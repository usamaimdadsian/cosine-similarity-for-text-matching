Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828–1833,
October 25-29, 2014, Doha, Qatar.
c
©2014 Association for Computational Linguistics
Joint Decoding of Tree Transduction Models for Sentence Compression
Jin-ge Yao Xiaojun Wan Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{yaojinge, wanxiaojun, xiaojianguo}@pku.edu.cn
Abstract
In this paper, we provide a new method for
decoding tree transduction based sentence
compression models augmented with lan-
guage model scores, by jointly decoding
two components. In our proposed so-
lution, rich local discriminative features
can be easily integrated without increasing
computational complexity. Utilizing an
unobvious fact that the resulted two com-
ponents can be independently decoded, we
conduct efficient joint decoding based on
dual decomposition. Experimental results
show that our method outperforms tradi-
tional beam search decoding and achieves
the state-of-the-art performance.
1 Introduction
Sentence compression is the task of generating a
grammatical and shorter summary for a long sen-
tence while preserving its most important informa-
tion. One specific instantiation is deletion-based
compression, namely generating a compression by
dropping words. Various approaches have been
proposed to challenge the task of deletion-based
compression. Earlier pioneering works (Knight
and Marcu, 2000) considered several insightful
approaches, including noisy-channel based gen-
erative models and discriminative decision tree
models. Structured discriminative compression
models (McDonald, 2006) are capable of inte-
grating rich features and have been proved effec-
tive for this task. Another powerful paradigm for
sentence compression should be mentioned here
is constraints-based compression,including inte-
ger linear programming solutions (Clarke and La-
pata, 2008) and first-order Markov logic networks
(Huang et al., 2012; Yoshikawa et al., 2012).
A notable class of methods that explicitly deal
with syntactic structures are tree transduction
models (Cohn and Lapata, 2007; Cohn and Lap-
ata, 2009). In such models a synchronous gram-
mar is extracted from a corpus of parallel syn-
tax trees with leaves aligned. Compressions are
generated from the grammar with learned weights.
Previous works have noticed that local coherence
is usually needed by introducing ngram language
model scores, which will make accurate decoding
intractable. Traditional approaches conduct beam
search to find approximate solutions (Cohn and
Lapata, 2009).
In this paper we propose a joint decoding strat-
egy to challenge this decoding task. We ad-
dress the problem as jointly decoding a simple
tree transduction model that only considers rule
weights and an ngram compression model. Al-
though either part can be independently solved by
dynamic programming, the naive way to integrate
two groups of partial scores into a huge dynamic
programming chart table is computationally im-
practical. We provide an effective dual decompo-
sition solution that utilizes the efficient decoding
of both parts. By integrating rich structured fea-
tures that cannot be efficiently involved in normal
formulation, results get significantly improved.
2 Motivation
Under the tree transduction models, the sentence
compression task is formulated as learning a map-
ping from an input source syntax tree to a target
tree with reduced number of leaves. This map-
ping is known as a synchronous grammar. The
synchronous grammar discussed through out this
paper will be synchronous tree substitution gram-
mar (STSG), as in previous studies.
In such formulations, sentence compression is
finding the best derivation from a syntax tree that
produces a simpler target tree, under the current
definition of grammar and learned parameters.
Each derivation is attached with a score. For the
sake of efficient decoding, the score often decom-
1828
poses with rules involved in the derivation. A typ-
ical score definition for a derivation y of source
tree x is in such form (Cohn and Lapata, 2008;
Cohn and Lapata, 2009):
S(x,y)=
?
r?y
w
T
?
r
(x)+logP (ngram(y)) (1)
The first term is a weighted sum of features ?
r
(x)
defined on each rule r. It is plausible to introduce
local scores from ngram models. The second term
in the above score definition is added with such
purpose.
Cohn and Lapata (2009) explained that ex-
act decoding of Equation 1 is intractable. They
proposed a beam search decoding strategy cou-
pled with cube-pruning heuristic (Chiang, 2007),
which can further improve decoding efficiency at
the cost of largely losing exactness in log probabil-
ity calculations. For efficiency reasons, rich local
ngram features have not been introduced as well.
3 Components of Joint Decoding
The score in Equation 1 consists of two parts: sum
of weighted rule features and local ngram scores
retrieved from a language model. There is an im-
plicit fact that either part can be used alone with
slight modifications to generate a coarse candidate
compression. Therefore, we can build a joint de-
coding system that consists of these two indepen-
dently decodable components.
In this section we will refer to these two in-
dependent models as the pure tree transduction
model and the pure ngram compression model,
described in Section 3.1 and Section 3.2 respec-
tively. There is a direct generalization of the
ngram model by introducing rich local features,
which results in the structured discriminative mod-
els (Section 3.3).
3.1 Pure Tree Transduction model
By merely considering scores from tree transduc-
tion rules, i.e. the first part of Equation 1, we can
have our scores factorized with rules. Then finding
the best derivation from a STSG grammar can be
easily solved by a dynamic programming process
described by Cohn and Lapata (2007).
This simplified pure tree transduction model
can still produce decent compressions if the rule
weights are properly learned during training.
3.2 Pure Ngram based Compression
The pure ngram based model will try to find
the most locally smooth compression, reflected
by having the maximum log probability score of
ngrams.
To avoid the trivial solution of deleting all
words, we find the target compression with speci-
fied length by dynamic programming.
Furthermore, we can integrate features other
than log probabilities. This is equivalent to using a
structured discriminative model with rich features
on ngrams of candidate compressions.
3.3 Structured Discriminative Model
The structured discriminative model proposed by
McDonald (2006) defines rich features on bigrams
of possible compressions. The score is defined as
weighted linear combination of those features:
f(x, z) =
|z|
?
j=2
w · f(x, L(z
j?1
), L(z
j
)) (2)
where the functionL(z
k
) maps a token z
k
in com-
pression z back to the index of the original sen-
tence x. Decoding can still be efficiently done by
dynamic programming.
With rich local structural information, the struc-
tured discriminative model can play a complemen-
tary role to the tree transduction model that focus
more on global syntactic structures.
4 Joint Decoding
From now on the remaining issue is jointly de-
coding the components. Either part factorizes
over local structures: rules for the tree transduc-
tion model and ngrams for the language model or
structured discriminative model. We may build a
large dynamic programming table to utilize this
kind of locality. Unfortunately this is computa-
tionally impractical. It is mathematically equiva-
lent to perform exact dynamic programming de-
coding of Equation 1, which would consume
asymptotically O(SRL
2(n?1)V
)
1
time for build-
ing the chart (Cohn and Lapata, 2009). Cohn and
Lapata (2009) proposed a beam search approxima-
tion along with cube-pruning heuristics to reduce
the time complexity down to O(SRBV )
2
.
1
S, R, L and V denote respectively for the number of
source tree nodes, the number of rules, size of target lexicon
and number of variables involved in each rule.
2
B denotes the beam width.
1829
In this work we utilize the efficiency of indepen-
dent decoding from the two components respec-
tively and then combine their solutions according
to certain standards. This naturally results in a
dual decomposition (Rush et al., 2010) solution.
Dual decomposition has been applied in sev-
eral natural language processing tasks, including
dependency parsing (Koo et al., 2010), machine
translation (Chang and Collins, 2011; Rush and
Collins, 2011) and information extraction (Re-
ichart and Barzilay, 2012). However, the strength
of this inference strategy has seldom been noticed
in researches on language generation tasks.
We briefly describe the formulation here.
4.1 Description
We denote the pure tree transduction part and the
pure ngram part as g(y) and f(z) respectively.
Then joint decoding is equivalent to solving:
max
y?Y,z?Z
g(y) + f(z) (3)
s.t. z
kt
= y
kt
, ?k ? {1, ..., n}, ?t ? {0, 1},
where y denotes a derivation which yields a final
compression {y
1
, ...,y
m
}. This derivation comes
from a pure tree transduction model. z denotes the
compression composed of {z
1
, ..., z
m
} from an
ngram compression model. Without loss of gener-
ality, we consider y
k
and z
k
as indicators that take
value 1 if the k’s token of original sentence has
been preserved in the compression and 0 if it has
been deleted. In the constraints of problem 3, y
kt
or z
kt
denote indicator variables that take value 1
if y
k
or z
k
= t and 0 otherwise.
Let L(u,y, z) be the Lagrangian of (3). Then
the dual objective naturally factorizes into two
parts that can be evaluated independently:
L(u) = max
y?Y,z?Z
L(u,y, z)
= max
y?Y,z?Z
g(y) + f(z) +
?
k,t
u
kt
(z
kt
? y
kt
)
= max
y?Y
(g(y)?
?
k,t
u
kt
y
kt
) +
max
z?Z
(f(z) +
?
k,t
u
kt
z
kt
)
With this factorization, Algorithm 1 tries to
solve the dual problem min
u
L(u) by alternatively
decoding each component.
This framework is feasible and plausible in that
the two subproblems (line 3 and line 4 in Algo-
rithm 1) can be easily solved with slight modifica-
Algorithm 1 Dual Decomposition Joint Decoding
1: Initialization: u
(0)
k
= 0, ?k ? {1, ..., n}
2: for i = 1 to MAX ITER do
3: y
(i)
? argmax
y?Y
(g(y)?
?
k,t
u
(i?1)
kt
y
kt
)
4: z
(i)
? argmax
z?Z
(f(z) +
?
k,t
u
(i?1)
kt
z
kt
)
5: if y
(i)
kt
= z
(i)
kt
?k ?t then
6: return (y
(i)
, z
(i)
)
7: else
8: u
(i)
kt
? u
(i?1)
kt
? ?
i
(z
(i)
kt
? y
(i)
kt
)
9: end if
10: end for
tions on the values of the original dynamic pro-
gramming chart. Joint decoding of a pure tree
transduction model and a structured discriminative
model is almost the same.
The asymptotic time complexity of Algorithm 1
is O(k(SRV + L
2(n?1)
)), where k denotes the
number of iterations. This is a significant re-
duction of O(SRL
2(n?1)V
) by directly solving
the original problem and is also comparable to
O(SRBV ) of conducting beam search decoding.
We apply a similar heuristic with Rush and
Collins (2012) to set the step size ?
i
=
1
t+1
, where
t < i is the number of past iterations that increase
the dual value. This setting decreases the step
size only when the dual value moves towards the
wrong direction. We limit the maximum iteration
number to 50 and return the best primal solution
y
(i)
among all previous iterations for cases that do
not converge in reasonable time.
5 Experiments
5.1 Baselines
The pure tree transduction model and the discrim-
inative model naturally become part of our base-
lines for comparison
3
. Besides comparing our
methods against the tree-transduction model with
ngram scores by beam search decoding, we also
compare them against the available previous work
from Galanis and Androutsopoulos (2010). This
state-of-the-art work adopts a two-stage method to
rerank results generated by a discriminative maxi-
mum entropy model.
5.2 Data Preparation
We evaluated our methods on two standard cor-
pora
4
, refer to as Written and Spoken respectively.
3
The pure ngram language model should not be consid-
ered here as it requires additional length constraints and in
general does not produce competitive results at all merely by
itself.
4
Available at http://jamesclarke.net/research/resources
1830
We split the datasets according to Table 1.
Table 1: Dataset partition (number of sentences)
Corpus Training Development Testing
Written 1,014 324 294
Spoken 931 83 254
All tree transduction models require parallel
parse trees with aligned leaves. We parsed all sen-
tences with the Stanford Parser
5
and aligned sen-
tence pairs with minimum edit distance heuristic
6
. Syntactic features of the discriminative model
were also taken from these parse trees.
For systems involving ngram scores, we trained
a trigram language model on the Reuters Corpus
(Volume 1)
7
with modified Kneser-Ney smooth-
ing, using the widely used tool SRILM
8
.
5.3 Model Training
The training process of a tree transduction model
followed similarly to Cohn and Lapata (2007) us-
ing structured SVMs (Tsochantaridis et al., 2005).
The structured discriminative models were trained
according to McDonald (2006).
5.4 Evaluation Metrics
We assessed the compression results by the F1-
score of grammatical relations (provided by a
dependency parser) of generated compressions
against the gold-standard compression (Clarke and
Lapata, 2006). All systems were controlled to pro-
duce similar compression ratios (CR) for fair com-
parison. We also reported manual evaluation on a
sampled subset of 30 sentences from each dataset.
Three unpaid volunteers with self-reported fluency
in English were asked to rate every candidate. Rat-
ings are in the form of 1-5 scores for each com-
pression.
6 Results
We report test set performance of the struc-
tured discriminative model, the pure tree transduc-
tion (T3), Galanis and Androutsopoulos (2010)’s
method (G&A2010), tree transduction with lan-
guage model scores by beam search and the pro-
posed joint decoding solutions.
5
http://nlp.stanford.edu/software/lex-parser.shtml
6
Ties were broken by always aligning a token in compres-
sion to its last appearance in the original sentence. This may
better preserve the alignments of full constituents.
7
http://trec.nist.gov/data/reuters/reuters.html
8
http://www-speech.sri.com/projects/srilm/
Table 2 shows the compression ratios and F-
measure of grammatical relations in average for
each dataset. Table 3 presents averaged human rat-
ing results for each dataset. We carried out pair-
wise t-test to examine the statistical significance
of the differences
9
. In both datasets joint decod-
ing with dual decomposition solution outperforms
other systems, especially when structured models
involved. We can also find certain improvements
of joint modeling with dual decomposition on the
original beam search decoding of Equation 1, un-
der very close compression ratios.
Joint decoding of pure tree transduction and dis-
criminative model gives better performance than
the joint model of tree transduction and language
model. From Table 3 we can see that integrat-
ing discriminative model will mostly improve the
preservation of important information rather than
grammaticality. This is reasonable under the fact
that the language model is trained on large scale
data and will often preserve local grammatical co-
herence, while the discriminative model is trained
on small but more compression specific corpora.
Table 2: Results of automatic evaluation. (†:
sig. diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD) for p < 0.01)
Written CR(%) GR-F1(%)
Discriminative 70.3 52.4
†?
G&A2010 71.6 60.2
?
Pure Tree-Transduction 72.6 52.3
†?
T3+LM (Beam Search) 70.4 58.8
?
T3+LM (Dual Decomp.) 70.7 60.5
T3+Discr. (Dual Decomp.) 71.0 62.3
Gold-Standard 71.4 100.0
Spoken CR(%) GR-F1(%)
Discriminative 69.5 50.6
†?
G&A2010 71.7 59.2
?
Pure Tree-Transduction 73.6 53.8
†?
T3+LM (Beam Search) 75.5 59.5
?
T3+LM (Dual Decomp.) 75.3 61.5
T3+Discr. (Dual Decomp.) 74.9 63.3
Gold-Standard 72.4 100.0
Table 4 shows some examples of compressed
sentences produced by all the systems in compar-
ison. The two groups of outputs are compressions
of one sentence from the Written corpora and
the Spoken corpora respectively. Ungrammatical
compressions can be found very often by several
baselines for different reasons, such as the outputs
from pure tree transduction and the discriminative
model in the first group. The reason behind the
9
For all multiple comparisons in this paper, significance
level was adjusted by the Holm-Bonferroni method.
1831
Table 3: Results of human rating. (†: sig.
diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD), for p < 0.01)
Written GR. Imp. CR(%)
Discriminative 3.92
†?
3.46
†?
70.6
G&A2010 4.11
†?
3.50
†?
72.4
Pure Tree-Transduction 3.85
†?
3.42
†?
70.1
T3+LM (Beam Search) 4.22
†?
3.69
?
73.0
T3+LM (Dual Decomp.) 4.63 3.98 73.2
T3+Discr. (Dual Decomp.) 4.62 4.25 73.5
Gold-Standard 4.89 4.76 72.9
Spoken GR. Imp. CR(%)
Discriminative 3.95
†?
3.62
†?
71.2
G&A2010 4.09
†?
3.96
?
72.5
Pure Tree-Transduction 3.92
†?
3.55
†?
71.4
T3+LM (Beam Search) 4.20
?
3.78
?
75.0
T3+LM (Dual Decomp.) 4.35 4.18 74.5
T3+Discr. (Dual Decomp.) 4.47 4.26 74.7
Gold-Standard 4.83 4.80 73.1
under generation of pure tree transduction is that it
mainly deals with global syntactic integrity merely
in terms of the application of synchronous rules.
Introducing language model scores will smooth
the candidate compressions and avoid many ag-
gressive decisions of tree transduction. Discrim-
inative models are good at local decisions with
poor consideration of grammaticality. We can see
that the joint models have collected their predic-
tive power together. Unfortunately we can still
observe some redundancy from our outputs in the
examples. The size of training corpus is not large
enough to provide enough lexicalized information.
On the other hand, the time consumption of
the joint model with dual decomposition decoding
in our experiments matched the aforementioned
asymptotic analysis. The training process based
on new decoding method consumes similar time
as beam search with cube-pruning heuristic.
7 Conclusion and Future Work
In this paper we propose a joint decoding scheme
for tree transduction based sentence compression.
Experimental results suggest that the proposed
framework works well. The overall performance
gets further improved under our framework by in-
troducing the structured discriminative model.
As several recent efforts have focused on ex-
tracting large-scale parallel corpus for sentence
compression (Filippova and Altun, 2013), we
would like to study how larger corpora can af-
fect tree transduction and our joint decoding so-
Table 4: Example outputs
Original: It was very high for people who took their
full-time education beyond the age of 18 , and higher
among women than men for all art forms except jazz
and art galleries .
Discr.: It was high for people took education higher
among women .
(Galanis and Androutsopoulos, 2010): It was high for
people who took their education beyond the age of 18 ,
and higher among women .
Pure T3: It was very high for people who took .
T3+LM-BeamSearch: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+LM-DualDecomp: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+Discr.: It was high for people who took education
beyond the age of 18 , and higher among women than
men .
Gold-Standard: It was very high for people who took
full-time education beyond 18 , and higher among
women for all except jazz and galleries .
Original: But they are still continuing to search the
area to try and see if there were , in fact , any further
shooting incidents .
Discr.: they are continuing to search the area to try and
see if there were , further shooting incidents .
(Galanis and Androutsopoulos, 2010): But they are still
continuing to search the area to try and see if there
were , in fact , any further shooting incidents .
Pure T3: they are continuing to search the area to try
and see if there were any further shooting incidents .
T3+LM-BeamSearch: But they are continuing to
search the area to try and see if there were , in fact ,
any further shooting incidents .
T3+LM-DualDecomp: But they are continuing to
search the area to try and see if there were any further
shooting incidents .
T3+Discr.: they are continuing to search the area to try
and see if there were further shooting incidents .
Gold-Standard: they are continuing to search the area
to see if there were any further incidents .
lution. Meanwhile, We would like to explore on
how other text-rewriting problems can be formu-
lated as a joint model and be applicable to similar
strategies described in this work.
Acknowledgements
This work was supported by National Hi-Tech Re-
search and Development Program (863 Program)
of China (2014AA015102, 2012AA011101) and
National Natural Science Foundation of China
(61170166, 61331011). We also thank the anony-
mous reviewers for very helpful comments.
The contact author of this paper, according to
the meaning given to this role by Peking Univer-
sity, is Xiaojun Wan.
1832
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 26–37, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377–384. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273–381.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
73–82, Prague, Czech Republic, June. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 137–144. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637–674.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1481–1491, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 885–893, Los Angeles, California,
June. Association for Computational Linguistics.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In AAAI/IAAI, pages 703–710.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1288–1298, Cambridge, MA, October.
Association for Computational Linguistics.
Ryan T McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 70–
79, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72–82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305–362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11, Cambridge, MA, October.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning Re-
search, pages 1453–1484.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers-Volume 2, pages
349–353. Association for Computational Linguis-
tics.
1833
