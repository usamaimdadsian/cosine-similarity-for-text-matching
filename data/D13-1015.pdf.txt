Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Studying the recursive behaviour of adjectival modification
with compositional distributional semantics
Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it
Abstract
In this study, we use compositional distribu-
tional semantic methods to investigate restric-
tions in adjective ordering. Specifically, we
focus on properties distinguishing Adjective-
Adjective-Noun phrases in which there is flex-
ibility in the adjective ordering from those
bound to a rigid order. We explore a number
of measures extracted from the distributional
representation of AAN phrases which may in-
dicate a word order restriction. We find that
we are able to distinguish the relevant classes
and the correct order based primarily on the
degree of modification of the adjectives. Our
results offer fresh insight into the semantic
properties that determine adjective ordering,
building a bridge between syntax and distri-
butional semantics.
1 Introduction
A prominent approach for representing the meaning
of a word in Natural Language Processing (NLP) is
to treat it as a numerical vector that codes the pat-
tern of co-occurrence of that word with other ex-
pressions in a large corpus of language (Sahlgren,
2006; Turney and Pantel, 2010). This approach to
semantics (sometimes called distributional seman-
tics) scales well to large lexicons and does not re-
quire words to be manually disambiguated (Schu¨tze,
1997). Until recently, however, this method had
been almost exclusively limited to the level of sin-
gle content words (nouns, adjectives, verbs), and had
not directly addressed the problem of composition-
ality (Frege, 1892; Montague, 1970; Partee, 2004),
the crucial property of natural language which al-
lows speakers to derive the meaning of a complex
linguistic constituent from the meaning of its imme-
diate syntactic subconstituents.
Several recent proposals have strived to ex-
tend distributional semantics with a component that
also generates vectors for complex linguistic con-
stituents, using compositional operations in the vec-
tor space (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al., 2012). All of
these approaches construct distributional represen-
tations for novel phrases starting from the corpus-
derived vectors for their lexical constituents and
exploiting the geometric quality of the representa-
tion. Such methods are able to capture complex se-
mantic information of adjective-noun (AN) phrases,
such as characterizing modification (Boleda et al.,
2012; Boleda et al., 2013), and can detect seman-
tic deviance in novel phrases (Vecchi et al., 2011).
Furthermore, these methods are naturally recursive:
they can derive a representation not only for, e.g.,
red car, but also for new red car, fast new red car,
etc. This aspect is appealing since trying to extract
meaningful representations for all recursive phrases
directly from a corpus will result in a problem of
sparsity, since most large phrases will never occur in
any finite sample.
Once we start seriously looking into recursive
modification, however, the issue of modifier order-
ing restrictions naturally arises. Such restrictions
have often been discussed in the theoretical linguis-
tic literature (Sproat and Shih, 1990; Crisma, 1991;
Scott, 2002), and have become one of the key in-
141
gredients of the ‘cartographic’ approach to syntax
(Cinque, 2002). In this paradigm, the ordering is
derived by assigning semantically different classes
of modifiers to the specifiers of distinct functional
projections, whose sequence is hard-wired. While
it is accepted that in different languages movement
can lead to a principled rearrangement of the linear
order of the modifiers (Cinque, 2010; Steddy and
Samek-Lodovici, 2011), one key assumption of the
cartographic literature is that exactly one intonation-
ally unmarked order for stacked adjectives should
be possible in languages like English. The possi-
bility of alternative orders, when discussed at all,
is attributed to the presence of idioms (high Amer-
ican building, but American high officer), to asyn-
detic conjunctive meanings (e.g. new creative idea
parsed as [new & creative] idea, rather than [new
[creative idea]]), or to semantic category ambiguity
for any adjective which appears in different orders
(see Cinque (2004) for discussion).
In this study, we show that the existence of both
rigid and flexible order cases is robustly attested at
least for adjectival modification, and that flexible or-
dering is unlikely to reduce to idioms, coordination
or ambiguity. Moreover, we show that at least for
some recursively constructed adjective-adjective-
noun phrases (AANs) we can extract meaning-
ful representations from the corpus, approximating
them reasonably well by means of compositional
distributional semantic models, and that the seman-
tic information contained in these models character-
izes which AA will have rigid order (as with rapid
social change vs. *social rapid change), or flexible
order (e.g. total estimated population vs. estimated
total population). In the former case, we find that
the same distributional semantic cues discriminate
between correct and wrong orders.
To achieve these goals, we consider various
properties of the distributional representation of
AANs (both corpus-extracted and compositionally-
derived), and explore their correlation with restric-
tions in adjective ordering. We conclude that mea-
sures that quantify the degree to which the modifiers
have an impact on the distributional meaning of the
AAN can be good predictors of ordering restrictions
in AANs.
2 Materials and methods
2.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row represents the meaning of an adjective, noun,
AN or AAN as a distributional vector, each column
a semantic dimension of meaning. We first introduce
the source corpus, then the vocabulary of words and
phrases that we represent in the space, and finally the
procedure adopted to build the vectors representing
the vocabulary items from corpus statistics, and ob-
tain the semantic space matrix. We work here with a
traditional, window-based semantic space, since our
focus is on the effect of different composition meth-
ods given a common semantic space. In addition,
Blacoe and Lapata (2012) found that a vanilla space
of this sort performed best in their composition ex-
periments, when compared to a syntax-aware space
and to neural language model vectors such as those
used for composition by Socher et al. (2011).
Source corpus We use as our source corpus the
concatenation of the Web-derived ukWaC corpus, a
mid-2009 dump of the English Wikipedia and the
British National Corpus1. The corpus has been tok-
enized, POS-tagged and lemmatized with the Tree-
Tagger (Schmid, 1995), and it contains about 2.8 bil-
lion tokens. We extract all statistics at the lemma
level, meaning that we consider only the canonical
form of each word ignoring inflectional information,
such as pluralization and verb inflection.
Semantic space vocabulary The words/phrases
in the semantic space must of course include the
items that we need for our experiments (adjectives,
nouns, ANs and AANs used for model training, as
input to composition and for evaluation). Therefore,
we first populate our semantic space with a core vo-
cabulary containing the 8K most frequent nouns and
the 4K most frequent adjectives from the corpus.
The ANs included in the semantic space are com-
posed of adjectives with very high frequency in the
corpus so that they are generally able to combine
with many classes of nouns. They are composed
of the 700 most frequent adjectives and 4K most
frequent nouns in the corpus, which were manually
1http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
142
controlled for problematic cases – excluding adjec-
tives such as above, less, or very, and nouns such
as cant, mph, or yours – often due to tagging errors.
We generated the set of ANs by crossing the filtered
663 adjectives and 3,910 nouns. We include those
ANs that occur at least 100 times in the corpus in
our vocabulary, which amounted to a total of 128K
ANs.
Finally, we created a set of AAN phrases com-
posed of the adjectives and nouns used to gener-
ate the ANs. Additional preprocessing of the gen-
erated AxAyNs includes: (i) control that both AxN
and AyN are attested in the corpus; (ii) discard any
AxAyN in which AxN or AyN are among the top
200 most frequent ANs in the source corpus (as in
this case, order will be affected by the fact that such
phrases are almost certainly highly lexicalized); and
(iii) discard AANs seen as part of a conjunction in
the source corpus (i.e., where the two adjectives ap-
pear separated by comma, and, or or; this addresses
the objection that a flexible order AAN might be a
hidden A(&)A conjunction: we would expect that
such a conjunction should also appear overtly else-
where). The set of AANs thus generated is then di-
vided into two types of adjective ordering:
1. Flexible Order (FO): phrases where both or-
ders, AxAyN and AyAxN, are attested (f>10
in both orders).
2. Rigid Order (RO): phrases with one order,
AxAyN, attested (20<f<200)2 and AyAxN
unattested.
All AANs that did not meet either condition were
excluded from our semantic space vocabulary. The
preserved set resulted in 1,438 AANs: 621 flexible
order and 817 rigid order. Note that there are almost
as many flexible as rigid order cases; this speaks
against the idea that free order is a marginal phe-
nomenon, due to occasional ambiguities that reas-
sign the adjective to a different semantic class. The
existence of freely ordered stacked adjectives is a ro-
bust phenomenon, which needs to be addressed.
2The upper threshold was included as an additional filter
against potential multiword expressions. Of course, the bound-
ary between phrases that are at least partially compositional and
those that are fully lexicalized is not sharp, and we leave it to
further work to explore the interplay between the semantic fac-
tors we study here and patterns of lexicalization.
Model ? M&L
CORP 0.41 0.43
W.ADD 0.41 0.44
F.ADD 0.40 –
MULT 0.33 0.46
LFM 0.40 –
Table 1: Correlation scores (Spearman’s ?, all signif-
icant at p<0.001) between cosines of corpus-extracted
or model-generated AN vectors and phrase similarity rat-
ings collected in Mitchell and Lapata (2010), as well as
best reported results from Mitchell & Lapata (M&L).
Semantic vector construction For each of
the items in our vocabulary, we first build 10K-
dimensional vectors by recording the item’s
sentence-internal co-occurrence with the top 10K
most frequent content lemmas (nouns, adjectives,
verbs or adverbs) in the corpus. We built a rank
of these co-occurrence counts, and excluded as
stop words from the dimensions any element of
any POS whose rank was from 0 to 300. The raw
co-occurrence counts were then transformed into
(positive) Pointwise Mutual Information (pPMI)
scores (Church and Hanks, 1990). Next, we reduce
the full co-occurrence matrix to 300 dimensions
applying the Non-negative Matrix Factorization
(NMF) operation (Lin, 2007). We did not tune the
semantic vector construction parameters, since we
found them to work best in a number of independent
earlier experiments.
Corpus-extracted vectors (corp) were computed
for the ANs and for the flexible order and attested
rigid order AANs, and then mapped onto the 300-
dimension NMF-reduced semantic space. As a san-
ity check, the first row of Table 1 reports the corre-
lation between the AN phrase similarity ratings col-
lected in Mitchell and Lapata (2010) and the cosines
of corpus-extracted vectors in our space, for the
same ANs. For the AAN vectors, which are sparser,
we used human judgements to build a reliable sub-
set to serve as our gold standard, as detailed in Sec-
tion 2.4.
2.2 Composition models
We focus on four composition functions proposed
in recent literature with high performance in a num-
ber of semantic tasks. We first consider meth-
ods proposed by Mitchell and Lapata (2010) in
143
which the model-generated vectors are simply ob-
tained through component-wise operations on the
constituent vectors. Given input vectors ~u and ~v, the
multiplicative model (MULT) computes a composed
vector by component-wise multiplication () of the
constituent vectors, where the i-th component of the
composed vector is given by pi = uivi.3 Given an
AxAyN phrase, this model extends naturally to the
recursive setting of this experiment, as seen in Equa-
tion (1).
~p = ~ax  ~ay  ~n (1)
This composition method is order-insensitive, the
formula above corresponding to the representation
of both AxAyN and AyAxN.
In the weighted additive model (W.ADD), we ob-
tain the composed vector as a weighted sum of the
two component vectors: ~p = ?~u+ ?~v, where ? and
? are scalars. Again, we can easily apply this func-
tion recursively, as in Equation (2).
~p = ?~ax + ?(?~ay + ?~n) = ?~ax + ??~ay + ?
2~n
(2)
We also consider the full extension of the addi-
tive model (F.ADD), presented in Guevara (2010)
and Zanzotto et al. (2010), such that the component
vectors are pre-multiplied by weight matrices before
being added: ~p = W1~u + W2~v. Similarly to the
W.ADD model, Equation (3) describes how we apply
this function recursively.
~p = W1~ax + W2(W1~ay + W2~n) (3)
= W1~ax + W2W1~ay + W22~n
Finally, we consider the lexical function model
(LFM), first introduced in Baroni and Zamparelli
(2010), in which attributive adjectives are treated as
functions from noun meanings to noun meanings.
This is a standard approach in Montague semantics
(Thomason, 1974), except noun meanings here are
distributional vectors, not denotations, and adjec-
tives are (linear) functions learned from a large cor-
pus. In this model, predicted vectors are generated
3We conjecture that the different performance of our multi-
plicative model and M&L’s (cf. Table 1) is due to the fact that
we use log-transformed pPMI scores, making their multiplica-
tive model more akin to our additive approach.
by multiplying a function matrix U with a compo-
nent vector: ~p = U~v. Given a weight matrix, A, for
each adjective in the phrase, we apply the functions
in sequence recursively as shown in Equation (4).
~p = Ax(Ay~n) (4)
Composition model estimation Parameters for
W.ADD, F.ADD and LFM were estimated following
the strategy proposed by Guevara (2010) and Ba-
roni and Zamparelli (2010), recently extended to all
composition models by Dinu et al. (2013b). Specif-
ically, we learn parameter values that optimize the
mapping from the noun to the AN as seen in ex-
amples of corpus-extracted N-AN vector pairs, us-
ing least-squares methods. All parameter estima-
tions and phrase compositions were implemented
using the DISSECT toolkit4 (Dinu et al., 2013a),
with a training set of 74,767 corpus-extracted N-
AN vector pairs, ranging from 100 to over 1K items
across the 663 adjectives. Importantly, while below
we report experimental results on capturing various
properties of recursive AAN constructions, no AAN
was seen during training, which was based entirely
on mapping from N to AN. Table 1 reports the re-
sults attained by our model implementations on the
Mitchell and Lapata AN similarity data set.
2.3 Measures of adjective ordering
Our general goal is to determine which
linguistically-motivated factors distinguish the
two types of adjective ordering. We hypothesize
that in cases of flexible order, the two adjectives
will have a similarly strong effect on the noun, thus
transforming the meaning of the noun equivalently
in the direction of both adjectives and component
ANs. For example, in the phrase creative new idea,
the idea is both new and creative, so we would
expect a similar impact of modification by both
adjectives.
On the other hand, we predict that in rigid order
cases, one adjective, the one closer to the noun, will
dominate the meaning of the phrase, distorting the
meaning of the noun by a significant amount. For
example, the phrase different architectural style in-
tuitively describes an architectural style that is dif-
4http://clic.cimec.unitn.it/composes/
toolkit
144
ferent, rather than a style that is to the same extent
architectural and different.
We consider a number of measures that could cap-
ture our intuitions and quantify this difference, ex-
ploring the distance relationship between the AAN
vectors and each of the AAN subparts. First, we
examine how the similarity of an AAN to its com-
ponent adjectives affects the ordering, using the co-
sine between the AxAyN vector and each of the
component A vectors as an expression of similarity
(we abbreviate this as cosAx and cosAy for the first
and second adjective, respectively).5 Our hypothe-
sis predicts that flexible order AANs should remain
similarly close to both component As, while rigid
order AANs should remain systematically closer to
their Ay than to their Ax.
Next, we consider the similarity between the
AxAyN vector and its component N vector (cosN ).
This measure is aimed at verifying if the degree to
which the meaning of the head noun is distorted
could be a property that distinguishes the two types
of adjective ordering. Again, vectors for flexible or-
der AANs should remain closer to their component
nouns in the semantic space, while rigid order AANs
should distort the meaning of the head noun more
notably.
We also inspect how the similarity of the AAN
to its component AN vectors affects the type of ad-
jective ordering (cosAxN and cosAyN ). Consid-
ering the examples above, we predict that the flex-
ible order AAN creative new idea will share many
properties with both creative idea and new idea, as
represented in our semantic space, while rigid or-
der AANs, like different architectural style, should
remain quite similar to the AyN, i.e., architectural
style, and relatively distant from the AxN, i.e., dif-
ferent style.
Finally, we consider a measure that does not ex-
ploit distributional semantic representations, namely
the difference in PMI between AxN and AyN
(?PMI). Based on our hypothesis described for the
other measures, we expect the association in the cor-
pus of AyN to be much greater than AxN for rigid
order AANs, resulting in a large negative ?PMI val-
ues. While flexible order AANs should have similar
5In the case of LFM, we compare the similarity of the AAN
with the AN centroids for each adjective, since the model does
not make use of A vectors (Baroni and Zamparelli, 2010).
association strengths for both AxN and AyN, thus
we expect ?PMI to be closer to 0 than for rigid or-
der AANs.
2.4 Gold standard
To our knowledge, this is the first study to use
distributional representations of recursive modifi-
cation; therefore we must first determine if the
composed AAN vector representations are seman-
tically coherent objects. Thus, for vector analysis,
a gold standard of 320 corpus-extracted AAN vec-
tors were selected and their quality was established
by inspecting their nearest neighbors. In order to
create the gold standard, we ran a crowdsourcing
experiment on CrowdFlower6 (Callison-Burch and
Dredze, 2010; Munro et al., 2010), as follows.
First, we gathered a randomly selected set of 600
corpus-extracted AANs, containing 300 flexible or-
der and 300 attested rigid order AANs. We then
extracted the top 3 nearest neighbors to the corpus-
extracted AAN vectors as represented in the seman-
tic space7. Each AAN was then presented with each
of the nearest neighbors, and participants were asked
to judge “how strongly related are the two phrases?”
on a scale of 1-7. The rationale was that if we
obtained a good distributional representation of the
AAN, its nearest neighbors should be closely related
words and phrases. Each pair was judged 10 times,
and we calculated a relatedness score for the AAN
by taking the average of the 30 judgments (10 for
each of the three neighbors).
The final set for the gold standard contains the 320
AANs (152 flexible order and 168 attested rigid or-
der) which had a relatedness score over the median-
split (3.9). Table 2 shows examples of gold stan-
dard AANs and their nearest neighbors. As these
example indicate, the gold standard AANs reside in
semantic neighborhoods that are populated by in-
tuitively strongly related expressions, which makes
them a sensible target for the compositional models
to approximate.
We also find that the neighbors for the AANs rep-
resent an interesting variety of types of semantic
6http://www.crowdflower.com
7The top 3 neighbors included adjectives, nouns, ANs and
AANs. The preference for ANs and AANs, as seen in Table 2,
is likely a result of the dominance of those elements in the se-
mantic space (c.f. Section 2.1).
145
medieval old town contemp. political issue
fascinating town cultural topic
impressive cathedral contemporary debate
medieval street contemporary politics
rural poor people British naval power
poor rural people naval war
rural infrastructure British navy
rural people naval power
friendly helpful staff last live performance
near hotel final gig
helpful staff live dvd
quick service live release
creative new idea rapid social change
innovative effort social conflict
creative design social transition
dynamic part cultural consequence
national daily newspaper new regional government
national newspaper regional government
major newspaper local reform
daily newspaper regional council
daily national newspaper fresh organic vegetable
national daily newspaper organic vegetable
well-known journalist organic fruit
weekly column organic product
Table 2: Examples of the nearest neighbors of the gold
standard, both flexible order (left column) and rigid order
(right column) AANs.
similarity. For example, the nearest neighbors to the
corpus-extracted vectors for medieval old town and
rapid social change include phrases which describe
quite complex associations, cf. Table 2. In addition,
we find that the nearest neighbors for flexible order
AAN vectors are not necessarily the same for both
adjective orders, as seen in the difference in neigh-
bors of national daily newspaper and daily national
newspaper. We can expect that the change in or-
der, when acceptable and frequent, does not neces-
sarily yield synonymous phrases, and that corpus-
extracted vector representations capture subtle dif-
ferences in meaning.
3 Results
3.1 Quality of model-generated AAN vectors
Our nearest neighbor analysis suggests that the
corpus-extracted AAN vectors in the gold standard
are meaningful, semantically coherent objects. We
can thus assess the quality of AANs recursively gen-
erated by composition models by how closely they
Gold FO RO
W.ADD 0.565 0.572 0.558
F.ADD 0.618 0.622 0.614
MULT 0.424 0.468 0.384
LFM 0.655 0.675 0.637
Table 3: Mean cosine similarities between the corpus-
extracted and model-generated gold AAN vectors. All
pairwise differences between models are significant ac-
cording to Bonferroni-corrected paired t-tests (p<0.001).
For MULT and LFM, the difference between mean flexible
order (FO) and rigid order (RO) cosines is also signifi-
cant.
approximate these vectors. We find that the perfor-
mances of most composition models in approximat-
ing the vectors for the gold AANs is quite satisfac-
tory (cf. Table 3). To put this evaluation into per-
spective, note that 99% of the simulated distribu-
tion of pairwise cosines of corpus-extracted AANs
is below the mean cosine of the worst-performing
model (MULT), that is, a cosine of 0.424 is very sig-
nificantly above what is expected by chance for two
random corpus-extracted AAN vectors. Also, ob-
serve that the two more parameter-rich models are
better than W.ADD, and that LFM also significantly
outperforms F.ADD.
Further, the results show that the models are able
to approximate flexible order AAN vectors better
than rigid order AANs, significantly so for LFM and
MULT. This result is quite interesting because it sug-
gests that flexible order AANs express a more lit-
eral (or intersective) modification by both adjectives,
which is what we would expect to be better captured
by compositional models. Clearly, a more complex
modification process is occurring in the case of rigid
order AANs, as we predicted to be the case.
3.2 Distinguishing flexible vs. rigid order
In the results reported below, we test how both our
baseline ?PMI measure and the distance from the
AAN and its component parts changes depending on
the type of adjective ordering to which the AAN be-
longs. From this point forward, we only use gold
standard items, where we are sure of the quality of
the corpus-extracted vectors. The first block of Ta-
ble 4 reports the t-normalized difference between
flexible order and rigid order mean cosines for the
corpus-extracted vectors.
146
Measure t sig.
CORP
cosAx 2.478
cosAy -4.348 * RO>FO
cosN 4.656 * FO>RO
cosAxN 5.913 * FO>RO
cosAyN 1.970
W.ADD
cosAx 4.805 * FO>RO
cosAy -1.109
cosN 1.140
cosAxN 1.059
cosAyN 0.584
F.ADD
cosAx 2.050
cosAy -1.451
cosN 4.493 * FO>RO
cosAxN -0.445
cosAyN 2.300
MULT
cosAx 3.830 * FO>RO
cosAy -0.503
cosN 5.090 * FO>RO
cosAxN 4.435 * FO>RO
cosAyN 3.900 * FO>RO
LFM
cosAx -1.649
cosAy -1.272
cosN 5.539 * FO>RO
cosAxN 3.336 * FO>RO
cosAyN 4.215 * FO>RO
?PMI 8.701 * FO>RO
Table 4: Flexible vs. Rigid Order AANs. t-normalized
differences between flexible order (FO) and rigid order
(FO) mean cosines (or mean ?PMI values) for corpus-
extracted and model-generated vectors. For significant
differences (p<0.05 after Bonferroni correction), the last
column reports whether mean cosine (or ?PMI) is larger
for flexible order (FO) or rigid order (RO) class.
These results show, in accordance with our con-
siderations in Section 2.3 above: (i) flexible or-
der AxAyNs are closer to AxN and the component
N than rigid order AxAyNs, and (ii) rigid order
AxAyNs are closer to their Ay (flexible order AANs
are also closer to Ax but the effect does not reach
significance).8 The results imply that the degree of
modification of the Ay on the noun is a significant
indicator of the type of ordering present.
8As an aside, the fact that mean cosines are significantly
larger for the flexible order class in two cases but for the rigid or-
der class in another addresses the concern, raised by a reviewer,
that the words and phrases in one of the two classes might sys-
tematically inhabit denser regions of the space than those of the
other class, thus distorting results based on comparing mean
cosines.
In particular, rigid order AxAyNs are heavily
modified by Ay, distorting the meaning of the head
noun in the direction of the closest adjective quite
drastically, and only undergoing a slight modifica-
tion when the Ax is added. In other words, in rigid
order phrases, for example rapid social change, the
AyN expresses a single concept (probably a “kind”,
in the terminology of formal semantics), strongly re-
lated to social, social change, which is then mod-
ified by the Ax. Thus, the change is not both so-
cial and rapid, rather, the social change is rapid. On
the other hand, flexible order AANs maintain the se-
mantic value of the head noun while being modi-
fied only slightly by both adjectives, almost equiv-
alently. For example, in the phrase friendly help-
ful staff, one is saying that the staff is both friendly
and helpful. Most importantly, the corpus-extracted
distributional representations are able to model this
phenomenon inherently and can significantly distin-
guish the two adjective orders.
The results of the composition models (cf. Ta-
ble 4) show that for all models at least some prop-
erties do distinguish flexible and rigid order AANs,
although only MULT and LFM capture the two prop-
erties that show the largest effect for the corpus-
extracted vectors, namely the asymmetry in similar-
ity to the noun and the AxN (flexible order AANs
being more similar to both).
It is worth remarking that MULT approximated the
patterns observed in the corpus vectors quite well,
despite producing order-insensitive representations
of recursive structures. For flexible order AANs, or-
der is indeed only slightly affecting the meaning, so
it stands to reason that MULT has no problems mod-
eling this class. For rigid order AANs, where we
consider here the attested-order only, evidently the
order-insensitive MULT representation is sufficient
to capture their relations to their constituents.
Finally, we see that the ?PMI measure is the best
at distinguishing between the two classes of AAN
ordering. This confirms our hypothesis that a lot has
to do with how integrated Ay and N are. While it
is somewhat disappointing that ?PMI outperforms
all distributional semantic cues, note that this mea-
sure conflates semantic and lexical factors, as the
high PMI of AyN in at least some rigid order AANs
might be also a cue of the fact that the latter bigram
is a lexicalized phrase (as discussed in footnote 2, it
147
is unlikely that our filtering strategies sifted out all
multiword expressions). Moreover, ?PMI does not
produce a semantic representation of the phrase (see
how composed distributional vectors approximate of
high quality AAN vectors in Table 3). Finally, this
measure will not scale up to cases where the ANs
are not attested, whereas measures based on compo-
sition only need corpus-harvested representations of
adjectives and nouns.
3.3 Properties of the correct adjective order
Having shown that flexible order and rigid order
AANs are significantly distinguished by various
properties, we proceed now to test whether those
same properties also allow us to distinguish between
correct (corpus-attested) and wrong (unattested) ad-
jective ordering in rigid AANs (recall that we are
working with cases where the attested-order occurs
more than 20 times in the corpus, and both adjec-
tives modify the nouns at least 10 times, so we are
confident that there is a true asymmetry).
We expect that the fundamental property that dis-
tinguishes the orders is again found in the degree
of modification of both component adjectives. We
predict that the single concept created by the AyN
in attested-order rigid AANs, such as legal status
in formal legal status, is an effect of the modifica-
tion strength of the Ay on the head noun, and when
seen in the incorrect ordering, i.e., ?legal formal sta-
tus, the strong modification of legal will still domi-
nate the meaning of the AAN. Composition models
should be able to capture this effect based on the dis-
tance from both the component adjectives and ANs.
Clearly, we cannot run these analyses on corpus-
extracted vectors since the unattested order, by def-
inition, is not seen in our corpus, and therefore we
cannot collect co-occurrence statistics for the AAN
phrase. Thus, we test our measures of adjective or-
dering on the model-generated AAN vectors, for all
gold rigid order AANs in both orders.
We also consider the ?PMI measure which was
so effective in distinguishing flexible vs. rigid or-
der AANs. We expect that the greater association
with AyN for attested-order AANs will again lead
to large, negative differences in PMI scores, while
the expectation that unattested-order AANs will be
highly associated with their AxN will correspond to
large, positive differences in PMI.
Measure t sig.
W.ADD
cosAx -7.840 * U>A
cosAy 7.924 * A>U
cosN 2.394
cosAxN -5.462 * U>A
cosAyN 3.627 * A>U
F.ADD
cosAx -8.418 * U>A
cosAy 6.534 * A>U
cosN -1.927
cosAxN -3.583 * U>A
cosAyN -2.185
MULT
cosAx -5.100 * U>A
cosAy 5.100 * A>U
cosN 0.000
cosAxN -0.598
cosAyN 0.598
LFM
cosAx -7.498 * U>A
cosAy 7.227 * A>U
cosN -2.172
cosAxN -5.792 * U>A
cosAyN 0.774
?PMI -11.448 * U>A
Table 5: Attested- vs. unattested-order rigid order
AANs. t-normalized mean paired cosine (or ?PMI) dif-
ferences between attested (A) and unattested (U) AANs
with their components. For significant differences (paired
t-test p<0.05 after Bonferroni correction), last column
reports whether cosines (or ?PMI) are on average larger
for A or U.
Across all composition models, we find that the
distance between the model-generated AAN and its
component adjectives, Ax and Ay, are significant in-
dicators of attested vs. unattested adjective ordering
(cf. Table 5). Specifically, we find that rigid order
AANs in the correct order are closest to their Ay,
while we can detect the unattested order when the
rigid order AAN is closer to its Ax. This finding
is quite interesting, since it shows that the order in
which the composition functions are applied does
not alter the fact that the modification of one ad-
jective in rigid order AANs (the Ay in the case of
attested-order rigid order AANs) is much stronger
than the other. Unlike the measures that differenti-
ated flexible and rigid order AANs, here we see that
the distance from the component N is not an indi-
cator of the correct adjective ordering (trivially so
for MULT, where attested and unattested AANs are
identical).
Next, we find that for W.ADD, F.ADD and LFM,
148
the distance from the component AxN is a strong
indicator of attested- vs. unattested-order rigid order
AANs. Specifically, attested-order AANs are further
from their AxN than unattested-order AANs. This
finding is in line with our predictions and follows
the findings of the impact of the distance from the
component adjectives.
?PMI, as seen in the ability to distinguish flexi-
ble vs. rigid order AANs, is the strongest indicator
of correct vs wrong adjective ordering. This mea-
sure confirms that the association of one adjective
(the Ay in attested-order AANs) with the head noun
is indeed the most significant factor distinguishing
these two classes. However, as we mentioned be-
fore, this measure has its limitations and is likely not
to be entirely sufficient for future steps in modeling
recursive modification.
4 Conclusion
While AN constructions have been extensively stud-
ied within the framework of compositional distri-
butional semantics (Baroni and Zamparelli, 2010;
Boleda et al., 2012; Boleda et al., 2013; Guevara,
2010; Mitchell and Lapata, 2010; Turney, 2012;
Vecchi et al., 2011), for the first time, we extended
the investigation to recursively built AAN phrases.
First, we showed that composition functions ap-
plied recursively can approximate corpus-extracted
AAN vectors that we know to be of high semantic
quality.
Next, we looked at some properties of the same
high-quality corpus-extracted AAN vectors, finding
that the distinction between “flexible” AANs, where
the adjective order can be flipped, and “rigid” ones,
where the order is fixed, is reflected in distributional
cues. These results all derive from the intuition that
the most embedded adjective in a rigid AAN has a
very strong effect on the distributional semantic rep-
resentation of the AAN. Most compositional models
were able to capture at least some of the same cues
that emerged in the analysis of the corpus-extracted
vectors.
Finally, similar cues were also shown to distin-
guish (compositional) representations of rigid AANs
in the “correct” (corpus-attested) and “wrong”
(unattested) orders, again pointing to the degree to
which the (attested-order) closest adjective affects
the overall AAN meaning as an important factor.
Comparing the composition functions, we find
that the linguistically motivated LFM approach has
the most consistent performance across all our tests.
This model significantly outperformed all others in
approximating high-quality corpus-extracted AAN
vectors, it provided the closest approximation to the
corpus-observed patterns when distinguishing flexi-
ble and rigid AANs, and it was one of the models
with the strongest cues distinguishing attested and
unattested orders of rigid AANs.
From an applied point of view, a natural next step
would be to use the cues we proposed as features to
train a classifier to predict the preferred order of ad-
jectives, to be tested also in cases where neither or-
der is found in the corpus, so direct corpus evidence
cannot help. For a full account of adjectival order-
ing, non-semantic factors should also be taken into
account. As shown by the effectiveness in our ex-
periments of PMI, which is a classic measure used
to harvest idioms and other multiword expressions
(Church and Hanks, 1990), ordering is affected by
arbitrary lexicalization patterns. Metrical effects are
also likely to play a role, like they do in the well-
studied case of “binomials” such as salt and pep-
per (Benor and Levy, 2006; Copestake and Herbe-
lot, 2011). In a pilot study, we found that indeed
word length (roughly quantified by number of let-
ters) is a significant factor in predicting adjective
ordering (the shorter adjective being more likely to
occur first), but its effect is not nearly as strong as
that of the semantic measures we considered here.
In our future work, we would like to develop an or-
der model that exploits semantic, metrical and lexi-
calization features jointly for maximal classification
accuracy.
Adjectival ordering information could be useful
in parsing: in English, it could tell whether an
AANN sequence should be parsed as A[[AN]N]
or A[A[NN]]; in languages with pre- and post-
N adjectives, like Italian or Spanish, it could tell
whether ANA sequences should be parsed as A[NA]
or [AN]A. The ability to detect ordering restric-
tions could also help Natural Language Generation
tasks (Malouf, 2000), especially for the generation
of unattested combinations of As and Ns.
From a theoretical point of view, we would like to
extend our analysis to adjective coordination (what’s
149
the difference between new and creative idea and
new creative idea?). Additionally, we could go more
granular, looking at whether compositional models
can help us to understand why certain classes of ad-
jectives are more likely to precede or follow others
(why is size more likely to take scope over color,
so that big red car sounds more natural than red big
car?) or studying the behaviour of specific adjectives
(can our approach capture the fact that strong alco-
holic drink is preferable to alcoholic strong drink
because strong pertains to the alcoholic properties
of the drink?).
In the meantime, we hope that the results we re-
ported here provide convincing evidence of the use-
fulness of compositional distributional semantics in
tackling topics, such as recursive adjectival modifi-
cation, that have been of traditional interest to theo-
retical linguists from a new perspective.
Acknowledgments
We would like to thank the anonymous reviewers,
Fabio Massimo Zanzotto, Yao-Zhong Zhang and the
members of the COMPOSES team. This research
was supported by the ERC 2011 Starting Indepen-
dent Research Grant n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Sarah Bunin Benor and Roger Levy. 2006. The chicken
or the egg? A probabilistic analysis of english binomi-
als. Language, pages 233–278.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the 2012 Joint Conference on
EMNLP and CoNLL, pages 546–556, Jeju Island, Ko-
rea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In Pro-
ceedings of the 2012 Joint Conference on EMNLP and
CoNLL, pages 1223–1233, Jeju Island, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35–46, Pots-
dam, Germany.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon’s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12, Los Angeles,
CA.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22–29.
Guglielmo Cinque, editor. 2002. Functional Structure in
DP and IP - The Carthography of Syntactic Structures,
volume 1. Oxford University Press.
Guglielmo Cinque. 2004. Issues in adverbial syntax.
Lingua, 114:683–710.
Guglielmo Cinque. 2010. The syntax of adjectives: a
comparative study. MIT Press.
Ann Copestake and Aure´lie Herbelot. 2011. Exciting
and interesting: issues in the generation of binomials.
In Proceedings of the UCNLG+ Eval: Language Gen-
eration and Evaluation Workshop, pages 45–53, Edin-
burgh, UK.
Paola Crisma. 1991. Functional categories inside the
noun phrase: A study on the distribution of nominal
modifiers. “Tesi di Laurea”, University of Venice.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013a. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, East Stroudsburg, PA.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the ACL 2013 Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2013), East Stroudsburg, PA.
Gottlob Frege. 1892. U¨ber sinn und bedeutung.
Zeitschrift fuer Philosophie un philosophische Kritik,
100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33–37, Uppsala, Sweden.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Computa-
tion, 19(10):2756–2779.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of ACL, pages 85–92, East Stroudsburg, PA.
150
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Richard Montague. 1970. Universal Grammar. Theoria,
36:373–398.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation of
linguistic data. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 122–130,
Los Angeles, CA.
Barbara Partee. 2004. Compositionality. In Compo-
sitionality in Formal Semantics: Selected Papers by
Barbara H. Partee. Blackwell, Oxford.
Magnus Sahlgren. 2006. The Word-Space Model. Dis-
sertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu¨tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Gary-John Scott. 2002. Stacked adjectival modification
and the structure of nominal phrases. In Guglielmo
Cinque, editor, Functional Structure in DP and IP. The
Carthography of Syntactic Structures, volume 1. Ox-
ford University Press.
Richard Socher, E.H. Huang, J. Pennington, Andrew Y.
Ng, and C.D. Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for paraphrase de-
tection. Advances in Neural Information Processing
Systems, 24:801–809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Edinburgh, UK.
Richard Sproat and Chilin Shih. 1990. The cross-
linguistics distribution of adjective ordering restric-
tions. In C. Georgopoulos and Ishihara R., editors,
Interdisciplinary approaches to language: essays in
honor of Yuki Kuroda, pages 565–593. Kluver, Dor-
drecht.
Sam Steddy and Vieri Samek-Lodovici. 2011. On the
ungrammaticality of remnant movement in the deriva-
tion of greenberg’s universal 20. Linguistic Inquiry,
42(3):445–469.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.
Peter Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533–585.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1–9, Portland,
OR.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-
chi, and Suresh Manandhar. 2010. Estimating linear
models for compositional distributional semantics. In
Proceedings of COLING, pages 1263–1271, Beijing,
China.
151
