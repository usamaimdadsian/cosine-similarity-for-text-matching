Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266,
Singapore, 6-7 August 2009. c©2009 ACL and AFNLP
Clustering to Find Exemplar Terms for Keyphrase Extraction
Zhiyuan Liu, Peng Li, Yabin Zheng, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, pengli09, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrases are widely used as a brief
summary of documents. Since man-
ual assignment is time-consuming, vari-
ous unsupervised ranking methods based
on importance scores are proposed for
keyphrase extraction. In practice, the
keyphrases of a document should not only
be statistically important in the docu-
ment, but also have a good coverage of
the document. Based on this observa-
tion, we propose an unsupervised method
for keyphrase extraction. Firstly, the
method finds exemplar terms by leverag-
ing clustering techniques, which guaran-
tees the document to be semantically cov-
ered by these exemplar terms. Then the
keyphrases are extracted from the doc-
ument using the exemplar terms. Our
method outperforms sate-of-the-art graph-
based ranking methods (TextRank) by
9.5% in F1-measure.
1 Introduction
With the development of Internet, information on
the web is emerging exponentially. How to effec-
tively seek and manage information becomes an
important research issue. Keyphrases, as a brief
summary of a document, provide a solution to help
organize, manage and retrieve documents, and are
widely used in digital libraries and information re-
trieval.
Keyphrases in articles of journals and books
are usually assigned by authors. However,
most articles on the web usually do not have
human-assigned keyphrases. Therefore, automatic
keyphrase extraction is an important research task.
Existing methods can be divided into supervised
and unsupervised approaches.
The supervised approach (Turney, 1999) re-
gards keyphrase extraction as a classification task.
In this approach, a model is trained to determine
whether a candidate term of the document is a
keyphrase, based on statistical and linguistic fea-
tures. For the supervised keyphrase extraction
approach, a document set with human-assigned
keyphrases is required as training set. However,
human labelling is time-consuming. Therefore, in
this study we focus on unsupervised approach.
As an example of an unsupervised keyphrase
extraction approach, the graph-based ranking (Mi-
halcea and Tarau, 2004) regards keyphrase extrac-
tion as a ranking task, where a document is repre-
sented by a term graph based on term relatedness,
and then a graph-based ranking algorithm is used
to assign importance scores to each term. Existing
methods usually use term cooccurrences within a
specified window size in the given document as an
approximation of term relatedness (Mihalcea and
Tarau, 2004).
As we know, none of these existing works
gives an explicit definition on what are appropri-
ate keyphrases for a document. In fact, the existing
methods only judge the importance of each term,
and extract the most important ones as keyphrases.
From the observation of human-assigned
keyphrases, we conclude that good keyphrases
of a document should satisfy the following
properties:
1. Understandable. The keyphrases are un-
derstandable to people. This indicates the
extracted keyphrases should be grammatical.
For example, “machine learning” is a gram-
matical phrase, but “machine learned” is not.
2. Relevant. The keyphrases are semantically
relevant with the document theme. For ex-
ample, for a document about “machine learn-
ing”, we want the keyphrases all about this
theme.
3. Good coverage. The keyphrases should
257
cover the whole document well. Sup-
pose we have a document describing “Bei-
jing” from various aspects of “location”,
“atmosphere” and “culture”, the extracted
keyphrases should cover all the three aspects,
instead of just a partial subset of them.
The classification-based approach determines
whether a term is a keyphrase in isolation, which
could not guarantee Property 3. Neither does the
graph-based approach guarantee the top-ranked
keyphrases could cover the whole document. This
may cause the resulting keyphrases to be inappro-
priate or badly-grouped.
To extract the appropriate keyphrases for a doc-
ument, we suggest an unsupervised clustering-
based method. Firstly the terms in a document are
grouped into clusters based on semantic related-
ness. Each cluster is represented by an exemplar
term, which is also the centroid of each cluster.
Then the keyphrases are extracted from the docu-
ment using these exemplar terms.
In this method, we group terms based on se-
mantic relatedness, which guarantees a good cov-
erage of the document and meets Property 2 and
3. Moreover, we only extract the keyphrases in ac-
cordance with noun group (chunk) patterns, which
guarantees the keyphrases satisfy Property 1.
Experiments show that the clustering-based
method outperforms the state-of-the-art graph-
based approach on precision, recall and F1-
measure. Moreover, this method is unsupervised
and language-independent, which is applicable in
the web era with enormous information.
The rest of the paper is organized as follows.
In Section 2, we introduce and discuss the re-
lated work in this area. In Section 3, we give an
overview of our method for keyphrase extraction.
From Section 4 to Section 7, the algorithm is de-
scribed in detail. Empirical experiment results are
demonstrated in Section 8, followed by our con-
clusions and plans for future work in Section 9.
2 Related Work
A straightforward method for keyphrase extrac-
tion is to select keyphrases according to frequency
criteria. However, the poor performance of this
method drives people to explore other methods. A
pioneering achievement is carried out in (Turney,
1999), as mentioned in Section 1, a supervised ma-
chine learning method was suggested in this paper
which regards keyphrase extraction as a classifi-
cation task. In this work, parameterized heuristic
rules are combined with a genetic algorithm into a
system for keyphrase extraction. A different learn-
ing algorithm, Naive Bayes method, is applied in
(Frank et al., 1999) with improved results on the
same data used in (Turney, 1999). Hulth (Hulth,
2003; Hulth, 2004) adds more linguistic knowl-
edge, such as syntactic features, to enrich term
representation, which significantly improves the
performance. Generally, the supervised methods
need manually annotated training set, which may
sometimes not be practical, especially in the web
scenario.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becom-
ing the most widely used unsupervised approach
for keyphrase extraction. The work in (Litvak
and Last, 2008) applies HITS algorithm on the
word graph of a document under the assumption
that the top-ranked nodes should be the document
keywords. Experiments show that classification-
based supervised method provides the highest key-
word identification accuracy, while the HITS al-
gorithm gets the highest F-measure. Work in
(Huang et al., 2006) also considers each document
as a term graph where the structural dynamics of
these graphs can be used to identify keyphrases.
Wan and Xiao (Wan and Xiao, 2008b) use a
small number of nearest neighbor documents to
provide more knowledge to improve graph-based
keyphrase extraction algorithm for single docu-
ment. Motivated by similar idea, Wan and Xiao
(Wan and Xiao, 2008a) propose to adopt cluster-
ing methods to find a small number of similar doc-
uments to provide more knowledge for building
word graphs for keyword extraction. Moreover,
after our submission of this paper, we find that
a method using community detection on seman-
tic term graphs is proposed for keyphrase extrac-
tion from multi-theme documents (Grineva et al.,
2009). In addition, some practical systems, such
as KP-Miner (Elbeltagy and Rafea, 2009), also
do not need to be trained on a particular human-
annotated document set.
In recent years, a number of systems are de-
veloped for extracting keyphrases from web docu-
ments (Kelleher and Luz, 2005; Chen et al., 2005),
email (Dredze et al., 2008) and some other spe-
cific sources, which indicates the importance of
keyphrase extraction in the web era. However,
258
none of these previous works has overall consid-
eration on the essential properties of appropriate
keyphrases mentioned in Section 1.
We should also note that, although the preci-
sion and recall of most current keyphrase extrac-
tors are still much lower compared to other NLP-
tasks, it does not indicate the performance is poor
because even different annotators may assign dif-
ferent keyphrases to the same document. As de-
scribed in (Wan and Xiao, 2008b), when two anno-
tators were asked to label keyphrases on 308 doc-
uments, the Kappa statistic for measuring inter-
agreement among them was only 0.70.
3 Algorithm Overview
The method proposed in this paper is mainly in-
spired by the nature of appropriate keyphrases
mentioned in Section 1, namely understandable,
semantically relevant with the document and high
coverage of the whole document.
Let’s analyze the document describing “Bei-
jing” from the aspects of “location”, “atmosphere”
and “culture”. Under the bag-of-words assump-
tion, each term in the document, except for func-
tion words, is used to describe an aspect of the
theme. Based on these aspects, terms are grouped
into different clusters. The terms in the same clus-
ter are more relevant with each other than with
the ones in other clusters. Taking the terms “tem-
perature”, “cold” and “winter” for example, they
may serve the aspect “atmosphere” instead of “lo-
cation” or some other aspects when talking about
“Beijing”.
Based on above description, it is thus reason-
able to propose a clustering-based method for
keyphrase extraction. The overview of the method
is:
1. Candidate term selection. We first filter out
the stop words and select candidate terms for
keyphrase extraction.
2. Calculating term relatedness. We use some
measures to calculate the semantic related-
ness of candidate terms.
3. Term clustering. Based on term relatedness,
we group candidate terms into clusters and
find the exemplar terms of each cluster.
4. From exemplar terms to keyphrases. Fi-
nally, we use these exemplar terms to extract
keyphrases from the document.
In the next four sections we describe the algo-
rithm in detail.
4 Candidate Term Selection
Not all words in a document are possible to be se-
lected as keyphrases. In order to filter out the noisy
words in advance, we select candidate terms using
some heuristic rules. This step proceeds as fol-
lows. Firstly the text is tokenized for English or
segmented into words for Chinese and other lan-
guages without word-separators. Then we remove
the stop words and consider the remaining single
terms as candidates for calculating semantic relat-
edness and clustering.
In methods like (Turney, 1999; Elbeltagy and
Rafea, 2009), candidate keyphrases were first
found using n-gram. Instead, in this method, we
just find the single-word terms as the candidate
terms at the beginning. After identifying the ex-
emplar terms within the candidate terms, we ex-
tract multi-word keyphrases using the exemplars.
5 Calculating Term Relatedness
After selecting candidate terms, it is important to
measure term relatedness for clustering. In this pa-
per, we propose two approaches to calculate term
relatedness: one is based on term cooccurrence
within the document, and the other by leveraging
human knowledge bases.
5.1 Cooccurrence-based Term Relatedness
An intuitive method for measuring term relat-
edness is based on term cooccurrence relations
within the given document. The cooccurrence
relation expresses the cohesion relationships be-
tween terms.
In this paper, cooccurrence-based relatedness is
simply set to the count of cooccurrences within a
window of maximum w words in the whole doc-
ument. In the following experiments, the window
size w is set from 2 to 10 words.
Each document can be regarded as a word se-
quence for computing cooccurrence-based relat-
edness. There are two types of word sequence
for counting term cooccurrences. One is the origi-
nal word sequence without filtering out any words,
and the other is after filtering out the stop words
or the words with specified part-of-speech (POS)
tags. In this paper we select the first type because
each word in the sequence takes important role for
measuring term cooccurrences, no matter whether
259
it is a stop word or something else. If we filter
out some words, the term relatedness will not be
as precise as before.
In experiments, we will investigate how the
window size influences the performance of
keyphrase extraction.
5.2 Wikipedia-based Term Relatedness
Many methods have been proposed for measuring
the relatedness between terms using external re-
sources. One principled method is leveraging hu-
man knowledge bases. Inspired by (Gabrilovich
and Markovitch, 2007), we adopt Wikipedia, the
largest encyclopedia collected and organized by
human on the web, as the knowledge base to mea-
sure term relatedness.
The basic idea of computing term related-
ness by leveragting Wikipedia is to consider each
Wikipedia article as a concept. Then the se-
mantic meaning of a term could be represented
as a weighted vector of Wikipedia concepts, of
which the values are the term’s TFIDF within cor-
responding Wikipedia articles. We could com-
pute the term relatedness by comparing the con-
cept vectors of the terms. Empirical evaluations
confirm that the idea is effective and practical
for computing term relatedness (Gabrilovich and
Markovitch, 2007).
In this paper, we select cosine similarity, Eu-
clidean distance, Point-wise Mutual Information
and Normalized Google Similarity Distance (Cili-
brasi and Vitanyi, 2007) for measuring term relat-
edness based on the vector of Wikipedia concepts.
Denote the Wikipedia-concept vector of the
term t
i
as C
i
= {c
i1
, c
i2
, ..., c
iN
}, where N in-
dicates the number of Wikipedia articles, and c
ik
is the TFIDF value of w
i
in the kth Wikipedia ar-
ticle. The cosine similarity is defined as
cos(i, j) =
C
i
· C
j
?C
i
??C
j
?
(1)
The definition of Euclidean distance is
euc(i, j) =
?
?
?
?
N
?
k=1
(c
ik
? c
jk
)
2 (2)
Point-wise Mutual Information (PMI) is a com-
mon approach to quantify relatedness. Here we
take three ways to measure term relatedness using
PMI. One is based on Wikipedia page count,
pmi
p
(i, j) = log
2
N × p(i, j)
p(i) × p(j)
(3)
where p(i, j) is the number of Wikipedia articles
containing both t
i
and t
j
, while p(i) is the number
of articles which contain t
i
. The second is based
on the term count in Wikipedia articles,
pmi
t
(i, j) = log
2
T × t(i, j)
t(i) × t(j)
(4)
where T is the number of terms in Wikipedia,
t(i, j) is the number of t
i
and t
j
occurred adja-
cently in Wikipedia, and t(i) is the number of t
i
in
Wikipedia. The third one is a combination of the
above two PMI ways,
pmi
c
(i, j) = log
2
N × pt(i, j)
p(i) × p(j)
(5)
where pt(i, j) indicates the number of Wikipedia
articles containing t
i
and t
j
as adjacency. It is ob-
vious that pmi
c
(i, j) ? pmi
p
(i, j), and pmi
c
(i, j)
is more strict and accurate for measuring related-
ness.
Normalized Google Similarity Distance (NGD)
is a new measure for measuring similarity between
terms proposed by (Cilibrasi and Vitanyi, 2007)
based on information distance and Kolmogorov
complexity. It could be applied to compute term
similarity from the World Wide Web or any large
enough corpus using the page counts of terms.
NGD used in this paper is based on Wikipedia ar-
ticle count, defined as
ngd(i, j) =
max(log p(i), log p(j)) ? logp(i, j)
logN ? min(logp(i), logp(j))
(6)
where N is the number of Wikipedia articles used
as normalized factor.
Once we get the term relatedness, we could then
group the terms using clustering techniques and
find exemplar terms for each cluster.
6 Term Clustering
Clustering is an important unsupervised learning
problem, which is the assignment of objects into
groups so that objects from the same cluster are
more similar to each other than objects from dif-
ferent clusters (Han and Kamber, 2005). In this
paper, we use three widely used clustering algo-
rithms, hierarchical clustering, spectral clustering
and Affinity Propagation, to cluster the candidate
terms of a given document based on the semantic
relatedness between them.
260
6.1 Hierarchical Clustering
Hierarchical clustering groups data over a variety
of scales by creating a cluster tree. The tree is a
multilevel hierarchy, where clusters at one level
are joined as clusters at the next level. The hier-
archical clustering follows this procedure:
1. Find the distance or similarity between every
pair of data points in the dataset;
2. Group the data points into a binary and hier-
archical cluster tree;
3. Determine where to cut the hierarchical tree
into clusters. In hierarchical clustering, we
have to specify the cluster number m in ad-
vance.
In this paper, we use the hierarchical cluster-
ing implemented in Matlab Statistics Toolbox.
Note that although we use hierarchical clustering
here, the cluster hierarchy is not necessary for the
clustering-based method.
6.2 Spectral Clustering
In recent years, spectral clustering has become one
of the most popular modern clustering algorithms.
Spectral clustering makes use of the spectrum of
the similarity matrix of the data to perform dimen-
sionality reduction for clustering into fewer di-
mensions, which is simple to implement and often
outperforms traditional clustering methods such as
k-means. Detailed introduction to spectral cluster-
ing could be found in (von Luxburg, 2006).
In this paper, we use the spectral clustering tool-
box developed by Wen-Yen Chen, et al. (Chen et
al., 2008) 1. Since the cooccurrence-based term
relatedness is usually sparse, the traditional eigen-
value decomposition in spectral clustering will
sometimes get run-time error. In this paper, we
use the singular value decomposition (SVD) tech-
nique for spectral clustering instead.
For spectral clustering, two parameters are re-
quired to be set by the user: the cluster number
m, and ? which is used in computing similarities
from object distances
s(i, j) = exp(
?d(i, j)
2
2?
2
) (7)
where s(i, j) and d(i, j) are the similarity and dis-
tance between i and j respectively.
1The package could be accessed via http://www.cs.
ucsb.edu/
˜
wychen/sc.html.
6.3 Affinity Propagation
Another powerful clustering method, Affinity
Propagation, is based on message passing tech-
niques. AP was proposed in (Frey and Dueck,
2007), where AP was reported to find clusters with
much lower error than those found by other meth-
ods. In this paper, we use the toolbox developed
by Frey, et al. 2.
Detailed description of the algorithm could be
found in (Frey and Dueck, 2007). Here we intro-
duced three parameters for AP:
• Preference. Rather than requiring prede-
fined number of clusters, Affinity Propaga-
tion takes as input a real number p for each
term, so that the terms with larger p are more
likely to be chosen as exemplars, i.e., cen-
troids of clusters. These values are referred
to as “preferences”. The preferences are usu-
ally be set as the maximum, minimum, mean
or median of s(i, j), i 6= j.
• Convergence criterion. AP terminates if (1)
the local decisions stay constant for I
1
itera-
tions; or (2) the number of iterations reaches
I
2
. In this work, we set I
1
to 100 and I
2
to
1, 000.
• Damping factor. When updating the mes-
sages, it is important to avoid numerical os-
cillations by using damping factor. Each
message is set to ? times its value from the
previous iteration plus 1 ? ? times its pre-
scribed updated value, where the damping
factor ? is between 0 and 1. In this paper we
set ? = 0.9.
7 From Exemplar Terms to Keyphrases
After term clustering, we select the exemplar
terms of each clusters as seed terms. In Affinity
Propagation, the exemplar terms are directly ob-
tained from the clustering results. In hierarchical
clustering, exemplar terms could also be obtained
by the Matlab toolbox. While in spectral cluster-
ing, we select the terms that are most close to the
centroid of a cluster as exemplar terms.
As reported in (Hulth, 2003), most manually
assigned keyphrases turn out to be noun groups.
Therefore, we annotate the document with POS
2The package could be accessed via http://www.
psi.toronto.edu/affinitypropagation/.
261
tags using Stanford Log-Linear Tagger 3, and then
extract the noun groups whose pattern is zero or
more adjectives followed by one or more nouns.
The pattern can be represented using regular ex-
pressions as follows
(JJ) ? (NN |NNS|NNP )+
where JJ indicates adjectives and various forms
of nouns are represented using NN , NNS and
NNP . From these noun groups, we select the
ones that contain one or more exemplar terms to
be the keyphrases of the document.
In this process, we may find single-word
keyphrases. In practice, only a small fraction of
keyphrases are single-word. Thus, as a part of
postprocessing process, we have to use a frequent
word list to filter out the terms that are too com-
mon to be keyphrases.
8 Experiment Results
8.1 Datasets and Evaluation Metric
The dataset used in the experiments is a collec-
tion of scientific publication abstracts from the In-
spec database and the corresponding manually as-
signed keyphrases 4. The dataset is used in both
(Hulth, 2003) and (Mihalcea and Tarau, 2004).
Each abstract has two kinds of keyphrases: con-
trolled keyphrases, restricted to a given dictionary,
and uncontrolled keyphrases, freely assigned by
the experts. We use the uncontrolled keyphrases
for evaluation as proposed in (Hulth, 2003) and
followed by (Mihalcea and Tarau, 2004).
As indicated in (Hulth, 2003; Mihalcea and
Tarau, 2004), in uncontrolled manually assigned
keyphrases, only the ones that occur in the cor-
responding abstracts are considered in evaluation.
The extracted keyphrases of various methods and
manually assigned keyphrases are compared after
stemming.
In the experiments of (Hulth, 2003), for her su-
pervised method, Hulth splits a total of 2, 000 ab-
stracts into 1, 000 for training, 500 for validation
and 500 for test. In (Mihalcea and Tarau, 2004),
due to the unsupervised method, only the test set
was used for comparing the performance of Tex-
tRank and Hulth’s method.
3The package could be accessed via http://http://
nlp.stanford.edu/software/tagger.shtml.
4Many thanks to Anette Hulth for providing us the dataset.
For computing Wikipedia-based relatedness,
we use a snapshot on November 11, 2005 5. The
frequent word list used in the postprocessing step
for filtering single-word phrases is also computed
from Wikipedia. In the experiments of this pa-
per, we add the words that occur more than 1, 000
times in Wikipedia into the list.
The clustering-based method is completely un-
supervised. Here, we mainly run our method on
test set and investigate the influence of relatedness
measurements and clustering methods with differ-
ent parameters. Then we compare our method
with two baseline methods: Hulth’s method and
TextRank. Finally, we analyze and discuss the per-
formance of the method by taking the abstract of
this paper as a demonstration.
8.2 Influence of Relatedness Measurements
We first investigate the influence of semantic re-
latedness measurements. By systematic experi-
ments, we find that Wikipedia-based relatedness
outperforms cooccurrence-based relatedness for
keyphrase extraction, though the improvement is
not significant. In Table 1, we list the perfor-
mance of spectral clustering with various related-
ness measurements for demonstration. In this ta-
ble, the w indicates the window size for counting
cooccurrences in cooccurrence-based relatedness.
cos, euc, etc. are different measures for com-
puting Wikipedia-based relatedness which we pre-
sented in Section 5.2.
Table 1: Influence of relatedness measurements
for keyphrase extraction.
Parameters Precision Recall F1-measure
Cooccurrence-based Relatedness
w = 2 0.331 0.626 0.433
w = 4 0.333 0.621 0.434
w = 6 0.331 0.630 0.434
w = 8 0.330 0.623 0.432
w = 10 0.333 0.632 0.436
Wikipedia-based Relatedness
cos 0.348 0.655 0.455
euc 0.344 0.634 0.446
pmi
p
0.344 0.621 0.443
pmi
t
0.344 0.619 0.442
pmi
c
0.350 0.660 0.457
ngd 0.343 0.620 0.442
5The dataset could be get from http://www.cs.
technion.ac.il/
˜
gabr/resources/code/
wikiprep/.
262
We use spectral clustering here because it out-
performs other clustering techniques, which will
be shown in the next subsection. The results in Ta-
ble 1 are obtained when the cluster number m =
2
3
n, where n is the number of candidate terms ob-
tained in Section 5. Besides, for Euclidean dis-
tance and Google distance, we set ? = 36 of For-
mula 7 to convert them to corresponding similari-
ties, where we get the best result when we conduct
different trails with ? = 9, 18, 36, 54, though there
are only a small margin among them.
As shown in Table 1, although the method using
Wikipedia-based relatedness outperforms that us-
ing cooccurrence-based relatedness, the improve-
ment is not prominent. Wikipedia-based related-
ness is computed according to global statistical in-
formation on Wikipedia. Therefore it is more pre-
cise than cooccurrence-based relatedness, which is
reflected in the performance of the keyphrase ex-
traction. However, on the other hand, Wikipedia-
based relatedness does not catch the document-
specific relatedness, which is represented by the
cooccurrence-based relatedness. It will be an in-
teresting future work to combine these two types
of relatedness measurements.
From this subsection, we conclude that, al-
though the method using Wikipedia-based related-
ness performs better than cooccurrence-based one,
due to the expensive computation of Wikipedia-
based relatedness, the cooccurrence-based one is
good enough for practical applications.
8.3 Influence of Clustering Methods and
Their Parameters
To demonstrate the influence of clustering meth-
ods for keyphrase extraction, we fix the relat-
edness measurement as Wikipedia-based pmi
c
,
which has been shown in Section 8.2 to be the best
relatedness measurement.
In Table 2, we show the performance of three
clustering techniques for keyphrase extraction.
For hierarchical clustering and spectral clustering,
the cluster number m are set explicitly as the pro-
portion of candidate terms n, while for Affinity
Propagation, we set preferences as the minimum,
mean, median and maximum of s(i, j) to get dif-
ferent number of clusters, denoted as min, mean,
median and max in the table respectively.
As shown in the table, when cluster number m
is large, spectral clustering outperforms hierarchi-
cal clustering and Affinity Propagation. Among
Table 2: Influence of clustering methods for
keyphrase extraction.
Parameters Precision Recall F1-measure
Hierarchical Clustering
m =
1
4
n 0.365 0.369 0.367
m =
1
3
n 0.365 0.369 0.367
m =
1
2
n 0.351 0.562 0.432
m =
2
3
n 0.346 0.629 0.446
m =
4
5
n 0.340 0.657 0.448
Spectral Clustering
m =
1
4
n 0.385 0.409 0.397
m =
1
3
n 0.374 0.497 0.427
m =
1
2
n 0.374 0.497 0.427
m =
2
3
n 0.350 0.660 0.457
m =
4
5
n 0.340 0.679 0.453
Affinity Propagation
p = max 0.331 0.688 0.447
p = mean 0.433 0.070 0.121
p = median 0.422 0.078 0.132
p = min 0.419 0.059 0.103
these methods, only Affinity Propagation under
some parameters performs poorly.
8.4 Comparing with Other Algorithms
Table 3 lists the results of the clustering-based
method compared with the best results reported
in (Hulth, 2003; Mihalcea and Tarau, 2004) on
the same dataset. For each method, the table lists
the total number of assigned keyphrases, the mean
number of keyphrases per abstract, the total num-
ber of correct keyphrases, and the mean number of
correct keyphrases. The table also lists precision,
recall and F1-measure. In this table, hierarchical
clustering, spectral clustering and Affinity Propa-
gation are abbreviated by “HC”, “SC” and “AP”
respectively.
The result of Hulth’s method listed in this ta-
ble is the best one reported in (Hulth, 2003) on the
same dataset. This is a supervised classification-
based method, which takes more linguistic fea-
tures in consideration for keyphrase extraction.
The best result is obtained using n-gram as candi-
date keyphrases and adding POS tags as candidate
features for classification.
The result of TextRank listed here is the best
one reported in (Mihalcea and Tarau, 2004) on the
same dataset. To obtain the best result, the authors
built an undirected graph using window w = 2
on word sequence of the given document, and ran
263
Table 3: Comparison results of Hulth’s method, TextRank and our clustering-based method.
Assigned Correct
Method Total Mean Total Mean Precision Recall F1-measure
Hulth’s 7,815 15.6 1,973 3.9 0.252 0.517 0.339
TextRank 6,784 13.7 2,116 4.2 0.312 0.431 0.362
HC 7,303 14.6 2,494 5.0 0.342 0.657 0.449
SC 7,158 14.3 2,505 5.0 0.350 0.660 0.457
AP 8,013 16.0 2,648 5.3 0.330 0.697 0.448
PageRank on it.
In this table, the best result of hierarchical clus-
tering is obtained by setting the cluster number
m =
2
3
n and using Euclidean distance for comput-
ing Wikipedia-based relatedness. The parameters
of spectral clustering are the same as in last sub-
section. For Affinity Propagation, the best result
is obtained under p = max and using Wikipedia-
based Euclidean distance as relatedness measure.
From this table, we can see clustering-
based method outperforms TextRank and Hulth’s
method. For spectral clustering, F1-measure
achieves an approximately 9.5% improvement as
compared to TextRank.
Furthermore, since the clustering-based method
is unsupervised, we do not need any set for train-
ing and validation. In this paper, we also carry out
an experiment on the whole Hulth’s dataset with
2, 000 abstracts. The performance is similar to
that on 500 abstracts as shown above. The best
result is obtained when we use spectral clustering
by setting m = 2
3
n with Wikipedia-based pmi
c
relatedness, which is the same in 500 abstracts. In
this result, we extract 29, 517 keyphrases, among
which 9, 655 are correctly extracted. The preci-
sion, recall and F1-measure are 0.327, 0.653 and
0.436 respectively. The experiment results show
that the clustering-based method is stable.
8.5 Analysis and Discussions
From the above experiment results, we can see the
clustering-based method is both robust and effec-
tive for keyphrase extraction as an unsupervised
method.
Here, as an demonstration, we use spectral clus-
tering and Wikipedia-based pmi
c
relatedness to
extract keyphrases from the abstract of this pa-
per. The extracted stemmed keyphrases under var-
ious cluster numbers are shown in Figure 1. In
this figure, we find that when m = 1
4
n,
1
3
n,
1
2
n,
the extracted keyphrases are identical, where the
exemplar terms under m = 1
3
n are marked in
boldface. We find several aspects like “unsuper-
vised”, “exemplar term” and “keyphrase extrac-
tion” are extracted correctly. In fact, “clustering
technique” in the abstract should also be extracted
as a keyphrase. However, since “clustering” is
tagged as a verb that ends in -ing, which disagrees
the noun group patterns, thus the phrase is not
among the extracted keyphrases.
When m = 2
3
n, the extracted keyphrases
are noisy with many single-word phrases. As
the cluster number increases, more exemplar
terms are identified from these clusters, and more
keyphrases will be extracted from the document
based on exemplar terms. If we set the cluster
number to m = n, all terms will be selected as
exemplar terms. In this extreme case, all noun
groups will be extracted as keyphrases, which
is obviously not proper for keyphrase extraction.
Thus, it is important for this method to appropri-
ately specify the cluster number.
In the experiments, we also notice that frequent
word list is important for keyphrase extraction.
Without the list for filtering, the best F1-measure
will decrease by about 5 percent to 40%. How-
ever, the solution of using frequent word list is
somewhat too simple, and in future work, we plan
to investigate a better combination of clustering-
based method with traditional methods using term
frequency as the criteria.
9 Conclusion and Future Work
In this paper, we propose an unsupervised
clustering-based keyphrase extraction algorithm.
This method groups candidate terms into clus-
ters and identify the exemplar terms. Then
keyphrases are extracted from the document based
on the exemplar terms. The clustering based on
term semantic relatedness guarantees the extracted
keyphrases have a good coverage of the document.
Experiment results show the method has a good ef-
264
Figure 1: Keyphrases in stemmed form extracted
from this paper’s abstract.
Keyphrases when m = 1
4
n,
1
3
n,
1
2
n
unsupervis method; various unsupervis rank
method; exemplar term; state-of-the-art
graph-bas rank method; keyphras; keyphras
extract
Keyphrases when m = 2
3
n
unsupervis method; manual assign; brief sum-
mari; various unsupervis rank method; exem-
plar term; document; state-of-the-art graph-bas
rank method; experi; keyphras; import score;
keyphras extract
fectiveness and robustness, and outperforms base-
lines significantly.
Future work may include:
1. Investigate the feasibility of clustering di-
rectly on noun groups;
2. Investigate the feasibility of combining
cooccurrence-based and Wikipedia-based re-
latedness for clustering;
3. Investigate the performance of the method on
other types of documents, such as long arti-
cles, product reviews and news;
4. The solution of using frequent word list
for filtering out too common single-word
keyphrases is undoubtedly simple, and we
plan to make a better combination of
the clustering-based method with traditional
frequency-based methods for keyphrase ex-
traction.
Acknowledgments
This work is supported by the National 863 Project
under Grant No. 2007AA01Z148 and the Na-
tional Science Foundation of China under Grant
No. 60621062. The authors would like to thank
Anette Hulth for kindly sharing her datasets.
References
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase extrac-
tion for web pages. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 277–278.
Wen Y. Chen, Yangqiu Song, Hongjie Bai, Chih J. Lin,
and Edward Chang. 2008. Psc: Paralel spectral
clustering. Submitted.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Transactions on
Knowledge and Data Engineering, 19(3):370–383.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th international conference on Intelligent user in-
terfaces, pages 199–206.
S. Elbeltagy and A. Rafea. 2009. Kp-miner: A
keyphrase extraction system for english and arabic
documents. Information Systems, 34(1):132–144.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668–673.
Brendan J J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6–12.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Ex-
tracting key terms from noisy and multi-theme docu-
ments. In Proceedings of the 18th international con-
ference on World wide web, pages 661–670. ACM
New York, NY, USA.
Jiawei Han and Micheline Kamber. 2005. Data Min-
ing: Concepts and Techniques, second edition. Mor-
gan Kaufmann.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275–284.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216–223.
A. Hulth. 2004. Reducing false positives by expert
combination in automatic keyword indexing. Re-
cent Advances in Natural Language Processing III:
Selected Papers from RANLP 2003, page 367.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence.
265
Marina Litvak and Mark Last. 2008. Graph-based
keyword extraction for single-document summariza-
tion. In Proceedings of the workshop Multi-source
Multilingual Information Extraction and Summa-
rization, pages 17–24.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Peter D. Turney. 1999. Learning to Extract Keyphrases
from Text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
U. von Luxburg. 2006. A tutorial on spectral clus-
tering. Technical report, Max Planck Institute for
Biological Cybernetics.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labrank: Towards a collaborative approach to single-
document keyphrase extraction. In Proceedings of
COLING, pages 969–976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
855–860.
266
