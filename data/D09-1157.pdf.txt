Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
Classifying Relations for Biomedical Named Entity Disambiguation
Xinglong Wang
†‡
Jun’ichi Tsujii
?†‡
Sophia Ananiadou
†‡
†
School of Computer Science, University of Manchester, UK
‡
National Centre for Text Mining, UK
?
Department of Computer Science, University of Tokyo, Japan
{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Named entity disambiguation concerns
linking a potentially ambiguous mention
of named entity in text to an unambigu-
ous identifier in a standard database. One
approach to this task is supervised classifi-
cation. However, the availability of train-
ing data is often limited, and the avail-
able data sets tend to be imbalanced and,
in some cases, heterogeneous. We pro-
pose a new method that distinguishes a
named entity by finding the informative
keywords in its surrounding context, and
then trains a model to predict whether each
keyword indicates the semantic class of
the entity. While maintaining a compara-
ble performance to supervised classifica-
tion, this method avoids using expensive
manually annotated data for each new do-
main, and thus achieves better portability.
1 Introduction
While technology on named entity recognition
(NER) matures, many researchers in the field of
information extraction (IE) gradually shifted their
focus to more complex tasks such as named en-
tity disambiguation and relation extraction. Both
tasks are particularly important for biomedical text
mining, which concerns automatically extracting
facts from the exponentially growing biomedical
literature (Hunter and Cohen, 2006). One type of
facts is relations between biomedical named en-
tities, such as disease-drug relation, gene-disease
relation, protein-protein interaction (PPI), etc. To
automatically extract these facts, advanced natu-
ral language processing techniques such as parsing
have been adopted to analyse the syntactic and se-
mantic structure of text. The idea is that linguistic
structures between the interacting biological enti-
ties may have common characteristics that can be
exploited by similarity measures or machine learn-
ing algorithms. For example, Erkan et al. (2007)
used the shortest path between two genes accord-
ing to edit distance in a dependency tree to de-
fine a kernel function for extracting gene interac-
tions. Miwa et al. (2008) comparably evaluated a
number of kernels for incorporating syntactic fea-
tures, including the bag-of-word kernel, the subset
tree kernel (Moschitti, 2006) and the graph ker-
nel (Airola et al., 2008), and they concluded that
combining all kernels achieved better results than
using any individual one. Miyao et al. (2008)
used syntactic paths as one of the features to train
a support vector machines (SVM) model for PPIs
and also discussed how different parsers and out-
put representations affected the end results.
Another crucial IE task is named entity disam-
biguation, which concerns grounding mentions of
named entities in text to unambiguous concepts as
defined in some standard dictionary or database.
For instance, given a search term Python, users
may like to see the results grouped into the fol-
lowing categories: a type of snake, a programming
language, or a film (Bunescu and Pas¸ca, 2006).
One approach to such lexical disambiguation tasks
is supervised classification. However, such tech-
niques suffer from the knowledge acquisition bot-
tleneck, meaning that manually annotating train-
ing data is costly and can never satisfy the need by
the machine learning algorithms. In addition, su-
pervised techniques may not yield reliable results
when the distributions of the semantic classes are
different in the training and test datasets (Agirre
and Martinez, 2004; Koeling et al., 2005). For ex-
ample, on the task of word sense disambiguation,
a model trained on a dataset where the predom-
inant sense of the word star is “heavenly body”,
may not work well on text mainly composed of
entertainment news. Such problems are also ma-
jor concerns when developing a system to disam-
biguate biomedical named entities (e.g., protein,
1513
gene, and disease), for which some researchers
rely on hand-crafted rules in addition to a small
amount of training data (Morgan and Hirschman,
2007; Hakenberg et al., 2008).
This paper proposes a new disambiguation
method that, instead of classifying each individual
occurrence of an entity, it classifies pair-wise re-
lations between the entity mention in question and
the “cue words” in its adjacent context, where each
cue word is assumed to bear a semantic class. We
then select the cue word that has a positive rela-
tion with the entity, and pass its semantic tag to it.
While an individual entity mention may belong to
a large number of semantic classes, a relation can
only take one of two values: positive or negative,
hence transforming a complex multi-classification
problem into a less complicated binary classifica-
tion task. The remainder of the paper is organised
as follows: Section 2 proposes the disambigua-
tion method and Section 3 introduces the task of
disambiguating the model organisms of biomedi-
cal named entities. Section 4 describes in detail
our proposed method and also a number of base-
line systems for comparison purposes. Section 5
shows the evaluation results and discusses the ad-
vantages and drawback of our system, and we fi-
nally conclude in Section 6.
2 Disambiguation as Relation
Classification
The named entity disambiguation task is defined
as follows: given a mention of a named entity in
text, we automatically assign a semantic tag d to
it, where d ? D, and D is a pre-compiled dic-
tionary with |D| entries. When |D| is small, the
problem can be approached by supervised classi-
fication. For example, to determine whether an
occurrence of an entity is a protein, a gene or an
RNA, Hatzivassiloglou et al. (2001) compared
performance of 3 supervised classification meth-
ods and reported results near the human agree-
ment rate. Nevertheless, when |D| is large (e.g.,
> 100), the performance of classification may de-
crease, especially when the distribution of d in
training dataset differs from that in the test set. In
other words, when |D| is large, named entity dis-
ambiguation becomes a multi-class classification
task on heterogeneous and imbalanced datasets,
which is challenging for a machine learning model
to learn to discriminate enough between the se-
mantic classes (Japkowicz, 2000).
We propose an alternative method for named
entity disambiguation. Intuitively, in the surround-
ing context of an ambiguous entity, one can of-
ten find “cue words” that are informative indica-
tors of the entity’s semantic category. These cue
words are provided by authors to remind readers
the semantic identity of a named entity. For ex-
ample, in an article about protein p53, phrase “hu-
man protein p53” may be mentioned, where both
human and protein contain semantic information
regarding p53: human indicates the model organ-
ism of p53, and protein suggests the type of this
entity. Such cue words may occur infrequently in
the training data, making it difficult for machine
learning classifiers to capture.
Our method exploits this observation. Given a
sentence, let E be the set of ‘target’ entities (e.g.,
p53) and W of the ‘cue’ words (e.g., human) that
co-occur in a sentence, we define a relation as a
pair r = ?e, w?, where e ? E and w ? W , and
r is a positive relation if e belongs to the semantic
class indicated by w, and is a negative one if not.
Then we can disambiguate e by accomplishing the
following steps: 1) identify W and build a set
of relations R = {?e, w
i
?|w
i
?W, i = 1, 2, .., n},
where n is the size of W ; and 2) classify every
r ? R and assign the semantic tag of w
j
to e such
that r
j
= ?e, w
j
? is positive. The first task can be
tackled by a dictionary lookup, or by an NER sys-
tem, if manually annotated data is available. The
second is essentially a binary relation classifica-
tion task, and in this work, we use an SVM model
exploiting bag-of-word and syntactic features.
3 Species Disambiguation
We show the performance of the proposed method
on a task of resolving one major source of am-
biguity in protein and gene entities: model or-
ganisms. Model organisms are species studied to
understand particular biological phenomena. Bi-
ological experiments are often conducted on one
species, with the expectation that the discover-
ies will provide insight into the workings of oth-
ers, including humans, which are more difficult
to study directly. From viruses, prokaryotes, to
plants and animals, there are dozens of organ-
isms commonly used in biological studies, such
as E. coli, Drosophila, Homo sapiens, and hun-
dreds more are frequently mentioned in biologi-
cal research papers. In biomedical articles, entities
of different species are commonly referred to us-
1514
ing the same name, causing great ambiguity. For
example, searching a protein sequence database,
RefSeq
1
with query “tumor protein p53” resulted
in over 100 proteins, as the name is shared by
many organisms.
The importance of distinguishing model organ-
isms has been recognised by the community of
biomedical text mining. Chen et al. (2005) col-
lected gene names from various source databases
and calculated intra- and inter-species ambigui-
ties. Overall, only 25 (0.02%) official symbols
were ambiguous within the organisms. However,
when official symbols from 21 organisms were
combined, the ambiguity increased substantially
to 21, 279 (14.2%) symbols. Hakenberg et al.
(2008) showed that species disambiguation is one
of the most important steps for term normalisa-
tion and identification, which concerns automat-
ically associating mentions of biomedical enti-
ties in text to unique database identifiers (Mor-
gan et al., 2008). Also, the task of extracting
PPIs in the recent BioCreative Challenge II work-
shop (Hirschman et al., 2007) requires protein
pairs to be recognised and normalised, which in-
evitably involves species disambiguation.
More specifically, given a text, in which men-
tions of biomedical named entities are annotated,
a species disambiguation system automatically as-
signs a species identifier, as in a standard database
of model organisms, to every entity mention. The
types of biomedical named entities concerned in
this study are protein, gene, protein complex and
mRNA/cDNA, and we used identifiers from the
NCBI Taxonomy of model organisms.
2
The work
focuses on species disambiguation and assumes
that the entities are already identified. In practice,
an automated named entity recogniser (e.g., AB-
NER (Settles, 2005)) should be used before apply-
ing the systems.
4 Approaches
This section describes a number of approaches to
species disambiguation, highlighting the relation
classification method proposed in Section 2.
4.1 Heuristics Baselines
The cue words for species are words denoting
names of model organisms (e.g., mouse as in
1
http://www.ncbi.nlm.nih.gov/RefSeq
2
http://www.ncbi.nlm.nih.gov/sites/
entrez?db=taxonomy
phrase “mouse p53”). Another clue is the pres-
ence of the species-indicating prefixes in gene and
protein names. For instance, prefix ‘h’ in en-
tity “hSos-1” suggests that it is a human protein.
Throughout this paper, we refer to such cue words
(e.g., mouse, hSos-1) as “species words”. Note
that a species “word” may contain multiple tokens
(e.g., E. Coli).
We encoded this knowledge in a rule-based
species tagging system (Wang and Grover, 2008).
The system takes a 2-step approach. First, it marks
up species words in the document using a species-
word detection program,
3
which searches every
word in a dictionary of model organisms and as-
signs a species ID to the word if a match is found.
The dictionary was built using the NCBI taxon-
omy
4
and the UniProt controlled vocabulary of
species,
5
and in total it contains 420,224 species
words for 324,157 species IDs. When species
words are identified, we disambiguate an entity
mention using one of the following rules:
1. previous species word: If the word preceding an entity
is a species word, assign the species ID indicated by
that word to the entity.
2. species word in the same sentence: If a species word
and an entity appear in the same sentence, assign its
species ID to the entity. When more than one species
word co-occurs in the sentence, priority is given to the
species word to the entity’s left with the smallest dis-
tance. If all species words occur to the right of the en-
tity, take the nearest one.
3. majority vote: assign the most frequently occurring
species ID in the document to all entity mentions.
It is expected that the first rule would produce
good precision. However, it can only disam-
biguate the fraction of entities that happen to have
a species word to their immediate left. The second
rule relaxes the first by allowing an entity to take
the species indicated by its nearest species word
in the same sentence, which should increase recall
but decrease precision. Statistics from our dataset
(see Section 5.1) show that only 5.68% entities can
potentially be resolved by rule 1 and 22.16% by
rule 2, while the majority rule can tackle every en-
tity mention in the dataset.
3
The species word detector identifies the cue words and
was used in all the systems studied in this paper. We could
not properly evaluate the detector due to the lack of man-
ually annotated data. Its performance, however, would not
affect the comparative evaluation results, and improvement
to species word detection should increase the performance of
these disambiguation systems.
4
ftp://ftp.ncbi.nih.gov/pub/taxonomy/
5
http://www.expasy.ch/cgi-bin/speclist
1515
4.2 Supervised Classification
The disambiguation problem can be approached as
a classification task. Given an entity mention and
its surrounding context, a machine learning model
classifies the entity into one of the classes, where
each class corresponds to a species ID. We car-
ried out experiments with two classification meth-
ods: multi-class classification and one-class clas-
sification, where a maximum entropy model
6
was
used for the former and SVM-light
7
for the lat-
ter. In one-class classification, we trained a se-
ries of binary SVM classifiers, each constructing
a separating hyperplane that maximises the mar-
gin between the instances of one specific species
(i.e., the target class) and a set of randomly se-
lected instances of other species (i.e., the outlier
class). We used equal numbers of instances for
both classes in training. The following types of
features were used in both multi-class and one-
class experiments, where the values of n were
set empirically by cross-validation on the training
data:
• leftContext The n word lemmas to the left of the entity
(n = 200).
• rightContext The n word lemmas to the right of the
entity (n = 200).
• leftSpeciesIDs The n species IDs to the left of the entity
(with order, n = 5).
• rightSpeciesIDs The n species IDs to the right of the
entity (with order, n = 5).
• leftNouns The n nouns to the left of the entity (with
order, n = 2).
• leftAdjs The n adjectives to the left of the entity (with
order, n = 2).
• leftSpeciesWords The n species word forms to the left
of the entity (n = 5).
• rightSpeciesWords The n species word forms to the
right of the entity (n = 5).
• firstLetter The first character of the entity itself (e.g.,
‘h’ in hP53).
• documentSpeciesIDs All species IDs that occur in the
document in question.
• useStopWords filter out function words.
• useStopPattern filter out words consisting only of digits
and punctuation characters.
Feature selection was also carried out for the
one-class classification experiments. We com-
pared two feature selection methods that report-
edly work well on the task of text classification:
information gain (IG) (Yang and Pedersen, 1997)
6
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
7
http://svmlight.joachims.org/
The ARG1 ARG1 ARG2
ARG1 ARG2ARG1
Drosophila Kip3 isorthologue of Klp67A.
Figure 1: Predicate argument structure (PAS).
and Bi-Normal separation (BNS) (Forman, 2003).
IG measures the decrease in entropy when the
feature is given vs. absent, and is defined as:
IG(Y |X) = H(Y ) ? H(Y |X) where H(Y ) is
the uncertainty about the value of Y (i.e., Y ’s en-
tropy), and H(Y |X) is Y ’s conditional entropy
given X . The BNS is defined as: |F
?1
(x) ?
F
?1
(y)|, where F
?1
is the standard Normal distri-
bution’s inverse cumulative probability function,
namely, z-score; x is the ratio between the number
of positive cases containing the feature in ques-
tion, and the total number of positive cases; and y
is the ratio between the number of negative cases
containing the feature, and the total number of
negative cases.
We computed a weight for each feature and then
ranked the features according to their weight, with
respect to each feature selection method. The top
10% features were used in training. Given a test
instance, the one-class classification method first
counts the species words in the document that the
instance appears in, and then applies in sequence
the binary models of each occurring species, start-
ing from the most frequent one. For example, if
a document contains 5 occurrences of human and
3 mouse, we first apply the human species model
to judge whether an entity mention is of human
species, and only if not, the mouse model was ap-
plied. The most-frequent species in the document
was used as backup when none of the binary mod-
els gives positive answers.
4.3 Relation Classification
4.3.1 Overview
As for the proposed relation classification method,
in the training phase, we first selected the sen-
tences in which an entity mention and a species
word co-occur, and constructed pair-wise entity-
species relations. We then assigned each relation a
binary label: a relation is positive if the species ID
inferred from the species word matches the gold-
standard species annotation on the entity, and is
negative otherwise. For example, for the sentence
shown in Figure 1, where Drosophila is a species
word, and Kip3 and Klp67A are proteins, relation
?Kip3, Drosophila? is a negative instance and the
1516
pair ?Klp67A, Drosophila? is a positive one.
8
For each relation, a vector of features were ex-
tracted. We followed the PPI extraction method
described in (Miyao et al., 2008), where two types
of features were used for a SVM classifier. The
first was bag-of-word features, i.e., the words be-
fore, between and after the pair of entities, where
the words were lemmatised. We added an ad-
ditional feature of the distance between the en-
tity and the cue word. The other type was syn-
tactic features obtained from parsers. For bag-
of-word features, a linear kernel was used, and
for syntactic ones, a subset tree kernel (Mos-
chitti, 2006) was adopted. The syntactic features
were represented in a flat tree format. Figure 2
shows such a feature for the negative instance
?Kip3, Drosophila? from Figure 1. Note that all
species words (e.g., Drosophila) were normalised
to “SPECIESWORD”, and entities (e.g., Kip3) to
“ENTITY”, which not only reduces the noise in
the feature set, but also makes the model more
species-generic. From the training dataset (see
Section 5.1), 25, 413 relations were extracted, of
which 63.3% were positive.
(ENJU(noun arg1(SPECIESWORD orthologue))
(prep arg12(of orthologue))
(prep arg12(of ENTITY)))
Figure 2: A syntactic feature obtained from the ENJU
parser.
To identify the species of an entity in unseen
text, we first parsed the sentence, and then listed
all pairs of species words and entities as relations.
Having extracted the bag-of-word and syntactic
features from the instance, the trained model was
applied to judge whether each species-entity rela-
tion was positive. The entity mention in a positive
relation would be tagged with the ID indicated by
the species word, while the mentions in negative
relations would be left untagged. The next section
describes in detail how we extracted the syntactic
features from text.
4.3.2 Syntactic Features
Given a sentence, a natural language parser au-
tomatically recognises its syntactic structure and
outputs a parse tree, in which nodes represent
words or syntactic constituents. A path between
8
Orthologues are genes/proteins in different species but
have similar sequences. In this example it implies that
Klp67A is a Drosophila protein but Kip3 is not.
Parser Input Output
C&C POS-tagged GR
ENJU POS-tagged PAS
ENJU-Genia POS-tagged PAS
Minipar Sentence-detected Minipar
RASP Tokenised GR
Stanford POS-tagged SD
Stanford-Genia POS-tagged SD
Table 1: Parsers and their input and output format
a pair of nodes can be interpreted as a syntactic re-
lation between sentence units, which was proved
useful to infer biological relations (e.g., Airola et
al., 2008; Miwa et al., 2008).
We experimented with the following parsers
(summarised in Table 1):
• Dependency parsers identify one word as the head
of a sentence and all other words are either a depen-
dent of that word, or else dependent on some other
word that connects to the headword through a sequence
of dependencies. We used Minipar (Lin, 1998) and
RASP (Briscoe et al., 2006) for the experiments;
• Constituent-structured parsers split a sentence into
syntactic constituents such as noun phrases or verb
phrases. We used the Stanford parser (Klein and Man-
ning, 2003), and also a variant of the Stanford parser
(i.e., Stanford-Genia), which was trained on the GE-
NIA treebank (Tateisi et al., 2005) for biomedical text;
• Deep parsers aim to compute in-depth syntactic and
semantic structures based on syntactic theories such as
HPSG (Pollard and Sag, 1994) and CCG (Steedman,
2000). We used the C&C parser (Clark and Curran,
2007), ENJU (Miyao and Tsujii, 2008), and a variant
of ENJU (Hara et al., 2007) adapted for the biomedical
domain (i.e., ENJU-Genia);
There were a number of practical issues to con-
sider when using parsers for this task. Firstly, be-
fore parsing, the text needs to be linguistically pre-
processed, and the quality of this process has a sig-
nificant impact on parsers’ performance. The pre-
processing steps include sentence boundary detec-
tion, tokenisation and part-of-speech (POS) tag-
ging, all of which can be tricky especially when
applied to biomedical text (Grover et al., 2003).
To avoid the noise that can be introduced in the
pre-processing steps and to concentrate on evalu-
ating the performance of the parsers, we used the
same pre-processing tools (Alex et al., 2008a)
9
whenever possible. The middle column in Ta-
ble 1 shows how the input text was linguisti-
cally pre-processed with respect to each parser.
A POS-tagged text implies that it was also sen-
tence boundary detected and tokenised Except for
9
These particular tools were chosen because they were
adopted to pre-process the ITI-TXM dataset, which we used
in our study.
1517
RASP and Minipar, all parsers took POS-tagged
text as input. RASP requires POS tags and punctu-
ation labels that were derived from the CLAWS-7
tagset,
10
whereas our dataset uses POS labels from
the Penn Treebank tagset (Marcus et al., 1994).
As RASP does not recognise the Penn tagset, we
used its build-in POS tagger. Minipar, on the other
hand, does not support input of tokenised or POS-
tagged text, and therefore took split sentences as
input.
Secondly, the output representations of the
parsers are different and we preferred a format
that depicts relations between words instead of
syntactic constituents. In total, 4 representations
were used: grammatical relation (GR) (Briscoe et
al., 2006), Stanford typed dependency (SD) (de
Marneffe et al., 2006), Minipar’s own representa-
tion (Lin, 1998), and ENJU’s predicate-argument
structure (PAS). All the above representations de-
fine relations of words in triples, where a depen-
dency triple (i.e., GR, SD and Minipar) consists
of head, dependent and relation, and a PAS triple
contains predicate, argument, and relation. Fig-
ure 1 shows a sentence parsed by ENJU in PAS
representation. The right-most column in Table 1
lists the output representation of each parser. A
syntactic path between an entity and a species
word was represented by a sequence of triples,
each following the order of head-dependent or
predicate-argument. These paths were used as
syntactic features for the SVM classifier.
4.4 Spreading Strategies
Except for the majority vote rule, the approaches
described in Sections 4.1 and 4.3 were expected
to yield low recall, because they can only detect
intra-sentential relations, and therefore only be ap-
plied to the entities having at least one species
word appearing in the same sentence.
Since our aim is to disambiguate as many entity
mentions as possible, we would like to “spread”
the decisions from the disambiguated mentions to
their “relatives” in the same document. We define
an entity mention e¯ as another mention e’s rela-
tive under either of the following conditions: a)
if e¯ has the same surface form with e; or, b) if
e¯ is an abbreviation or an antecedent of e, where
abbreviation/antecedent pairs were detected using
the algorithm described in (Schwartz and Hearst,
10
http://ucrel.lancs.ac.uk/claws7tags.
html
2003). Given the set of disambiguated mentions,
we then “spread” their species IDs to their rela-
tives in the same document. After this process, the
mentions that do not have any disambiguated rela-
tives would still be missed by the system. In such
cases, we used a “default” species, as determined
by the rule of majority vote (see Section 4.1).
5 Evaluation
5.1 Data and Ontology
The species disambiguation experiments were
conducted using the ITI-TXM corpus (Alex et al.,
2008b), a collection of full-length biomedical re-
search articles manually annotated with linguistic
and biomedical information for developing auto-
matic information extraction systems. The cor-
pus contains two datasets covering slightly dif-
ferent domains: enriched protein-protein interac-
tion (EPPI) and tissue expression (TE). When-
ever possible, protein, protein complex, gene, and
mRNA/cDNA entities were tagged with NCBI
Taxonomy IDs, denoting their species, and it was
the species annotation that this study used.
The EPPI and TE datasets have different distri-
butions of species. The entities in EPPI belong to
118 species with human being the most frequent at
51.98%. In TE, the entities are across 67 species
and mouse is the most frequent at 44.67%.
11
The
inter-annotator agreement of species annotation on
EPPI and TE are 86.45% and 95.11%, respectively.
The species disambiguation systems were de-
veloped on the training portions of the EPPI and
TE corpora, each containing 221 articles, and eval-
uated on a dataset combining the development
test (DEVTEST) datasets of EPPI and TE, contain-
ing 58 and 48 articles, respectively. The com-
bined training dataset contains 96, 992 entity men-
tions belonging to 138 model organisms, while the
DEVTEST dataset contains 23, 118 entities of 54
species. The diversity of model organisms in this
corpus highlights the fact that a primary consid-
eration when developing a species disambiguation
system is its ability to distinguish a wide range of
species with minimal additional manual effort.
5.2 Results
5.2.1 Evaluation Metrics
The evaluation was carried out on the DEVTEST
dataset, and the systems are compared using av-
11
These figures were obtained from the training split of the
datasets.
1518
micro-avg. macro-avg.
Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01
SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09
SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34
Table 2: Evaluation results of the classification systems on
DEVTEST (precision/recall/F1-score, in %)
eraged precision, recall and F1 scores over all
species. In more detail, for each model organism
that appears in the DEVTEST dataset, we collect
two lists of entity mentions of that species: one
from the gold-standard DEVTEST dataset, and the
other from the output of a disambiguation system.
Then the list of system output is compared against
the gold-standard list to obtain precision, recall
and F1 score. For each system, the scores ob-
tained from all species are averaged using micro-
average and macro-average. The micro-average is
the mean of the summation of contingency metrics
for all model organisms, so that scores of the more
frequent species influence the mean more than
those of less frequent ones. The macro-average is
the mean of precision, recall, or F1 over all labels,
thus attributing equal weights to each species, and
measuring a system’s adaptability across different
model organisms.
5.2.2 Evaluation Results
First of all, Table 2 shows the results of the clas-
sification methods described in Section 4.2. The
multi-classification system using a maximum en-
tropy model (Maxent) yielded the highest overall
micro-averaged F1. Among the SVM-based sys-
tems, the one using IG feature selection achieved
better performance. In particular, it outperformed
the Maxent model in term of macro-averages. The
performance of the SVM model with BNS feature
selection is disappointing, perhaps because the oc-
currences of a feature in each instance are not nor-
mally distributed. As the Maxent system obtained
better results, it was used to compare with other
disambiguation systems.
Table 3 shows the results of a number of meth-
ods described in the previous sections. The meth-
ods are categorised into 4 groups: rule-based
baseline systems, a Maxent classification model,
relation-classification methods, and a hybrid sys-
tem. The difference between the relation classifi-
cation systems is the features adopted. Rel-Context
was trained on only bag-of-word and distance fea-
tures, whereas each other system also used syn-
tactic features provided by a specific parser. For
example, the Rel-RASP system identifies an entity’s
species by finding positive relations between the
entity and its neighbouring species words, using
features including bag-of-word, distance, and de-
pendency paths generated by RASP. The hybrid
system (Hbrd) ran the Rel-ENJU-Genia system on top
of the outcome of Maxent. When a conflict oc-
curs, the species ID is chosen by Rel-ENJU-Genia.
The idea is that the relation classification system
is more accurate than Maxent when it is applica-
ble, and hence would improve precision on dis-
ambiguating the species with few or no training
instances.
Without spreading (shown in the “NO SPRD”
columns of Table 3), most of the rule-based and re-
lation classification systems only work on a subset
of DEVTEST, resulting in low recall: Rule-Sp works
on the small proportion of entities (5.68%) with a
preceding species word, while the other systems
only work on the collection of sentences contain-
ing at least one species word and one entity, which
covers 4.60% sentences and 22.16% entity men-
tions. Rule-Majority, Maxent, and Hbrd, on the other
hand, apply to all entity mentions, and therefore
they are only compared against the others when
spreading was applied.
The results shown in the “NO SPRD” columns
can be viewed as a comparative evaluation of
the usefulness of the syntactic features supplied
by the parsers on this particular task. The rule-
based systems set high baselines: Rule-Sp pro-
duced good precision and Rule-SpSent achieved the
highest micro-averaged F1, thanks to its high
coverage, which is also an upperbound of recall
for the relation classification systems. Neverthe-
less, it is encouraging that the relation classifica-
tion systems obtained higher precision than Rule-
SpSent, which is important, considering the de-
cisions will be transfered to the untagged entity
mentions across the document. Indeed, as shown
in the SPRD columns in Table 3, most relation
classification systems outperformed the Rule-SpSent
baseline when spreading was used. The scores
of the systems using different parser outputs only
vary slightly. Rel-Context, on the other hand, sur-
passed others in terms of micro-averaged preci-
sion, while sacrificing micro-averaged recall and
macro-averaged scores.
Next, the SPRD columns in Table 3 show the re-
sults when the spreading rules were applied, which
1519
METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)
Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08
Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00
Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Rel-C&C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14
Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67
Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21
Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44
Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80
Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45
Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64
Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80
Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)
effectively improved recall (see Section 5.2.3
for discussion on statistical significance tests on
the results). The Maxent system achieved very
good micro-averaged precision, but low macro-
averaged scores. In fact, as shown in Table 4, Max-
ent can only disambiguate 7 species (out of a total
of 54) that have relatively large amount of train-
ing instances,
12
and failed completely on other
species. This suggests that Maxent may not be able
to generate good micro-averaged scores when ap-
plied to a dataset where the dominant species are
different from those in the training set. On the
other hand, the relation-classification approaches
have a clear advantage over Maxent as measured
by macro-averaged scores. As shown in Table 4,
Rel-ENJU-Genia worked well on most of the species,
displaying its good adaptability, while achieving
comparable micro-averaged F1 to Maxent. Over-
all, Hbrd, which combines the strengths of relation
classification and the Maxent classification model,
obtained the highest points as measured by every
metric.
5.2.3 Statistical Significance
To see whether our methods significantly im-
proved the baseline systems, we performed ran-
domisation tests (Noreen, 1989; Yeh, 2000) on
some of the results shown in Table 3. The in-
tuition of randomisation test is as follows: when
comparing two systems (e.g., A and B), we erase
the labels “output of A” or “output of B” from all
observations. The null hypothesis is that there is
no difference between A and B, and thus any re-
sponse produced by one of the systems could have
as likely come from the other. We shuffle these re-
12
The following 7 species occur most frequently in the
training set: H. sapiens (43.25%), M. musculus (27.05%),
R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis
(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).
Species Name Pct Mxt Rel Hbrd
H. sapiens 50.13% 76.25 65.33 79.51
M. musculus 13.99% 66.41 58.29 68.27
X. tropicalis 7.35% 64.80 77.72 71.39
D. melanogaster 6.34% 93.17 78.46 95.15
S. cerevisiae 4.79% 90.12 83.32 87.68
R. norvegicus 2.97% 44.04 38.69 51.77
T. aestivum 2.62% 0.00 89.68 23.35
P. americana 2.27% 0.00 98.50 7.76
C. elegans 2.08% 96.83 95.88 97.50
H. herpesvirus 5 1.58% 0.00 54.46 4.27
R. virus 1.45% 0.00 28.54 6.45
H. spumaretrovirus 1.17% 0.00 99.37 2.49
... ... ... ... ...
Macro-average 9.85 30.21 31.80
Micro-average 70.48 67.13 73.70
Table 4: The micro-averaged F1 scores (%) of Maxent
(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd with
respect to each of the most frequent 12 species in DEVTEST.
sponses R times, reassign each response to A or
B and see how likely such a shuffle produces a
difference in the metric of interest that is at least
as large as the difference observed when using A
and B on the test data. Let r denote the number
of times that such a difference occurred, then as
R ? ?,
r+1
R+1
approaches the significance level.
In our case, the metrics tested were micro- and
macro-averaged precision, recall and F1.
Following this procedure, we tested whether the
improvements made by a relation classification
based system (i.e., Rel-ENJU-Genia with SPRD) and
the hybrid system (i.e., Hbrd) over the baseline sys-
tems were statistically significant. We carried out
approximate randomisation with 10,000 shuffles
and the test results are shown in Table 5. The nu-
merical figures in the cells are differences in pre-
cision, recall and F1 between a pair of systems.
The significance levels (i.e., p-values) are indi-
cated by superscript marks, whose correspond-
ing values are displayed in Table 6. For exam-
1520
Rule-Majority Rule-Sp Rule-SpSent Maxent
Rel
micro-avg 2.77
?
/3.46
?
/3.13
?
1.95
?
/2.04
?
/2.00
?
1.57
?
/2.22
?
/1.92
?
-1.57
?
/ -5.02
?
/ -3.35
?
macro-avg 15.24
?
/13.12
?
/12.13
?
3.75
a
/4.21
a
/3.20
a
9.35
?
/8.44
?
/7.10
?
21.92
?
/24.87
?
/20.35
?
Hbrd
micro-avg 8.01
?
/11.27
?
/9.70
?
7.19
?
/9.85
?
/8.57
?
6.81
?
/10.04
?
/8.49
?
3.67
?
/2.78
?
/2.82
b
macro-avg 27.22
?
/15.72
c
/13.72
d
15.73
?
/6.82
e
/4.80
f
21.33
?
/11.05
g
/ 8.70
h
33.91
i
/27.47
?
/21.95
?
Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-
proved the baseline systems. The numerical figures in the cells show the differences between the two systems as measured by
precision/recall/F1 in percentage. The superscript marks indicate the significance levels and are explained in Table 6.
ple, the difference in micro-averaged precision be-
tween Rel-ENJU-Genia and Rule-Majority on the test
data was 2.77%, and in 10,000 approximate ran-
domisation trials, there was zero times
13
that Rel-
ENJU-Genia’s micro-averaged precision is greater
than Rule-Majority’s by at least 2.77% (p < 0.0001).
MARK VALUE MARK VALUE
* p < 0.0001 a p < 0.06
b p < 0.002 c p < 0.0003
d p < 0.0002 e p < 0.03
f p < 0.05 g p < 0.003
h p < 0.005 i p < 0.07
Table 6: p-values.
The test results confirmed that, the improve-
ments made by Hbrd are statistically significant
with at least 95% confidence as measured by all
metrics except for macro-averaged precision. The
relation classification approach achieved signifi-
cantly lower performance than Maxent in terms of
micro-averaged scores (hence the “-” sign in the
corresponding cell in Table 5), but in all other
cases it can reject the null hypothesis with very
high confidence (i.e., p < 0.0001).
6 Conclusions and Future Work
This paper proposes a method that tackles a com-
plex disambiguation problem by breaking it into
two cascaded simpler tasks of cue word discov-
ery and binary relation classification. We evalu-
ated the method on the task of disambiguating the
model organisms of biomedical named entities,
along with a number of other approaches. As mea-
sured by micro-averaged F1 score, a supervised
classification approach (Maxent) yielded the second
best result. However, it can only disambiguate
a small number of species that have abundant
training instances. With spreading rules, a rela-
tion classification system (Rel-ENJU-Genia) trained
on word and syntactic features from ENJU-Genia
also obtained good micro-averaged F1, while sur-
13
The numbers of times are not shown in Table5 for
brevity.
passing Maxent significantly in terms of macro-
averaged scores. Combining these two systems
achieved the best overall performance. Neverthe-
less, we combined the two methods in a rather
crude way, leaving ample room for exploring bet-
ter strategies in the future.
One drawback of the relation classification sys-
tems is that they can not cover all entity mentions
but only the ones with informative keywords co-
occurring in the same sentence. We overcame the
drawback by using spreading rules. For some ap-
plications, however, it may be sufficient to make
predictions exclusively for cases where the sys-
tems are applicable. Also, the predictions with
high confidence can be used as seed training ma-
terial for automatically harvesting more training
data.
Acknowledgments
The work reported in this paper is funded by Pfizer
Ltd.. The UK National Centre for Text Mining is
funded by JISC. The ITI-TXM corpus used in the
experiments was developed at School of Informat-
ics, University of Edinburgh, in the TXM project,
which was funded by ITI Life Sciences, Scotland.
References
E. Agirre and D. Martinez. 2004. Unsupervised WSD based
on automatically retrieved examples: The importance of
bias. In Proceedings of EMNLP.
A. Airola, S. Pyysalo, J. Bj¨orne, T. Pahikkala, F. Ginter, and
T. Salakoski. 2008. A graph kernel for protein-protein
interaction extraction. In Proceedings of BioNLP.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a.
Assisted curation: does text mining really help? In Pro-
ceedings of the Pacific Symposium on Biocomputing.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b.
The ITI TXM corpus: Tissue expression and protein-
protein interactions. In Proceedings of the Workshop on
Building and Evaluating Resources for Biomedical Text
Mining at LREC.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL Interactive Presentation Sessions.
1521
R. Bunescu and M. Pas¸ca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name
ambiguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248–256.
S. Clark and J. R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models. Com-
putational Linguistics, 33(4).
M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006.
Generating typed dependency parses from phrase struc-
ture. In Proceedings of LREC.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interaction
sentences using dependency parsing. In Proceedings of
the Joint Conference of EMNLP and CoNLL.
G. Forman. 2003. An extensive empirical study of feature se-
lection metrics for text classification. Journal of Machine
Learning Research, 3:1289–1305.
C. Grover, M. Lapata, and A. Ascarides. 2003. A compar-
ison of parsing technologies for the biomedical domain.
Natural Language Engineering, 1(1):1–38.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of gene
mentions with GNAT. Bioinformatics, 24(16).
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact
of re-training a lexical disambiguation model on domain
adaptation of an HPSG parser. In Proceedings of the 10th
International Conference on Parsing Technology.
V. Hatzivassiloglou, PA Dubou´e, and A. Rzhetsky. 2001.
Disambiguating proteins, genes, and RNA in text: a ma-
chine learning approach. Bioinformatics, 17(Suppl 1).
L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-
itors. 2007. The BioCreative II - Critical Assessment
for Information Extraction in Biology Challenge, volume
9(Suppl 2). Genome Biology.
L. Hunter and K. B. Cohen. 2006. Biomedical language
processing: what’s beyond PubMed. Molecular Cell,
21(5):589–594.
N. Japkowicz. 2000. Learning from imbalanced data sets: a
comparison of various strategies. In Proceedings of AAAI
Workshop on Learning from Imbalanced Data Sets.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of ACL.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of HLT/EMNLP.
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–330.
M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008.
Combining multiple layers of syntactic information for
protein-protein interaction extraction. In Proceedings of
SMBM.
Y. Miyao and J. Tsujii. 2008. Feature forest models for prob-
abilistic HPSG parsing. Computational Linguistics, 34(1).
Y. Miyao, R. Sætre, K. Sagae, T. Matsuzaki, and J. Tsujii.
2008. Task-oriented evaluation of syntactic parsers and
their representations. In Proceedings of ACL-08: HLT.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreAtIvE II gene normalisation. In Proceedings of the
BioCreAtIvE II Workshop, Madrid.
A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.
Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and
L. Hirschman. 2008. Overview of BioCreAtIvE II gene
normalization. Genome Biology, 9(Suppl 2).
A. Moschitti. 2006. Making tree kernels practical for natural
language learning. In Proceedings of EACL.
E. W. Noreen. 1989. Computer Intensive Methods for Test-
ing Hypothesis. John Wiley & Sons.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
A. S. Schwartz and M. A. Hearst. 2003. Identifying abbrevi-
ation definitions in biomedical text. In Proceedings of the
Pacific Symposium on Biocomputing.
B. Settles. 2005. ABNER: An open source tool for automat-
ically tagging genes, proteins, and other entity names in
text. Bioinformatics, 21(14):3191–3192.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syn-
tax annotation for the GENIA corpus. In Proceedings of
IJCNLP.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings of LREC.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings of
ICML.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of COLING.
1522
