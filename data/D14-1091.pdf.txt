Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844–853,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Syllable weight encodes mostly the same information for English word
segmentation as dictionary stress
John K Pate Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{john.pate,mark.johnson}@mq.edu.au
Abstract
Stress is a useful cue for English word
segmentation. A wide range of computa-
tional models have found that stress cues
enable a 2-10% improvement in segmen-
tation accuracy, depending on the kind of
model, by using input that has been anno-
tated with stress using a pronouncing dic-
tionary. However, stress is neither invari-
ably produced nor unambiguously iden-
tifiable in real speech. Heavy syllables,
i.e. those with long vowels or syllable
codas, attract stress in English. We de-
vise Adaptor Grammar word segmentation
models that exploit either stress, or sylla-
ble weight, or both, and evaluate the util-
ity of syllable weight as a cue to word
boundaries. Our results suggest that sylla-
ble weight encodes largely the same infor-
mation for word segmentation in English
that annotated dictionary stress does.
1 Introduction
One of the first skills a child must develop in the
course of language acquisition is the ability to seg-
ment speech into words. Stress has long been
recognized as a useful cue for English word seg-
mentation, following the observation that words
in English are predominantly stress-initial (Cutler
and Carter, 1987), together with the result that 9-
month-old English-learning infants prefer stress-
initial stimuli (Jusczyk et al., 1993). A range of
statistical (Doyle and Levy, 2013; Christiansen et
al., 1998; B¨orschinger and Johnson, 2014) and
rule-based (Yang, 2004; Lignos and Yang, 2010)
models have used stress information to improve
word segmentation. However, that work uses
stress-marked input prepared by marking vowels
that are listed as stressed in a pronouncing dic-
tionary. This pre-processing step glosses over the
fact that stress identification itself involves a non-
trival learning problem, since stress has many pos-
sible phonetic reflexes and no known invariants
(Campbell and Beckman, 1997; Fry, 1955; Fry,
1958). One known strong correlate of stress in
English is syllable weight: heavy syllables, which
end in a consonant or have a long vowel, at-
tract stress in English. We present experiments
with Bayesian Adaptor Grammars (Johnson et al.,
2007) that suggest syllable weight encodes largely
the same information for word segmentation that
dictionary stress information does.
Specifically, we modify the Adaptor
Grammar word segmentation model of
B¨orschinger and Johnson (2014) to compare
the utility of syllable weight and stress cues for
finding word boundaries, both individually and in
combination. We describe how a shortcoming of
Adaptor Grammars prevents us from comparing
stress and weight cues in combination with the full
range of phonotactic cues for word segmentation,
and design two experiments to work around this
limitation. The first experiment uses grammars
that provide parallel analyses for syllable weight
and stress, and learns initial/non-initial phonotac-
tic distinctions. In this first experiment, syllable
weight cues are actually more useful than stress
cues at larger input sizes. The second experiment
focuses on incorporating phonotactic cues for
typical word-final consonant clusters (such as
inflectional morphemes), at the expense of parallel
structures. In this second experiment, weight cues
merely match stress cues at larger input sizes,
and the learning curve for the combined weight-
and-stress grammar follows almost perfectly with
the stress-only grammar. This second experiment
suggests that the advantage of weight over stress
in the first experiment was purely due to poor
modeling of word-final consonant clusters by
the stress-only grammar, not weight per se. All
together, these results indicate that syllable weight
844
is highly redundant with dictionary-based stress
for the purposes of English word segmentation;
in fact, in our experiments, there is no detectable
difference between relying on syllable weight and
relying on dictionary stress.
2 Background
Stress is the perception that some syllables are
more prominent than others, and reflects a com-
plex, language-specific interaction between acous-
tic cues (such as loudness and duration), and
phonological patterns (such as syllable shapes).
The details on how stress is assigned, produced,
and perceived vary greatly across languages.
Three aspects of the English stress system are
relevant for this paper. First, although English
stress can shift in different contexts (Liberman and
Prince, 1977), such as from the first syllable of
‘fourteen’ in isolation to the second syllable when
followed by a stressed syllable, it is largely stable
across different tokens of a given word. Second,
most words in English end up being stress-initial
on a type and token basis. Third, heavy syllables
(those with a long vowel or a consonant coda) at-
tract stress in English.
There is experimental evidence that English-
learning infants prefer stress-initial words from
around the age of seven months (Jusczyk et al.,
1993; Juszcyk et al., 1999; Jusczyk et al., 1993;
Thiessen and Saffran, 2003). A variety of com-
putational models have subsequently been devel-
oped that take stress-annotated input and use this
regularity to improve segmentation accuracy. The
earliest Simple Recurrent Network (SRN) mod-
eling experiments of Christiansen et al. (1998)
and Christiansen and Curtin (1999) found that
stress improved word segmentation from about
39% to 43% token f-score (see Evaluation). Ryt-
ting et al. (2010) applied the SRN model to prob-
ability distributions over phones obtained from a
speech recognition system, and found that the en-
tropy of the probability distribution over phones,
as a proxy to local hyperarticulation and hence a
stress cue, improved token f-score from about 16%
to 23%. In a deterministic approach using pre-
syllabified input, Yang (2004), with follow-ups in
Lignos and Yang (2010) and Lignos (2011; 2012),
showed that a ‘Unique Stress Constraint’ (USC),
or assuming each word has at most one stressed
syllable, leads to an improvement of about 2.5%
boundary f-score.
Among explicitly probabilistic models,
Doyle and Levy (2013) incorporated stress into
Goldwater et al.’s (2009) Bigram model. They
did this by modifying the base distribution over
lexical forms to generate not simply phone strings
but a sequence of syllables that may or may
not be stressed. The resulting model can learn
that some sequences of syllables (in particular,
sequences that start with a stressed syllable)
are more likely than others. However, observed
stress improved token f-score by only 1%.
B¨orschinger and Johnson (2014) used Adaptor
Grammars (Johnson et al., 2007), a generalization
of Goldwater et al.’s (2009) Bigram model that
will be described shortly, and found a clearer
4-10% advantage in token f-score, depending on
the amount of training data.
Together, the experimental and computational
results suggest that infants in fact pay attention
to stress, and that stress carries useful information
for segmenting words in running speech. How-
ever, stress identification is itself a non-trivial
task, as stress has many highly variable, context-
sensitive, and optional phonetic reflexes. How-
ever, one strong phonological cue in English is
syllable weight: heavy syllables attract stress.
Heavy syllables, in turn, are syllables with a
coda and/or a long vowel, which, in English,
are tense vowels. Turk et al. (1995) replicated
the Jusczyk et al. (1993) finding that English-
learning infants prefer stress-initial stimuli (using
non-words), and then examined how stress inter-
acted with syllable weight. They found that sylla-
ble weight was not a necessary condition to trig-
ger the preference: infants preferred stress-initial
stimuli even if the initial syllable was light. How-
ever, they also found that infants most strongly
preferred stimuli whose first syllable was both
stressed and heavy: infants preferred stress-initial
and heavy-initial stimuli to stress-initial and light-
initial stimuli. This result suggests that infants are
sensitive to syllable weight in determining typical
stress and rythmic patterns in their language.
2.1 Models
We will adopt the Adaptor Grammar framework
used by B¨orschinger and Johnson (2014) to ex-
plore the utility of syllable weight as a cue
to word segmentation by way of its covariance
with stress. Adaptor Grammars are Probabilis-
tic Context Free Grammars (PCFGs) with a spe-
845
Syll
Onset
k
Rhyme
Nucleus
æ
Coda
ts
(a) Basic syllable.
SyllIF
OnsetI
k
RhymeIF
NucleusI
æ
CodaF
ts
(b) Mono-syllable with initial Rhyme.
SyllIF
OnsetI
k
RhymeF
NucleusF
æ
CodaF
ts
(c) Mono-syllable with final Rhyme.
Figure 1: Different ways to incorporate phonotactics. It is not possible to capture word-final codas and
word initial rhymes in monosyllabic words with factors the size of a PCFG rule.
cial set of adapted non-terminal nodes. We un-
derline adapted non-terminals (X) to distinguish
them from non-adapted non-terminals (Y). While
a vanilla PCFG can only directly model regular-
ities that are expressed by a single re-write rule,
an Adaptor Grammar model caches entire subtrees
that are rooted at adapted non-terminals. Adaptor
Grammars can thus learn the internal structure of
words, such as syllables, syllable onsets, and syl-
lable rhymes, while still learning entire words as
well.
In Adaptor Grammars, parameters are associ-
ated with PCFG rules. While this has been a useful
factorization in previous work, it makes it difficult
to integrate syllable weight and syllable stress in
a linguistically natural way. A syllable is typically
analyzed as having an optional onset followed by a
rhyme, with the rhyme rewriting to a nucleus (the
vowel) followed by an optional coda, as in Fig-
ure 1a. We expect stress and syllable weight to be
useful primarily because initial syllables tend to be
different from non-initial syllables. However, dis-
tinguishing final from non-final codas should be
useful as well, due to the frequency of suffixes in
English, and the importance of edge phenomena in
phonology more generally (Brent and Cartwright,
1996). These principles come into conflict when
modeling monosyllabic words. If we say that a
monosyllable is an Initial and Final SyllIF, and
has an initial Onset and an initial Rhyme, as in
Figure 1b, then we can learn the initial/non-initial
generalization about stressed or heavy rhymes at
the expense of the generalization about final and
non-final codas. If we say that a monosyllable is
an initial onset with a final rhyme, the reverse oc-
curs: we can learn the final/non-final coda gen-
eralization at the expense of the initial/non-initial
regularities. If we split the symbols further, we’d
generalize even less: we’d essentially have to learn
the initial/non-initial patterns separately for mono-
syllables and polysyllables.
The most direct solution would introduce fac-
tors that are ‘smaller’ than a single PCFG rule. Es-
sentially, we would compute the score of a PCFG
rule in terms of multiple features of its right-hand
side, rather than a single ‘one-hot’ feature identi-
fying the expansion. We left this direction for fu-
ture work and instead carried out two experiments
using Adaptor Grammars that were designed to
work around this limitation.
Our first experiment focuses on modeling
the initial/non-initial distinction, leaving the
final/non-final coda distinction unmodeled. The
models in this experiment assume parallel struc-
tures for syllable weight and stress, and focus on
providing the most direct comparison between syl-
lable weight and stress with a strictly initial/non-
initial distinction. This first experiment shows that
observing dictionary stress is better early in learn-
ing, but that modeling syllable weight is better
later in learning. However, it is possible that sylla-
ble weight was more useful because modeling syl-
lable weight involves modeling the characteristics
of codas; the advantage may not have been due to
weight per se but due to having learned something
about the effects of suffixes on final codas.
Our second experiment focuses on modeling
some aspects of final codas at the expense of main-
taining a rigid parallelism in the structures for syl-
lable weight and stress. The models in this exper-
iment split only those symbols that are necessary
to bring stress or weight patterns into the expres-
sive power of the model, and focus on comparing
richer models of syllable weight and stress that
account for inital/internal/final distinctions. This
second experiment shows that observing dictio-
nary stress is better early in learning, and that
modeling syllable weight merely catches up to
846
Sentence ? Collocations3
+
(1)
Collocations3 ? Collocations2
+
(2)
Collocations2 ? Collocation
+
(3)
Collocation ? Word
+
(4)
Figure 2: Three levels of collocation; symbols fol-
lowed by
+
may occur one or more times.
stress without surpassing it. Moreover, a com-
bined stress-and-weight model does no better than
a stress model, suggesting that the weight gram-
mar’s contribution is fully redundant, for the pur-
poses of word segmentation, with the stress obser-
vations.
Together, these experiments suggest that sylla-
ble weight eventually encodes everything about
word segmentation that dictionary stress does, and
that any advantage that syllable weight has over
observing dictionary stress is entirely redundant
with knowledge of word-final codas.
3 Experiments
3.1 Adaptor Grammars
We follow B¨orschinger and Johnson (2014) in us-
ing a 3-level collocation Adaptor Grammar, as in-
troduced by Johnson and Goldwater (2009) and
presented in Figure 2, as the backbone for all
models, including the baseline. A 3-level collo-
cation grammar assumes that words are grouped
into collocations of words that tend to appear with
each other, and that the collocations themselves
are grouped into larger collocations, up to three
levels of collocations. This collocational struc-
ture allows the model to capture strong word-
to-word dependencies without having to group
frequently-occuring word sequences into a single,
incorrect, undersegmented ‘word’ as the unigram
model tends to do (Johnson and Goldwater, 2009)
Word rewrites in different ways in Experiment I
and Experiment II, which will be explained in the
relevant experiment section.
3.2 Experimental Set-up
We applied the same experimental set-up used by
B¨orschinger and Johnson (2014), to their dataset,
as described below. To understand how different
modeling assumptions interact with corpus size,
we train on prefixes of each corpus with increas-
ing input size: 100, 200, 500, 1,000, 2,000, 5,000,
and 10,000 utterances. Inference closely fol-
lowed B¨orschinger and Johnson (2014) and John-
son and Goldwater (2009). We set our hyperpa-
rameters to encourage onset maximization. The
hyperparameter for syllable nodes to rewrite to
an onset followed by a rhyme was 10, and the
hyperparameter for syllable nodes to rewrite to a
rhyme only was 1. Similarly, the hyperparame-
ter for rhyme nodes to include a coda was 1, and
the hyperparameter for rhyme nodes to exclude
the coda was 10. All other hyperparameters spec-
ified vague priors. We ran eight chains of each
model for 1,000 iterations, collecting 20 samples
with a lag of 10 iterations between samples and a
burn-in of 800 iterations. We used the same batch-
initialization and table-label resampling to encour-
age the model to mix.
After gathering the samples, we used them to
perform a single minimum Bayes risk decoding
of a separate, held-out test set. This test set was
constructed by taking the last 1,000 utterances of
each corpus. We use a common test-set instead
of just evaluating on the training data to ensure
that performance figures are comparable across in-
put sizes; when we see learning curves slope up-
ward, we can be confident that the increase is due
to learning rather than easier evaluation sets.
We measured our models’ performance with the
usual token f-score metric (Brent, 1999), the har-
monic mean of how many proposed word tokens
are correct (token precision) and how many of the
actual word tokens are recovered (token recall).
For example, a model may propose “the in side”
when the true segmentation is “the inside.” This
segmentation would have a token precision of
1
3
,
since one of three predicted words matches the
true word token (even though the other predicted
words are valid word types), and a token recall of
1
2
, since it correctly recovered one of two words,
yield a token f-score of 0.4.
3.3 Dataset
We evaluated on a dataset drawn from the Alex
portion of the Providence corpus (Demuth et al.,
2006). This dataset contains 17, 948 utterances
with 72, 859 word tokens directed to one child
from the age of 16 months to 41 months. We used
a version of this dataset that contained annota-
tions of primary stress that B¨orschinger and John-
son (2014) added to this input using an extended
847
RhymeI ? HeavyRhyme
RhymeI ? LightRhyme
Rhyme ? HeavyRhyme
Rhyme ? LightRhyme
HeavyRhyme ? LongVowel
HeavyRhyme ? Vowel Coda
LightRhyme ? ShortVowel
(a) Weight-sensitive grammar
RhymeI ? RhymeS
RhymeI ? RhymeU
Rhyme ? RhymeS
Rhyme ? RhymeU
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
(b) Stress-sensitive grammar
RhymeI ? Vowel (Coda)
Rhyme ? Vowel (Coda)
(c) Baseline grammar
RhymeI ? HeavyRhymeS
RhymeI ? HeavyRhymeU
RhymeI ? LightRhymeS
RhymeI ? LightRhymeU
Rhyme ? HeavyRhymeS
Rhyme ? HeavyRhymeU
Rhyme ? LightRhymeS
Rhyme ? LightRhymeU
HeavyRhymeS ? LongVowel Stress
HeavyRhymeS ? LongVowel Stress Coda
HeavyRhymeU ? LongVowel
HeavyRhymeU ? LongVowel Coda
LightRhymeS ? ShortVowel Stress
LightRhymeU ? ShortVowel
(d) Combined grammar
Figure 3: Experiment I Grammars
version of CMUDict (cmu, 2008).
1
The mean
number of syllables per word token was 1.2, and
only three word tokens had more than five sylla-
bles. Of the 40, 323 word tokens with a stressed
syllable, 27, 258 were monosyllabic. Of the
13, 065 polysyllabic word tokens with a stressed
syllable, 9, 931 were stress-initial. Turning to the
32, 536 word tokens with no stress (i.e., the func-
tion words), all but 23 were monosyllabic (the 23
were primarily contractions, such as “couldn’t”).
3.4 Experiment I: Parallel Structures
The goal of this first experiment is to provide the
most direct comparison possible between gram-
mars that attend to stress cues and grammars that
attend to syllable weight cues. As these are both
hypothesized to be useful by way of an initial/non-
initial distinction, we defined a word to be an ini-
tial syllable SyllI followed by zero to three sylla-
bles, and syllables to consist of an optional onset
1
This dataset and these Adap-
tor Grammar models are available at:
http://web.science.mq.edu.au/˜jpate/stress/
and a rhyme:
Word ? SyllI (Syll)
{0,3}
(5)
SyllI ? (OnsetI) RhymeI (6)
Syll ? (Onset) Rhyme (7)
In the baseline grammar, presented in Figure 3c,
rhymes rewrite to a vowel followed by an optional
consonant coda. Rhymes then rewrite to be heavy
or light in the weight grammar, as in Figure 3a, to
be stressed or unstressed in the stress grammar, as
in Figure 3b. In the combination grammar, rhymes
rewrite to be heavy or light and stressed or un-
stressed, as in Figure 3d. LongVowel and Short-
Vowel both re-write to all vowels. An additional
grammar that restricted them to rewrite to long and
short vowels, respectively, led to virtually identi-
cal performance, suggesting that vowel quantity
can be learned for the purposes of word segmenta-
tion from distributional cues. We will also present
evidence that the model did manage to learn most
of the contrast.
Figure 4 presents learning curves for the gram-
mars in this parallel structured comparison. We
see that observing stress without modeling weight
848
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctions
between Stressed/Unstressed and Heavy/Light syllable rhymes.
?
?
?
?
?
a?
a?
æ
e?
?
i?
o?
?
??
u?
LongVowel ShortVowel Vowel
Vo
we
l
1 10 100 1000 7000
Vowel counts by quantity
Figure 5: Heatmap of learned vowels in the Ex-
periment I weight-only grammar. Each cell cor-
responds to the count of a particular vowel being
analyzed as one of the three vowel types. Diph-
thongs are rarely ShortVowel.
outperforms both the baseline and the weight-only
grammar early in learning. The weight-only gram-
mar rapidly improves in performance at larger
training data sizes, increasing its advantage over
the baseline, while the advantage of the stress-
only grammar slows and appears to disappear at
the largest training data size. At 10,000 utterances,
the improvement of the weight-only grammar over
the stress-only grammar is significant according to
an independent samples t-test (t = 7.2, p < 0.001,
14 degrees of freedom). This pattern suggests that
annotated dictionary stress is easy to take advan-
tage of at low data sizes, but that, with sufficient
data, syllable weight can provide even more in-
formation about word boundaries. The best over-
all performance early in learning is obtained by
the combined grammar, suggesting that syllable
weight and dictionary stress provide information
about word segmentation that is not redundant.
An examination of the final segmentation sug-
gests that the weight grammar has learned that
initial syllables tend to be heavy. Specifically,
across eight runs, 98.1% of RhymeI symbols
rewrote to HeavyRhyme, whereas only 54.5% of
Rhyme symbols (i.e. non-initial rhymes) rewrote
to HeavyRhyme.
849
Model Mean TF Std. Dev.
noweight:nostress 0.830 0.005
noweight:stress 0.831 0.008
weight:nostress 0.861 0.008
weight:stress 0.861 0.008
Table 1: Segmentation Token F-score for Experi-
ment I at 10,000 utterances across eight runs.
We also examined the final segmentation to see
well the model learned the distinction between
long vowels and short vowels. Figure 5 presents a
heatmap, with colors on a log-scale, showing how
many times each vowel label rewrote to each pos-
sible vowel in the (translated to IPA). Although the
quantity generalisations are not perfect, we do see
a general trend where ShortVowel rarely rewrites
to diphthongs.
3.5 Experiment II: Word-final Codas
Experiment I suggested that, under a ba-
sic initial/non-initial distinction, syllable weight
eventually encodes more information about word
boundaries than does dictionary stress. This is
a surprising result, since we initially investigated
syllable weight as a noisy proxy for dictionary
stress. One possible source of the ‘extra’ advan-
tage that the syllable weight grammar exhibited
has to do with the importance of word-final codas,
which can encode word-final morphemes in En-
glish (Brent and Cartwright, 1996). Even though
the grammars did not explicitly model them, the
weight grammar could implicitly capture a bias for
or against having a coda in non-initial position,
while the stress grammar could not. This is be-
cause most word tokens are one or two syllables,
and only one of the two rhyme types of the weight
grammar included a coda. Thus, the HeavyRhyme
symbol could simultaneously capture the most im-
portant aspects of both stress and coda constraints.
To see if the extra advantage of the syllable
weight grammar can be attributed to the influence
of word-final codas, we formulated a set of gram-
mars that model word-final codas and also can
learn stress and/or syllable weight patterns. These
grammars are more similar in structure to the ones
that B¨orschinger and Johnson (2014) used. For the
baseline and weight grammar, we again defined
words to consist of up to four syllables with an ini-
tial SyllI syllable, but this time distinguished final
syllables SyllF in polysyllabic words. The non-
stress grammars use the following rules for pro-
ducing syllables:
Word ? SyllIF (8)
Word ? SyllI (Syll)
{0,2}
SyllF (9)
SyllIF ? (OnsetI) RhymeI (10)
SyllI ? (OnsetI) RhymeI (11)
Syll ? (Onset) Rhyme (12)
SyllF ? (Onset) RhymeF (13)
For the stress grammar, we followed
B¨orschinger and Johnson (2014) in distin-
guishing stressed and unstressed syllables, rather
than simply stressed rhymes as in Experiment I,
to allow the model to learn likely stress patterns
at the word level. A word can consist of up to
four syllables, and any syllable and any number
of syllables may be stressed, as in Figure 6a.
The baseline grammar is similar to the previous
one, except it distinguishes word-final codas, as
in Figure 6b. The weight grammar, presented in
Figure 6c, rewrites rhymes to a nucleus followed
by an optional coda and distinguishes nuclei in
open syllables according to their position in the
word. The stress grammar, presented in Figure 6d,
is the all-stress-patterns model (without the unique
stress constraint) B¨orschinger and Johnson (2014).
This grammar introduces additional distinctions at
the syllable level to learn likely stress patterns,
and distinguishes final from non-final codas. The
combined model is identical to the stress model,
except Vowel non-terminals in closed and word-
internal syllables are replaced with Nucleus non-
terminals, and Vowel non-terminals in word-inital
(-final) open syllables are replaced with NucleusI
(NucleusF) non-terminals.
To summarize, the stress models distinguish
stressed and unstressed syllables in initial, final,
and internal position. The weight models distin-
guish the vowels of initial open syllables, the vow-
els of final open syllables, and other vowels, al-
lowing them to take advantage of an important cue
from syllable weight for word segmentation: if an
initial vowel is open, it should usually be long.
Figure 7 shows segmentation performance on
the Alex corpus with these more complete models.
While the performance of the weight grammars is
virtually unchanged compared to Figure 4, the two
grammars that do not model syllable weight im-
prove dramatically. This result supports our pro-
posal that much of the advantage of the weight
850
Word ? {SyllUIF|SyllSIF}
Word ? {SyllUI|SyllSI} {SyllU|SyllS}
{0,2}
{SyllUF|SyllSF}
(a) The all-patterns stress model
Rhyme ? Vowel (Coda)
RhymeF ? Vowel (CodaF)
(b) Baseline grammar
RhymeI ? NucleusI
RhymeI ? Nucleus Coda
Rhyme ? Nucleus (Coda)
RhymeF ? NucleusF
RhymeF ? Nucleus CodaF
(c) Weight-sensitive grammar
SyllSIF ? OnsetI RhymeSF
SyllUIF ? OnsetI RhymeUF
SyllSI ? Onset RhymeS
SyllUI ? Onset RhymeU
SyllSF ? Onset RhymeSF
SyllUF ? Onset RhymeUF
RhymeSI ? Vowel Stress (Coda)
RhymeUI ? Vowel (Coda)
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
RhymeSF ? Vowel Stress (CodaF)
RhymeUF ? Vowel (CodaF)
(d) Stress-sensitive grammar
Figure 6: Experiment II Grammars.
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotactics
that exploit Stress and Weight.
851
Model Mean TF Std. Dev.
noweight:nostress 0.846 0.007
noweight:stress 0.880 0.005
weight:nostress 0.865 0.011
weight:stress 0.875 0.005
Table 2: Segmentation Token F-score for Experi-
ment II at 10,000 utterances across eight runs.
grammars over stress in Experiment I was due to
modeling of word-final coda phonotactics.
Table 2 presents token f-score at 10,000 train-
ing utterances averaged across eight runs, along
with the standard deviation in f-score. We see that
the noweight:nostress grammar is several standard
deviations than the grammars that model sylla-
ble weight and/or stress, while the syllable weight
and/or stress grammars exhibit a high degree of
overlap.
4 Conclusion
We have presented computational modeling exper-
iments that suggest that syllable weight (eventu-
ally) encodes nearly everything about word seg-
mentation that dictionary stress does. Indeed,
our experiments did not find a persistent advan-
tage to observing stress over modeling syllable
weight. While it is possible that a different mod-
eling approach might find such a persistent advan-
tage, this advantage could not provide more than
13% absolute F-score. This result suggests that
children may be able to learn and exploit impor-
tant rhythm cues to word boundaries purely on
the basis of segmental input. However, this result
also suggests that annotating input with dictionary
stress has missed important aspects of the role
of stress in word segmentation. As mentioned,
Turk et al. (1995) found that infants preferred ini-
tial light syllables to be stressed. Such a prefer-
ence obviously cannot be learned by attending to
syllable weight alone, so infants who have learned
weight distinctions must also be sensitive to non-
segmental acoustic correlates to stress. There was
no long-term advantage to observing stress in ad-
dition to attending to syllable weight in our mod-
els, however, suggesting that annotated dictionary
stress does not capture the relevant non-segmental
phonetic detail. More modeling is necessary to as-
sess the non-segmental phonetic features that dis-
tinguish stressed light syllables from unstressed
light syllables.
This investigation also highlighted a weakness
of current Adaptor Grammar models: the ‘small-
est’ factors are the size of one PCFG rule. Allow-
ing further factorizations, perhaps using feature
functions of a rule’s right-hand side, would allow
models to capture finer-grained distinctions with-
out fully splitting the symbols that are involved.
References
Benjamin B¨orschinger and Mark Johnson. 2014. Ex-
ploring the role of stress in Bayesian word segmen-
tation using Adaptor Grammars. Transactions of the
ACL, 2:93–104.
Michael R Brent and Timothy A Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition, 61:93–125.
Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71–105.
Nick Campbell and Mary Beckman. 1997. Stress,
prominence, and spectral tilt. In Proceedings of an
ESCA workshop, pages 67–70, Athens, Greece.
Morten H. Christiansen and Suzanne L Curtin. 1999.
The power of statistical learning: No need for alge-
braic rules. In Proceedings of the 21st annual con-
ference of the Cognitive Science Society.
Morten H. Christiansen, Joseph Allen, and Mark S.
Seidenberg. 1998. Learning to segment speech
using multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13:221—268.
2008. The CMU pronouncing dictionary.
http://www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Anne Cutler and David M Carter. 1987. The predom-
inance of strong initial syllables in the English vo-
cabulary. Computer Speech & Language, 2(3):133–
142.
Katherine Demuth, Jennifer Culbertson, and Jennifer
Alter. 2006. Word-minimality, epenthesis, and coda
licensing in the acquisition of English. Language
and Speech, 49:137–174.
Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL 2013, pages 117–
126. Association for Computational Linguistics.
D B Fry. 1955. Duration and intensity as physical cor-
relates of linguistic stress. J. Acoust. Soc. of Am.,
27:765–768.
D B Fry. 1958. Experiments in the perception of stress.
Language and Speech, 1:126–152.
852
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparametric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 317–325. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B Schoelkopf, J Platt, and T Hoffmann,
editors, Advances in Neural Information Processing
Systems, volume 19. The MIT Press.
Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.
1993. Infants’ preference for the predominant stress
patterns of English words. Child Development,
64(3):675–687.
Peter W Juszcyk, Derek M Houston, and Mary New-
some. 1999. The beginnings of word segmentation
in English-learning infants. Cognitive Psychology,
39(3–4):159–207.
Mark Liberman and Alan Prince. 1977. On stress and
linguistic rhythm. Linguistic Inquiry, 8(2):249–336,
Spring.
Constantine Lignos and Charles Yang. 2010. Reces-
sion segmentation: simpler online word segmenta-
tion using limited resources. In Proceedings of ACL
2010, pages 88–97. Association for Computational
Linguistics.
Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the fifteenth confer-
ence on computational natural language learning,
pages 29–38. Association for Computational Lin-
guistics.
Constantine Lignos. 2012. Infant word segmentation:
An incremental, integrated model. In Proceedings
of the West Coast Conference on Formal Linguistics
30.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: sub-
segmental variation in segmental cues. Journal of
Child Language, 37(3):513–543.
Erik D Thiessen and Jenny R Saffran. 2003. When
cues collide: use of stress and statistical cues to word
boundaries by 7-to-9-month-old infants. Develop-
mental Psychology, 39(4):706–716.
Alice Turk, Peter W Jusczyk, and Louann Gerken.
1995. Do English-learning infants use syllable
weight to determine stress? Language and Speech,
38(2):143–158.
Charles Yang. 2004. Universal grammar, statistics or
both? Trends in Cognitive Science, 8(10):451–456.
853
