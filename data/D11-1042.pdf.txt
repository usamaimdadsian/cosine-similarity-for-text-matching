Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455–466,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Corroborating Text Evaluation Results with Heterogeneous Measures
Enrique Amigo´ † Julio Gonzalo † Jesu´s Gime´nez ‡ Felisa Verdejo†
† UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
‡ UPC, Barcelona
{jgimenez}@lsi.upc.edu
Abstract
Automatically produced texts (e.g. transla-
tions or summaries) are usually evaluated with
n-gram based measures such as BLEU or
ROUGE, while the wide set of more sophisti-
cated measures that have been proposed in the
last years remains largely ignored for practical
purposes. In this paper we first present an in-
depth analysis of the state of the art in order
to clarify this issue. After this, we formalize
and verify empirically a set of properties that
every text evaluation measure based on simi-
larity to human-produced references satisfies.
These properties imply that corroborating sys-
tem improvements with additional measures
always increases the overall reliability of the
evaluation process. In addition, the greater the
heterogeneity of the measures (which is mea-
surable) the higher their combined reliability.
These results support the use of heterogeneous
measures in order to consolidate text evalua-
tion results.
1 Introduction
The automatic evaluation of textual outputs is a
core issue in many Natural Language Processing
(NLP) tasks such as Natural Language Generation,
Machine Translation (MT) and Automatic Sum-
marization (AS). State-of-the-art automatic evalu-
ation methods all operate by rewarding similari-
ties between automatically-produced candidate out-
puts and manually-produced reference solutions, so-
called human references or models.
Over the last decade, a wide variety of measures,
based on different quality assumptions, have been
proposed. Recent work suggests exploiting exter-
nal knowledge sources and/or deep linguistic an-
notation, and measure combination (see Section 2).
However, original measures based on lexical match-
ing, such as BLEU (Papineni et al., 2001a) and
ROUGE (Lin, 2004) are still preferred as de facto
standards in MT and AS, respectively. There are,
in our opinion, two main reasons behind this fact.
First, the use of a common measure certainly allows
researchers to carry out objective comparisons be-
tween their work and other published results. Sec-
ond, the advantages of novel measures are not easy
to demonstrate in terms of correlation with human
judgements.
Our goal is not to answer which is the most re-
liable metric or to propose yet another novel mea-
sure. Rather than this, we first analyze in depth the
state of the art, concluding that it is not easy to de-
termine the reliability of a measure. In absence of a
clear proof of the advantages of novel measures, sys-
tem developers naturally tend to prefer well-known
standard measures. Second, we formalize and check
empirically two intrinsic properties that any evalua-
tion measure based on similarity to human-produced
references satisfies. Assuming that a measure satis-
fies a set of basic formal constraints, these properties
imply that corroborating a system comparison with
additional measures always increases the overall re-
liability of the evaluation process, even when the
added measures have a low correlation with human
judgements. In most papers, evaluation results are
corroborated with similar n-gram based measures
(eg. BLEU and ROUGE). However, according to
our second property, the greater the heterogeneity of
455
the measures (which is measurable) the higher their
reliability. The practical implication is that, corrob-
orating evaluation results with measures based on
higher linguistic levels increases the heterogeneity,
and therefore, the reliability of evaluation results.
2 State of the Art
2.1 Individual measures
Among NLP disciplines, MT probably has the
widest set of automatic evaluation measures. The
dominant approach to automatic MT evaluation is,
today, based on lexical metrics (also called n-gram
based metrics). These metrics work by rewarding
lexical similarity between candidate translations and
a set of manually-produced reference translations.
Lexical metrics can be classified according to how
they compute similarity. Some are based on edit dis-
tance, e.g., WER (Nießen et al., 2000), PER (Till-
mann et al., 1997), and TER (Snover et al., 2006).
Other metrics are based on computing lexical preci-
sion, e.g., BLEU (Papineni et al., 2001b) and NIST
(Doddington, 2002), lexical recall, e.g., ROUGE
(Lin and Och, 2004a) and CDER (Leusch et al.,
2006), or a balance between the two, e.g., GTM
(Melamed et al., 2003; Turian et al., 2003b), ME-
TEOR (Banerjee and Lavie, 2005), BLANC (Lita et
al., 2005), SIA (Liu and Gildea, 2006), MAXSIM
(Chan and Ng, 2008), and Ol (Gime´nez, 2008).
The lexical measure BLEU has been criticized in
many ways. Some drawbacks of BLEU are the lack
of interpretability (Turian et al., 2003a), the fact that
it is not necessary to increase BLEU to improve sys-
tems (Callison-burch and Osborne, 2006), the over-
scoring of statistical MT systems (Le and Przybocki,
2005), the low reliability over rich morphology lan-
guages (Homola et al., 2009), or even the fact that a
poor system translation of a book can obtain higher
BLEU results than a manually produced translation
(Culy and Riehemann, 2003).
The reaction to these criticisms has been focused
on the development of more sophisticated measures
in which candidate and reference translations are
automatically annotated and compared at different
linguistic levels. Some of the features employed
include parts of speech (Popovic and Ney, 2007;
Gime´nez and Ma`rquez, 2007), syntactic dependen-
cies (Liu and Gildea, 2005; Gime´nez and Ma`rquez,
2007; Owczarzak et al., 2007a; Owczarzak et al.,
2007b; Owczarzak et al., 2008; Chan and Ng,
2008; Kahn et al., 2009), CCG parsing (Mehay and
Brew, 2007), syntactic constituents (Liu and Gildea,
2005; Gime´nez and Ma`rquez, 2007), named entities
(Reeder et al., 2001; Gime´nez and Ma`rquez, 2007),
semantic roles (Gime´nez and Ma`rquez, 2007), dis-
course representations (Gime´nez, 2008), and textual
entailment features (Pado´ et al., 2009). In general,
when a higher linguistic level is incorporated, lin-
guistic features at lower levels are preserved.
The proposals for summarization evaluation are
less numerous. Some proposals for AS tasks are
based on syntactic units (Tratz and Hovy, 2008), de-
pendency triples (Owczarzak, 2009) or convolution
kernels (Hirao et al., 2005) which reported some re-
liability improvement over ROUGE in terms of cor-
relation with human judgements.
In general, however, it is not easy to determine
clearly the contribution of deeper linguistic knowl-
edge in those proposals. In the case of MT, im-
provements versus BLEU have been reported (Liu
and Gildea, 2005; Kahn et al., 2009), but not over
a more elaborated metric such as METEOR (Mehay
and Brew, 2007; Chan and Ng, 2008). Besides, con-
troversial results on their performance at sentence vs
system level have been reported in shared evaluation
tasks (Callison-Burch et al., 2008; Callison-Burch et
al., 2009; Callison-Burch et al., 2010).
2.2 Combined measures
Several researchers have suggested integrating het-
erogeneous measures. Some of them optimize the
measure combination function according to the met-
ric’s ability to emulate the behavior of human as-
sessors (i.e., correlation with human assessments).
For instance, using linear combinations (Pado´ et al.,
2009; Liu and Gildea, 2007; Gime´nez and Ma`rquez,
2008), Decision Trees (Akiba et al., 2001; Quirk,
2004), regression based algorithms (Paul et al.,
2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b) or a variety of supervised machine learn-
ing algorithms(Quirk et al., 2005; Corston-Oliver et
al., 2001; Kulesza and Shieber, 2004; Gamon et al.,
2005; Amigo´ et al., 2005).
Some of these works report evidence on the con-
tribution of combining heterogeneous measures. For
instance, Albrecht and Hwa included syntax-based
456
measures together with lexical measures, outper-
forming other combination schemes (Albrecht and
Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and
Gildea, after examining the contribution of each
component metric, found that “metrics showing dif-
ferent properties of a sentence are more likely to
make a good combined metric”(Liu and Gildea,
2007). Akiba et al., which combined multiple edit-
distance features based on lexical, morphosyntac-
tic and lexical semantic information, observed that
their approach improved single editing distance for
several data sets (Akiba et al., 2001). More evi-
dence was provided by Corston and Oliver. They
showed that results on the task of discriminating be-
tween manual and automatic translations improve
when combining linguistic and n-gram based fea-
tures. In addition, they showed that this mixed com-
bination improved over the combination of linguistic
or n-gram based measures alone (Corston-Oliver et
al., 2001). (Pado´ et al., 2009) reported a reliability
improvement by including measures based on tex-
tual entailment in the set. In (Gime´nez and Ma`rquez,
2008), a simple arithmetic mean of scores for com-
bining measures at different linguistic levels was ap-
plied with remarkable results in recent shared evalu-
ation tasks (Callison-Burch et al., 2010).
2.3 Meta-evaluation criteria
Meta-evaluation methods have been gradually intro-
duced together with evaluation measures. For in-
stance, Papineni et al. (2001b) evaluated the reliabil-
ity of the BLEU metric according to its ability to em-
ulate human assessors, as measured in terms of Pear-
son correlation with human assessments of adequacy
and fluency at the document level. The measure
NIST (Doddington, 2002) was meta-evaluated also
in terms of correlation with human assessments, but
over different document sources and for a varying
number of references and segment sizes. Melamed
et al. (2003) argued, at the time of introducing the
GTM metric, that Pearson correlation coefficients
can be affected by scale properties. They suggested
using the non-parametric Spearman correlation co-
efficients instead. Lin and Och meta-evaluated
ROUGE over both Pearson and Spearman correla-
tion over a wide set of metrics, including NIST,
WER, PER, and variants of ROUGE, BLEU and
GTM. They obtained similar results in both cases
(Lin and Och, 2004a). Banerjee and Lavie (2005)
argued that the reliability of metrics at the document
level can be due to averaging effects but might not
be robust across sentence translations. In order to
address this issue, they computed the translation-by-
translation correlation with human assessments (i.e.,
correlation at the sentence level).
However, correlation with human judgements is
not enough to determine the reliability of measures.
First, correlation at sentence level (unlike correla-
tion at system level) tends to be low and difficult to
interpret. Second, correlation at system and segment
levels can produce contradictory results. In (Amigo´
et al., 2009) it is observed that higher linguistic lev-
els in measures increases the correlation with human
judgements at the system level at the cost of corre-
lation at the segment level. As far as we know, a
clear explanation for these phenomena has not been
provided yet.
Third, a high correlation at system level does
not ensure a high reliability. Culy and Rieheman
observed that, although BLEU can achieve a high
correlation at system level in some test suites, it
over-scores a poor automatic translation of “Tom
Sawyer” against a human produced translation (Culy
and Riehemann, 2003). This meta-evaluation crite-
rion based on the ability to discern between man-
ual and automatic translations have been referred to
as human likeness (Amigo´ et al., 2006), in contrast
to correlation with human judgements which is re-
ferred to as human acceptability. Examples of meta-
measures based on this criterion are ORANGE (Lin
and Och, 2004b) and KING (Amigo´ et al., 2005).
In addition, many of the approaches to metric com-
bination described in Section 2.2 take human like-
ness as the optimization criterion (Corston-Oliver
et al., 2001; Kulesza and Shieber, 2004; Gamon et
al., 2005). The main advantage of meta-evaluation
based on human likeness is that, since human as-
sessments are not required, metrics can be evaluated
over larger test beds. However, the meta-evaluation
in terms of human likeness is difficult to interpret.
2.4 The use of evaluation measures
In general, the state of the art includes a wide set
of results that show the drawbacks of n-gram based
measures as BLEU, and a wide set of proposals for
new single and combined measures which are meta-
457
evaluated in terms of human acceptability (i.e., their
ability to emulate human judges, typically measured
in terms of correlation with human judgements) or
human-likeness (i.e., their ability to discern between
automatic and human translations) (Amigo´ et al.,
2006). However, the original measures BLEU and
ROUGE are still preferred.
We believe that one of the reasons is the lack of
an in-depth study on to what extent providing ad-
ditional evaluation results with other metrics con-
tributes to the reliability of such results. The state of
the art suggests that the use of heterogeneous mea-
sures can improve the evaluation reliability. How-
ever, as far as we know, there is no comprehen-
sive analysis on the contribution of novel measures
when corroborating evaluation results with addi-
tional measures.
3 Similarity Based Evaluation Measures
In general, automatic evaluation measures applied
in tasks like MT or AS are similarity measures be-
tween system outputs and human references. These
measures are related with precision, recall or overlap
over specific types of linguistic units. For instance,
ROUGE measures n-gram recall. Other measures
that work at higher linguistic levels apply precision,
recall or overlap of linguistic components such as
dependency relations, grammatical categories, se-
mantic roles, etc.
In order to delimit our hypothesis, let us first de-
fine what is a similarity measure in this context. Un-
fortunately, as far as we know, there is no formal
concept covering the properties of current evaluation
similarity measures. A close concept is that of “met-
ric” or “distance function”. But, actually, measures
such as ROUGE or BLEU are not proper “metrics”,
because they do not satisfy the symmetry and the tri-
angle inequality properties. Therefore, we need a
new definition.
Being ? the universe of system outputs s and
gold-standards g, we assume that a similarity mea-
sure, in our context, is a function x : ?2 ?? < such
that there exists a decomposition function f : ? ??
{e1..en} (e.g., words or other linguistic units or
relationships) satisfying the following constraints:
(i) maximum similarity is achieved only when then
the decomposition of the system output resembles
exactly the gold-standard decomposition; and (ii)
growing overlap or removing non overlapped ele-
ments implies growing x. Formally, if x ranges from
0 to 1:
f(s) = f(g)? x(s, g) = 1
(f(s) = f(s?) ? {e ? f(g) \ f(s?)})? x(s, g) > x(s?, g)
(f(s) = f(s?)? {e ? f(s?) \ f(g)})? x(s, g) > x(s?, g)
For instance, a random function and the reversal
of a similarity funtion (f ?(s) = 1f(s) ) do not satisfythese constraints. While the F measure over Pre-
cision and Recall satisfies these constraints1, pre-
cision and recall in isolation do not satisfy all of
them: maximum recall can be achieved without re-
sembling the goldstandard text decomposition; and
maximum precision can be achieved with only a few
overlapped elements.
BLEU (Papineni et al., 2001a) computes the n-
gram precision while the metric ROUGE (Lin and
Och, 2004a) computes the n-gram recall. How-
ever, in general, both metrics satisfy all the con-
straints, given that BLEU includes a brevity penalty
and ROUGE penalizes or limits the system output
length. The measure METEOR creates an align-
ment between the two strings (Banerjee and Lavie,
2005). This overlap-based measure satisfies also the
previous constraints. Measures based on edit dis-
tance over n-grams (Tillmann et al., 1997; Nießen
et al., 2000) or other linguistic units (Akiba et al.,
2001; Popovic and Ney, 2007) match also our def-
inition of similarity measure. The editing distance
is minimum when the two compared text are equal.
The more the evaluated text contains elements from
the gold-standard the more the editing distance is re-
duced (higher similarity). The word ordering can be
also expressed in terms of a decomposition function.
A similar reasoning applies to every relevant mea-
sure in the state-of-the art.
4 Data Sets and Measures
4.1 Data sets
In this paper, we provide empirical results for
MT and AS. For MT, we use the data sets from
the Arabic-to-English (AE) and Chinese-to-English
(CE) NIST MT Evaluation campaigns in 2004 and
1There is an exception. In an extreme case, when recall is
zero, removing non overlapped elements does not modify the F
measure.
458
AE2004 CE2004 AE2005 CE2005
#human-references 5 5 5 4
#systems 5 10 7 10
#system-outputs-assessed 5 10 6 5
#system-outputs 1,353 1,788 1,056 1,082
#outputs-assessed per-system 347 447 266 272
Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments
throughout the paper.
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-
out the paper.
20052. Both include two translations exercises: for
the 2005 campaign we contacted each participant
individually and asked for permission to use their
data3. In our experiments, we take the sum of ad-
equacy and fluency, both in a 1-5 scale, as a global
measure of quality (LDC, 2005). Thus, human as-
sessments are in a 2-10 scale. For AS, we have used
the AS test suites developed in the DUC 2005 and
DUC 2006 evaluation campaigns4. This AS task
was to generate a question focused summary of 250
words from a set of 25-50 documents to a complex
question. Summaries were evaluated according to
several criteria. Here, we will consider the respon-
siveness judgements, in which the quality score was
an integer between 1 and 5. See Tables 1 and 2 for a
brief quantitative description of these test beds.
2http://www.nist.gov/speech/tests/mt
3We are grateful to a number of groups and companies who
responded positively: University of Southern California Infor-
mation Sciences Institute (ISI), University of Maryland (UMD),
Johns Hopkins University & University of Cambridge (JHU-
CU), IBM, University of Edinburgh, University of Aachen
(RWTH), National Research Council of Canada (NRC), Chi-
nese Academy of Sciences Institute of Computing Technology
(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-
entifica e Tecnologica(ITC-IRST), MITRE.
4http://duc.nist.gov/
4.2 Measures
As for evaluation measures, for MT we have used a
rich set of 64 measures provided within the ASIYA
Toolkit (Gime´nez and Ma`rquez, 2010)5. This in-
cludes measures operating at different linguistic lev-
els: lexical, syntactic, and semantic. At the lexical
level this set includes variants of 8 measures em-
ployed in the state of the art: BLEU, NIST, GTM,
METEOR, ROUGE, WER, PER and TER. In addi-
tion, we have included a basic measure Ol that com-
putes the lexical overlap without considering word
ordering. All these measures have similar granular-
ity. They use n-grams of a varying length as the ba-
sic unit with additional information provided by lin-
guistic tools. The underlying similarity criteria in-
clude precision, recall, overlap, or edit rate, and the
decomposition functions include words, dependency
tree nodes (DP HWC, DP-Or, etc.), constituency
parsing (CP-STM), discourse roles (DR-Or), seman-
tic roles (SR-Or), named entities, etc. Further details
on the measure set may be found in the ASIYA tech-
nical manual (Gime´nez and Ma`rquez, 2010).
According to our computations, our measures
cover high and low correlations at both levels. Cor-
relation at system level spans between 0.63 and 0.95.
Correlations at sentence level ranges from 0.18 up to
0.54. We will discriminate between two subsets of
5http://www.lsi.upc.edu/˜nlp/Asiya
459
measures. The first one includes those that decom-
pose the text into words, n-grams, stems or lexical
semantic tags. This set includes BLEU, ROUGE,
NIST, GTM, PER and WER families. We will re-
fer to them as “lexical” measures. The second set
are those that consider deeper linguistic levels such
as parts of speech, syntactic dependencies, syntactic
constituents, etc. We will refer to them as “linguis-
tic” measures.
In the case of automatic summarization (AS), we
have employed the standard variants of ROUGE
(Lin, 2004). These 7 measures are ROUGE-{1..4},
ROUGE-SU, ROUGE-L and ROUGE-W. In addi-
tion we have included the reversed precision version
for each variant and the F measure of both. Notice
that the original ROUGE measures are oriented to
recall. In total, we have 21 measures for the sum-
marization task. All of them are based on n-gram
overlap.
5 Additive reliability
As discussed in Section 2, a number of recent pub-
lications address the problem of measure combi-
nation with successful results, specially when het-
erogeneous measures are combined. The following
property clarifies this issue and justifies the use of
heterogeneous measures when corroborating evalu-
ation results. It asserts that the reliability of system
improvements always increases when the evaluation
result is corroborated by an additional similarity
measure, regardless of the correlation achieved by
the additional measure in isolation.
For the sake of clarity, in the rest of the paper,
we will denote the similarity x(s, g) between sys-
tem output s and human reference g by x(s). The
quality of a system output s will be referred to as
Q(s). Let us define the reliability R(X) of a mea-
sure set as the probability of a real improvement (as
measured by human judges) when a score improve-
ment is observed simultaneously for all measures in
the set X. :
R(X) ? P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X)
According to this definition, we may not be able
to predict the quality of any system output (i.e. a
translation) with a highly reliable measure set, but
we can ensure a system improvement when all mea-
sures corroborate the result. Then the additive relia-
bility property can be stated as:
R(X ? {x}) ? R(X)
We could think of violating this property by
adding, for instance, a measure consisting of a ran-
dom function (x?(s) = rand(0..1)) or a reversal of
the original measure (x?(s) = 1/x(s)). These kind
of measures, however, would not satisfy the con-
straints defined in Section 3.
This property is based on the idea that similar-
ity with human references according to any aspect
should not imply statistically a quality decrease. Al-
though our test suites includes measures with low
correlation at segment and system level, we can con-
firm empirically that all of them satisfy this property.
We have developed the following experiment:
taking all possible measure pairs in the test suites,
we have compared their reliability as a set versus the
maximal reliability of any of them (by computing
the difference R(X)?max(R(x1), R(x2)). Figure
1 shows the obtained distribution of this difference
for our MT and AS test suites. Remarkably, in al-
most every case this difference is positive.
This result has a key implication: Corroborating
evaluation results with a new measure, even when
it has lower correlation with human judgements, in-
creases the reliability of results. Therefore, if the
correlation with judgements is not determinant, the
question is now what factor determines the contri-
bution of the new measures. According to the fol-
lowing property, this factor is the heterogeneity of
measures.
6 Heterogeneity
This property states that the reliability of any mea-
sure combination is lower bounded by the hetero-
geneity of the measure set. In other words, a single
measure can be more or less reliable, but a system
improvement according to all measures in an het-
erogeneous set is reliable.
Let us define the heterogeneity H(X) of a set of
measures X as, given two system outputs s and s?
such that g 6= s 6= s? 6= g (g is the reference
text), the probability that there exist two measures
that contradict each other. That is:
H(X) ? P (?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
460
Figure 1: Additive reliability for metric pairs.
Thus, given a set X of measures, the property
states that there exists a strict growing function F
such that:
R(X) ? F (H(X)) and H(X) = 1? R(X) = 1
In other words, the more the similarity measures
tend to contradict each other, the more a unanimous
improvement over all similarity measures is reliable.
Clearly, the harder it is that measures agree, the more
meaningful it is when they do.
The first part is derived from the Additive Re-
liability property. Intuitively, any individual mea-
sure has zero heterogeneity. Increasing the hetero-
geneity implies joining measures or measure sets
progressively. According to the Additive Reliabil-
ity property, this joining implies a reliability in-
crease. Therefore, the higher the heterogeneity, the
higher the minimum Reliability achieved by the cor-
responding measure sets.
The second part is derived from the Heterogeneity
definition. If H(X) = 1 then, for any distinct pair
of outputs that differ from the reference, there exist
at least two measures in the set contradicting each
other. That is, H(X) = 1 implies that:
?s 6= s? 6= g(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
Therefore, if one output improves the other ac-
cording to all measures, then the output must be
equal than the reference.
¬(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))?
Figure 2: Heterogeneity vs. reliability in MT test suites.
¬(g 6= s 6= s? 6= g)? g = s ? g = s?
According to the first constraint of similarity mea-
sures, a text that is equal to the reference achieves
the maximum score:
g = s? f(g) = g(s)? ?x.x(s) ? x(s?)
Finally, if we assume that the reference (human pro-
duced texts) has a maximum quality, then it will
have equal or higher quality than the other output.
g = s? Q(s) ? Q(s?)
Therefore, the reliability of the measure set is maxi-
mal. In summary, if H(X) = 1 then:
R(X) = P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X) =
= P (Q(s) ? Q(s?)|s = g) = 1
Figures 2 and 3 show the relationship between the
heterogeneity of randomly selected measure sets and
their reliability for the MT and summarization test
suites. As the figures show, the higher the hetero-
geneity, the higher the reliability of the measure set.
The results in AS are less pronounced due to the re-
dundancy in ROUGE measure.
Notice that the heterogeneity property does not
necessarily imply a high correlation between reli-
ability and heterogeneity. For instance, an ideal
single measure would have zero heterogeneity and
461
Figure 3: Heterogeneity vs. reliability in summarization
test suites.
achieve maximum reliability, appearing in the top
left area. The property rather brings us to the fol-
lowing situation: let us suppose that we have a set
of single measures available which achieve a certain
range of reliability. We can improve our system ac-
cording to any of these measures. Without human
assessments, we do not know what is the most re-
liable measure. But if we combine them, increas-
ing the heterogeneity, the minimal reliability of the
selected measures will be higher. This implies that
combining heterogeneous measures (e.g. at high lin-
guistic levels) that do not achieve high correlation
in isolation, is better than corroborating results with
any individual measure alone, such as ROUGE and
BLEU, which is the common practice in the state of
the art.
The main drawback of this property is that in-
creasing the heterogeneity implies a sensitivity re-
duction. For instance, if H(X) = 0.9, then only
for 10% of output pairs in the corpus there exists
an improvement according to all measures. In other
words, unanimous evaluation results from heteroge-
neous measures are reliable but harder to achieve for
the system developer. The next section investigates
on this issue.
Finally, Figure 4 shows that linguistic measures
increase the heterogeneity of measure sets. We have
generated sets of metrics of size 1 to 10 made up
by lexical or lexical and linguistic metrics. As the
figure shows, in the second case, the measure sets
achieve a higher heterogeneity.
Figure 4: Heterogeneity of lexical measures vs. lexical
and linguistic measures.
7 Score thresholds vs. Additive Reliability
According to the previous properties, corroborating
evaluation results with several measures increases
the reliability of evaluation results at the cost of sen-
sitivity. On the other hand, increasing the score
threshold of a single measure should have a similar
effect. Which is then the best methodology to im-
prove reliability? In this section we provide exper-
imental evidence on the relationship between both
ways of increasing reliability: we have found that,
corroborating evaluation results over single texts
with additional measures is more reliable than re-
quiring higher score differences according to any in-
dividual measure in the set. More specifically, we
have found that the reliability of a measure set is
higher than the reliability of each of the individual
measures at a similar level of sensitivity.
Formally, we define the sensitivity S(X) of a met-
ric set X as the probability of finding a score im-
provement within text pairs with a real (i.e. human
assessed) quality improvement:
S(X) = P (x(s) ? x(s?)?x ? X|Q(s) ? Q(s?))
Being Rth(x) and Sth(x) the reliability and sen-
sitivity of a single measure x for a certain increase
score threshold th:
462
Figure 5: Heterogeneity vs. reliability Gain for MT test
suites.
Rth(x) = P (Q(s) ? Q(s?)|x(s)? x(s?) ? th)
Sth(x) = P (x(s)? x(s?) ? th|Q(s) ? Q(s?))
The property that we want to check is that, at the
same sensitivity level, combining measures is more
reliable than increasing the score threshold of single
measures:
S(X) = Sth(x).x ? X ?? R(X) ? Rth(x)
Note that if we had a perfect measure xp such that
R(xp) = S(xp) = 1, then combining this measure
with a low reliability measure xl would produce a
lower sensitivity, but the maximal reliability would
be preserved.
In order to confirm empirically this property, we
have developed the following experiment: (i) We
compute the reliability and sensitivity of randomly
chosen measure sets over single text pairs. We have
generated sets of 2,3,5,10,20 and 40 measures. In
the case of summarization corpora we have com-
bined up to 20 measures. In addition, we com-
pute also the heterogeneity H(X) of each measure
set; (ii) Experimenting with different values for the
threshold th, we compute the reliability of single
measures for all potential sensitivity levels; (iii) For
each measure set, we compare the reliability of the
measure set versus the reliability of single measures
at the same sensitivity level. We will refer to this as
the Reliability Gain:
Figure 6: Heterogeneity vs. reliability Gain for MT test
suites.
Reliability Gain =
R(X)?max{Rth(x)/x ? X ? Sth(x) = S(X)}
If there are several reliability values with the same
sensitivity for a given single measures, we choose
the highest reliability value for the single measure.
Figures 5 and 6 illustrate the results for the MT
and AS corpora. The horizontal axis represents the
Heterogeneity of measure sets, while the vertical
axis represents the reliability gain. Remarkably, the
reliability gain is positive for all cases in our test
suites. The maximum reliability gain is 0.34 in the
case of MT and 0.08 for AS (note that summariza-
tion measures are more redundant in our corpora).
In both test suites, the largest information gains are
obtained with highly heterogeneous measure sets.
In summary, given comparable measures in terms
of reliability, corroborating evaluation results with
several measures is more effective than optimizing
systems according to the best measure in the set.
This empirical property provides an additional ev-
idence in favour of the use of heterogeneous mea-
sures and, in particular, of the use of linguistic mea-
sures in combination with standard lexical measures.
8 Conclusions
In this paper, we have analyzed the state of the art in
order to clarify why novel text evaluation measures
463
are not exploited by the community. Our first con-
clusion is that it is not easy to determine the reliabil-
ity of measures, which is highly corpus-dependent
and often contradictory when comparing correlation
with human judgements at segment vs. system lev-
els.
In order to tackle this issue, we have studied a
number of properties that suggest the convenience of
using heterogeneous measures to corroborate eval-
uation results. According to these properties, we
can ensure that, even when if we can not determine
the reliability of individual measures, corroborating
a system improvement with additional measures al-
ways increases the reliability of the results. In ad-
dition, the more heterogeneous the measures em-
ployed (which is measurable), the higher the relia-
bility of the results. But perhaps the most impor-
tant practical finding is that the reliability at similar
sensitivity levels by corroborating evaluation results
with several measures is always higher than improv-
ing systems according to any of the combined mea-
sures in isolation.
These properties point to the practical advantages
of considering linguistic knowledge (beyond lexi-
cal information) in measures, even if they do not
achieve a high correlation with human judgements.
Our experiments show that linguistic knowledge in-
creases the heterogeneity of measure sets, which
in turn increases the reliability of evaluation results
when corroborating system comparisons with sev-
eral measures.
Acknowledgements
This work has been partially funded by the Spanish
Government (Holopedia, TIN2010-21128-C02 and
OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement number
247762 (FAUST project, FP7-ICT-2009-4-247762).
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceedings of
Machine Translation Summit VIII, pages 15–20.
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 880–887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 296–303.
Enrique Amigo´, Julio Gonzalo, Anselmo Pe nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Summarization. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 280–289.
Enrique Amigo´, Jesu´s Gime´nez, Julio Gonzalo, and Llu?´s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of the Joint 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 17–
24.
Enrique Amigo´, Jesu´s Gime´nez, Julio Gonzalo, and Fe-
lisa Verdejo. 2009. The contribution of linguis-
tic features to automatic machine translation evalua-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, ACL ’09,
pages 306–314, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249–256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70–106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53. Revised August 2010.
464
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55–62.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to the
Automatic Evaluation of Machine Translation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 140–147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1–8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
national Conference on Human Language Technology,
pages 138–145.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT, pages 103–111.
Jesu´s Gime´nez and Llu?´s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation, pages 256–264.
Jesu´s Gime´nez and Llu?´s Ma`rquez. 2008. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319–326.
Jesu´s Gime´nez and Llu?´s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, 1(94):77–86.
Jesu´s Gime´nez. 2008. Empirical Machine Transla-
tion and its Evaluation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145–152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Petr Homola, Vladislav Kubon?, and Pavel Pecina. 2009.
A simple automatic mt evaluation metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, StatMT ’09, pages 33–36, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2009. Expected Dependency Pair Match: Predicting
translation quality with expected syntactic structure.
Machine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), pages 75–84.
LDC. 2005. Linguistic Data Annotation Spec-
ification: Assessment of Adequacy and Flu-
ency in Translations. Revision 1.5. Tech-
nical report, Linguistic Data Consortium.
http://www.ldc.upenn.edu/Projects/
TIDES/Translation/TransAssess04.pdf.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Official
release of automatic evaluation scores for all submis-
sions, August.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL), pages 241–248.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of the 20th
International Conference on Computational Linguis-
tics (COLING).
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
BLANC: Learning Evaluation Metrics for MT. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP), pages 740–747.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
25–32.
Ding Liu and Daniel Gildea. 2006. Stochastic Iter-
ative Alignment for Machine Translation Evaluation.
In Proceedings of the Joint 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 539–546.
465
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 41–48.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Sonja Nießen, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC).
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, pages 80–87.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007b. Labelled Dependencies in Machine Transla-
tion Evaluation. In Proceedings of the ACL Workshop
on Statistical Machine Translation, pages 104–111.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2008. Evaluating machine translation with lfg depen-
dencies. Machine Translation, 21(2):95–119.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ’09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190–198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sebastian Pado´, Michael Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 297–305.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311–318, Philadelphia, jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001b. Bleu: a method for automatic evalu-
ation of machine translation, RC22176. Technical re-
port, IBM T.J. Watson Research Center.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007.
Reducing Human Assessments of Machine Transla-
tion Quality to Binary Classifiers. In Proceedings of
the 11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation (TMI).
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Applica-
tions for Error Analysis. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
48–55, Prague, Czech Republic, June. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 271–279.
Chris Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825–828.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Confu-
sion of Tongues: an MT Metric. In Proceedings of
the Workshop on MT Evaluation ”Who did what to
whom?” at Machine Translation Summit VIII, pages
55–59.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 223–231.
Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-
biaga, and H. Sawaf. 1997. Accelerated DP based
Search for Statistical Translation. In Proceedings of
European Conference on Speech Communication and
Technology.
Stephen Tratz and Eduard Hovy. 2008. Summarization
evaluation using transformed basic elements. In In
Proceedings of TAC-08. Gaithersburg, Maryland.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a.
Evaluation of machine translation and its evaluation.
In In Proceedings of MT Summit IX, pages 386–393.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003b. Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT SUMMIT IX.
466
