Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820–828,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Integrating Multi-level Linguistic Knowledge with a Unified Framework for
Mandarin Speech Recognition
Xinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu?
Speech and Hearing Research Center,
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing, 100871, China
{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cn
Abstract
To improve the Mandarin large vocabulary
continuous speech recognition (LVCSR), a
unified framework based approach is intro-
duced to exploit multi-level linguistic knowl-
edge. In this framework, each knowledge
source is represented by a Weighted Finite
State Transducer (WFST), and then they are
combined to obtain a so-called analyzer for in-
tegrating multi-level knowledge sources. Due
to the uniform transducer representation, any
knowledge source can be easily integrated into
the analyzer, as long as it can be encoded
into WFSTs. Moreover, as the knowledge in
each level is modeled independently and the
combination is processed in the model level,
the information inherently in each knowledge
source has a chance to be thoroughly ex-
ploited. By simulations, the effectiveness
of the analyzer is investigated, and then a
LVCSR system embedding the presented ana-
lyzer is evaluated. Experimental results reveal
that this unified framework is an effective ap-
proach which significantly improves the per-
formance of speech recognition with a 9.9%
relative reduction of character error rate on
the HUB-4 test set, a widely used Mandarin
speech recognition task.
1 Introduction
Language modeling is essential for large vocabu-
lary continuous speech recognition (LVCSR), which
aims to determine the prior probability of a supposed
word string W , p(W ). Although the word-based n-
gram language model remains the mainstream for
?Corresponding author: Xihong Wu
most speech recognition systems, the utilization of
linguistic knowledge is too limited in this model.
Consequently, many researchers have focused on
introducing more linguistic knowledge in language
modeling, such as lexical knowledge , syntax and
semantics of language (Wang and Vergyri, 2006;
Wang et al., 2004; Charniak, 2001; Roark, 2001;
Chelba, 2000; Heeman, 1998; Chelba et al., 1997).
Recently, structured language models have been
introduced to make use of syntactic hierarchi-
cal characteristics (Roark, 2001; Charniak, 2001;
Chelba, 2000). Nevertheless, the computational
complexity of decoding will be heavily increased, as
they are parser-based models. In contrast, the class-
based language model groups the words that have
similar functions of syntax or semantics into mean-
ingful classes. As a result, it handles the questions of
data sparsity and generalization of unseen event. In
practice, the part-of-speech (POS) information, cap-
turing the syntactic role of words, has been widely
used in clustering words (Wang and Vergyri, 2006;
Maltese et al., 2001; Samuelsson and Reichl, 1999).
In Heeman’s POS language model (Heeman, 1998),
the joint probability of word sequence and associ-
ated POS sequence was estimated directly, which
has been demonstrated to be superior to the condi-
tional probability previously used in the class-based
models (Johnson, 2001). Moreover, a SuperARV
language model was presented (Wang and Harper,
2002), in which lexical features and syntactic con-
straints were tightly integrated into a linguistic struc-
ture of SuperARV serving as a class in the model.
Thus, these knowledge was integrated in the rep-
resentation level, and then the joint probabilities
820
of words and corresponding SuperARVs were esti-
mated. However, in the class-based language mod-
els, words are taken as the model units, while other
units smaller or larger than words are unfeasible for
modeling simultaneously, such as the Chinese char-
acters for Chinese names.
Usually, speech recognition systems can only rec-
ognize the words within a predefined dictionary.
With the increase of unknown words, i.e., out-of-
vocabulary (OOV) words, the performance will de-
grade dramatically. This is because not only those
unknown words cannot be recognized correctly, but
the words surrounding them will be affected. Thus,
many efforts have been made to deal with the is-
sue of OOV words (Martins et al., 2006; Galescu,
2003; Bazzi and Glass, 2001), and various model
units smaller than words have been examined to rec-
ognize OOVs from speech, such as phonemes (Bazzi
and Glass, 2000a), variable-length phoneme se-
quence (Bazzi and Glass, 2001), syllable (Bazzi and
Glass, 2000b) and sub-word (Galescu, 2003). Since
the proper name is a typical category of OOV words
and usually takes a very large proportion among all
kinds of OOV words, it has been specially addressed
in (Hu et al., 2006; Tanigaki et al., 2000).
All those attempts mentioned above succeed in
utilizing linguistic knowledge in language modeling
in some degree respectively. In this study, a uni-
fied framework based approach, which aims to ex-
ploit information from multi-level linguistic knowl-
edge, is presented. Here, the Weighted Finite State
Transducer (WFST) turns to be an ideal choice for
our purpose. WFSTs were formerly introduced to
simplify the integration of models in speech recog-
nition, including acoustic models, phonetic mod-
els and word n-gram (Mohri, 1997; Mohri et al.,
2002). In recent years, the WFST has been suc-
cessfully applied in several state-of-the-art speech
recognition systems, such as systems developed by
the AMI project (Hain et al., 2006), IBM (Saon et
al., 2003) and AT&T (Mohri et al., 1996), and in
various fields of natural language processing, such
as smoothed n-gram model, partial parsing (Abney,
1996), named entities recognition (Friburger and
Maurel, 2004), semantic interpretation (Raymond et
al., 2006) and machine translation (Tsukada and Na-
gata, 2004). In (Takaaki Hori and Minami, 2003),
the WFST has been further used for language model
adaptation, where language models of different vo-
cabularies that represented different styles were in-
tegrated through the framework of speech transla-
tion. In WFST-based systems, all of the models are
represented uniformly by WFSTs, and the general
composition algorithm (Mohri et al., 2000) com-
bines these representations flexibly and efficiently.
Thereby, rather than integrating the models step by
step in decoding stage, a complete search network is
constructed in advance. The combined WFST will
be more efficient by optimizing with determiniza-
tion, minimization and pushing algorithms of WF-
STs (Mohri, 1997). Besides, the researches on opti-
mizing the search space and improving WFST-based
speech recognition has been carried out, especially
on how to perform on-the-fly WFSTs composition
more efficiently (Hori et al., 2007; Diamantino Ca-
seiro, 2002).
In this study, we extend the linguistic knowledge
used in speech recognition. As WFSTs provide a
common and natural representation for lexical con-
straints, n-gram language model, Hidden Markov
Model models and context-dependency, multi-level
knowledge sources can be encoded into WFSTs un-
der the uniform transducer representation. Then this
group of WFSTs is flexibly combined together to
obtain an analyzer representing knowledge of per-
son and location names as well as POS information.
Afterwards, the presented analyzer is incorporated
into LVCSR to evaluate the linguistic correctness of
recognition candidates by an n-best rescoring.
Unlike other methods, this approach holds two
distinct features. Firstly, as all multi-level knowl-
edge sources are modeled independently, the model
units such as character, words, phrase, etc., can be
chosen freely. Meanwhile, the integration of these
information sources is conducted in the model level
rather than the representation level. This setup will
help to model each knowledge source sufficiently
and may promote the accuracy of speech recogni-
tion. Secondly, under this unified framework, it is
easy to combine additional knowledge source into
the framework with the only requirement that the
new knowledge source can be represented by WF-
STs. Moreover, since all knowledge sources are fi-
nally represented by a single WFST, additional ef-
forts are not required for decoding the new knowl-
edge source.
821
The remainder of this paper is structured as fol-
lows. In section 2, we introduce our analyzer in de-
tail, and incorporate it into a Mandarin speech recog-
nition system. In section 3, the simulations are per-
formed to evaluate the analyzer and test its effective-
ness when being applied to LVCSR. The conclusion
appears in section 4.
2 Incorporation of Multi-level linguistic
knowledge in LVCSR
In this section, we start by giving a brief descrip-
tion on WFSTs. Then some special characteristics
of Chinese are investigated, and the model units are
fixed. Afterwards, each knowledge source is rep-
resented with WFSTs, and then they are combined
into a final WFST, so-called analyzer. At last, this
analyzer is incorporated into Mandarin LVCSR.
2.1 Weighted Finite State Transducers
The Weighted Finite State Transducer (WFST) is the
generalization of the finite state automata, in which,
besides of an input label, an output label and a
weight are also placed on each transition. With these
labels, a WFST is capable of realizing a weighted re-
lation between strings. In our system, log probabili-
ties are adopted as transition weights and the relation
between two strings is associated with a weight indi-
cating the probability of the mapping between them.
Given a group of WFSTs, each of which models a
stage of a mapping cascade, the composition opera-
tion provides an efficient approach to combine them
into a single one (Mohri et al., 2002; Mohri et al.,
1996). In particular, for two WFSTs R and S, the
composition T = RoS represents the composition
of relations realized by R and S. The combination
is performed strictly on R’s output and S’s input. It
means for each path in T, mapping string r to string
s, there must exist a path mapping r to some string
t in R and a path mapping t to s in S. Decoding on
the combined WFST enables to find the joint opti-
mal results for multi-level weighted relations.
2.2 Model Unit Selection
This study primarily takes the person and location
names as well as the POS information into account.
To deal with Chinese OOV words, different from
the western language in which the phoneme, sylla-
ble or sub-word are used as the model units (Bazzi
and Glass, 2000a; Bazzi and Glass, 2000b; Galescu,
2003), Chinese characters are taken as the basic
units. In general, a person name of Han nation-
ality consists of a surname and a given name usu-
ally with one or two characters. Surnames com-
monly come from a fixed set that has been histori-
cally used. According to a recent investigation on
surnames involving 296 million people, 4100 sur-
names are found, and 129 most used surnames ac-
count for 87% (conducted by the Institute of Genet-
ics and Developmental Biology, Chinese Academy
of Sciences). In contrast, the characters used in
given names can be selected freely, and in many situ-
ations, some commonly used words may also appear
in names, such as ”??” (victory) and ”??” (the
Changjiang River). Therefore, both Chinese charac-
ters and words are considered as model units in this
study, and a word re-segmentation process on recog-
nition hypotheses is necessary, where an n-gram lan-
guage model based on word classes is adopted.
2.3 Representation and Integration of
Multi-level Knowledge
In this work, we ignore the word boundaries of n-
best hypotheses and perform a word re-segmentation
for names recognition. Given an input Chinese
character, it is encoded by a finite state acceptor
FSAinput. For example, the input ”?????”
(while synthesizing molecule) is represented as in
Figure 1(a). Then a dictionary is represented by a
50 321
0
?:??:?
?:??
?:?
(a)
(b)
4
?:?
?:? ?:? ?:?
?:??
?:?
?:?
?:?
?:? ?:?
?:??
?:??
?? ? ? ?
1 3
6
5
4
2
10
9
8
7
Figure 1: (a) is an example of the FSA representing a
given input; (b) is the FST representing a toy dictionary.
822
transducer with empty weights, denoted as FSTdict.
Figure 1(b) illustrates a toy dictionary listed in Ta-
ble 1, in which a successful path encodes a mapping
from a Chinese character sequence to some word
in the dictionary. In practice, all Chinese charac-
Chinese Words English Words
?? synthesize
?? element
?? molecule
?? the period of the day from11 p.m.to l a.m.
? together
? present
Table 1: The Toy dictionary
ters should appear in the dictionary for further in-
corporating models of names. Then the combination
of FSAinput and FSTdict, FSTseg = FSAinput ?
FSTdict, will result in a WFST embracing all the
possible candidate segmentations. Afterwards an n-
gram language model based on word classes is used
to weight the candidate segmentations. As in Fig-
ure 2, a toy bigram with three words is depicted by
WFSTn?gram, and the word classes are defined in
Table 2. Here, both in the training and test stages,
0
w1/un(w1)
w2/un(w2)
w3/un(w3)
4
w3/un(w3)
?/back(w1)
w1/un(w1)
?/back(w3)
w2/un(w2)
?/back(w2)
w1/bi(w2,w1)
w2/bi(w3,w2)
w2/bi(w1,w2)
w3/bi(w2,w3)
w1/bi(w3,w1)
w3/bi(w1,w3)
2
3
1
Figure 2: The WFST representing a toy bigram language
model, in which un(w1) denotes the unigram of w1;
bi(w1, w2) and back(w1) respectively denotes the bi-
gram of w2 and the backoff weight given the word history
w1.
the strings of numbers or letters in sentences are ex-
Classes Description
wi Each word wi listed in the dictionary
CNAME Person names of Han nationality
TNAME Translated person names
LOC Location names
NUM Number expressions
LETTER Letter strings
NON Other non Chinese character strings
BEGIN Beginning of sentence
END End of sentence
Table 2: The Definition of word classes
tracted according to the rules, and then substituted
with the class tags, ”NUM” and ”LETTER” respec-
tively. At the same time, the words, such as ”??”
and ”A?”, are replaced with ”NUM?” and ”LET-
TER?” in the dictionary. In addition, name classes,
including ”CNAME”, ”TNAME” and ”LOC”, will
be set according to names recognition.
Hidden Markov Models (HMMs) are adopted
both for names recognition and POS tagging. Here,
each HMM is represented with two WFSTs. Tak-
ing the POS tagging as an example, the toy POS
WFSTs with 3 different tags are illustrated in Fig-
ure 3. The emission probability of a word by a POS,
(P (word/pos)), is represented as in Figure 3(a),
and the bigram transition probabilities between POS
tags are represented as in Figure 3(b), similar to the
word n-gram. In terms of names recognition, the
HMM states correspond to 30 role tags of names,
some for model units of Chinese characters, such as
surname, the first or second character of a given per-
son name with two characters, the first or last charac-
ter of a location name and so on, but others for model
units of words, such as the word before or after a
name, the words in a name and so on. When rec-
ognizing the person names, since there is a big dif-
ference between the translated names and the names
of Han nationality, two types of person names are
modeled separately, and substituted with two differ-
ent class tags in the segmentation language model,
as ”TNAME” and ”CNAME”. Some rules, which
can be encoded into WFSTs, are responsible for the
transformation from a role sequence to correspond-
ing name class (for example, a role sequence might
consist of the surname, the first character of the
823
0pos1/un(pos1)
pos2/un(pos2)
pos3/un(pos3)
pos1/bi(pos2,pos1)
pos3/bi(pos2,pos3)
pos2/bi(pos1,pos2)
pos3/bi(pos1,pos3)
pos1/bi(pos3,pos1)pos2/bi(pos3,pos2)
(a)
(b)
word: pos/p(word/pos)
3
2
1
0
Figure 3: The toy POS WFSTs. (a) is the WFST rep-
resenting the relationship between the word and the pos;
(b) is the WFSA representing the bigram transition prob-
abilities between POS tags
given name, and the second character of the given
name, which will be transformed to ”CNAME” in
FSTseg). Hence, taking names recognition into ac-
count, a WFST, including all possible segmentations
as well as recognized candidates of names, can be
obtained as below, denoted as WFSTwords:
FSAinput ? FSTdict ?WFSTne ?WFSAn?gram
(1)
POS information is integrated as follows.
(? ?WFSTwords) ?WFSTPOS (2)
Consequently, the desired analyzer, a combined
WFST that represents multi-level linguistic knowl-
edge sources, has been obtained.
2.4 Incorporation in LVCSR
The presented analyzer models linguistic knowledge
at different levels, which will be useful to find an
optimal words sequence among a large number of
speech recognition hypotheses. Thus in this re-
search, the analyzer is incorporated after the first
pass recognition, and the n-best hypotheses are
reranked according to the total path scores adjusted
with the analyzer scores as follows.
Wˆ = argmax
W
?
??
log (PAM (O|W ))
+? ? log (PLM (W ))
+? ? log (PAnalyzer (W ))
?
??
(3)
where PAM (O|W ) and PLM (W ) are the acoustic
and language scores produced in first pass decoding,
and PAnalyzer (W ) reflects the linguistic correctness
of one hypothesis scored by the analyzer. Through
the reranking paradigm, a new best sentence hypoth-
esis is obtained.
3 Simulation
Under the unified framework, multi-level linguistic
knowledge is represented by the analyzer as men-
tioned above. To guarantee the effectiveness of
the introduced framework in integrating knowledge
sources, the analyzer is evaluated in this section.
Then the experiments using an LVCSR system in
which the analyzer is embedded are performed.
3.1 Analyzer Evaluation
Considering the function of the analyzer, cascaded
subtasks of word segmentation, names recognition
and POS tagging can be processed jointly, while
they are traditionally handled in a pipeline manner.
Hence, a comparison between the analyzer and the
pipeline system can be used to evaluate the effec-
tiveness of the introduced framework for knowledge
integration. As illustrated in Figure 4, two systems
based on the presented analyzer and the pipeline
manner are constructed respectively.
The evaluation data came from the People’s Daily
of China in 1998 from January to June (annotated by
the Institute of Computational Linguistics of Peking
University1), among which the January to May data
was taken as the training set, and the June data was
taken as the test set (consisted of 21,143 sentences
and about 1.2 million words). The first two thou-
sand sentences from the June data were extracted
as the development set, used to fix the composition
weight ? in equation 2. A dictionary including about
113,000 words was extracted from the training data,
1http://icl.pku.edu.cn/icl res/
824
input d ict ne n gramF SA F ST W F ST W F ST q q q
Decode
The best segmentation
posWFST
CCompose ompose
Decode Decode
Pipeline System Presented Analyzer
output output
Figure 4: The pipeline system vs The analyzer
in which a person or location name was accounted
as a word in vocabulary, only when the number of
its appearances was no less than three.
In Figure 5, the analyzer is compared with the
pipeline system, where the analyzer outperforms the
pipeline manner on all the subtasks in terms of F1-
score metric. Furthermore to detect the differences,
the statistical significance test using approximate
randomization approach (Yeh, 2000) is done on the
word segmentation results. Since there are more
than 21,000 sentences in the test set, which is not
appropriate for approximate randomization test, ten
sets (500 sentences for each) are randomly selected
from the test corpus. For each set, we run 1048576
shuffles twice and calculate the significance level p-
value according to the shuffled results. It has been
shown that all p-value are less than 0.001 on the ten
sets. Accordingly the improvement is statistically
significant. Actually, this significant improvement
is reasonable, since the joint processing avoids error
propagation and provides the opportunity of shar-
ing information between different level knowledge
sources. The superiority of this analyzer also shows
that the integration of multi-level linguistic knowl-
edge under the unified framework is effective, which
may lead to improved LVCSR.
95.9
91.1
89.9
96.8
91.8
88.5
90.9
88
92
96 Pipeline Analyzer
Integrated Analyzer
83.3
80
84
Word Segmentation POS Tagging Person Name Recognition Location Name Recognition
Figure 5: The Performance comparison between the
pipeline system and the analyzer. The system perfor-
mances are measured with the F1-score in the tasks
of word segmentation, POS tagging, the person names
recognition and the location names recognition.
3.2 Experimental Setup for Mandarin Speech
Recognition
In the baseline speech recognition system, the
acoustic models consisted of context-dependent
Initial-Final models, in which the left-to-right model
topology was used to represent each unit. Accord-
ing to the phonetic structures, the number of states
in each model was set to 2 or 3 for initials, and 4
or 5 for tonal finals. Each state was trained to have
32 Gaussian mixtures. The used 39-dimension fea-
ture vector comprised 12 MFCC coefficients, en-
ergy, and their first-order and second-order deltas.
Since in this work we focused on modeling knowl-
edge of language in Mandarin LVCSR, only clean
male acoustic models were trained with a speech
database that contained about 360 hours speech of
over 750 male speakers. This training data was
picked up from three continuous Mandarin speech
corpora: the 863-I, 863-II and Intel corpora. The
brief information about these three speech corpora
was listed in Table 3. As in this work, the eval-
uation data was the 1997 HUB-4 Mandarin broad-
cast news evaluation data (HUB-4 test set), to bet-
ter fit this task, the acoustic models were adapted
by the approach of maximum a posterior (MAP)
adaptation. The adaption data was drawn from the
HUB4 training set, excluding the HUB-4 develop-
825
Corpus Speakers Amount of Speech
(hours)
863-I (male) 83 56.67
863-II(male) 120 78.08
Intel (male) 556 227.30
total 759 362.05
Table 3: The information of the speech training data
ing set, where only the cleaned male speech data
(data under condition f0 defined as (Doddington,
1996)) was used. The partition for the clean data
was done with the acoustic segmentation software
CMUseg 0.52 (Siegler et al., 1997), and finally 8.6
hours adaptation data was obtained.
The language model was a word-based trigram
built on 60,000 words entries and trained with a cor-
pus about 1.5 billion characters. The training set
consisted of broadcast news data from the Xinhua
News Agency released by LDC (Xinhua part of Chi-
nese Gigaword), seven years data of People’s Daily
of China from 1995 to 2002 released by People’s
Daily Online3, and some other data from news web-
sites, such as yahoo, sina and so on.
In addition, the analyzer incorporated in speech
recognition was trained with a larger corpus from
People’s Daily of China, including the data in 1998
from January to June and the data in 2000 from
January to November (annotated by the Institute
of Computational Linguistics of Peking University).
The December data in 2000 was taken as the devel-
opment set used to fix the composition weight ? in
equation 2.
3.3 Experimental Results
In our experiments, the clean male speech data from
the Hub-4 test set was used, and 238 sentences were
finally extracted for testing. The weight of the ana-
lyzer was empirically derived from the development
set, including 649 clean male sentences from the de-
vSet of HUB-4 Evaluation. The recognition results
are shown in Table 4. The baseline system has a
character error rate (CER) of 14.85%. When the an-
alyzer is incorporated, a 9.9% relative reduction is
2Acoustic segmentation software downloaded from
http://www.nist.gov/speech/tools/CMUseg 05targz.htm.
3http://www.people.com.cn
System Err. Sub. Del. Ins.
Baseline 14.85 13.02 0.76 1.07
Analyzer 13.38 11.78 1.00 0.60incorporation
Table 4: The Speech recognition results
achieved. Furthermore, we ran the statistical signif-
icance test to detect the performance improvement,
in which the approximate randomization approach
(Yeh, 2000) was modified to output the significance
level, p-value, for the CER metric. The p-levels pro-
duced through two rounds of 1048576 shuffles are
0.0058 and 0.0057 respectively, both less than 0.01.
Thus the performance improvement imposed by the
utilization of the analyzer is statistically significant.
4 Conclusion
Addressing the challenges of Mandarin large vocab-
ulary continuous speech recognition task, within the
unified framework of WFSTs, this study presents
an analyzer integrating multi-level linguistic knowl-
edge. Unlike other methods, model units, such as
characters and words, can be chosen freely in this
approach since multi-level knowledge sources are
modeled independently. As a consequence, the fi-
nal analyzer can be derived from the combination
of better optimized models based on proper model
units. Along with two level knowledge sources, i.e.,
the person and location names as well as the part-of-
speech information, the analyzer is built and evalu-
ated by a comparative simulation. Further evaluation
is also conducted on an LVCSR system in which the
analyzer is embedded. Experimental results consis-
tently reveal that the approach is effective, and suc-
cessfully improves the performance of speech recog-
nition by a 9.9% relative reduction of character error
rate on the HUB-4 test set. Also, the unified frame-
work based approach provides a property of integrat-
ing additional linguistic knowledge flexibly, such as
organization name and syntactic structure. Further-
more, the presented approach has a benefit of ef-
ficiency that additional efforts are not required for
decoding as new knowledge comes, since all knowl-
edge sources are finally encoded into a single WFST.
826
Acknowledgments
The work was supported in part by the National
Natural Science Foundation of China (60435010;
60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the Na-
tional Key Basic Research Program of China
(2004CB318005), and the New-Century Training
Program Foundation for the Talents by the Ministry
of Education of China.
References
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4):337–344.
Issam Bazzi and James R. Glass. 2000a. Modeling out-
of-vocabulary words for robust speech recognition. In
Proc. of 6th International Conference on Spoken Lan-
guage Processing, pages 401–404, Beijing, China, Oc-
tober.
Issam Bazzi and James Glass. 2000b. Heterogeneous
lexical units for automatic speech recognition: prelim-
inary investigations. In Proc. of ICASSP, pages 1257–
1260, Istanbul, Turkey, June.
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Proc. of EUROSPEECH, pages 61–64, Aal-
borg, Denmark, September.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of ACL, pages 116–123,
Toulouse, France, July.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, and Dekai Wu. 1997. Structure and performance
of a dependency language model. In Proc. of EU-
ROSPEECH, pages 2775–2778, Rhodes, Greece.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns Hop-
kins University.
Isabel Trancoso Diamantino Caseiro. 2002. Using dy-
namic WFST composition for recognizing broadcast
news. In Proc. of ICSLP, pages 1301–1304, Denver,
Colorado, USA, September.
George Doddington. 1996. The 1996 hub-
4 annotation specification for evaluation of
speech recognition on broadcast news. In
ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.
N. Friburger and D. Maurel. 2004. Finite-state trans-
ducer cascades to extract named entities in texts. The-
oretical Computer Science, 313(1):93–104.
Lucian Galescu. 2003. Recognition of out-of-vocabulary
words with sub-lexical language models. In Proc.
of EUROSPEECH, pages 249–252, Geneva, Switzer-
land, September.
Thomas Hain, Lukas Burget, John Dines, Giulia Garau,
Martin Karafiat, Mike Lincoln, Jithendra Vepa, and
Vincent Wan. 2006. The AMI meeting transcription
system: Progress and performance. In Proc. of Rich
Transcription 2006 Spring Meeting Recognition Eval-
uation.
Peter A. Heeman. 1998. Pos tagging versus classes in
language modeling. In Proc. of the 6th Workshop on
very large corpora, pages 179–187, Montreal, Canada.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring in
extremely large vocabulary continuous speech recog-
nition. IEEE Transactions on audio, speech, and lan-
guage processing, 15(4):1352–1365.
Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and
Yoshinori Sagisaka. 2006. Language modeling of
chinese personal names based on character units for
continuous chinese speech recognition. In Proc. of
INTERSPEECH, pages 249–252, Pittsburgh, USA,
September.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL, pages
322 – 329, Toulouse, France.
G. Maltese, P. Bravetti, H. Crépy, B. J. Grainger, M. Her-
zog, and F. Palou. 2001. Combining word- and
class-based language models: A comparative study in
several languages using automatic and manual word-
clustering techniques. In Proc. of EUROSPEECH,
pages 21–24, Aalborg, Denmark, September.
Ciro Martins, Antonio Texeira, and Joao Neto. 2006.
Dynamic vocabulary adaptation for a daily and real-
time broadcast news transcription system. In Proc. of
Spoken Language Technology Workshop, pages 146–
149, December.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17–32.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69–88.
Mehrya Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269–311.
827
Christan Raymond, Fre de ric Be chet, Renato D. Mori,
and Ge raldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288–304.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Christer Samuelsson and Wolfgang Reichl. 1999. A
class-based language model for large-vocabulary
speechrecognition extracted from part-of-speech
statistics. In Proc. of ICASSP, pages 537–540,
Phoenix, Arizona, USA, March.
George Saon, Geoffrey Zweig, Brain KingsBury, Lidia
Mangu, and Upendra Canudhari. 2003. An architec-
ture for rapid decoding of large vocabulary conversa-
tional speech. In Proc. of Eurospeech, pages 1977–
1980, Geneva, Switzerland, September.
Matthew A. Siegler, Uday Jain, Bhiksha Raj, and
Richard M. Stern. 1997. Automatic segmentation,
classification and clustering of broadcast news audio.
In Proc. of DARPA Speech Recognition Workshop,
pages 97–99, Chantilly, Virginia, February.
Daniel Willett Takaaki Hori and Yasuhiro Minami.
2003. Language model adaptation using WFST-based
speaking-style translation. In Proc. of ICASSP, pages
I.228–I.231, Hong Kong, April.
Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori
Sagisaka. 2000. A hierarchical language model incor-
porating class-dependent word models for oov words
recognition. In Proc. of 6th International Conference
on Spoken Language Processing, pages 123–126, Bei-
jing, China, October.
Hajime Tsukada and Masaaki Nagata. 2004. Efficient
decoding for statistical machine translation with a fully
expanded WFST model. In Proc. of EMNLP, pages
427–433, Barcelona, Spain, July.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. of
EMNLP, pages 238–247, Philadelphia, USA, July.
Wen Wang and Dimitra Vergyri. 2006. The use of word
n-grams and parts of speech for hierarchical cluster
language modeling. In Proc. of ICASSP, pages 1057–
1060, Toulouse, France, May.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pages 261–264, Montreal, Canada, May.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proc. of
COLING, pages 947–953, Saarbrücken, August.
828
