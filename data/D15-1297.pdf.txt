Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2503–2508,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
JEAM: A Novel Model for Cross-Domain Sentiment Classification 
Based on Emotion Analysis 
 
 
Kun-Hu Luo, Zhi-Hong Deng?, Liang-Chen Wei, Hongliang Yu 
School of Electronic Engineering and Computer Science 
Peking University, Beijing, China 
 {dr.tiger126@gmail.com, zhdeng@cis.pku.edu.cn, pkuhaywire@gmail.com, 
yuhongliang324@gmail.com}  
 
  
 
Abstract 
Cross-domain sentiment classification 
(CSC) aims at learning a sentiment 
classifier for unlabeled data in the target 
domain based on the labeled data from a 
different source domain. Due to the 
differences of data distribution of two 
domains in terms of the raw features, the 
CSC problem is difficult and challenging. 
Previous researches mainly focused on 
concepts mining by clustering words 
across data domains, which ignored the 
importance of authors’ emotion contained 
in data, or the different representations of 
the emotion between domains. In this 
paper, we propose a novel framework to 
solve the CSC problem, by modelling the 
emotion across domains. We first develop 
a probabilistic model named JEAM to 
model author’s emotion state when 
writing. Then, an EM algorithm is 
introduced to solve the likelihood 
maximum problem and to obtain the latent 
emotion distribution of the author. Finally, 
a supervised learning method is utilized to 
assign the sentiment polarity to a given 
online review. Experiments show that our 
approach is effective and outperforms 
state-of-the-art approaches. 
1 Introduction 
Cross-domain sentiment classification (CSC) is 
the task that learns a sentiment classifier for 
unlabeled data in the target domain based on the 
labeled data from the source domain. With the 
increasing amount of opinion information 
                                                          
? Corresponding author 
available on the Internet, CSC has become a hot 
spot in recent years. Traditional machine learning 
algorithms often train a classifier utilizing the 
labeled data for CSC. However, in some practical 
cases, we may have many labeled data for some 
domains (source domains) but very few or no 
labeled data for other domains (target domains). 
Due to the differences of the distribution of two 
domains in terms of raw features, e.g. raw term 
frequency, the classifier trained from the source 
domain often performs badly on the target domain. 
To overcome this issue, several feature-based 
studies have been proposed to improve the 
sentiment classification domain adaptation   
[Zhuang et al., 2013; He et al., 2011; Gao and Li, 
2011; Li et al., 2012; Dai et al., 2007; Zhuang et 
al., 2010; Pan et al., 2010; Wang et al., 2011; Long 
et al., 2012; Lin and He, 2009]. 
 Existing studies build various generative 
models to solve the domain adaptation problems 
for CSC. In most cases, the models are trained by 
using the whole corpora without specifying on the 
sentiment of the texts. For example, [Zhuang et al., 
2013] propose a general framework HIDC to mine 
high-level concepts (e.g. word clusters) across 
various domains. However, their learned concepts 
contain many topics not restricted to the sentiment. 
On the other hand, some researchers focus on the 
usage of the sentiment in CSC study [Mitra et al., 
2013a; Mitra et al., 2013b; He et al., 2011]. [He et 
al., 2011] modify JST model [Lin and He, 2009] 
by incorporating word polarity priors through 
adjusting the topic-word Dirichlet priors. 
However, they fail to consider the expression 
differences among various domains.  
To overcome the above issues, we employ 
“emotion”, for its ubiquity among domains. The 
sentiment words in different domains might vary 
2503
significantly, but the emotion can be effectively 
transferred. For example, when expressing the 
emotion “happiness”, one uses “bravo” in the 
domain of sport, while “yummy” in the domain of 
food. Therefore, we propose an EA framework to 
model the latent emotions which are commonly 
contained in subjective articles and expressed by 
“emotional words”. We infer the sentiment 
polarity of a document based on the emotion state. 
The hierarchy of EA is composed by four layers: 
(1) Sentiment Layer 
Normally, the sentiment of a document is the 
general opinion towards a certain event or object. 
For example, a movie review in IMDB might 
voice the feeling about the movie by a reviewer 
[Yu et al., 2013].  
(2) Emotion Layer 
Based on the emotion classification theories 
in psychology [Plutchik, 2002], the emotion can 
be classified into the basic ones influenced by the 
physiological factors, e.g. happiness, sadness, 
anger, etc., and dozens of complicated ones 
formed under some specific social conditions, e.g. 
shame, guilt, abashment, etc. Additionally, the 
emotion can be classified as positive and negative 
(similar to the sentiment classification) based on 
dimensional models of emotion [Schlosberg, 
1954; Plutchik, 2002; Rubin and Talerico, 2009]. 
Intuitively, we assume that a document tends to 
contain the emotions of similar polarity.  
(3) Lexicon Layer 
To build the connection between words and 
the emotion, we introduce emotional words 
instead of raw word features into our model. By 
utilizing the emotional lexicon MPQA [Wiebe et 
al., 2005], we select groups of strong polar words, 
which get high scores in the emotional lexicon. 
These words are considered highly correlated to 
the certain emotion of the same polarity. And 
these strong polar words have invariant polarity 
across domains. Therefore, the emotion can be 
substantialized by a series of emotional words 
drawn from corresponding probability distribution. 
(4) Expression Layer 
In many practical cases, data come from 
different domains. We suppose that the 
correlation between emotion state and sentiment 
orientation is stable over domains, but one 
emotion may have different expressions when 
domain varies. E.g., “satisfaction” may be 
expressed as “interesting” or “attractive” for a 
book; meanwhile, it may be expressed “efficient” 
for an electronics device. Formally, we have 
?(?|?, ?1) = ?(?|?, ?2) = ?(?|?)     (1) 
?(??|?, ?1) ? ?(??|?, ?2)         (2) 
where ?  denotes the emotion, y denotes the 
author’s sentiment orientation, ?1  and ?2 
denotes two different domains, and ??  denotes 
the emotional words.  
 Along this line, we propose the Joint 
Emotion Analysis Model (named JEAM for 
abbreviation) utilizing the probabilistic methods. 
See details in the next section. 
2 Proposed Model 
2.1 Problem Formulation 
The CSC problem can be formulated as follows: 
Suppose we have two sets of data, denoted as ?? 
and ??, which represent the source domain data 
and the target domain data respectively. In the 
CSC problem, the source domain data consist of 
labeled instances, denoted by  ?? =
{(??
(?)
, ??
(?)
)}|?=1
?? , where ??
(?)
? ??  is an input 
vector, ??
(?)
? {0,1} is the output label, and ?? 
is the number of documents in ??. Unlike that of 
the source domain, the target domain data consists 
of samples without any label information, denoted 
by ?? = {??
(?)
}|?=1
?? , where ??
(?)
? ?? is an input 
vector, and ?? is the number of documents in ??. 
The task of CSC is to leverage the training data of 
source domain ??  to predict the label ??
(?)
 
corresponding to input vector ??
(?)
 of target 
domain ??.  
2.2 The JEAM Model 
To model the author’s emotion state contained in 
the document, we propose the JEAM model based 
on the probabilistic graphical principle. Note that 
all the factors and edges in JEAM are derived 
from the specific concepts and relations in EA, 
e.g., Eq(1) and Eq(2). We draw the graphical 
representation of JEAM in Figure 1, and show the 
notations of this paper in Table 1. 
In Figure 1, y denotes the sentiment 
orientation of the author, which is a latent variable 
in this model. ?   denotes any emotion (topic) 
generated by y from a conditional 
probability ?(?|?). ? is also a latent variable in 
this model. ?  denotes any data domain, e.g., 
books, dvd, kitchen, and electronics etc. ? 
denotes any document chosen from domain r with 
label y. For documents from the source domain, 
the conditional probability ?(?|?, ?) is known, 
which can be used to supervise the modeling 
process. ? denotes the prior sentiment polarity of 
the corresponding emotional word. In practice, ? 
can be obtained from the emotional lexicon,  
2504
 
 
 
 
 
 
Figure 1. The Graphical representation of JEAM. 
All the latent variables are marked in white, and 
all the observed variables are marked in gray. 
 
Table 1.  Means of Symbols 
 
which classifies a series of words into positive and 
negative categories.  ?? denotes any emotional 
word with polarity u, which is chosen over words 
conditioned on emotion e and domain r from 
conditional probability ?(??|?, ?, ?) . In this 
paper, we only select emotional words with strong 
sentiment polarities to represent the vector of the 
document. Therefore, we rebuild the data with the 
help of emotional lexicon cutting out the non-
emotional words. As a result, any word chosen 
from the rebuilt data will be an emotional word, 
which is supposed not to change its polarity in 
different domains. Additionally, the joint 
probability over all the observed variables can be 
defined as follows based on the hidden variables: 
?(?? , ?, ?, ?) = ? ?(?, ?, ?? , ?, ?, ?)?,?   (3) 
Based on the graphical model, we have: 
?(?, ?, ?? , ?, ?, ?) = 
?(??|?, ?, ?)?(?|?, ?)?(?|?)?(?)?(?)?(?) (4) 
We need to learn the unobservable 
probabilities (e.g.,  ?(??|?, ?, ?), ?(?|?, ?),  
?(?|?), ?(?) ) to infer the hidden emotion 
distribution. Therefore, we develop an 
Expectation-Maximization (EM) algorithm to 
maximize the log likelihood of generating the 
whole dataset and obtain the iterative formula in 
E-step as follows: 
?(?, ?|?? , ?, ?, ?) =  
?(??|?, ?, ?)?(?|?, ?)?(?|?)?(?)?(?)?(?)
? ?(??|?, ?, ?)?(?|?, ?)?(?|?)?(?)?(?)?(?)?,?
 (5) 
where all the factors are calculated in M-step 
similar to PLSA and HIDC (Hoffman, 1999; 
Zhuang et al., 2013). 
2.3 CSC via JEAM 
To use JEAM to solve CSC problems, we adopt 
two optimizations: 
First, we supervise the EM optimization with 
the polarity information of emotional words and 
instances respectively in the source domain. On 
the one hand, we estimate ?(?, ?|?? , ?, ?, ?) 
utilizing the polarity label of the emotional words.  
Let the emotion set ? be divided into positive set 
??  and negative set  ?? . We set 
?(?? , ?|?? , ?, ?, ???) = 0 during the whole EM 
process when the polarities of the emotion and 
current emotional word are different. On the other 
hand, we estimate the probability ?(?|?, ?) with 
the label information of instances in the source 
domain. When the document is from the source 
domain, we set ?(?|?, ?) = 0  if  ?   is 
different with the ground truth. 
Second, we reconstruct the document as 
follows, 
?? = [?1, ?2, … ??], ?? = {
    1 +
|???
?|
? |???
?|??=1
, ?? ???
? ? ?
 
0, ??????
     (6) 
where [?1, ?2, … ??]  is the distribution over 
emotions, ???
? = {??|???
? = ?} , ???
? =
???????  ?(?|?? , ?) , and ?(?|?? , ?)  can be 
computed based on  ?(??|?, ?, ?) , ?(?|?), ?(?) 
and ?(?) obtained after EM algorithm. The main 
function of this step is to process a new given 
document faster, avoiding training JEAM again 
with the new input. Finally, a machine learning 
method Support Vector Machine (SVM) is 
introduced to train a classifier with the labeled 
data from the source domain and assign polarities 
to documents from the target domain based on our 
reconstructed data.  
3 Experiments 
3.1 Experimental Setup 
We demonstrate the effectiveness of JEAM on the 
Multi-Domain sentiment data set [Blitzer et al., 
2007] which contains four types (domains) of 
real-world product documents taken from 
Amazon.com, which are books, dvd, electronics 
and kitchen. We randomly select 1800 documents 
from the one domain (source domain) and 200 
documents from another domain (target domain). 
Then, we train a sentiment classifier using 
documents selected from the source domain and 
? Emotion 
?? Emotional word 
? Domain 
? Document  
? Prior sentiment polarity of the emotional word 
? Sentiment polarity of the document 
? All the observed variables 
? All the model parameters 
2505
assign labels to documents selected from the 
target domain, which generates 12 classification 
tasks. We preform 10 random selections and 
report the average results over 10 different runs. 
We use MPQA subjective lexicon 1  as the 
emotional lexicon. In our experiments, only 
strongly subjective clues are considered as 
emotional words, consisting of 1717 positive and 
3621 negative words. We rebuild the dataset by 
cutting out the non-emotional words. For 
experiment parameters, we set ? = 25, ? = 25, 
and ? = 100  after plenty of experiments. 
Considering the data in practice, the sentiment 
orientation y has only two forms, positive or 
negative. Note that we do neither instance 
selection nor complicated feature selection (only 
filter the low-frequency words) to our proposed 
method and other methods in comparison. 
3.2 Experimental Result 
Performance of Emotional Words 
We show the effectiveness of introducing 
emotional words to solve the CSC problem. In 
JEAM, we reconstruct the documents by cutting 
out the non-emotional words. To compare the 
classification accuracy on the original documents 
and the reconstructed (emotional) documents, we 
choose two common classification algorithms, 
linear SVM and PLSA (topic size=10) for 
experiment respectively. The experiment results 
shows that both SVM and PLSA perform better 
on the emotional documents (60.43% and 60.48%) 
than on the original documents (57.73% and 
56.69%) for the average accuracy over 12 
classification tasks.  
Effectiveness of using domain information 
and word polarity 
We show the effectiveness of using domain 
information and word polarity, which are 
employed in our approach. For this purpose, we 
repeat the experiment without introducing domain 
and word polarity (node u and node r) into the 
model. Figure 2 shows the results. As it is clear, 
the highest performance can be achieved when 
domain information and word polarity are both 
used, while the lowest performance is obtained 
when neither of them is used.  
Comparison with the Baselines 
We compare our proposed approach with PLSA, 
SVM, SFA [Pan et al., 2010], JST [He et al., 2011] 
and HIDC [Zhuang et al., 2013]. The 
experimental results of the 12 classification tasks 
are shown in Figure 3. It can be observed that our  
                                                          
1 http://www.cs.pitt.edu/mpqa 
 
 
 
 
 
 
 
 
 
 
Figure 2: Effectiveness of using domain 
information and word polarity 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Comparison with the Baselines 
 
 
proposed approach outperforms all the other 
approaches in general. Note that in order to obtain 
a more precise comparison of the algorithms, we 
do neither the instance selection nor the 
complicated feature selection. The result of our 
proposed approach can possibly be improved with 
the help of these selection strategies. 
4 Conclusion 
In this paper, we propose a novel framework to 
solve the CSC problem, by modelling emotions 
across domains. We deeply analyze the relation 
between the author and the document based on the 
emotion theories in the field of psychology. Along 
this line, we propose a framework named EA, 
which takes the emotions and domains into 
account. Based on EA, we propose a novel model 
named JEAM to model the author’s emotion state 
for Cross-domain sentiment classification. We 
conduct extensive experiments on real datasets to 
evaluate JEAM. The experiment results show that 
emotion plays an important role in CSC and 
JEAM outperforms existing state-of-the-art 
methods on the task of CSC. 
2506
5 Acknowledgement 
This work is partially supported by Project 
61170091 supported by National Natural Science 
Foundation of China and Project 2015AA015403 
supported by the National High Technology 
Research and Development Program of China 
(863 Program). We would also like to thank the 
anonymous reviewers for their helpful comments. 
 
References 
John Blitzer, Mark Dredze, Fernando Pereira, et 
al.2007. Biographies, bollywood, boom-boxes and 
blenders: Domain adaptation for sentiment 
classification. In ACL, volume 7, pages 440–447. 
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong 
Yu. 2007. Co-clustering based classification for out-
of-domain documents. In Proceedings of the 13th 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pages 210– 
219. ACM. 
Hal Daum?e III. 2009. Frustratingly easy domain 
adaptation. arXiv preprint arXiv:0907.1815.  
Sheng Gao and Haizhou Li. 2011. A cross-domain 
adaptation method for sentiment classification using 
probabilistic latent analysis. In Proceedings of the 
20th ACM international conference on Information 
and knowledge management, pages 1047–1052. 
ACM. 
Boqing Gong, Kristen Grauman, and Fei Sha. 2013. 
Connecting the dots with landmarks: 
Discriminatively learning domain-invariant features 
for unsupervised domain adaptation. In Proceedings 
of The 30th International Conference on Machine 
Learning, pages 222–230. 
Yulan He, Chenghua Lin, and Harith Alani. 2011. 
Automatically extracting polarity-bearing topics for 
cross-domain sentiment classification. In 
Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics: Human 
Language Technologies-Volume 1, pages 123–131. 
Association for Computational Linguistics.  
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual 
international ACM SIGIR conference on Research 
and development in information retrieval, pages 50–
57. ACM. 
Shoushan Li, Chu-Ren Huang, Guodong Zhou, and 
Sophia Yat Mei Lee. 2010. Employing personal/ 
impersonal views in supervised and semisupervised 
sentiment classification. In Proceedings of the 48th 
annual meeting of the association for computational 
linguistics, pages 414–423. Association for 
Computational Linguistics. 
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and 
Xiaoyan Zhu. 2012. Cross-domain co-extraction of 
sentiment and topic lexicons. In Proceedings of the 
50th Annual Meeting of the Association for 
Computational Linguistics: Long Papers-Volume 1, 
pages 410–419. Association for Computational 
Linguistics. 
Chenghua Lin and Yulan He. 2009. Joint sentiment/ 
topic model for sentiment analysis. In Proceedings 
of the 18th ACMconference on Information and 
knowledge management, pages 375–384. ACM. 
Mingsheng Long, Jianmin Wang, Guiguang Ding, Wei 
Cheng, Xiang Zhang, and Wei Wang. 2012. Dual 
transfer learning. In SDM, pages 540–551. SIAM. 
Mitra Mohtarami. Man lan, and chew lim tan. 2013a. 
from semantic to emotional space in probabilistic 
sense sentiment analysis. In the 27th AAAI 
Conference on Artificial Intelligence. 
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang 
Yang, and Zheng Chen. 2010. Cross-domain 
sentiment classification via spectral feature 
alignment. In Proceedings of the 19th international 
conference on World wide web, pages 751–760. 
ACM. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and trends in 
information retrieval, 2(1-2):1–135. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings of the 
ACL-02 conference on Empirical methods in 
natural language processing-Volume 10, pages 79–
86. Association for Computational Linguistics. 
Robert Plutchik. 1980. Emotion: A psychoevolutionary 
synthesis. Harpercollins College Division.  
Robert Plutchik. 2002. Emotion and life. 
David C Rubin and Jennifer M Talarico. 2009. A 
comparison of dimensional models of emotion: 
Evidence from emotions, prototypical events, 
autobiographical memories, and words. Memory, 
17(8):802–808. 
Harold Schlosberg. 1954. Three dimensions of 
emotion. Psychological review, 61(2):81. 
Hua Wang, Heng Huang, Feiping Nie, and Chris Ding. 
2011. Cross-language web page classification via 
dual knowledge transfer using nonnegative matrix 
trifactorization. In Proceedings of the 34th 
international ACM SIGIR conference on Research 
and development in Information Retrieval, pages 
933–942. ACM. 
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 
2005. Annotating expressions of opinions and 
emotions in language. Language resources and 
evaluation, 39(2-3):165–210. 
2507
Hongliang Yu, Zhi-Hong Deng, and Shiyingxue Li. 
2013. Identifying sentiment words using an 
optimization-based model without seed words. In 
ACL (2), pages 855–859. 
Fuzhen Zhuang, Ping Luo, Peifeng Yin, Qing He, and 
Zhongzhi Shi. 2013. Concept learning for 
crossdomain text classification: A general 
probabilistic framework. In Proceedings of the 
Twenty-Third international joint conference on 
Artificial Intelligence, pages 1960–1966. AAAI 
Press. 
Fuzhen Zhuang, Ping Luo, Changying Du, Qing He, 
Zhongzhi Shi, and Hui Xiong. 2014. Triplex transfer 
learning: exploiting both shared and distinct 
concepts for text classification. Cybernetics, IEEE 
Transactions on, 44(7):1191–1203. 
2508
