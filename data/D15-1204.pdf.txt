Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1763–1773,
Lisbon, Portugal, 17-21 September 2015.
c
©2015 Association for Computational Linguistics.
CORE: Context-Aware Open Relation Extraction
with Factorization Machines
Fabio Petroni
Sapienza University of Rome
Rome, Italy
petroni@dis.uniroma1.it
Luciano Del Corro
Max Planck Institute for
Informatics
Saarbr¨ucken, Germany
delcorro@mpi-inf.mpg.de
Rainer Gemulla
University of Mannheim
Mannheim, Germany
rgemulla@uni-mannheim.de
Abstract
We propose CORE, a novel matrix fac-
torization model that leverages contextual
information for open relation extraction.
Our model is based on factorization ma-
chines and integrates facts from various
sources, such as knowledge bases or open
information extractors, as well as the con-
text in which these facts have been ob-
served. We argue that integrating contex-
tual information—such as metadata about
extraction sources, lexical context, or type
information—significantly improves pre-
diction performance. Open information
extractors, for example, may produce ex-
tractions that are unspecific or ambiguous
when taken out of context. Our experi-
mental study on a large real-world dataset
indicates that CORE has significantly bet-
ter prediction performance than state-of-
the-art approaches when contextual infor-
mation is available.
1 Introduction
Open relation extraction (open RE) is the task of
extracting new facts for a potentially unbounded
set of relations from various sources such as
knowledge bases or natural language text. The
task is closely related to targeted information ex-
traction (IE), which aims to populate a knowledge
base (KB) with new facts for the KB’s relations,
such as wasBornIn(Sepp Herberger, Mannheim).
Existing methods either reason within the KB it-
self (Franz et al., 2009; Nickel et al., 2011; Dru-
mond et al., 2012) or leverage large text corpora
to learn patterns that are indicative of KB rela-
tions (Mintz et al., 2009; Surdeanu et al., 2012;
Min et al., 2013). In both cases, targeted IE meth-
ods are inherently limited to an (often small) set
of predefined relations, i.e., they are not “open”.
The open RE task is also related to open infor-
mation extraction (open IE) (Banko et al., 2007;
Del Corro and Gemulla, 2013), which extracts
large amounts of surface relations and their ar-
guments from natural language text; e.g., “critiz-
ices”(“Dante”, “Catholic Church”).
1
Although
open IE is a domain-independent approach, the ex-
tracted surface relations are purely syntactic and
often ambiguous or noisy. Moreover, open IE
methods usually do not “predict” facts that have
not been explicitly observed in the input data.
Open RE combines the above tasks by predicting
new facts for an open set of relations. The key
challenge in open RE is to reason jointly over the
universal schema consisting of KB relations and
surface relations (Riedel et al., 2013).
A number of matrix or tensor factorization mod-
els have recently been proposed in the context of
relation extraction (Nickel et al., 2012; Riedel et
al., 2013; Huang et al., 2014; Chang et al., 2014).
These models use the available data to learn la-
tent semantic representations of entities (or entity
pairs) and relations in a domain-independent way;
the latent representations are subsequently used to
predict new facts. Existing models often focus
on either targeted IE or open RE. Targeted mod-
els are used for within-KB reasoning; they rely
on the closed-world assumption and often do not
scale with the number of relations. Open RE mod-
els use the open-world assumption, which is more
suitable for the open RE task because the avail-
able data is often highly incomplete. In this paper,
we propose CORE, a novel open RE factorization
model that incorporates and exploits contextual in-
formation to improve prediction performance.
Consider for example the sentence “Tom Peloso
joined Modest Mouse to record their fifth studio
album”. Open IE systems may extract the sur-
face fact “join”(TP, MM) from this sentence. Note
1
We mark (non-disambiguated) mentions of entities and
relations in quotation marks throughout this paper.
1763
that surface relation “join” is unspecific; in this
case, it refers to becoming a member of a music
band (as opposed to, say, an employee of a com-
pany). Most existing open RE systems use the
extracted surface fact for further reasoning, but
they ignore the context from which the fact was
extracted. We argue in this paper that exploiting
contextual information is beneficial for open RE.
For our example, we may use standard NLP tools
like a named entity recognizer to detect that TP is
a person and MM an organization. These coarse-
grained types give us hints about the domain and
range of the “join” relation for the surface fact, al-
though the actual meaning of “join” still remains
opaque. Now imagine that the above sentence was
extracted from a newspaper article published in
the music section. This information can help to
infer that “join” indeed refers to joining a band.
Other contextual information, such as the words
“record” and “album” that occur in the sentence,
further strengthen this interpretation. A context-
aware open RE system should leverage such in-
formation to accurately predict facts like “is band
member of”(TP, MM) and “plays with”(TP, MM).
Note that the prediction of the fact “is band
member of”(TP, MM) is facilitated if we make use
of a KB that knows that TP is a musician and MM
is a music band. If TP and/or MM are not present
in the knowledge base, however, such a reason-
ing does not apply. In our work, we consider both
linked entities (in-KB) and non-linked entity men-
tions (out of-KB). Since KB are often incomplete,
this open approach to handle named entities allows
us to extract facts for all entities, even if they do
not appear in the KB.
In this paper, we propose CORE, a flexible
open RE model that leverages contextual infor-
mation. CORE is inspired by the combined fac-
torization and entity model (FE) of Riedel et al.
(2013). As FE, CORE associates latent semantic
representations with entities, relations, and argu-
ments. In contrast to FE, CORE uses factorization
machines (Rendle, 2012) as its underlying frame-
work, which allows us to incorporate context in a
flexible way. CORE is able to leverage and inte-
grate arbitrary contextual information associated
with the input facts into its open RE factoriza-
tion model. To support reasoning under the open-
world assumption, we propose an efficient method
for parameter estimation in factorization machines
based on Bayesian personalized ranking (Rendle
et al., 2009).
We conducted an experimental study on a real-
world dataset using contextual information along
the lines mentioned above. Our model is exten-
sible, i.e., additional contextual information can
be integrated when available. Even with limited
amount of contextual information used in our ex-
periments, our CORE model provided higher pre-
diction performance than previous models. Our
findings validate the usefulness of contextual in-
formation for the open RE task.
2 Related Work
There is a large body of related work on relation
extraction; we restrict attention to methods that are
most similar to our work.
Targeted IE. Targeted IE methods aim to ex-
tract from natural-language text new instances of
a set of predefined relations, usually taken from
a KB. Most existing methods make use of distant
supervision, i.e., they start with a set of seed in-
stances (pairs of entities) for the relations of inter-
est, search for these seed instances in text, learn
a relation extractor from the so-obtained training
data, and optionally iterate (Mintz et al., 2009;
Surdeanu et al., 2012; Min et al., 2013). Open RE
models are more general then targeted IE meth-
ods in that they additionally reason about surface
relations that do not correspond to KB relations.
For this reason, Riedel et al. (2013) argued and
experimentally validated that open RE models can
outperform targeted IE methods.
Open IE. In contrast to targeted IE, the goal of
open IE is to extract all (or most) relations ex-
pressed in natural-language text, whether or not
these relations are defined in a KB (Banko et al.,
2007; Fader et al., 2011; Del Corro and Gemulla,
2013). The facts obtained by open IE meth-
ods are often not disambiguated, i.e., the enti-
ties and/or the relation are not linked to a knowl-
edge base; e.g., “criticizes”(“Dante”, “Catholic
Church”). The goal of our work is to reason about
extracted open-IE facts and their contextual infor-
mation. Our method is oblivious to the actual open
IE method being used.
Relation clustering. One way to reason about
KB and surface relations is to cluster the relations:
whenever two relations appear in the same clus-
ter, they are treated as synonymous (Hasegawa
et al., 2004; Shinyama and Sekine, 2006; Yao
1764
et al., 2011; Takamatsu et al., 2011; Min et al.,
2012; Akbik et al., 2012; de Lacalle and La-
pata, 2013). For example, if “criticizes” and
“hates” are clustered together, then we may pre-
dict “hates”(“Dante”, “Catholic Church”) from the
above fact (which is actually not true). The general
problem with relation clustering is its “black and
white” approach to relations: either two relations
are the same or they are different. This assumption
generally does not hold for the surface relations
extracted by open IE systems (Riedel et al., 2013);
examples of other types of relationships between
relations include implication or mutual exclusion.
Tensor factorization. Matrix or tensor factor-
ization approaches try to address the above prob-
lem: instead of clustering relations, they directly
predict facts. Both matrix and tensor models learn
and make use of semantic representations of rela-
tions and their arguments. The semantic represen-
tations ideally captures all the information present
in the data; it does not, however, establish a direct
relationship (such as synonymy) between different
KB or surface relations.
Tensor factorization models conceptually
model the input data as a subject×relation×object
tensor, in which non-zero values correspond to
input facts. The tensor is factored to construct
a new tensor in which predicted facts take large
non-zero values. Examples of such tensor fac-
torization models are TripleRank (Franz et al.,
2009), RESCAL (Nickel et al., 2011; Nickel et
al., 2012), or PITF (Drumond et al., 2012). Tensor
factorization models are generally well-suited to
reason within a KB because they are able to pre-
dict relations between arbitrary pairs of subjects
and objects. In the context of open RE, however,
these methods suffer from limited scalability with
the number of relations as well as from their large
prediction space (Chang et al., 2014).
Matrix factorization. The key difference be-
tween matrix and tensor factorization models is
that the former restrict the prediction space, i.e.,
these models generally cannot predict arbitrary
facts. Similar to distant supervision approaches,
matrix factorization models focus on predicting
facts for which some direct evidence exists. In
more detail, most methods restrict the prediction
space to the set of facts for which the subject and
the object share at least some relation in the input
data. For this reason, matrix factorization models
are not suited for in-KB reasoning; an individual
pair of entities usually does not occur in more than
one KB relation. In the open RE context, how-
ever, input relations are semantically related so
that many subject-object pairs belong to multiple
relations. The key advantage of matrix methods
is (1) that this restriction allows them to use addi-
tional features—such as features for each subject-
object pair—and (2) that they scale much better
with the number of relations. Examples of such
matrix factorization models include (Tresp et al.,
2009; Jiang et al., 2012; Fan et al., 2014; Huang
et al., 2014). Chang et al. (2014) have also shown
that a combination of matrix and tensor factoriza-
tion models can be fruitful. Closest to our work
is the “universal schema” matrix factorization ap-
proach of Riedel et al. (2013), which combines a
latent features model, a neighborhood model and
an entity model but does not incorporate context.
Our CORE model follows the universal schema
idea, but uses a more general factorization model,
which includes the information captured by the la-
tent features and entity model (but not the neigh-
borhood model), and incorporates contextual in-
formation.
Using contextual information. It is well known
that contextual information can improve IE meth-
ods. Information such as bag-of-words, part-of-
speech tags, entity types, or parse trees have been
integrated into many existing systems (Mintz et
al., 2009; Zhang et al., 2012; Takamatsu et al.,
2011; Zhou et al., 2007; de Lacalle and Lapata,
2013; Akbik et al., 2012). Our work differs in
that we integrate contextual information into an
open RE system. To do so, we leverage factoriza-
tion machines (Rendle et al., 2011; Rendle, 2012),
which have been successfully applied to exploit
contextual information in the context of recom-
mender systems. We show how to model open RE
data and context with factorization machines and
provide a method for parameter estimation under
the open-world assumption.
3 The CORE Model
Input data. We model the input data as a set
of observations of the form (r, t, c), where r re-
fer to a KB or surface relation, t refer to a
subject-object pair of entities (or entity mentions)
and c to contextual information. An observa-
tion obtained from the example of the introduction
may be (“join”, (TP, MM), { types:(person,org),
1765
1 0 0 1 0 0 0.5 0.5 0 0 0 1 0 1 
0 1 0 0 0 1 0 0 0.5 0.5 1 0 1 0 
1 0 0 0 1 0 0 0.5 0.5 0 0 1 0.6  0.4 
0 0 1 0 0 1 0 0 0.5 0.5 1 0 1 0 
“born in”(x,y) employee(x,y) Caesar,Rome Fermi,Rome Fermi,Sapienza Caesar 
 
Rome Fermi Sapienza person, organization 
person, 
location 
physics history 
relations tuples entities tuple types tuple topics 
x1 
x2 
x3 
x4 
Surface KB Context 
… … 
“professor at”(x,y) 
Figure 1: Example for representing a context-aware open RE problem with CORE
topic:music, word:record, word:album, . . . }). De-
note by R the set of all observed relations, by E
the set of all observed entities, and by T ? E×E
the set of all observed entity pairs, which we refer
to as tuples. A fact takes form r(t) and is com-
posed of a relation r ? R and a tuple t ? T ; e.g.,
“join”(TP, MM). Note that there may be multiple
observations for a fact. Finally, denote by C the
set of all contextual variables; each observation is
associated with a set c ? C of context variables.
In this paper we restrict attention to categorical
context variables; our model can potentially han-
dle continuous context as in (Rendle, 2012).
Problem definition. The open RE task is to pro-
duce a ranked list of tuples T
r
? T for each rela-
tion r ? R; the list is restricted to new tuples, i.e.,
tuples t ? T for which r(t) has not been observed
in the input. The rank of each tuple reflects the
model’s prediction of the likelihood that the corre-
sponding fact is indeed true. A good model thus
ranks correct facts higher than incorrect ones.
Modeling facts. Denote by V = R?T ?E ?C
the set of all observed relations, tuples, entities,
and contextual variables. For ease of exposition,
we refer to the elements of V as variables. We
model the input data in terms of a matrix in which
each row corresponds to a fact (i.e., not an obser-
vation) and each column to a variable. We group
columns according to the type of the variables; e.g,
there are relation columns, tuple columns, entity
columns, and a group of columns for each type of
contextual information. The matrix is populated
such that in each row the values of each column
group sum up to unity, i.e., we normalize values
within column groups. In particular, we set to 1
the values of the variable of the relation and the
tuple of the corresponding fact. We set to 0.5 the
variables corresponding to the two entities referred
to by the fact. An example is shown in Fig. 1.
Here the first row, for instance, corresponds to the
fact “born in”(Caesar, Rome). Note that we model
tuples and entities separately: the entity variables
expose which arguments belong to the fact, the tu-
ple variables expose their order.
Modeling context. As described above, we
model the data in terms of a matrix in which rows
corresponds to facts (instead of observations). The
reasoning behind this approach is as follows. First,
we may see a fact in multiple observations; our
goal is to leverage all the available context. Sec-
ond, facts but not observations are the target of our
predictions. Finally, we are interested in predict-
ing new facts, i.e., facts that we have not seen in
the input data. For these facts, there is no corre-
sponding observation so that we cannot directly
obtain contextual information. To address these
points, our model aggregates the context of rele-
vant observations for each fact; this approach al-
lows us to provide comprehensive contextual in-
formation for both observed and unobserved facts.
We group contextual information by the type of
information: examples include metadata about the
extraction sources (e.g., from an article on music),
types of the entities of a tuple (e.g., (person, lo-
cation)), or the bag-of-words in the sentence from
which an extraction has been obtained. We ag-
gregate the contextual information for each tuple
t ? T ; this tuple-level approach allows us to pro-
vide contextual information for unobserved facts.
In more detail, we count in how many observations
each contextual variable has been associated with
the tuple, and then normalize the count values to
1 within each group of columns. The so-obtained
values can be interpreted as the relative frequen-
cies with which each contextual variable is associ-
ated with the tuple. The contextual information as-
1766
sociated with each fact is given by the aggregated,
normalized context of its tuple.
Fig. 1 shows context information arranged in
two groups: tuple types and tuple topics. We cap-
ture information such as that the tuple (Caesar,
Rome) has only been seen in articles on history
or that tuple (Fermi, Rome) is mentioned in both
physics and history articles (slightly more often in
the former). Since context is associated with tu-
ples, facts 2 and 4 on (Fermi, Sapienza) share con-
textual information. This form of context sharing
(as well as entity sharing) allows us to propagate
information about tuples across various relations.
Factorization model. CORE employs a matrix
factorization model based on factorization ma-
chines and the open-world assumption to capture
latent semantic information about the individual
variables. In particular, we associate with each
variable v ? V a bias term b
v
? R and a latent
feature vector f
v
? R
d
, where the dimensionality
d of the latent feature space is a hyperparameter of
our model. Denote by X the set of rows in the in-
put matrix, which we henceforth refer to as train-
ing points. For each training point x ? X , denote
by x
v
the value of variable v ? V in the corre-
sponding row of the matrix. Our model associates
with training point x ? X a score s(x) computed
as follows:
s(x) =
?
v?V
x
v
b
v
+
?
v
1
?V
?
v
2
?V \{v
1
}
x
v
1
x
v
2
fT
v
1
f
v
2
(1)
Here the bias terms models the contribution of
each individual variable to the final score, whereas
the latent feature vectors model the contribution of
all pairwise interactions between variables. Note
that only bias terms and feature vectors corre-
sponding to non-zero entries in x affect the score
and that x is often sparse. Since we can compute
s(x) in time linear to both the number of nonzero
entries in x and the dimensionality d (Rendle,
2012), score computation is fast. As discussed
below, we (roughly) estimate bias terms and fea-
ture vectors such that observed facts achieve high
scores. We may thus think of each feature vector
as a low-dimensional representation of the global
information contained in the corresponding vari-
able.
Prediction. Given estimates for bias terms and
latent feature vectors, we rank unobserved facts as
follows. Fix a relation r ? R and a tuple t ? T
such that r(t) has not been observed. As indicated
above, our model overcomes the key problem that
there is no observation, and thus no context, for
r(t) by context aggregation and sharing. In par-
ticular, we create an test point xˆ for tuple r(t) in
a way similar to creating data points, i.e., we set
the relation, tuple, and entity variables accordingly
and add the aggregated, normalized context of t.
Once test point xˆ has been created, we can predict
its score s(xˆ) using Eq. (1). We then rank each un-
observed tuple by its so-obtained score, i.e., tuples
with higher scores are ranked higher. The resulting
ranking constitutes the list T
r
of predicted facts for
relation r.
Bayesian personalized ranking. The parame-
ters of our model are given by ? = { b
v
,f
v
| v ?
V }. In approaches based on the closed-world as-
sumption, ? is estimated by minimizing the error
between model predictions and target values (e.g.,
1 for true facts, 0 for false facts). In our setting
of open RE, all our observations are positive, i.e.,
we do not have negative training data. One way
to handle the absence of negative training data is
to associate a target value of 0 to all unobserved
facts. This closed-world approach essentially as-
sumes that all unobserved facts are false, which
may not be a suitable assumption for the sparsely
observed relations of open RE. Following Riedel
et al. (2013), we adopt the open-world assump-
tion instead, i.e., we treat each unobserved facts
as unknown. Since factorization machines origi-
nally require explicit target values (e.g., feedback
in recommender systems), we need to adapt pa-
rameter estimation to the open-world setting.
In more detail, we employ a variant of the
Bayesian personalized ranking (BPR) optimiza-
tion criterion (Rendle et al., 2009). We asso-
ciate with each training point x a set of negative
samples X
?
x
. Each negative sample x
?
? X
?
x
is an unobserved fact with its associated context
(constructed as described in the prediction section
above). Generally, the negative samples x
?
should
be chosen such that they are “less likely” to be true
than fact x. We maximize the following optimiza-
tion criterion:
1
|X|
?
x?X
?
?
?
x
?
?X
?
x
ln?(?(x, x
?
))
|X
?
x
|
? ???
x
?
2
?
?
(2)
where ?(x) =
1
1+e
?x
denotes the logistic func-
tion, ?(x, x
?
) = s(x)? s(x
?
) denotes the differ-
1767
ence of scores, and ?
x
= { b
v
,f
v
| x
v
6= 0 } the
subset of the model parameters relevant for train-
ing point x. Here we use L2 regularization con-
trolled by a single hyperparameter ?. In essence,
the BPR criterion aims to maximize the average
“difference” ln?(?(x, x
?
)) between the score of
fact x and each of its negative samples x
?
, av-
eraged over all facts. In other words, we aim to
score x higher than each x
?
. (Note that under the
closed-world assumption, we would instead con-
sider x
?
as being false.) For a more in-depth dis-
cussion of BPR, see (Rendle et al., 2009).
Sampling negative evidence. To make BPR ef-
fective, the set of negative samples needs to be
chosen carefully. A naive approach is to take the
set of all unobserved facts between each relation
r ? R and each tuple t ? T (or E × E) as the
set X
?
x
. The reasoning is that, after all, we ex-
pect “random” unobserved facts to be less likely
to be true than observed facts. This naive approach
is problematic, however, because the set of nega-
tive samples is independent of x and thus not suf-
ficiently informative (i.e., it contains many irrele-
vant samples).
To overcome this problem, the negative sample
set needs to be related to x in some way. Since we
ultimately use our model to rank tuples for each
relation individually, we consider as negative evi-
dence for x only unobserved facts from the same
relation (Riedel et al., 2013). In more detail, we
(conceptually) build a negative sample set X
?
r
for
each relation r ? R. We include into X
?
r
all facts
r(t)—again, along with their context—such that
t ? T is an observed tuple but r(t) is an unob-
served fact. Thus the subject-object pair t of enti-
ties is not observed with relation r in the input data
(but with some other relation). The set of negative
samples associated with each training point x is
defined by the relation r of the fact contained in x,
that is X
?
x
= X
?
r
. Note that we do not actually
construct the negative sample sets; see below.
Parameter estimation. We maximize Eq. (2)
using stochastic gradient ascent. This allows us
to avoid constructing the sets X
?
x
, which are of-
ten infeasibly large, and worked well in our ex-
periments. In particular, in each stochastic gra-
dient step, we randomly sample a training point
x ? X , and subsequently randomly sample a neg-
ative sample x
?
? X
?
x
. This sampling procedure
can be implemented very efficiently. We then per-
size info
facts 453.9k
14.7k Freebase,
174.1k surface linked,
184.5k surface partially-linked,
80.6k surface non-linked.
relations 4.7k
94 Freebase,
4.6k surface.
tuples 178.5k
69.5k linked,
71.5k partially-linked,
37.5k non-linked.
entities 114.2k
36.8k linked,
77.4k non-linked.
Table 1: Dataset statistics.
form the following ascent step with learning rate
?:
?? ? + ??
?
(
ln?(d(x, x
?
))? ???
x
?
2
)
One can show that the stochastic gradient used in
the formula above is an unbiased estimate of the
gradient of Eq. (2). To speed up parameter es-
timation, we use a parallel lock-free version of
stochastic gradient ascent as in Recht et al. (2011).
This allows our model to handle (reasonably) large
datasets.
4 Experiments
We conducted an experimental study on real-
world data to compare our CORE model with
other state-of-the-art approaches.
2
Our experi-
mental study closely follows the one of Riedel et
al. (2013).
4.1 Experimental Setup
Dataset. We made use of the dataset of Riedel
et al. (2013), but extended it with contextual in-
formation. The dataset consisted of 2.5M sur-
face facts extracted from the New York Times cor-
pus (Sandhaus, 2008), as well as 16k facts from
Freebase. Surface facts have been obtained by
using a named-entity recognizer, which addition-
ally labeled each named entity mention with its
coarse-grained type (i.e., person, organization, lo-
cation, miscellaneous). For each pair of entities
found within a sentence, the shortest dependency
path between these pairs was taken as surface rela-
tion. The entity mentions in each surface fact were
linked to Freebase using a simple string matching
2
Source code, datasets, and supporting material are avail-
able at http://dws.informatik.uni-mannheim.
de/en/resources/software/core/
1768
Relation # PITF NFE CORE CORE+m CORE+t CORE+w CORE+mt CORE+mtw
person/company 208 70 (0.47) 92 (0.81) 91 (0.83) 90 (0.84) 91 (0.87) 92 (0.87) 95 (0.93) 96 (0.94)
person/place of birth 117 1 (0.0) 92 (0.9) 90 (0.88) 92 (0.9) 92 (0.9) 89 (0.87) 93 (0.9) 92 (0.9)
location/containedby 102 7 (0.0) 63 (0.47) 62 (0.47) 63 (0.46) 61 (0.47) 61 (0.44) 62 (0.49) 68 (0.55)
parent/child 88 9 (0.01) 64 (0.6) 64 (0.56) 64 (0.59) 64 (0.62) 64 (0.57) 67 (0.67) 68 (0.63)
person/place of death 71 1 (0.0) 67 (0.93) 67 (0.92) 69 (0.94) 67 (0.93) 67 (0.92) 69 (0.94) 67 (0.92)
person/parents 67 20 (0.1) 51 (0.64) 52 (0.62) 51 (0.61) 49 (0.64) 47 (0.6) 53 (0.67) 53 (0.65)
author/works written 65 24 (0.08) 45 (0.59) 49 (0.62) 51 (0.69) 50 (0.68) 50 (0.68) 51 (0.7) 52 (0.67)
person/nationality 61 21 (0.08) 25 (0.19) 27 (0.17) 28 (0.2) 26 (0.2) 29 (0.19) 27 (0.18) 27 (0.21)
neighbor./neighborhood of 39 3 (0.0) 24 (0.44) 23 (0.45) 26 (0.5) 27 (0.47) 27 (0.49) 30 (0.51) 30 (0.52)
film/directed by 15 7 (0.06) 7 (0.15) 11 (0.22) 9 (0.25) 10 (0.27) 15 (0.52) 11 (0.28) 12 (0.31)
company/founders 11 0 (0.0) 10 (0.34) 10 (0.34) 10 (0.26) 10 (0.21) 10 (0.22) 10 (0.22) 10 (0.24)
sports team/league 11 1 (0.0) 7 (0.24) 10 (0.23) 10 (0.3) 7 (0.22) 10 (0.27) 8 (0.29) 9 (0.3)
structure/architect 11 7 (0.63) 7 (0.63) 9 (0.7) 11 (0.84) 11 (0.73) 11 (0.9) 11 (0.8) 10 (0.77)
team/arena stadium 9 2 (0.01) 6 (0.14) 6 (0.19) 6 (0.18) 6 (0.15) 6 (0.18) 7 (0.29) 7 (0.2)
team owner/teams owned 9 4 (0.05) 6 (0.17) 7 (0.18) 7 (0.33) 6 (0.27) 7 (0.19) 6 (0.22) 8 (0.34)
film/produced by 8 1 (0.03) 4 (0.06) 3 (0.13) 2 (0.12) 3 (0.03) 6 (0.09) 3 (0.13) 6 (0.15)
roadcast/area served 5 0 (0.0) 4 (0.71) 4 (0.73) 4 (0.65) 4 (0.66) 4 (0.66) 5 (0.64) 5 (0.72)
person/religion 5 2 (0.0) 3 (0.21) 2 (0.22) 1 (0.2) 3 (0.22) 3 (0.25) 2 (0.21) 3 (0.21)
composer/compositions 3 2 (0.1) 2 (0.34) 2 (0.35) 2 (0.34) 2 (0.35) 1 (0.33) 2 (0.22) 2 (0.36)
Average MAP
100
#
0.09 0.46 0.47 0.49 0.47 0.49 0.49 0.51
Weighted Average MAP
100
#
0.14 0.64 0.64 0.66 0.67 0.66 0.70 0.70
Table 2: True facts and MAP
100
#
(in parentheses) in the top-100 evaluation-set tuples for Freebase rela-
tions. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best
value per relation in bold (unique winner) or italic (multiple winners). Average weighs are # column
values.
method. If no match was found, the entity men-
tion was kept as is. There were around 2.2M tu-
ples (distinct entity pairs) in this dataset, out of
which 580k were fully linked to Freebase. For
each of these tuples, the dataset additionally in-
cluded all of the corresponding facts from Free-
base. Using the metadata
3
of each New York
Times article, we enriched each surface fact by
the following contextual information: news desk
(e.g., sports desk, foreign desk), descriptors (e.g.,
finances, elections), online section (e.g., sports,
business), section (e.g., a, d), publication year, and
bag-of-words of the sentence from which the sur-
face fact has been extracted.
Training data. From the raw dataset described
above, we filtered out all surface relations with
less than 10 instances, and all tuples with less than
two instances, as in Riedel et al. (2013). Tab. 1
summarizes statistics of the resulting dataset. Here
we considered a fact or tuple as linked if both of
its entities were linked to Freebase, as partially-
linked if only one of its entities was linked, and
as non-linked otherwise. In contrast to previous
work (Riedel et al., 2013; Chang et al., 2014), we
retain partially-linked and non-linked facts in our
dataset.
3
Further information can be found at htps://
catalog.ldc.upenn.edu/LDC2008T19.
Evaluation set. Open RE models produce pre-
dictions for all relations and all tuples. To keep
the experimental study feasible and comparable to
previous studies, we use the full training data but
evaluate each model’s predictions on only the sub-
sample of 10k tuples (? 6% of all tuples) of Riedel
et al. (2013). The subsample consisted of 20%
linked, 40% partially-linked and 40% non-linked
tuples. For each (surface) relation and method, we
predicted the top-100 new facts (not in training)
for the tuples in the subsample.
Considered methods. We compared various
forms of our CORE model with PITF and the
matrix factorization model NFE. Our study fo-
cused on these two factorization models because
they outperformed other models (including non-
factorization models) in previous studies (Riedel
et al., 2013; Chang et al., 2014). All models were
trained with the full training data described above.
PITF (Drumond et al., 2012). PITF is a recent
tensor factorization method designed for within-
KB reasoning. PITF is based on factorization ma-
chines so that we used our scalable CORE imple-
mentation for training the model.
NFE (Riedel et al., 2013). NFE is the full
model proposed in the “universal schema” work
of Riedel et al. (2013). It uses a linear combina-
tion of three component models: a neighborhood
1769
Relation # PITF NFE CORE CORE+m CORE+t CORE+w CORE+mt CORE+mtw
head 162 34 (0.18) 80 (0.66) 83 (0.66) 82 (0.63) 76 (0.57) 77 (0.57) 83 (0.69) 88 (0.73)
scientist 144 44 (0.17) 76 (0.6) 74 (0.55) 73 (0.56) 74 (0.6) 73 (0.59) 78 (0.66) 78 (0.69)
base 133 10 (0.01) 85 (0.71) 86 (0.71) 86 (0.78) 88 (0.79) 85 (0.75) 83 (0.76) 89 (0.8)
visit 118 4 (0.0) 73 (0.6) 75 (0.61) 76 (0.64) 80 (0.68) 74 (0.64) 75 (0.66) 82 (0.74)
attend 92 11 (0.02) 65 (0.58) 64 (0.59) 65 (0.63) 62 (0.6) 66 (0.63) 62 (0.58) 69 (0.64)
adviser 56 2 (0.0) 42 (0.56) 47 (0.58) 44 (0.58) 43 (0.59) 45 (0.63) 43 (0.53) 44 (0.63)
criticize 40 5 (0.0) 31 (0.66) 33 (0.62) 33 (0.7) 33 (0.67) 33 (0.61) 35 (0.69) 37 (0.69)
support 33 3 (0.0) 19 (0.27) 22 (0.28) 18 (0.21) 19 (0.28) 22 (0.27) 23 (0.27) 21 (0.27)
praise 5 0 (0.0) 2 (0.0) 2 (0.01) 4 (0.03) 3 (0.01) 3 (0.02) 5 (0.03) 2 (0.01)
vote 3 2 (0.01) 3 (0.63) 3 (0.63) 3 (0.32) 3 (0.49) 3 (0.51) 3 (0.59) 3 (0.64)
Average MAP
100
#
0.04 0.53 0.53 0.51 0.53 0.53 0.55 0.59
Weighted Average MAP
100
#
0.08 0.62 0.61 0.63 0.63 0.61 0.65 0.70
Table 3: True facts and MAP
100
#
(in parentheses) in the top-100 evaluation-set tuples for surface relations.
We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best value
per relation in bold (unique winner) or italic (multiple winners). Average weighs are # column values.
model (N), a matrix factorization model (F), and
an entity model (E). The F and E models together
are similar (but not equal) to our CORE model
without context. The NFE model outperformed
tensor models (Chang et al., 2014) as well as clus-
tering methods and distantly supervised methods
in the experimental study of Riedel et al. (2013)
for open RE tasks. We use the original source code
of Riedel et al. (2013) for training.
CORE. We include multiple variants of our
model in the experimental study, each differing
by the amount of context being used. We con-
sider as context the article metadata (m), the tu-
ple types (t) and the bag-of-words (w). Each tuple
type is a pair of subject-object types of (e.g. (per-
son, location)). The basic CORE model uses rela-
tions, tuples and entities as variables. We addition-
ally consider the CORE+t, CORE+w, CORE+mt,
and CORE+mtw models, where the suffix indi-
cates which contextual information has been in-
cluded. The total number of variables in the re-
sulting models varied between 300k (CORE) to
350k (CORE+mtw). We used a modified version
of libfm for training.
4
Our version adds support
for BPR and parallelizes the training algorithm.
Methodology. To evaluate the prediction perfor-
mance of each method, we followed Riedel et al.
(2013). We considered a collection of 19 Freebase
relations (Tab. 2) and 10 surface relations (Tab. 3)
and restrict predictions to tuples in the evaluation
set.
Evaluation metrics. For each relation and
method, we computed the top-100 evaluation set
predictions and labeled them manually. We used
4
http://www.libfm.org
as evaluation metrics the mean average precision
defined as:
MAP
100
#
=
?
100
k=1
I
k
· P@k
min{100,#}
(3)
where indicator I
k
takes value 1 if the k-th pre-
diction is true and 0 otherwise, and # denotes the
number of true tuples for the relation in the top-
100 predictions of all models. The denominator
is included to account for the fact that the eval-
uation set may include less than 100 true facts.
MAP
100
#
reflects how many true facts are found
by each method as well as their ranking. If all #
facts are found and ranked top, then MAP
100
#
= 1.
Note that our definition of MAP
100
#
differs slightly
from Riedel et al. (2013); our metric is more ro-
bust because it is based on completely labeled
evaluation data. To compare the prediction per-
formance of each system across multiple rela-
tions, we averaged MAP
100
#
values, in both an un-
weighted and a weighted (by #) fashion.
Parameters. For all systems, we used d = 100
latent factors, ? = 0.01 for all variables, a
constant learning rate of ? = 0.05, and ran
1000 epochs of stochastic gradient ascent. These
choices correspond to the ones of Riedel et al.
(2013); no further tuning was performed.
4.2 Results.
Prediction performance. The results of our ex-
perimental study are summarized in Tab. 2 (Free-
base relations) and Tab. 3 (surface relations). As
mentioned before, all reported numbers are with
respect to our evaluation set. Each entry shows
the number of true facts in the top-100 predictions
and, in parentheses, the MAP
100
#
value. The # col-
1770
author(x,y)  
 
ranked list of tuples 
 
1 (Winston Groom, Forrest Gump) 
2 (D. M. Thomas, White Hotel) 
3 (Roger Rosenblatt, Life Itself) 
4 (Edmund White, Skinned Alive) 
5 (Peter Manso, Brando: The Biography) 
6 (Edward J. Renehan Jr., The Lion's Pride) 
7 (Richard Taruskin, Stravinsky and …) 
… 
 
 
similar relations 
 
0.98  “reviews x by y”(x,y) 
0.97  “book by”(x,y) 
0.95  “author of”(x,y) 
0.95  ” ‘s novel”(x,y) 
0.95  “ ‘s book”(x,y) 
0.91  “who wrote”(x,y) 
0.89  “ ’s poem”(x,y) 
... 
 
 
similar relations 
 
0.87  “scientist”(x,y) 
0.84  “scientist with”(x,y) 
0.80  “professor at”(x,y) 
0.79  “scientist for”(x,y) 
0.78  “neuroscientist at”(x,y) 
0.76  “geneticist at”(x,y) 
0.75  “physicist at”(x,y) 
… 
 
 
ranked list of tuples 
 
1 (Riordan Roett, Johns Hopkins University) 
2 (Dr. R. M. Roberts, University of Missouri) 
3 (Linda Mayes, Yale University) 
4 (Daniel T. Jones, Cardiff Business School) 
5 (Russell Ross, University of Iowa) 
6 (Eva Richter, Kingsborough College) 
7 (M.L. Weidenbaum, Washington University) 
… 
“scientist at”(x,y) 
Figure 2: Some facts predicted by our model for the Freebase relation author(x,y) and the surface relation
”scientist at”(x,y). Most similar relations also reported, using cosine similarity between the correspond-
ing latent feature vectors as distance.
umn list the total number of true facts found by at
least one method. The last two lines show the ag-
gregated MAP
100
#
scores.
We start our discussion with the results for Free-
base relations (Tab. 2). First note that the PITF
model generally did not perform well; as dis-
cussed before, tensor factorization models such
as PITF suffer from a large prediction space and
cannot incorporate tuple-level information. NFE
and CORE, both matrix factorization models, per-
formed better and were on par with each other.
This indicates that our use of factorization ma-
chines does not affect performance in the ab-
sence of context; after all, both methods essen-
tially make use of the same amount of informa-
tion. The key advantage of our model over NFE
is that we can incorporate contextual informa-
tion. Our results indicate that using such informa-
tion indeed improves prediction performance. The
CORE+mtw model performed best overall; it in-
creased the average MAP
100
#
by four points (six
points weighted) compared to the best context-
unware model. Note that for some relations, in-
cluding only subsets of the contextual informa-
tion produced better results than using all contex-
tual information (e.g., film/directed by). We thus
conjecture that extending our model by variable-
specific regularization terms may be beneficial.
Tab. 3 summarizes our results for surface rela-
tions. In general, the relative performance of the
models agreed with the one on Freebase relations.
One difference is that using bag-of-word context
significantly boosted prediction performance. One
reason for this boost is that related surface rela-
tions often share semantically related words (e.g.,
“professor at” and “scientist at”) and may occur in
similar sentences (e.g., mentioning “university”,
“research”, ...).
Anecdotal results. Fig. 2 shows the top test-set
predictions of CORE+mtw for the author and “sci-
entist at” relations. In both cases, we also list re-
lations that have a similar semantic representation
in our model (highest cosine similarity). Note that
semantic similarity of relations is one aspect of our
model; predictions incorporate other aspects such
as context (i.e., two “similar” relations in different
contexts are treated differently).
Training time. We used a machine with 16-
cores Intel Xeon processor and 128GB of mem-
ory. Training CORE took roughly one hour, NFE
roughly six hours (single core only), and training
CORE+mtw took roughly 20 hours. Our imple-
mentation can handle reasonably large data, but
an investigation of faster, more scalable training
methods appears worthwhile.
5 Conclusion
We proposed CORE, a matrix factorization model
for open RE that incorporates contextual informa-
tion. Our model is based on factorization ma-
chines and the open-world assumption, integrates
various forms of contextual information, and is ex-
tensible. Our experimental study suggests that ex-
ploiting context can significantly improve predic-
tion performance.
References
Alan Akbik, Larysa Visengeriyeva, Priska Herger,
Holmer Hemsen, and Alexander L¨oser. 2012. Un-
supervised discovery of relations and discriminative
extraction patterns. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012).
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI).
1771
Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and
Christopher Meek. 2014. Typed tensor decom-
position of knowledge bases for relation extrac-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Oier Lopez de Lacalle and Mirella Lapata. 2013. Un-
supervised relation extraction with general domain
knowledge. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of the 22nd International Conference
on World Wide Web (WWW).
Lucas Drumond, Steffen Rendle, and Lars Schmidt-
Thieme. 2012. Predicting rdf triples in incomplete
knowledge bases with tensor factorization. In Pro-
ceedings of the 27th Annual ACM Symposium on Ap-
plied Computing (SAC).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,
Thomas Fang Zheng, and Edward Y Chang. 2014.
Distant supervision for relation extraction with ma-
trix completion. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In Proceedings of the
8th International Semantic Web Conference (ISWC).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics (ACL).
Yi Huang, Volker Tresp, Maximilian Nickel, Achim
Rettinger, and Hans-Peter Kriegel. 2014. A scalable
approach for statistical learning in semantic graphs.
Semantic Web, 5(1):5–22.
Xueyan Jiang, Volker Tresp, Yi Huang, and Maxi-
milian Nickel. 2012. Link prediction in multi-
relational graphs using additive models. In Proceed-
ings of the 2012 International Workshop on Seman-
tic Technologies meet Recommender Systems & Big
Data (SeRSy).
Bonan Min, Shuming Shi, Ralph Grishman, and Chin-
Yew Lin. 2012. Ensemble semantics for large-scale
unsupervised relation extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (HLT-NAACL).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP).
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th international conference on machine learn-
ing (ICML).
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable ma-
chine learning for linked data. In Proceedings of the
21st International Conference on World Wide Web
(WWW).
Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Pro-
ceedings of the 25th Annual Conference on Neural
Information Processing Systems (NIPS).
Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. Bpr:
Bayesian personalized ranking from implicit feed-
back. In Proceedings of the 25th Conference on Un-
certainty in Artificial Intelligence (UAI).
Steffen Rendle, Zeno Gantner, Christoph Freuden-
thaler, and Lars Schmidt-Thieme. 2011. Fast
context-aware recommendations with factorization
machines. In Proceedings of the 34th international
ACM conference on Research and development in
Information Retrieval (SIGIR).
Steffen Rendle. 2012. Factorization machines with
libfm. ACM Transactions on Intelligent Systems and
Technology (TIST), 3(3):57.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (HLT-
NAACL).
E. Sandhaus. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia,
6(12).
1772
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the 2006 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (HLT-NAACL).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2011. Probabilistic matrix factorization leveraging
contexts for unsupervised relation extraction. In Ad-
vances in Knowledge Discovery and Data Mining,
pages 87–99. Springer.
Volker Tresp, Yi Huang, Markus Bundschus, and
Achim Rettinger. 2009. Materializing and query-
ing learned knowledge. In Proceedings of the 2009
International Workshop on Inductive Reasoning and
Machine Learning for the Semantic Web (IRMLeS).
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Congle Zhang, Raphael Hoffmann, and Daniel S.
Weld. 2012. Ontological smoothing for relation
extraction with minimal supervision. In Proceed-
ings of the 26th Conference on Artificial Intelligence
(AAAI).
GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
1773
