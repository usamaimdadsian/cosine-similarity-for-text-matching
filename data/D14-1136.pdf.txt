Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307,
October 25-29, 2014, Doha, Qatar.
c
©2014 Association for Computational Linguistics
Semantic Parsing Using Content and Context:
A Case Study from Requirements Elicitation
Reut Tsarfaty
Weizmann Institute
Rehovot, Israel
Ilia Pogrebezky
Interdisciplinary Center
Herzliya, Israel
Guy Weiss
Weizmann Institute
Rehovot, Israel
Yaarit Natan
Weizmann Institute
Rehovot, Israel
Smadar Szekely
Weizmann Institute
Rehovot, Israel
David Harel
Weizmann Institute
Rehovot, Israel
Abstract
We present a model for the automatic se-
mantic analysis of requirements elicitation
documents. Our target semantic repre-
sentation employs live sequence charts, a
multi-modal visual language for scenario-
based programming, which can be directly
translated into executable code. The ar-
chitecture we propose integrates sentence-
level and discourse-level processing in a
generative probabilistic framework for the
analysis and disambiguation of individual
sentences in context. We show empiri-
cally that the discourse-based model con-
sistently outperforms the sentence-based
model when constructing a system that re-
flects all the static (entities, properties) and
dynamic (behavioral scenarios) require-
ments in the document.
1 Introduction
Requirements elicitation is a process whereby a
system analyst gathers information from a stake-
holder about a desired system (software or hard-
ware) to be implemented. The knowledge col-
lected by the analyst may be static, referring to
the conceptual model (the entities, their properties,
the possible values) or dynamic, referring to the
behavior that the system should follow (who does
what to whom, when, how, etc). A stakeholder in-
terested in the system typically has a specific static
and dynamic domain in mind, but he or she cannot
necessarily prescribe any formal models or code
artifacts. The term requirements elicitation we use
here refers to a piece of discourse in natural lan-
guage, by means of which a stakeholder commu-
nicates their desiderata to the system analyst.
The role of a system analyst is to understand
the different requirements and transform them into
formal constructs, formal diagrams or executable
code. Moreover, the analyst needs to consolidate
the different pieces of information to uncover a
single shared domain. Studies in software engi-
neering aim to develop intuitive symbolic systems
with which human agents can encode require-
ments that would then be unambiguously trans-
lated into a formal model (Fuchs and Schwitter,
1995; Bryant and Lee, 2002).
More recently, Gordon and Harel (2009) de-
fined a natural fragment of English that can be
used for specifying requirements which can be
effectively translated into live sequence charts
(LSC) (Damm and Harel, 2001; Harel and
Marelly, 2003), a formal language for specifying
the dynamic behavior of reactive systems. How-
ever, the grammar that underlies this language
fragment is highly ambiguous, and all disam-
biguation has to be conducted manually by a hu-
man agent. Indeed, it is commonly accepted that
the more natural a controlled language fragment
is, the harder it is to develop an unambiguous
translation mechanism (Kuhn, 2014).
In this paper we accept the ambiguity of re-
quirements descriptions as a premise, and aim to
answer the following question: can we automati-
cally recover a formal representation of the com-
plete system — one that best reflects the human-
perceived interpretation of the entire document?
Recent advances in natural language processing,
with an eye to semantic parsing (Zettlemoyer and
Collins, 2005; Liang et al., 2011; Artzi and Zettle-
moyer, 2013; Liang and Potts, 2014), use differ-
ent formalisms and various kinds of learning sig-
nals for statistical semantic parsing. In particu-
lar, the model of Lei et al. (2013) induces input
parsers from format descriptions. However, rarely
do these models take into account the entire docu-
ment’s context.
The key idea we promote here is that discourse
context provides substantial disambiguating infor-
mation for sentence analysis. We suggest a novel
1296
Figure 1: An LSC scenario: ”When the user clicks
the button, the display color must change to red.”
model for integrated sentence-level and discourse-
level processing, in a joint generative probabilistic
framework. The input for the requirements elici-
tation task is given in a simplified, yet highly am-
biguous, fragment of English, as specified in Gor-
don and Harel (2009). The output, in contrast, is
a sequence of unambiguous and well-formed live
sequence charts (LSC) (Damm and Harel, 2001;
Harel and Marelly, 2003) describing the dynamic
behavior of the system, tied to a single shared
code-base called a system model (SM).
Our solution takes the form of a hidden markov
model (HMM) where emission probabilities re-
flect the grammaticality and interpretability of tex-
tual requirements via a probabilistic grammar and
transition probabilities model the overlap between
SM snapshots of a single, shared, domain. Using
efficient viterbi decoding, we search for the best
sequence of domain snapshots that has most likely
generated the entire requirements document. We
empirically show that such an integrated model
consistently outperforms a sentence-based model
learned from the same set of data.
The remainder of this document is organized as
follows. In Section 2 we describe the task, and
spell out our formal assumptions concerning the
input and the output. In Section 3 we present
our target semantic representation and a specially
tailored notion of grounding for anchoring the
requirements in a code-base. In Section 4 we
develop our sentence-based and discourse-based
models, and in Section 5 we evaluate the models
on various case studies. In Section 6 we discuss
applications and future extensions, and in Sec-
tion 7 we summarize and conclude.
2 Parsing Requirements Elicitation
Documents: Task Description
There is an inherent discrepancy between the in-
put and the output of the software engineering pro-
cess. The input, system requirements, is specified
in a natural, informal, language. The output, the
system, is ultimately implemented in a formal un-
ambiguous programming language. Can we auto-
matically recover such a formal representation of
a complete system from a set of requirements? In
this work we explore this challenge empirically.
The Input. We assume a scenario-based pro-
gramming paradigm (a.k.a behavioral program-
ming (BP) (Harel et al., 2012)) in which system
development is seen as a process whereby humans
describe the expected behavior of the system by
means of “short-stories”, formally called scenar-
ios (Harel, 2001). We further assume that a given
requirements document describes exactly one sys-
tem, and that each sentence describes a single,
possibly complex, scenario. The requirements we
aim to parse are given in a simplified form of En-
glish (specifically, the English fragment described
in Gordon and Harel (2009)). Contrary to strictly
formal specification languages, which are closed
and unambiguous, this fragment of English em-
ploys an open-ended lexicon and exhibits exten-
sive syntactic and semantic ambiguity.
1
The Output. Our target semantic representation
employs live sequence charts (LSC), a diagram-
matic formal language for scenario-based pro-
gramming (Damm and Harel, 2001). Formally,
LSCs are an extension of the well-known UML
message sequence diagrams (Harel and Maoz,
2006), and they have a direct translation into ex-
ecutable code (Harel and Marelly, 2003).
2
Using
LSC diagrams for software modelling enjoys the
advantages of being easily learnable (Harel and
Gordon, 2009), intuitively interpretable (Eitan et
al., 2011) and straightforwardly amenable to exe-
cution (Harel et al., 2002) and verification (Harel
et al., 2013). The LSC language is particularly
suited for representing natural language require-
ments, since its basic formal constructs, scenar-
ios, nicely align with events, the primitive objects
of Neo-Davidsonian Semantics (Parsons, 1990).
1
Formally, this variant may be viewed as a CNL of degree
P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12).
2
It can be shown that the execution semantics of the LSC
language is embedded in a fragment of a branching temporal
logic called CTL* (Kugler et al., 2005).
1297
Live Sequence Charts and Code Artifacts. A
live sequence chart (LSC) is a diagram that de-
scribes a possible or necessary run of a specified
system. In a single LSC diagram, entities are rep-
resented as vertical lines called lifelines, and inter-
actions between entities are represented using hor-
izontal arrows between lifelines called messages,
connecting a sender to a receiver. Messages may
refer to other entities (or properties of entities) as
arguments. Time in LSCs proceeds from top to
bottom, imposing a partial order on the execution
of messages. LSC messages can be hot (red, “must
happen”) or cold (blue, “may happen”). A mes-
sage may have an execution status, which desig-
nates it as monitored (dashed arrow, “wait for”)
or executed (full arrow, “execute”). The LSC lan-
guage also encompasses conditions and control
structures, and it allows defining requirements in
terms of the negation of charts. Figure 1 illustrates
the LSC for the scenario “When the user clicks
the button, the display color must change to red.”.
The respective system model (SM) is a code-base
hierarchy containing the classes USER, BUTTON,
DISPLAY, the method BUTTON.CLICK() and the
property DISPLAY.COLOR.
3 Formal Settings
In the text-to-code generation task, we aim to im-
plement a prediction function f : D ? M, such
thatD ? D is a piece of discourse consisting of an
ordered set of requirements D = d
1
, d
2
...d
n
, and
f(D) = M ? M is a code-base hierarchy that
grounds the semantic interpretation of D; we de-
note this by M . sem(d
1
, ..., d
n
). We now define
the objects D,M , and describe how to construct
the semantic interpretation function (sem(.)). We
then spell out the notion of grounding (.).
Surface Structures: Let ? be a finite lexicon
and let L
req
? ?
?
be a language for specifying
requirements. We assume the sentences in L
req
have been generated by a context-free grammar
G = ?N ,?, S ? N ,R?, where N is a set of non-
terminals, ? is the aforementioned lexicon, S ?
N is the start symbol andR is a set of context-free
rules {A ? ?|A ? N , ? ? (N ? ?)
?
}. For each
utterance u ? L
req
, we can find a sequential appli-
cation of rules that generates it: u = r
1
? ... ? r
k
;
?i : r
i
? R. We call such a sequence a deriva-
tion of u. These derivations may be graphically
depicted as parse trees, where the utterance u de-
fines the sequence of tree terminals in the leaves.
We define T
req
to be the set of trees strongly
generated by G, and utilize an auxiliary yield
function yield : T
req
? L
req
returning the leaves
of the given tree t ? .L
req
. Different parse-trees
can generate the same utterance, so the task of an-
alyzing the structure of an utterance u ? L
req
is
modeled via a function syn : L
req
? T
req
that
returns the correct, human-perceived, parse of u.
Semantic Structures: Our target semantic rep-
resentation of a requirement d ? L
req
is a dia-
grammatic structure called a live sequence chart
(LSC). The LSC formal definition we provide here
is based on the appendix of Harel and Marelly
(2003), but rephrased in set-theoretic, event-based,
terms. We defined this alternative formalization
in order to make LSCs compatible with Neo-
Davidsonian, event-based, semantic theories. As
a result, this form of LSC formalization is well-
suited for representing the semantics of natural
language utterances.
Let us assume that L is a dictionary of entities
(lifelines), A is a dictionary of actions, P is a dic-
tionary of attribute names and V a dictionary of
attribute values. The set of simple events in the
LSC formal system is defined as follows:
E
active
? L×A× L× (L× P × V )
?
×{hot, cold} × {executed,monitored}
where e = ?l
1
, a, l
2
, {l
i
: p
i
: v
i
}
k
i=3
, temp, exe?
and l
i
? L, a ? A, p
i
? P, temp ? {hot, cold},
exe ? {executed,monitored}. The event e is
called a message in which an action a is carried
over from a sender l
1
to a receiver l
2
.
3
The set
{l
i
: p
i
: v
i
}
k
i=3
depicts a set of attribute:value
pairs provided as arguments to action a. The tem-
perature temp marks the modality of the action
(may, must), and the status exe distinguishes ac-
tions to be taken from actions to be waited for.
An event e can also refer to a state, where a
logical expression is being evaluated over a set of
property:value pairs. We call such an event a con-
dition, and specify the set of possible conditions
as follows:
E
cond
? Exp× (L× P × V )
?
×{hot, cold} × {executed,monitored}
3
The LSC language also distinguishes static lifelines from
dynamically-bound lifelines. For brevity, we omit this from
the formal description of events, and simply assert that it may
be listed as one of the properties of the relevant lifeline.
1298
Specifically, e = ?exp, {l : p : v}
k
i=0
, temp, exe?
is a condition to be evaluated, where l
i
? L, p
i
?
P, v
i
? V, temp ? {hot, cold} and exe ?
{executed,monitored} are as specified above.
The condition exp ? Exp is a first-order logic for-
mula using the usual operators (?,?,?,¬,?,?).
The set {l : p : v}
k
i=0
depicts a (possibly empty)
set of attribute:value pairs that participates as pred-
icates in exp. Executing a condition, that is, evalu-
ating the logical expression specified by exp, also
has a modality (may/must) and an execution status
(performed/waited for).
The LSC language further allows us to define
more complex events by combining partially or-
dered sets of events with control structures.
E
complex
? N × E
cond
×
{?E
c
, <?|?E
c
, <? is a poset }
N is a set of non-negative integers, E
cond
is a set
of conditions as described above, and each ele-
ment ?E
c
, <? is a partially ordered set of events.
This structure allows us to derive three kinds of
control structures:
• e = ?#, ?, ?E,<?? is a loop in which ?E,<?
is executed # times.
• e = ?0, cond, ?E,<?? is a conditioned exe-
cution. If cond holds, ?E,<? is executed.
• e = ?#, {cond}
#
i=1
, {?E
c
, <?}
#
i=1
? is a
switch: in case i, if the condition i holds,
?E
c
, <?
i
is executed.
Definition 1 (LSC) An LSC c = ?E,<? is a
partially ordered set of events such that
?e ? E : e ? E
active
? e ? E
cond
? e ? E
complex
Grounded Semantics: The information repre-
sented in the LSC provides the recipe for a rig-
orous construction of the code-base that will im-
plement the program. This code-base is said to
ground the semantic representation. For exam-
ple, if our target programming language is an
Object-Oriented programming language such as
Java, then the code-base will include the objects,
the methods and the properties that are minimally
required for executing the scenario that is repre-
sented by the LSC. We refer to this code-base as
a system model (henceforth, SM), and define it as
follows.
Definition 2: (SM) Let L
m
be a set of imple-
mented objects, A
m
a set of implemented meth-
ods, P
m
a set of arguments and V
m
argument
values. We further define the auxiliary functions
methods : A
m
? L
m
, props : P
m
? L
m
and
values : V
m
? L
m
× P
m
, for identifying the
entity l ? L
m
that implements the method a ?
A
m
, the entity l ? L
m
that contains the property
p ? P
m
, and the entity property ?l, p? ? L
m
×P
m
that assumes that value v ? V
m
, respectively. A
system model (SM) is a tuple m representing the
implemented architecture.
m = ?L
m
, A
m
, P
m
, V
m
,methods, props, values?
Analogously to interpretation functions in logic
and natural language semantics, we assume here
an implementation function, denoted [[.]], which
maps each formal entity in the LSC semantic rep-
resentation to its instantiation in the code-base.
Using this function we define a notion of ground-
ing that captures the fact that a certain code-base
permits the execution of a given LSC c.
Definition 3(a): (Grounding) LetM be the set
of system models and let C be the set of LSC
charts. We say that m grounds c = ?E,<?, and
write m . c, if ?e ? E : m . e, where:
• if e ? E
active
then
m . e?
[[l
1
]], [[l
2
]] ? L &
[[a]] ? methods([[l
2
]]) &
?i : ?l : p : v?
i
? [[l]] ? L
m
&[[p]] ?
props[[l]]&v ? values([[l]], [[p]])
• if e ? E
cond
then
m . e?
?i : ?l : p : v?
i
? [[l]] ? L
m
&[[p]] ?
props[[l]]&v ? values([[l]], [[p]])
• if e = ?#, e
s
, ?E
c
, <? ? E
complex
then
m . e?m . e
s
& ?e
?
? E
c
: m . e
?
We have thus far defined how the semantics of
a single LSC can be grounded in a single SM. In
the real world, however, a requirements document
typically contains multiple different requirements,
but it is interpreted as a complete whole. The de-
sired SM is then one that represents a single do-
main shared by all the specified requirements. Let
us then assume a document d = d
1
, ..., d
n
con-
taining n requirements, where ?i : d
i
? L
req
, and
1299
let unionsq be a unification operation that returns the for-
mal unification of two SMs if such exists, and an
empty SM otherwise. We define a discourse in-
terpretation function sem(d) that returns a single
SM for the entire document, where different men-
tions across sentences may share the same refer-
ence. The discourse interpretation function sem
can be as simple as unifying all individual SMs
for d
i
, and asserting that all elements that have the
same name in different SMs refer to a single ele-
ment in the overall SM. Or, it can be as complex as
taking into account synonyms (“clicks the button”
and “presses the button”), anaphora (“when the
user clicks the button, it changes colour”), bind-
ing (“when the user clicks any button, this button
is highlighted”), and so on. We can now define the
grounding of an entire requirements document.
Definition 3(b): (Grounding) Let d = d
1
...d
n
be a requirements document and let m = m
1
...m
n
be a sequence of system models. M = ?m,unionsq? is
a sequence of models and a unification operation,
and M . sem(d) if and only if ?i : m
i
. sem(d
i
)
and ((m
1
unionsqm
2
).... unionsqm
n
) . sem(d
1
, ...., d
n
).
In this work we assume that sem(d) is a simple
discourse interpretation function, where entities,
methods, properties, etc. that are referred to using
the same name in different local SMs refer to a sin-
gle element in the overall code-base. This simple
assumption already carries a substantial amount of
disambiguating information concerning individual
requirements. For example, assume that we have
seen a “click” method over a “button” object in
sentence i. This may help us disambiguate future
attachment ambiguity, favoring structures where
a “button” is attached to “click” over other at-
tachment alternatives. Our goal is then to model
discourse-level context for supporting the accurate
semantic analysis of individual requirements.
4 Probabilistic Modeling
In this section we set out to explicitly model
the requirement’s context, formally captured as a
document-level SM, in order to support the accu-
rate disambiguation of the requirements’ content.
We first specify our probabilistic content model,
a sentence-level model which is based on a prob-
abilistic grammar augmented with compositional
semantic rules. We then specify our probabilistic
context model, a document-level sequence model
that takes into account the content as well as the
relation between SMs at different time points.
4.1 Sentence-Based Modeling
The task of our sentence-based model is to learn
a function that maps each requirement sentence
to its correct LSC diagram and SM snapshot.
In a nutshell, we do this via a (partially lexi-
calized) probabilistic context-free grammar aug-
mented with a semantic interpretation function.
More formally, given a discourse D = d
1
...d
n
we think of each d
i
as having been generated by
a probabilistic context-free grammar (PCFG) G.
The syntactic analysis of d
i
may be ambiguous,
so we first implement a syntactic analysis function
syn : L
req
? T
req
using a probabilistic model
that selects the most likely syntax tree t of each
d individually. We can simplify syn(d), with d
constant with respect to the maximization:
syn(d) = argmax
t?T
req
P (t|d)
= argmax
t?T
req
P (t,d)
p(d)
= argmax
t?T
req
P (t, d)
= argmax
t?{t|t?T
req
,yield(t)=d}
P (t)
Because of the context-freeness assumption, it
holds that P (t) =
?
r?der(t)
P (r), where der(t)
returns the rules that derive t. The resulting proba-
bility distribution P : T
req
? [0, 1] defines a prob-
abilistic language model over all requirements d ?
L
req
, i.e.,
?
d?L
req
?
t?T
req
,yield(t)=d
P (t) = 1.
We assume a function sem : T ? C mapping
syntactic parse trees to semantic constructs in the
LSC language. Syntactic parse trees are complex
entities, assigning structures to the flat sequences
of words. The principle of compositionality as-
serts that the meaning of a complex syntactic en-
tity is a function of the meaning of its parts and
their mode of combination. Here, the semantics of
a tree t ? T
req
is derived compositionally from the
interpretation of the rules in the grammar G. We
overload the sem notation to define sem : R ? C
as a function assigning rules to LSC constructs
(events or parts of events),
4
with
ˆ
? merging the
resulting sets of events. Our sentence-based com-
positional semantics is summarized as:
sem(u) = sem(syn(u)) = sem(r
1
? ... ? r
n
) =
sem(r
1
)
ˆ
?...
ˆ
?sem(r
n
) = c
1
ˆ
?...
ˆ
?c
n
= c
4
Here, it suffices to say that sem maps edges in the
syntax tree to functions in the API of an existing LSC
editor. For example: sem(NP ? DET NN) =
fCreateObject(DET.sem,NN.sem). We specify the
function sem in the supplementary materials. The code of
sem is available as part of PlayGo (www.playgo.co).
1300
For a single chart c, one can easily construct an
implementation for every entity, action and prop-
erty in the chart. Then, by design, we get an
SM m such that m . c. To construct the SM of
the entire discourse in the sentence-based model
we simply return f(d
1
, ..., d
n
) = unionsq
n
i=1
m
i
where
?i : m
i
. sem(syn(d
i
)) and unionsq unifies different
mentions of the same string to a single element.
4.2 Discourse-Based Modeling
We assume a given document D ? D and aim to
find the most probable system modelM ?M that
satisfies the requirements. We assume that M re-
flects a single domain that the stakeholders have in
mind, and we are provided with an ambiguous nat-
ural language evidence, an elicited discourseD, in
which they convey it. We instantiate this view as a
noisy channel model (Shannon, 1948), which pro-
vides the foundation for many NLP applications,
such as speech recognition (Bahl et al., 1983) and
machine translation (Brown et al., 1993).
According to the noisy channel model, when a
signal is received it does not uniquely identify the
message being sent. A probabilistic model is then
used to decode the original message. In our case,
the signal is the discourse and the message is the
overall system model. In formal terms, we want to
find a model M that maximises the following:
f(D) = argmax
M?M
P (M |D)
We can simplify further, using Bayes law, where
D is constant with respect to the maximisation.
f(D) = argmax
M?M
P (M |D)
= argmax
M?M
P (D|M)×P (M)
P (D)
= argmax
M?M
P (D|M)× P (M)
We would thus like to estimate two types of prob-
ability distributions, P (M) over the source and
P (D|M) over the channel.
BothM andD are structured objects with com-
plex internal structure. In order to assign prob-
abilities to objects involving such complex struc-
tures it is customary to break them down into sim-
pler, more basic, events. We know that D =
d
1
, d
2
, ..., d
n
is composed of n individual sen-
tences, each representing a certain aspect of the
model M . We assume a sequence of snapshots of
M that correspond to the timestamps 1...n, that is:
m
1
,m
2
, ...,m
n
? M where ?i : m
i
. sem(d
i
).
The complete SM is given by the union of the
different snapshots reflected in different require-
ments, i.e., M =
?
i
m
i
. We then rephrase:
P (M) = P (m
1
, ...,m
n
)
P (D|M) = P (d
1
, ...., d
n
|m
1
, ...,m
n
)
These events may be seen as points in a high di-
mensional space. In actuality, they are too com-
plex and would be too hard to estimate directly.
We then define two independence assumptions.
First, we assume that a system model snapshot at
time i depends only on k previous snapshots (a
stationary distribution). Secondly, we assume that
each sentence i depends only on the SM snapshot
at time i. We now get:
P (m
1
...m
n
) ?
?
i
P (m
i
|m
i?1
...m
i?k
)
P (d
1
...d
n
|m
1
...m
n
) ?
?
i
P (d
i
|m
i
)
Furthermore, assuming bi-gram transitions, our
objective function is now represented as follows:
f(D) = argmax
M?M
n
?
i=1
P (m
i
|m
i?1
)P (d
i
|m
i
)
Note that m
0
may be empty if the system is im-
plemented from scratch, and non-empty if the re-
quirements assume an existing code-base, which
makes p(m
1
|m
0
) a non-trivial transition.
4.3 Training and Decoding
Our model is in essence a Hidden Markov Model
in which states capture SM snapshots, state-
transition probabilities model transitions between
SM snapshots, and emission probabilities model
the verbal description of each state. To implement
this, we need to implement a decoding algorithm
that searches through all possible state sequences,
and a training algorithm that can automatically
learn the values of the still rather complex param-
eters P (m
i
|m
i?1
), P (d
i
|m
i
) from data.
f(D) = argmax
M?M
? ?? ?
decoding
n
?
i=1
P (m
i
|m
i?1
)P (d
i
|m
i
)
? ?? ?
training
Training: We assume a supervised training set-
ting in which we are given a set of examples anno-
tated by a human expert. For instance, these can
be requirements an analyst has formulated and en-
coded using an LSC editor, while manually pro-
viding disambiguating information. We are pro-
vided with a set of pairs {D
i
,M
i
}
n
i=1
containing n
documents, where each of the pairs in i = 1..n is a
1301
tuple set {d
ij
, t
ij
, c
ij
,m
ij
}
n
i
j=1
. For all i, j, it holds
that t
ij
= syn(d
ij
), c
ij
= sem(t
ij
), and m
ij
.
sem(syn(d
ij
)). The union of the n
i
SM snapshots
yields the entire model unionsq
j
m
i
j
= M
i
, that satisfies
the set of requirements M
i
. sem(d
i1
, ..., d
in
i
).
(i) Emission Parameters Our emission parame-
ters P (d
i
|m
i
) represent the probability of a verbal
description of a requirement given an SM snap-
shot which grounds the semantics of the descrip-
tion. A single SM may result from different syn-
tactic derivations. We calculate this probability
by marginalizing over the syntactic trees that are
grounded in the same SM snapshot.
P (d,m)
P (m)
=
?
t?{t|yield(t)=d,m.sem(t)}
P (t)
?
t?{t|t?T
req
,m.sem(t)}
P (t)
The probability of P (t) is estimated using a tree-
bank PCFG (Charniak, 1996), based on all pairs
?d
ij
, t
ij
? in the annotated corpus. We estimate
rule probabilities using maximum-likelihood es-
timates, and use simple smoothing for unknown
lexical items, using rare-words distributions.
(ii) Transition Parameters Our transition pa-
rameters P (m
i
|m
i?1
) represent the amount of
overlap between the SM snapshots. We look at the
current and the previous system model, and aim
to estimate how likely the current SM is given the
previous one. There are different assumptions that
may underly this probability distribution, reflect-
ing different principles of human communication.
We first define a generic estimator as follows:
ˆ
P (m
i
|m
j
) =
gap(m
i
,m
j
)
?
m
j
gap(m
i
,m
j
)
where gap(.) quantifies the information sharing
between SM snapshots. Regardless of the im-
plementation of gap, it can be easily shown that
ˆ
P is a conditional probability distribution where
ˆ
P : M × M ? [0, 1] and, for all m
i
,m
j
, :
?
m
j
ˆ
P (m
i
|m
j
) = 1. (For efficiency reasons, we
considerM to be a restricted universe that is con-
sidered be the decoder, as specified shortly.)
We define different gap implementations, re-
flecting different assumptions about the discourse.
Our first assumption here is that different SM
snapshots refer to the same conceptual world, so
there should be a large overlap between them. We
call this the max-overlap assumption. A second
assumption is that, in collaborative communica-
tion, a new requirement will only be stated if it
Transition: gap(m
curr
,m
prev
)
max-overlap
|set(m
curr
)?set(m
prev
)|
|set(m
curr
)|
max-expansion 1?
|set(m
curr
)?set(m
prev)
|
|set(m
prev
)?set(m
curr
)|
min-distance 1?
ted(m
prev
,m
curr
)
|set(m
prev
)|+|set(m
curr
)|
Table 1: Quantifying the gap between snapshots.
set(m
i
) is a set of nodes marked by path to root.
provides new information, akin to Grice (1975).
This is the max-expansion assumption. An addi-
tional assumption prefers “easy” transitions over
“hard” ones, this is the min-distance assumption.
The different gap calculations are listed in Table 1.
Decoding An input document contains n re-
quirements. Our decoding algorithm considers the
N-best syntactic analyses for each requirement. At
each time step i = 1...n we assume N, states rep-
resenting the semantics of the N best syntax trees,
retrieved via a CKY chart parser. Thus, setting
N = 1 is equal to a sentence-based model, in
which for each sentence we simply select the most
likely tree according to a probabilistic grammar,
and construct a semantic representation for it.
For each document of length n, we assume that
our entire universe of system models M is com-
posed of N × n SM snapshots, reflecting the N
most-likely analyses of n sentences, as provided
by the probabilistic syntactic model. (As shall be
seen shortly, even with this restricted
5
universe ap-
proximating M, our discourse-based model pro-
vides substantial improvements over a sentence-
based model).
Our discourse-based model is an HMM where
each requirement is an observed signal, and each
i = 1..N is a state representing the SM that
grounds the i th best tree. Because of the
Markov independence assumption our setup satis-
fies the optimal subproblem and overlapping prob-
lem properties, and we can use efficient viterbi de-
coding to exhaustively search through the differ-
ent state sequences, and find the most probable
sequence that has generated the sequence of re-
quirements according to our discourse-based prob-
abilistic model.
5
This restriction is akin to pseudo-likelihood estimation,
as in Arnold and Strauss (1991). In pseudo-likelihood estima-
tion, instead of normalizing over the entire set of elements,
one uses a subset that reflects only the possible outcomes.
Here, instead of summing SM probabilities over all possible
sentences in the language, we sum up the SM analyses of the
sentences observed in the document only. This estimation
could also be addressed via, e.g., sampling methods.
1302
The overall complexity decoding a document
with n sentences of which max length is l, using a
grammar G of size |G| and a fixed N , is given by:
O(n× l
3
× |G|
3
+ l
2
×N
2
× n+ n
3
×N
2
)
We can break this expression down as follows: (i)
In O(n × l
3
× |G|
3
) we generate N best trees for
each one of the n requirements, using a CKY chart
(Younger, 1967). (ii) In O(l
2
×N
2
×n) we create
the universe M based on the N best trees of the
n requirements, and calculate N × N transitions.
(iii) InO((N×n)
2
×n) = O(N
2
×n
3
) we decode
the n×N grid using Viterbi (1967) decoding.
5 Experiments
Goal. We set out to evaluate the accuracy of a se-
mantic parser for requirements documents, in the
two modes of analysis presented above. Our eval-
uation methodology is as standardly assumed in
machine learning and NLP: given a set of anno-
tated examples — that is, given a set of require-
ments documents, where each requirement is an-
notated with its correct LSC representation and
each document is associated with a complete SM
— we partition this set into a training set and a test
set that are disjoint. We train our statistical model
on the examples in the training set and automati-
cally analyze the requirements in the test set. We
then compare the predicted semantic analyses of
the test set with the human-annotated (henceforth,
gold) semantic analyses of this test set, and empir-
ically quantify our prediction accuracy.
Metrics. Our semantic LSC objects have the
form of a tree (reflecting the sequence of nested
events in our scenarios). Therefore, we can use
standard tree evaluation metrics, such as ParseE-
val (Black et al., 1992), to evaluate the accuracy
of the prediction. Overall, we define three metrics
to evaluate the accuracy of the LSC trees:
POS: the POS metric is the percentage of
part-of-speech tags predicted correctly.
LSC-F1: F1 is the harmonic means of the
precision and recall of the predicted tree.
LSC-EM: EM is 1 if the predicted tree is an
exact match to the gold tree, and 0 otherwise.
In the case of SM trees, as opposed to the LSC
trees, we cannot assume identity of the yield be-
tween the gold and parse trees for the same sen-
System #Scenarios avg sentence length
Phone 21 24.33
WristWatch 15 29.8
Chess 18 15.83
Baby Monitor 14 20
Total 68 22.395
Table 2: Seed Gold-Annotated Requirements
N=1 POS LSC-F1 LSC-EM SM-TED SM-EM
Gen-Only 85.52 84.40 9.52 84.25 9.52
Gen+Seed 91.59 88.05 14.29 85.17 14.29
Table 3: Sentence-Based modeling: Accuracy re-
sults on the Phone development set.
tence,
6
so we cannot use ParseEval. Therefore, we
implement a distance-based metrics in the spirit of
Tsarfaty et al. (2012). Then, to evaluate the accu-
racy of the SM, we use two kinds of scores:
SM-TED: TED is the normalized edit dis-
tance between the predicted and gold SM
trees, subtracted from a unity.
SM-EM: EM is 1 if the predicted SM is an
exact match with the gold SM, 0 otherwise.
Data. We have a small seed of correctly anno-
tated requirements-specification case studies that
describe simple reactive systems in the LSC lan-
guage. Each document contains a sequence of
requirements, each of which is annotated with
the correct LSC diagram. The entire program is
grounded in a java implementation. As training
data, we use the case studies provided by Gordon
and Harel (2009). Table 2 lists the case studies and
basic statistics concerning these data.
As our annotated seed is quite small, it is hard to
generalize from it to unseen examples. In particu-
lar, we are not guaranteed to have observed all pos-
sible structures that are theoretically permitted by
the assumed grammar. To cope with this, we cre-
ate a synthetic set of examples using the grammar
of Gordon and Harel (2009) in generation mode,
and randomly generate trees t ? T
req
.
The grammar we use to generate the synthetic
examples clearly over-generates. That is to say,
it creates many trees that do not have a sound in-
terpretation. In fact, only 3000 our of 10000 gen-
erated examples turn out to have a sound seman-
tic interpretation grounded in an SM. Nonetheless,
these data allow us to smooth the syntactic distri-
butions that are observed in the seed, and increase
the coverage of the grammar learned from it.
6
This is because the LSC trees are predicted bottom up
and the SM trees are predicted top-down.
1303
Results. Table 3 presents the results for pars-
ing the Phone document, our development set,
with the sentence-based model, varying the train-
ing data. We see that despite the small size of the
seed, adding it to our set if synthetics examples
substantially improves results over a model trained
on synthetic examples only.
In our next experiment, we provide empirical
upper-bounds and lower-bounds for the discourse-
based model. Table 4 presents the results of
the discourse-based model for N > 1 on the
Phone example. Gen-Only presents the results of
the discourse-based model with a PCFG learned
from synthetic trees only, incorporating transitions
obeying the max-overlap assumption. Already
here, we see a mild improvement for N > 1 rel-
ative to the N = 1 results, indicating that even a
weak signal such as the overlap between neighbor-
ing sentences already improves sentence disam-
biguation in context. We next present the results of
an Oracle experiment, where every requirement is
assigned the highest scoring tree in terms of LSC-
F1 with respect to the gold tree, keeping the same
transitions. Again we see that results improve with
N , indicating that the syntactic model alone does
not provide optimal disambiguation. These re-
sults provides an upper bound on the parser perfor-
mance for each N . Gen+Seed presents results of
the discourse-based model where the PCFG inter-
polates the seed set and the synthetic train set, with
max-overlap transitions. Here, we see larger im-
provements over the synthetic-only PCFG. That is,
modeling grammaticality of individual sentences
improves the interpretation of the document.
Next we compare the performance for differ-
ent implementations of the gap(m
i
,m
j
) function.
We estimate probability distributions that reflect
each of the assumptions we discussed, and add
an additional method called hybrid, in which we
interpolate the max-expansion and max-overlap
estimates (equal weights). In Table 5, the trend
from the previous experiment persists. Notably,
the hybrid model provides a larger error reduc-
tion than its components used separately, indicat-
ing that in order to capture discourse context we
may need to balance possibly conflicting factors.
In no emissions we rely solely on the probability
of state transitions, and again increasing N leads
to improvement. This result confirms that con-
text is indispensable for sentence interpretation —
even when probabilities for the sentence’s seman-
System N=2 4 8 16 32 64 128
Gen-Only
POS 85.52 86.30 87.67 88.45 88.85 88.85 88.85
LSC-F1 84.40 85.35 86.31 87.51 88.81 89.30 89.51
LSC-EM 9.52 9.52 14.29 14.29 14.29 14.29 14.29
SM-TED 84.25 85.94 89.14 91.90 92.81 93.31 92.70
SM-EM 9.52 19.05 33.33 33.33 33.33 38.10 33.33
Gen+Seed
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Oracle
POS 91.98 93.54 94.91 95.30 96.09 96.67 96.87
LSC-F1 88.73 91.33 93.19 94.39 95.11 95.91 96.70
LSC-EM 23.81 42.86 61.90 61.90 66.67 76.19 76.19
SM-TED 86.54 91.28 94.28 94.88 96.24 97.51 98.80
SM-EM 23.81 42.86 66.67 71.43 76.19 76.19 76.19
Table 4: Discourse-Based Modeling: Accuracy re-
sults on the Phone dev set. The Oracle selects the
highest scoring LSC tree among the N-candidates,
providing an upper bound on accuracy. Gen-Only
selects the most probable tree, relying on synthetic
examples only, providing a lower bound.
tics (content) are entirely absent.
We finally perform a cross-fold experiment in
which we leave one document out as a test set
and take the rest as our seed. The results are pro-
vided in Table 6. The discourse-based model out-
performs the sentence-based model N = 1 in all
cases. Moreover, the drop in N = 128 for Phone
seems incidental to this set, and the other cases
level off beforehand. Despite our small seed, the
persistent improvement on all metrics is consistent
with our hypothesis that modeling the interpreta-
tion process within the discourse has substantial
benefits for automatic understanding of the text.
6 Applications and Discussion
The statistical models we present here are ap-
plied in the context of PlayGo,
7
a comprehensive
tool for behavioral, scenario-based, programming.
PlayGo now provides two modes of playing-in
natural language requirements: interactive play-in,
where a user manually disambiguates the analyses
of the requirements (Gordon and Harel, 2009), and
statistical play-in, where disambiguation decisions
are taken using our discourse-based model.
The fragment of English we use is very ex-
pressive. It covers not only entities and predi-
cates, but also temporal and aspectual information,
modalities, and program flow. Beyond that, we as-
sume an open-ended lexicon. Overall, we are not
7
www.playgo.co.
1304
Transitions N=2 4 8 16 32 64 128
Min Dist
POS 91.98 92.76 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.39 89.77 91.00 90.99 91.81 92.09 91.73
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.71 94.38 93.81 95.57 96.43 94.53
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Max Overlap
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Max Expand
POS 91.98 92.76 93.74 93.54 94.32 94.52 93.93
LSC-F1 88.39 89.71 91.00 90.99 91.68 91.96 91.60
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.93 93.75 93.18 94.79 95.66 93.75
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Hybrid
POS 91.78 92.95 93.93 93.74 94.72 94.91 94.32
LSC-F1 88.11 90.18 91.34 91.33 92.15 92.42 92.07
LSC-EM 19.05 38.10 47.62 47.62 47.62 47.62 47.62
SM-TED 85.49 90.78 93.66 93.09 94.87 95.75 93.83
SM-EM 19.05 38.10 57.14 57.14 57.14 57.14 57.14
No Emissions
POS 91.78 91.98 92.37 92.37 92.17 92.76 93.15
LSC-F1 88.11 88.79 89.12 89.12 89.39 89.67 89.89
LSC-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
SM-TED 85.49 85.74 85.82 85.82 85.87 86.85 86.92
SM-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
Table 5: Discourse-Based modeling: Experiments
on the Phone development set. Estimation proce-
dure for transition probabilities. All experiments
use the Gen+Seed emission probablities.
only translating English sentences into executable
LSCs — we provide a fully generative model for
translating a complete document (text) into a com-
plete system model (code).
This text-to-code problem may be thought of as
a machine translation (MT) problem, where one
aims to translate sentences in English to the formal
language of LSCs. However, standard statistical
MT techniques rely on the assumption that textual
requirements and code are aligned at a sentence
level. Creating a formal model that aligns text and
code on a sentence-by-sentence basis is precisely
our technical contribution in Section 3.
To our knowledge, modeling syntax and dis-
course processing via a fully joint generative
model, where a discourse level HMM is in-
terleaved with PCFG sentence-based emissions,
is novel. By plugging in different models for
p(d|m), different languages may be parsed. This
method may further be utilized for relating content
and context in other tasks: parsing and document-
level NER, parsing and document-level IE, etc. To
do so, one only needs to redefine the PCFG (emis-
sions) and state-overlap (transition) parameters, as
appropriate for their data.
8
8
Our code, annotated data, four case studies, and the LSC
Data Set N=1 32 64 128
Baby Monitor
POS 94.29 96.07 96.07 96.07
LSC-F1 91.50 94.96 94.96 94.96
LSC-EM 14.29 21.43 21.43 21.43
SM-TED 88.63 91.11 91.11 91.11
SM-EM 28.57 50.00 50.00 50.00
Chess
POS 92.63 93.68 93.68 93.68
LSC-F1 95.79 96.16 96.16 96.16
LSC-EM 5.56 11.11 11.11 11.11
SM-TED 94.90 97.10 97.10 97.10
SM-EM 61.11 66.67 66.67 66.67
Phone
POS 91.59 94.72 94.91 94.32
LSC-F1 88.05 92.15 92.42 92.07
LSC-EM 14.29 47.62 47.62 47.62
SM-TED 85.17 94.87 95.75 93.83
SM-EM 14.29 57.14 57.14 57.14
WristWatch
POS 34.23 34.45 34.45 34.45
LSC-F1 50.06 51.05 51.05 51.05
LSC-EM 26.67 26.67 26.67 26.67
SM-TED 71.15 72.73 72.73 72.73
SM-EM 26.67 33.33 33.33 33.33
Table 6: Cross-Fold Validation for N=1..128.
Seed+Generated emissions, Hybrid transitions.
7 Conclusion
The requirements understanding task presents an
exciting challenge for CL/NLP. We ought to au-
tomatically discover the entities in the discourse,
the actions they take, conditions, temporal con-
straints, and execution modalities. Furthermore, it
requires us to extract a single ontology that satis-
fies all individual requirements. The contributions
of this paper are three-fold: we formalize the text-
to-code prediction task, propose a semantic rep-
resentation with well-defined grounding, and em-
pirically evaluate models for this prediction. We
show consistent improvement of discourse-based
over sentence-based models, in all case studies.
In the future, we intend to extend this model for
interpreting requirements in un-restricted, or less-
restricted, English, endowed with a more sophisti-
cated discourse interpretation function.
Acknowledgements
We thank Shahar Maoz, Rami Marelly, Yoav
Goldberg and three anonymous reviewers for
their insightful comments on an earlier draft.
This research was supported by an Advanced
Research Grant to D. Harel from the Euro-
pean Research Council (ERC) under the Eu-
ropean Community‘s Seventh Framework Pro-
gramme (FP7/2007-2013), and by a grant to D.
Harel from the Israel Science Foundation (ISF).
visual editor are available via http://wiki.weizmann.
ac.il/playgo/index.php/Download_PlayGo.
1305
References
B. C. Arnold and D. Strauss. 1991. Pseudolikelihood
Estimation: Some Examples. Sankhy¯a: The Indian
Journal of Statistics, Series B (1960-2002), 53(2).
Y. Artzi and L. Zettlemoyer. 2013. Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. TACL, 1:49–62.
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A
maximum likelihood approach to continuous speech
recognition. IEEE Trans. Pattern Anal. Mach. In-
tell., 5(2):179–190.
E. Black, J. D. Lafferty, and S. Roukos. 1992. De-
velopment and evaluation of a broad-coverage prob-
abilistic grammar of English-language computer
manuals. In Proceedings of ACL, pages 185–192.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Comput.
Linguist., 19(2):263–311, June.
B. Bryant and B.-S. Lee. 2002. Two-level gram-
mar as an object-oriented requirements specifica-
tion language. In Proceedings of the 35th Annual
Hawaii International Conference on System Sci-
ences (HICSS’02)-Volume 9 - Volume 9, HICSS ’02,
pages 280–, Washington, DC, USA. IEEE Computer
Society.
E. Charniak. 1996. Tree-bank grammars. In Proceed-
ings of the Thirteenth National Conference on Arti-
ficial Intelligence, pages 1031–1036.
W. Damm and D. Harel. 2001. LSCs: Breathing life
into message sequence charts. Form. Methods Syst.
Des., 19(1):45–80, July.
N. Eitan, M. Gordon, D. Harel, A. Marron, and
G. Weiss. 2011. On visualization and compre-
hension of scenario-based programs. In Proceed-
ings of the 2011 IEEE 19th International Conference
on Program Comprehension, ICPC ’11, pages 189–
192, Washington, DC, USA. IEEE Computer Soci-
ety.
N. E. Fuchs and R. Schwitter. 1995. Attempto: Con-
trolled natural language for requirements specifica-
tions. In Markus P. J. Fromherz, Marc Kirschen-
baum, and Anthony J. Kusalik, editors, LPE.
M. Gordon and D. Harel. 2009. Generating executable
scenarios from natural language. In Proceedings of
the 10th International Conference on Computational
Linguistics and Intelligent Text Processing, CICLing
’09, pages 456–467, Berlin, Heidelberg. Springer-
Verlag.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics:
Vol. 3: Speech Acts, pages 41–58. Academic Press,
San Diego, CA.
D. Harel and M. Gordon. 2009. On teaching visual
formalisms. IEEE Softw., 26(3):87–95, May.
D. Harel and S. Maoz. 2006. Assert and negate revis-
ited: Modal semantics for UML sequence diagrams.
In Proceedings of the 2006 International Workshop
on Scenarios and State Machines: Models, Algo-
rithms, and Tools, SCESM ’06, pages 13–20, New
York, NY, USA. ACM.
D. Harel and R. Marelly. 2003. Come, Let’s Play:
Scenario-Based Programming Using LSCs and the
Play-Engine. Springer-Verlag New York, Inc., Se-
caucus, NJ, USA.
D. Harel, H. Kugler, R. Marelly, and A. Pnueli. 2002.
Smart play-out of behavioral requirements. In Pro-
ceedings of the 4th International Conference on For-
mal Methods in Computer-Aided Design, FMCAD
’02, pages 378–398, London, UK. Springer-Verlag.
D. Harel, A. Marron, and G. Weiss. 2012. Behavioral
programming. Commun. ACM, 55(7):90–100, July.
D. Harel, A. Kantor, G. Katz, A. Marron, L. Mizrahi,
and G. Weiss. 2013. On composing and proving
the correctness of reactive behavior. In Embedded
Software (EMSOFT), 2013 Proceedings of the Inter-
national Conference on, pages 1–10, Sept.
D. Harel. 2001. From play-in scenarios to code: An
achievable dream. Computer, 34(1):53–60, January.
H. Kugler, D. Harel, A. Pnueli, Y. Lu, and Y. Bon-
temps. 2005. Temporal logic for scenario-based
specifications. In Proceedings of the 11th In-
ternational Conference on Tools and Algorithms
for the Construction and Analysis of Systems,
TACAS’05, pages 445–460, Berlin, Heidelberg.
Springer-Verlag.
T. Kuhn. 2014. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics, 40(1):121–170.
T. Lei, F. Long, R. Barzilay, and M. C. Rinard. 2013.
From natural language specifications to program in-
put parsers. In ACL (1), pages 1294–1303.
P. Liang and C. Potts. 2014. Bringing machine learn-
ing and compositional semantics together. Annual
Reviews of Linguistics (submitted), 0.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.
T. Parsons. 1990. Events in the Semantics of English:
A study in subatomic semantics. MIT Press, Cam-
bridge, MA.
C. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379–
423, 623–656, July, October.
1306
R. Tsarfaty, J. Nivre, and E. Andersson. 2012. Cross-
framework evaluation for statistical parsing. In
W. Daelemans, M. Lapata, and L. M`arquez, editors,
Proceedings of EACL, pages 44–54. The Associa-
tion for Computer Linguistics.
A. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Trans. Inf. Theor.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI,
pages 658–666. AUAI Press.
1307
