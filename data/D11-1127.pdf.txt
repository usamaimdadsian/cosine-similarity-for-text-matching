Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias? Cyril Allauzen‡ William Byrne?
Adria` de Gispert? Michael Riley‡
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{gi212,wjb31,ad465}@eng.cam.ac.uk
‡ Google Research, 76 Ninth Avenue, New York, NY 10011
{allauzen,riley}@google.com
Abstract
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
1 Introduction
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M , this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} ? G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
2. Applying the language model to these target
translations: L=T ?M, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M .
3. Searching for the translation and language
model combination with the highest-probablity
path: Lˆ=argmaxl?LL
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf . In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al., 2009a; de Gispert et al.,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
1373
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) – the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
1.1 Related Work
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al. (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al., 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al. (2009a)
and de Gispert et al. (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al., 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al., 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al.,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
2 Pushdown Automata
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
1374
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
(a) (b)
0
1
(
3
?
2
a
4(
)
5
b
)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
(c) (d)
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ? N}. (b) Regular (but not bounded-stack)
PDA accepting a?b?. (c) Bounded-stack PDA accepting
a?b? and (d) its expansion as an FSA.
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al., 2007).
2.1 Definitions
A (restricted) Dyck language consist of “well-
formed” or “balanced” strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a¯ denote f(a) if
a ? A and f?1(a) if a ? A. The Dyck language
DA over the alphabet A? = A ? A is then the lan-
guage defined by the following context-free gram-
mar: S ? , S ? SS and S ? aSa¯ for all a?A.
We define the mapping cA : A?? ? A?? as follow.
cA(x) is the string obtained by iteratively deleting
from x all factors of the form aa¯ with a ? A. Ob-
serve that DA=c?1A ().
Let A and B be two finite alphabets such that
B ? A, we define the mapping rB : A? ? B?
by rB(x1 . . . xn) = y1 . . . yn with yi = xi if xi ?B
and yi= otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ? {?},min,+,?, 0) is
a 9-tuple (?,?,?, Q,E, I, F, ?) where ? is the fi-
nite input alphabet, ? and ? are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ?Q the initial state, F ? Q the set of final states,
E ? Q × (? ? ?? ? {}) × (R ? {?}) × Q a fi-
nite set of transitions, and ? : F ? R ? {?} the
final weight function. Let e= (p[e], i[e], w[e], n[e])
denote a transition in E.
A path pi is a sequence of transitions pi=e1 . . . en
such that n[ei]=p[ei+1] for 1 ? i < n. We then de-
fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] · · · i[en],
and w[pi]=w[e1] + . . . + w[en].
A path pi is accepting if p[pi] = I and n[pi] ? F .
A path pi is balanced if r??(i[pi]) ?D?. A balanced
path pi accepts the string x ? ?? if it is a balanced
accepting path such that r?(i[pi])=x.
The weight associated by T to a string x ? ??
is T (x) = minpi?P (x) w[pi] + ?(n[pi]) where P (x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |= |Q|+|E|.
A PDA T has a bounded stack if there exists K ?
N such that for any sub-path pi of any balanced path
in T : |c?(r??(i[pi]))| ? K . If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weighted finite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
1375
2.2 Expansion Algorithm
Given a bounded-stack PDA T , the expansion of T
is the FSA T ? equivalent to T defined as follows.
A state in T ? is a pair (q, z) where q is a state in T
and z ???. A transition (q, a, w, q?) in T results in
a transition ((q, z), a?, w, (q?, z?)) in T ? only when:
(a) a?? ? {}, z? =z and a? =a, (b) a??, z? =za
and a? = , or (c) a ? ?, z? is such that z = z?a
and a? = . The initial state of T ? is I ? = (I, ). A
state (q, z) in T ? is final if q is final in T and z = 
(??((q, ))=?(q)). The set of states of T ? is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z| ? K).
The complexity of the algorithm is linear in
O(|T ?|)=O(e|T |). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
2.3 Intersection Algorithm
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al., 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1?T2, the intersection of T1 and T2, such
that for all x ? ??: (T1?T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input- transitions. When
T2 has input- transitions, an epsilon filter (Mohri,
2009; Allauzen et al., 2011) generalized to handle
parentheses can be used.
A state in T =T1?T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I=(I1, I2). Given a transition e1=(q1, a, w1, q?1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ? ?, then e1 can be matched with a tran-
sition (q2, a, w2, q?2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q?1, q?2)) in T .
If a = , then e1 is matched with staying in q2
resulting in a transition ((q1, q2), , w1, (q?1, q2)).
Finally, if a ? ??, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q?1, q2)) in T .
A state (q1, q2) in T is final when both q1 and q2
are final, and then ?((q1, q2))=?1(q1)+?2(q2).
SHORTESTDISTANCE(T )
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 GETDISTANCE(T, I)
4 return d[f, I ]
RELAX(q, s, w,S)
1 if d[q, s] > w then
2 d[q, s]? w
3 if q 6? S then
4 ENQUEUE(S , q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[q, s]??
3 d[s, s]? 0
4 Ss ? s
5 while Ss 6=? do
6 q ? HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e ? E[q] do
9 if i[e] ? ? ? {} then
10 RELAX(n[e], s, d[q, s] + w[e],Ss)
11 elseif i[e] ? ? then
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[q, s] +w[e] + d[p[e?], n[e]] + w[e?]
18 RELAX(n[e?], s, w,Ss)
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and ?(f)=0 to simplify the presentation.
The complexity of the algorithm is in O(|T1||T2|).
2.4 Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T |3 log |T |) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by Cs the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in Cs to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in Cs. The
1376
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
Ss, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e], i[e]]| is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N |Q| |E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is ?(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N |T |3 ?(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T |). When all
the Cs’s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T | becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T , there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e? such that e?B[n[e?], i[e?]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T |) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
2.5 Replacement Algorithm
A recursive transition network (RTN) can be speci-
fied by (N,?, (T?)??N , S) where N is an alphabet
of nonterminals, ? is the input alphabet, (T?)??N is
a family of FSAs with input alphabet ? ? N , and
S?N is the root nonterminal.
A string x ? ?? is accepted by R if there exists
an accepting path pi in TS such that recursively re-
placing any transition with input label ? ?N by an
accepting path in T? leads to a path pi? with input x.
The weight associated by R is the minimum over all
such pi? of w[pi?]+?S(n[pi?]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(?,?,?, Q,E, I, F, ?, ?) with ?=Q=???N Q? ,
I = IS , F = FS , ?= ?S , and E =
?
??N
?
e?E? Ee
where Ee = {e} if i[e] 6? N and Ee =
{(p[e], n[e], w[e], Iµ), (f, n[e], ?µ(f), n[e])|f ?Fµ}
with µ= i[e]?N otherwise.
The complexity of the construction is in O(|T |).
If |F? | = 1, then |T | = O(
?
??N |T? |) = O(|R|).
Creating a superfinal state for each T? would lead to
a T whose size is always linear in the size of R.
3 Hierarchical Phrase-Based Translation
Representation
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S?abXdg, S?acXfg, and X?bc.
Figure 3 shows several alternative representations of
T : Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
1377
SX
3 3
a
1 1
b
2
1
c
2
2
d
4
f
4
g
5 5
(a) Hypergraph
0
1a
6
a
2b
7c
3X 4d 5g
8X 9f 10g 11 12b 13c
S X
(b) RTN
0
1a
6
a
2b
7c
11
(
12b
3 4d 5g
[
8 9f 10g
13c
)
]
(c) PDA
0,?
1,?a
6,?
a
2,?b
7,?c
11,(?
11,[?
12,(b
12,[b
13,(c
13,[c
3,??
8,??
4,?d
9,?f
5,?g
10,?g
(d) FSA
Figure 3: Alternative translation representations
duced by the HiFST decoder (Iglesias et al., 2009a;
de Gispert et al., 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
0 1a
2b
3
c
4X
5X
6
d
f 7
g
0 1b 2c
S X
(a) RTN
0 1a
2b
3
c 8
(
[ 9
b
4
6
d
7g
5
f1 0c
)
]
(b) PDA
0 1a
2b
3
c
4b
5b
6c
7c
8
d
f 9
g
(c) FSA
Figure 4: Optimized translation representations
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al., 1964) with time and space complex-
ity O(|Th||M |3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M |). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M |) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M |.2
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M |4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al. (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M |. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq . Given a state (q1, q2)
in Tp ?M , (q1, q2) ? C(s1,s2) only if s1 = sq1 , and hence
(q1, q2) belongs to at most |M | components.
1378
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)
PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)
FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)
Table 1: Complexity using various target translation rep-
resentations.
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
4 Experimental Framework
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al., 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
0 7.5× 10?9 7.5× 10?8 7.5× 10?7
207.5 20.2 4.1 0.9
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different ? values (top row).
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al., 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
• A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p > 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
• An unrestricted one without the previous con-
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al., 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
1379
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of ? to reduce the size of
M1. We will call the resulting language model
as M?1 . Table 2 shows the number of n-grams
(in millions) obtained for different ? values.
3. We translate with M?1 using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and M?1 .
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width ?.
5. We remove the M?1 scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
Note that if ?=? or if ?=0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase ? to reduce the
size of the language model used at Step 3, ? will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If ? defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with M?1 would be possible without incurring search
errors M1. This is investigated next.
5 Entropy-Pruned LM in Rescoring
In Table 3 we show translation performance under
grammar G1 for different values of ?. Performance
is reported after first-pass decoding with M?1 (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses ? = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the M?1 involved in
decoding and the ? applied prior to rescoring.
As shown in row number 2, for ? ? 10?9 the
system provides the same performance to the base-
line when ? > 8, while decoding time is reduced
by roughly 40%. This is because M?1 is 10% of the
size of the original language model M1, as shown
in Table 2. As M?1 is further reduced by increas-
ing ? (see rows number 3 and 4), decoding time is
also reduced. However, the beam width ? required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as ? is wide enough,
given the particular M?1 used in first-pass decoding.
Interestingly, results show that a similar ? value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with ?=10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam ? is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (? =12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al., 2010).
6 Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
1380
HiFST (G1 + M?1 ) +M1 +M2
# ? tune-nw test-nw time ? tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5× 10?9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5× 10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5× 10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various ? is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
Exact search for G2 + M?1 with memory usage under 10 GB
# ? HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5× 10?9 12 51 37 40 8 52
3 7.5× 10?8 16 53 31 76 1 23
4 7.5× 10?7 18 53 29 99.8 0 0.2
Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M?1 ;
HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.
HiPDT (G2 + M?1 ) +M1 +M2
? tune-nw test-nw ? tune-nw test-nw tune-nw test-nw
7.5× 10?7 25.7 26.3 15 34.6 34.8 35.2 36.1
Table 5: HiPDT performance on grammar G2 with ? = 7.5 × 10?7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various M?1 . With
?=7.5× 10?9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
M?1 is more reduced, and for ?=7.5×10?7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
7 Discussion and Conclusion
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under ? = 7.5 × 10?8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
1381
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
?=7.5× 10?8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
8 Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11–23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 of LNCS, pages 28–38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116–150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858–867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242–2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494–507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343–354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144–151.
1382
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273–283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ’05, pages 65–73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1–18.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of NAACL-HLT, pages 433–441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380–388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288–1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35–75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings of Eurospeech, pages 1251–1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al. (Drosde et al., 2009), chapter 6, pages
213–254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137–148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160–167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al. (Drosde et
al., 2009), chapter 7, pages 257–289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings of IEEE
International Conference on Portable Information De-
vices, pages 1 –5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72–82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270–274.
1383
