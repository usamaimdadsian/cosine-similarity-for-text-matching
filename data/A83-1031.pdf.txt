INTERACTIVE NATURAL LANGUAGE PROBLEM SOLVING: 
A PRAGMATIC APPROACH 
* ** 8allard*, ** A. Biermann , R. Rodman , B. T. Betancourt , 
**  • Fineman t G. Bi lbro , H. Deas , L. 
* * * Heidlage* P. Fink , K. Gi lbert , D. Gregory , F. 
* Department of Computer Science 
Duke Univers i ty  
Durham, North Carol ina 
** Department of Computer Science 
North Carol ina State Univers i ty  
Raleigh, North Carol ina 
ABSTRACT 
I NTRODUCT ION 
A class of natural language proces- 
sors is descr ibed which al low a user to 
display objects of interest on a computer 
terminal and manipulate them via typed or 
spoken Engl ish sentences. 
This paper concerns itself with the 
implementat ion of the voice input faci l i ty 
using an automatic speech recognizer, and 
the touch input faci l i ty using a touch 
sensit ive screen. To overcome the high 
error rates of the speech recognizer under 
condit ions of actual problem solving in 
natural language, error correct ion 
software has been designed and is 
descr ibed here. Also described are prob- 
lems involving the resolution of voice 
input with touch input, and the identif i -  
cation of the intended referents of touch 
input. 
To measur~ system performance we have 
considered two classes of factors: the 
various condit ions of testing, and the 
level and quality of training of the sys- 
tem user. In the paper a sequence of five 
different testing situations is observed, 
each one result ing in a lowering of system 
performance by several percentage points 
below the previous one. A training pro- 
cedure for potential  users is described. 
and an experiment is discussed which util- 
izes the training procedure to enable 
users to solve actual non-trivial problems 
using natural language voice communica- 
tion. 
A class of natural language proces- 
sors is under development which al low a 
user to display objects of interest on a 
computer terminal and manipulate them via 
typed or spoken Engl ish imperative sen- 
tences. Such a processor is designed to 
respond within one to four seconds by exe- 
cuting the input command and updating the 
displayed world for user veri f icat ion. If 
an undesired action is observed, a 
"backup" command makes it possible to undo 
any action and return the system to a pre- 
vious state. The domains of interest 
include matrix computation, where one can 
display tables of data and manipulate 
them: off ice automation, where one can 
work with texts, files, calendars, or mes- 
sages: and machine control, where one 
might wish to command a robot or other 
equipment via natural language input. 
The first such system (Biermann and 
Ballard \[6\]), cal led NLC, provides a 
matr ix computat ion faci l ity and al lows 
users to display matrices, enter data, and 
manipulate the entries, rows, and columns. 
It became operat ive in Ig79 and includes a 
variety of special purpose features 
1 ~his Work was supported by National 
Science Foundat ion Grants MCS 7904120 and 
MCS 8113491, by the IBM Corporat ion under 
GSD agreement no. 260880, and by the 
Univers i te de Paris-Sud, Laboratoire de 
Recherche en Informatique during the sum- 
mer of Ig82. 
1~0 
including arbitrari ly deep nesting of noun 
groups, extensive conjunction processing, 
user defined imperative verbs, and looping 
and branching features. More recently, a 
domain independent abstraction of the NLC 
system has been constructed and now is 
being specialized to handle a text pro- 
cesslng task. In this system, text can be 
displayed and modified or formatted with 
natural language commands. 
Current work emphasizes the addition 
of voice input, voice output, and a touch 
sensitive display screen. Speech recogni- 
tion is being done on an experimental 
basis with the Nippon Electric DP-200 Con- 
nected Speech Recognizer in both discrete 
and connected speech modes, and with the 
Votan Corporation V-SO00 Development Sys- 
tem. The touch sensitive screen being 
used is a Carroll touch panel mounted on a 
19-inch color monitor. Voice response is 
also provided by the Votan V-5000 which 
assembles and vocalizes digital ly recorded 
human voice messages. The work has pro- 
gressed to the point where OUr natural 
language matrix computer NLC is operative 
under voice control using the DP-200 and 
the text processing system is beginning to 
function using the V-5000 speech recog- 
nizer. The touch panel interface and 
voice response systems are still in the 
design phase. 
The goal of the project is to make 
possible voice and touch interactions of  
the following kind: 
Retrieve file Budget83. 
Find the largest number in this 
column and zero it. (with touch 
input) 
Add this column putting the result 
here. (with two touch inputs) 
Send this file to Jones and file it 
as Budget83. (touch input) 
~at  is, imperative sentences are to be 
processed that operate on domain objects 
to produce modifications to the existing 
objects or their relationship to each 
other. The objects are, for example, 
rows, columns, numbers, entries, labels, 
etc. in the matrix domain or sections, 
paragraphs, sentences, margins, pages, 
etc. in the text processing domain. The 
execution of each command is accompanied 
by an update of the displayed data with 
highlighting to indicate changes. Prompts 
and error messages will be given by voice 
response, gystem design is aimed at 
allowing fast interactive control of the 
objects on the screen while the user main- 
tains uninterrupted eye contact with th~ 
events as they happen. 
A continuous program of human factors 
testing has been maintained by the pro jec t  
in order to build a realistic view of 
potential users and to measure Progress in 
achieving usability. For example, in a 
test of the matrix computation system with 
typed input, twenty-three subjects solved 
problems similar to those that might be 
assigned in a first course in programming 
(Biermann, Ballard, and Sigmon \[7\]). In 
this test, the NLC system correctly pro- 
cessed 81 percent of the sentences and 
users were quite satisfied with its gen- 
eral performance. Other tests of the sys- 
tem are described in Fink \[14\] and Geist 
et el. \[IS\]. In another test (Fineman 
\[13\]), a simulator for a voice driven 
office automation system was used to 
obtain data on user behaviors when problem 
solving is with discrete and slow con- 
netted speech. It was found that users 
quickly adapted their speech to the 
required discipl ine of slow, methodical, 
and simple sentences which can be recog- 
nized by machine. Since the data obtained 
in any system test is heavily dependent on 
the amount and kind of training given to 
subjects, it is necessary to have a stand- 
ardlzed training procedure. In the 
current work, a voice tutorial has been 
developed for training users to use a 
voice interactive system (Deas \[Ii\]). 
This paper reports on the current 
status of these projects with emphasis on 
system design, speech input facilities and 
their performance, the touch input system 
and human factors considerations. 
SYSTEM OVERVZEW 
181 
The basic system design includes 
modules to do the following tasks: 
(i) token acquisit ion 
(2) parsing 
(3) noun group resolution 
(4) imperative verb execution 
(5) flow-of-control semantics 
(6) system output 
The token acquisit ion phase receives 
typed inputs, word guesses ~com the voice 
recognizer, and screen coordinates from 
the touch panel. These inputs are prepro- 
cessed and passed tO the parser which uses 
an augmented transition network to ~is- 
cover the structure of the command and the 
roles of the individual tokens. ~oun 
group resolution attempts to discover what 
domain objects are being referred to, and 
the verb execution module transforms those 
objects as requested by the imperative 
verb. The f low-of-control semantics 
module manages the execution of meta- 
imperative verbs such as ~ ,  and han- 
dles user-defined imperatives. Finally, 
system output displays the state of the 
world on the screen. Any module may issue 
prompts and error messages via text or 
spoken output. Backup from any given 
module to an earlier stage may occur in 
unusual situations. More detai ls appear 
in Ballard \[i\], Biermann \[5\], Biermann and 
Ballard \[6\], and Eallard and 8iermann \[3\]. 
SPEECH INPUT 
An automatic speech recognizer such 
as the DP-200 or V-5000 recognizes speech 
by means of pattern matching algorithms. 
A subject is introduced to the device for 
a training session, and asked to repeat 
the various words of the vocabulary into a 
microphone. The device extracts and 
stores bit patterns corresponding to each 
vocabulary word uttered by that part icular 
speaker. After training, when a speaker 
wishes to use the device, the appropriate 
bit patterns are loaded. Each utterance 
of the speaker is compared with the pre- 
stored bit patterns and the best match 
above a threshold limit is presented as 
the recognized word. Depending on the 
device being used, the speaker may be 
required to talk with discrete or con- 
nected speech. The results descr ibed 
below were obtained primari ly in the 
discrete mode with a pause of at least 200 
mil l iseconds after each word. 
Error Handlin~ 
The major di f f iculty facing users of 
automatic speech recognit ion equipment is 
the high error rate. Even the best dev- 
ices in the best of circumstances are not 
entirely free of error, and when cir- 
cumstances are less than optimal, and more 
like the real world, the error rate rises. 
Thus, a good part of the project effort 
has gone into coping with errors in recog- 
nition. In our view the speech recogni- 
tion device is a component of the larger 
natural language computing system, and our 
goal is to reduce the system error rate as 
much as possible. We have therefore 
designed error correction software that 
corrects for certain kinds of errors, and 
error messages that elicit repetit ion from 
the human subject in less tractable cases. 
Error correction essential ly func- 
tions by start ing with a sequence of word 
guesses from the input system and filter- 
ing out the meaningless alternatives at 
the appropriate stages of processing. 
Beginning in the token acquisit ion phase, 
certain unacceptable word sequences can be 
disal lowed. For example, a noun such as 
"matrix" or "row" would be disal lowed as 
the first word in the sentence since this 
is i l legal in the system grammar. In the 
parsing phase, a grammatical sequence of 
words is selected from the incoming sets 
of word guesses. Thus all ungrammatical  
word sequences are el iminated. The parser 
also disal lows phrases containing certain 
semantical ly  unacceptable relat ionships 
such as 
the second row in 6. 
or phrases containing disal lowed opera- 
tions such as 
Add the matr ix to 6. 
In the noun group processor and Later 
stages, various other semantic errors can 
be el iminated such as references to nonex- 
istent objects or impossible operations. 
For discrete mode operations, errors 
are c lassi f ied into four types: 
a. Substitutions. 
The device reports word B when 
word A was actual ly spoken. 
b. Re~ections. 
The device sends a reject ion 
code when a vocabulary word was 
spoken. 
c. Insertions. 
The device reports a vocabulary 
word when a non-vocabulary word, 
or noise, was uttered: 
d. Fusions. Two (or more) words are 
spoken but only one word is 
reported. 
Subst i tut ion Errors 
Subst i tut ion errors are the easiest 
to correct since the subst ituted word 
often resembles the actual word phoneti-  
cally. Some of the substitut ions are 
fairly predictable, e.g. "by" for 
"five", "and" for "add", or "up" for 
"of". We have coined the term synophone 
to describe such sets. Many synophone 
pairs are symmetr ical ly  interchangable: 
however, some are not. For example, with 
some speakers, the word "a" is fre- 
quently reported as "eight" although the 
converse seldom occurs. 
Synophones of a part icular word 
utterance come from two sources: alter- 
nate guesses offered by the recognit ion 
device based on its pattern matching com- 
putation, and a set of words stored in the 
system that are known to be confused with 
the selected word. Whenever a token is 
col lected by the scanner, its synophone 
182 
llst is compiled. Passing the complete 
set of synophones for each word to the 
parser would result in excessive parse 
time so it is desirable to  eliminate 
beforehand any synophones whose occurrence 
can be determined to be impossible based 
on grammatical or contextual considera- 
tions. For example the syntax of English 
(and of NLC) prevents certai~ words from 
occurring next to each other, or beginning 
or ending sentences. This information is 
recorded i n  a table of adJacencies. If 
there is a synophone in a word slot that 
cannot be preceded by any of the syno- 
phones in the previous word slot that 
synophone is deleted. This process is 
repeated until no more deletlons are pos- 
sible. On average, roughly one-half of 
the candidate synophones are deleted. 
Since parsing time may increase exponen- 
tlally with the number of candidate syno- 
phones, and this table driven el imination 
process is very quick, considerable sav- 
ings result. 
For reasons of indivldual speech 
variation some vocabulary words will have 
synophones peculiar to an individual 
speaker. The set of synophones of each 
vocabulary word is therefore augmented to 
accommodate this situation so that each 
speaker has personalized synophone sets. 
Early training includes a tutorial intro- 
ductlon, part of which  requires the sub- 
Ject to repeat sentences word for word. 
In this mode, the software has a priori 
knowledge of the correct token--for each 
word slot. If a given word slot does not 
contain the correct token, the substituted 
word can be added to the appropriate syno- 
phone set for that subject. Thereafter, 
if the same substitution error recurs dur- 
ing a session with that subject, the 
correct word will be included in the syno- 
phone list for that word slot. 
Re~ection Errors 
The occurrence of one or more rejec- 
tions in a sentence almost always results 
in a request for repetition. However, we 
are designing a number of facilities to 
handle rejections. I n  some cases, the 
rejected word can be determined from con- 
text, and processing can continue uninter- 
rupted. Otherwise, the current plan is to 
handle a single rejection by returning an 
audio response that repeats all of the 
sentence with the word "what" in place of 
the rejected element. The speaker will 
then .be able to choose to repeat the 
rejected word or, in case other errors are 
apparent, to repeat the entire utterance. 
I n  cases of multiple rejection 
errors, the speaker is requested to repeat 
the entire utterance. In all cases previ- 
ous utterances will not be. discarded. The 
scanner will merge them, complete with 
synophones, in an attempt to el iminate 
reJectione and provide the broadest amount 
of information from which to extract what 
the speaker actual ly said. For example, 
if the actual utterance were 
ABC D E FG 
and the recognizer returned 
Am * Z E*  G 
where * s tands  for rejection, the speaker 
will be asked to repeat. If 
ABC*  EFH 
is then recognized, it will be combined 
with the first utterance so that the 
scanner considers the seven word slots to 
contain= 
s(A) sis) sic) s(z) sis) s(F) s(G) 
sin) 
where siX) is the union of X with its 
synophones. (Hopefully D is in s(Z).) If 
subsequent utterances are so different 
from previous ones that they are unl ikely 
to be word-for-word repetitions (for exam- 
ple, by containing a different number of 
words), previous utterances will be dis- 
carded and processing will be started 
over .  
It may also be possible to predict a 
rejected word with some degree of cer -  
tainty based on semantic or pragmatic 
information. (We consider pragmatics to 
involve discourse dependent contextual 
factors.) For example suppose the scanner 
receives from the recognizer: 
Double * nine and add column four to it. 
The most l ikely possibi l it ies for the 
rejection are entry, row and column. 
Entry can be ellmz~'~ate~--on semantic 
grounds since it is meaningless to a<\]d a 
column to an entry. Row is semantical ly 
possible, but pragma-'~cally less likely 
than column since adding columns to 
columns is much more common than adding 
columns to rows. Thus column may be 
chosen. Furthermore if t-h-e matrix in 
focus is six by seven, then the nine is a 
substitution error, and the sentence will 
be rejected on pragmatic grounds ini- 
tially. However, since five is a syno- 
phone of nine the sentence ~ be tried 
with flve 'in the place of nine. Ulti- 
mate!y t't~'~e~e user will see displayS, on the 
screen the result From: 
Double column five and add column 
four to it. 
183 
The act iv ity descr ibed above is tran- 
sparent to the user. If the results are 
unsat is factory to the user, the command 
"backup" will undo them. 
An addit ional source of pragmatic 
error correct ion comes from utterances in 
h is tor ica l ly  similar dialogs. We are 
developing a method for ut i l iz ing this 
type of information. Consider ing the last 
example, if the user had been adding 
columns to rows quite freque~: "~-" in the 
current and/or recent sessions, but rarely 
if ever adding columns to columns, the 
system would choose row as the rejected 
word. 
Insertion Errors and Fusion Errors 
Most speech recognizers al low the 
threshold value to be adjusted that deter- 
mines whether the best match is "recog- 
nized" or is rejected. Since rejections 
are harder to correct for than substitu- 
tions there is reason to lower this value. 
Too low a value, however, aggravates the 
insert ion problem. When the speaker 
utters a non-vocabulary word, or emits a 
grunt or uncouth sound, the correct 
response is a rejection. A non-reject ion 
in this s i tuat ion may be dif f icult  to deal 
with. 
In our experience users have l ittle 
trouble in conf ining themselves to the 
trained vocabulary. Most insert ion errors 
occur between sentences, rather than 
between words within a sentence. This 
results in extraneous "words" in the first 
one or two word slots. These can often be 
el iminated because neither they nor their 
synophones can begin a sentence in the NLC 
grammar. Timing considerations, too, 
could be used to eliminate, or at least 
cast suspicion on, inter-sentence inser- 
tions, though we have not found the need 
for such measures. 
Raw Error Rate 
Although a good deal of our interest 
i s  in correct ing or compensat ing for the 
various kinds of errors in recognit ion, we 
are also working on ways to reduce the 
actual number of errors made by the recog- 
nit ion devices (the raw error rate). 
Careful vocabulary choice and proper tun- 
ing of the hardware such as threshold 
level select ions are crucial factors. 
It is important to choose vocabulary 
words as widely separated phonet ic~l ly  as 
c ircumstances allow. Addit ional ly,  we 
have found that words containing non- 
str ident fr icatives (e.g. the th in 
fifth), af fr icates (e.g. the c--h in 
c u-'~-~r'ch), l iquids (r and I) and nasals-'(m,n 
an~q)  are mort  di f fTcult  to recognize  
than words contain ing other sounds. 
Monosyl lab ic  words, in general, are not 
recognized as readi ly as po lysy l lab ic  
ones, though words that are long and dif- 
ficult to pronounce (e.g. anaesthet ist)  
are also to be avoided. Often the domain 
leaves l ittle lat itude for vocabulary 
choice. If ordinal numbers are needed it 
is necessary to have fifth and sixth, 
which are di f f icult  to - -~ ingu ish .  But 
instead of a word like rate which is 
easi ly confused with eig t~-~-, tax rate or 
rate-of-pay (pronounced as a s ingle-- '~rd) 
m%--~t~ a better choice. 
Correct training procedures are 
instrumental  in reducing the raw error 
rate as are such factors as whether the 
user receives immediate feedback from the 
recognizer, the form and frequency of 
error messages request ing repetit ion, and 
the degree of comfort fett by the user 
insofar as att itude toward computers is 
concerned. Some of these are d iscussed 
below in the section Measurin@ System Per- 
formance. 
We have observed fusion errors in 
discrete mode. They arise when the 
speaker neglects to pause long enough 
between words. In our experience they 
occur so infrequently we have not tried to 
compensate for them. This type of error 
is more crucial when operat ing in con- 
nected mode. It may be the case that two 
(or possibly more) words are reported as a 
single word different from either of the 
two or ig inal ly  uttered words. It may also 
happen that two words, A and B, are 
reported as either A or 8. In this case 
the fusion error takes on the appearance 
of an omission. Our connected speech 
parser, current ly under construction, will 
have the abi l i ty z9 guess an omission and 
inser t  a correct ion if suff icient contex- 
tual information is available. 
Some Miscel laneous Questions 
Apart from error correction, a number 
of other questions have arisen during our 
implementat ion of the voice driven system. 
Among these are: 
a) How is the beginning of a sen- 
tence detected? 
b) How is the end of a sentence 
detected? 
c) How can a user make a correct ion 
in mid-sentence? 
Current ly  a sentence begins with any 
input after the end of the previous sen- 
tence. The instances of inter- or pre- 
sentence insertions were discussed above. 
Sentences are terminated by the 
mete-word over. This word has few syno- 
18 ~. 
phones in the current word set and has the 
advantage of being widely understood to 
mean "end of transmission." However, we 
plan to experiment with other kinds of 
termination such as use of touch input or 
t iming information. 
A user may misspeak in instructing 
the computer to perform a task and may 
wish to repeat all or part of the command. 
Also, if the words from the woice recog- 
nizer are displayed as they are spoken, 
the user may desire to correct a misrecog- 
n i t ion .  '~ne metaword cor rect ion  i s  
currently used to implement this facility. 
There are several levels of correction. 
Some may be accomplished by the scanner, 
while others require more information than 
is avai lable to the scanner and must 
therefore be handled by the parser. The 
simplest type of correction consists of 
changing one word at the end of the sen- 
tence: 
Add row one to row four 
correction three. 
Here the scanner merely deletes the word 
slot before the metaword. If several 
words follow "correction" as in 
Add row one to row two correction 
row one to column three. 
the scanner detects this fact and scans 
backward in the sentence, attempting tO 
match the largest possible number of word 
slots before and immediately after the 
metaword. In this example the tokens for 
row, one and to match, so the scanner 
copies--t'~e last ~r t  of the sentence into 
the earlier part of the buffer to arrive 
at 
Add row one to column three. 
In the case of an utterance such as 
Add row one to row two 
correction column three. 
it is impossible to match the tokens 
before and after the metaword. The 
scanner therefore deletes the token 
\[~Ime,\]iately before the metaword, flags the 
word slot preceding that token and passes 
the result to the parser. In the example, 
Add row one to row column three. 
is passed, with the word slot containing 
ro w flagged. The parser attempts to make 
185 
sense of the set of tokens passed. If it 
cannot, the flagged word slot is deleted, 
the word previous to it is flagged and 
another parse is attempted. The process 
is repeated until a successful Parse is 
found. If none is found, an error message 
is issued. Thus in the example, after 
fail ing tO Parse the tokens as passed, the 
parser tries 
Add row one to column three. 
which is parsed successful ly. 
TOUCH INPUT 
An important aspect of natural 
language communication is pointing, which 
is often used in connection with words 
such as this, that, here and there. 
Pointing may---'f'~ncto~-o'n--as em-'m'~asis, as in 
Put the dog out. 
where either the dog, the outside, or pos- 
sibly both are pointed to. Pointing also 
functions to put objects into focus, 
al lowing subsequent references to use a 
definite pronoun: for example, 
Move that there and cover it. 
with a point to the object to be moved and 
covered. 
A point ing abi l i ty would fit in very 
nicely with voice driven NLC and our pro- 
Ject includes a touch sensit ive screen so 
that the user can say "double this", point 
to a row, and cause the processor to dou- 
ble every element in that row. More com- 
plex sentences such as 
Add this row to that row putting 
the results here. (with three 
touchee) 
also become possible. 
Apart from being "natural" in the 
sense that ordinary language users point 
often, pointing may increase the effi- 
ciency of communication. 
There has  been a good deal of 
interest among human factors scientists as 
to the eff ic iency of various modes of com- 
munication. Past experiments, for exam- 
ple, have compared the eff ic iency of typed 
versus voice messages (voice messages are 
more efficient). We carried out an exper- 
iment to verify the hypothesis that voice 
input together with touch input is more 
efficient than voice input alone, and we 
attempted to quantify the results, we 
solved eight different types of matrix 
problems including Gaussian elimination, 
div ided dif ferences and matrix inversion, 
using NLC without touch. We then went 
back and rewrote the solutions using the 
touch facility, but without any other 
changes. On the average 29% fewer words 
were needed to solve the problem, and 
individual sentences were shortened by 
23%. 
A number of interest ing problems 
arise when a touch faci l i ty is imple- 
mented. One is how to pair up tacti le and 
verbal input in the way intended by the 
user. Another problem is identi fying the 
actual object the user intends to refer to 
once the tacti le and verbal input have 
been resolved. 
An example of the latter problem 
would be the command 
Double this 
accompanied by a touch of element <3,2> of 
a displayed matrix. Does the user want to 
double element (3,2>, double row 3, double 
column 2, or even double the entire 
matr ix? The same touch paired with 
Double this entry. 
Double this matrix. 
Double this column. 
or 
Double this matrix. 
would be unambiguous. If the demonstra-  
tive is not accompanied by a nominal some 
strategy is needed to process the sen- 
tence. We opt for the smallest possible 
noun group encompassed by the touch (the 
<3,2> entry in the above case), and rely 
on our "backup" facil ity in case the 
user's intentions are not fulfi l led. If 
the utterance "double this" is accompanied 
by a touch of the displayed name of a row, 
column or matrix, then the named object 
will be referenced. 
Pair ing up touches with spoken 
phrases is straightforward when a single 
noun group is used with a single touch, as 
in "double this entry." In a more compli- 
cated case we might have 
Add this entry to that row 
and put the result here. 
accompanied by three touches. The stra- 
tegy here is to -air touches and utter- 
ances in the order given by the user. 
In the last example all touches func- 
t ioned to establ ish focus or resol~=e no,~n 
group reference. If the emphasis function 
of touch is mixed in, a more dif f icult  
s i tuat ion arises. If three touches accom- 
pany 
Add this entry to the first row 
and put the result here. 
then the second touch was presumably to 
emphasize the first row or even to estab- 
lish a rhythm of touching. In any case 
the faci l i ty to match touches with non- 
deict ic expressions iS needed. If only 
two touches accompany this last sentence 
then the focusing funct ion should take 
precedence, and the touches should be 
matched with "this entry" and "here." 
The s i tuat ion is made even more com- 
plex by the abi l i ty  to establ ish focus 
verbally. In NLC the user can say 
Consider row four. 
Double that row. 
and the express ion "that row" will refer 
to row four. ~f the same utterance is 
accompanied by a touch to a row other than 
four a potential  confl ict results. Our 
strategy is to give precedence to touch, 
since it is the more immediate focussing 
mechanism. Thus the sequence 
Consider row four. 
Double that row. (touching row three) 
will result in the doubl ing of row three. 
When both verbal and touch focus are 
present, nearly unresolvable ambiguit ies 
may result. The sequence 
Consider row four. 
Add this row to that row. 
accompanied by one touch, gives rise to 
the problem as to which demonstrat ive noun 
group to associate with row four, and 
which to associate with the touch. One 
strategy is to associate with a demonstra-  
tive noun group the touch that occurred 
closest to the time of utterance. Another 
possible strategy is to assume that the 
expression with that refers to the more 
distant element in focus (the one esta- 
bl ished verbal ly in this case). This 
takes advantage of the ~act that this and 
that can be dist inguished in Engl ish gram- 
mar by the feature +NEAR. Unfortunate ly  
by a simple change iF stress pattern a 
speaker can undo this fairly weak regular- 
ity. Thus the sequence 
Consider row four. 
~dd th{s row to that row. 
186 
plus a single touch, where this bears prl- 
mary stress and that bears secondary 
stress, should flnd t-- e~touch  referring to 
"this row." If the stress pattern were 
Add th i s  row to  that  row. 
with primary stress on Add, the touch 
would more llkely be assoc--~-ated with that 
row. It is unfortunate that to date we 
~w of  no vo ice  equ ipment  sens i t i ve  
enough to  d i s t ingu ish  between two such 
s t ress  patterns. 
Somewhat more complicated cases are 
possible= 
Consider row three. 
~ld this row to that row and 
put the result in the first row. 
accompanied by two touches. Since we 
allow a touch to occur with expressions 
such as "the first row," and since it is 
poss ib le  to d is regard  the element i n  ver- 
bal focus altogether, such a case produces 
mult iple ambiguities. Although we foresee 
being able to resolve these ambiguit ies 
effectively, and ca~ always fall back on 
our "backup" facil ity in case of mistakes, 
we also believe that such complex cases 
will be extremely rare. No sentence of 
such complexity was produced in our solu- 
tions to  the e ight  prob lems ment ioned 
above. With a voice and touch facility, 
sentences tend to be shorter and simpler. 
NLC has implemented plurals, but we 
have not considered their use in touch 
input. Such sentences as 
or 
Multiply these elements by 
this element. 
Add these elements up. 
with multiple touches, would be useful. 
In the trial run of eight problems, the 
introduction of plural ity resulted in up 
to fifty percent reduction in number of 
words needed and sentence length. 
MEASURING SYSTEM PERFORMANCE 
Progress in any endeavor is greatly 
aided if the level of accomplishment can 
be measured in some meaningful way. It is 
desirable to give a figure of merit for a 
system both so that a project can indicate 
to the world the degree of the achievement 
and also so that the project can in ter -  
nally Judge its own improvements over 
time. In voice language processing, one 
can attempt to measure performance by the 
word and sentence error rates. However, 
exper ience  shows that these measures are 
highly dependent on two factors and that 
almost any level of performance can be 
reached if those factors are appropr iately 
adjusted. Those factors are 
(a) the environment and type of test 
within which the measurement is 
made, and 
(b} the level of training of the 
system user. 
Type O~f Testln~ Environment 
Consider ing (a), we tend to classify 
the type of test for a recognizer into one 
of the fol lowing five categories and we 
expect signif icant differences in device 
response in each case. 
187 
(1) Lists of words are read in tests 
performed by the manufacturer. 
(2) Lists of words are read in our 
laboratory. 
(3) Sentences are read in our labora- 
tory. (discrete or connected) 
(4) Sentences are uttered in a prob- 
lem solving situation in our 
laboratory. (discrete or con- 
nected) 
(5) Sentences are uttered in a prob- 
lem solving situation in the user 
environment. (discrete or con- 
nected) 
In the first situation, a manufac- 
turer is interested in advertis ing the 
best performance achievable. Tests are 
performed in control led conditions with 
microphone placement and all system param- 
eters set for optimum performance, and an 
expert speaker is used. In our labora- 
tory, we are not interested in the best 
possible system performance but rather 
what we can real ist ical ly expect. The 
parameters are set at medium levels, there 
is some ambient noise, the microphone ~ay 
move during the test, and the user wil\] be 
anyone we happen to bring in regardless of 
their speech characteristics. 
As soon as the sequential words 
become organized as sentences, situation 
(3), the speaker begins to impose inflec- 
tions on the utterance that will affect 
recognition. Certain words may be 
stressed, and intonation may rise an~\] fall 
as the sequential parts of each sentence 
are voiced. Training samples based on 
reading lists of vocabulary items tend to 
be inaccurate templates for words spoken 
in context. When sentences are spoken in 
a problem solving environment, s ituation 
(4), these effects increase and other 
aspects of word pronunciat ion change. 
When voice control stops being the central 
concern of the speaker, largeT variations 
in speech are bound to occur with accom- 
panying larger error rates. 
The most dif f icult  s ituation of all 
occurs in s ituation (5) where the user 
might not even be a person who could be 
brought into a voice laboratory. In this 
case, the user has on ly  one concern, 
achieving the desired machine performance. 
Encouragement to speak careful ly could be 
met with impatience, and a few system 
errors could result in even worse speech 
qual i ty and further degraded performance. 
Our experience has been that word 
error rates increase from about three to 
seven percent as one moves to each more 
di f f icult  s ituation type depending on the 
vocabulary, t~e equipment, and other fac- 
tors. Consequently, we tend to distrust 
any figures gathered in the easier classes 
of environments and attempt to do our own 
testing in the more diff icult  and more 
interesting situations. Most of our 
recent data is of type (4) and we hope to 
gain some type (5) experience in the com- 
ing year. 
Training the System User 
The second major factor affecting 
voice recognit ion performance is the level 
of training of the system user. Humans 
are extremely adaptive and capable of 
learning behaviors to a high degree of 
perfection. Thus the designer of a voice 
system might, over the years, learn to 
chat with it like an old friend whereas 
others might not be able to use the system 
at all. ~gain, almost any level of system 
performance can be observed depending on 
the quality of training of the user. 
Our approach to control l ing this fac- 
tor has been to develop a standardized 
training procedure and to only report 
statist ics on uninit iated users whose 
experience with the system is l imited to 
this procedure. Ideally this procedure 
would be administered by machine to obtain 
maximum uniformity in training but this 
has not yet been possible. 
The training procedure has two parts. 
The first part is an informal session in 
which the user is told how to speak indi- 
vidual words to the system and examples of 
the complete vocabulary are col lected by 
the recognit ion system. ~he second part 
is administered very mechanical ly  by read- 
ing a tutorial document to the user and 
request ing the utterance of trial sen- 
tences. This port ion of the training 
introduces the user to the interactive 
system's capabi l i t ies and is speci f ical ly 
designed to be administered by the 
machine. 
Some Performance Data 
An experiment was run during the sum- 
mer of 1982 to obtain DP-200 performance 
data in an environment of type (4) as 
described above. Beca~ise no voice 
interactive system was yet available, a 
system simulat ion was used. After the 
first part of the training session in 
which the voice samples were collected, 
the subject was placed in a room behind a 
display terminal with a head mounted 
microphone. The voice tutorial was read 
to the subject through a loudspeaker at 
the terminal introducing the capabi l i t ies 
of the simulated system and the types of 
voice commands that could be executed. 
The subject's commands were recognized by 
the DP-200 and executed by the simulation. 
Thus each user command resulted in either 
appropr iate act ion visible on the screen 
or a voice error message. In the final 
port ion of the experiment, the subject was 
asked to solve an invoice pro61em that 
involved computing costs for a series of 
individual items and finding the tax and 
total. The experiment gave a reasonably 
accurate simulat ion of the expected NLC 
system behavior when it becomes completely 
voice interactive. The experiment 
attempted to simulate a syntact ic level of 
voice error correct ion but nothing deeper. 
It was fo,lnd that the DP-~00 word 
error rate rose to about 20 percent in 
this test with about 14 of the 20 percent 
being automatical ly  correctable. The 
vocabulary size was 80, with three samples 
of most words, and six samples of a few of 
the diff icult words, stn1=ed in the DP-200. 
This means that roughly every two to four 
sentences will have a single word error 
not correctable at shal low levels. This 
data comes from the first two hours o~ 
usage for these subjects and we expect 
signif icant improvement as usage experi-  
ence increases over time. 
More recently, the ~LC system has 
become operat ive in a voice driven mode 
and subject testing has begun using the 
same training procedure. It is too early 
to report results but it appears that the 
performance predicted in the s imulat iou 
will be approximately achieved. This 
experiment will include longer usage by 
the subjects and thus indicate how much 
error rates decrease over time. 
188 
In conclusion, we have at this time 
only fragmentary information regarding 
what levels of performance can be 
achieved. HOwever, we have developed some 
tools for making measurements and will 
report the results as they become avail- 
able. 
systems has been refined to the point that 
it could actual ly support user interac- 
tions in real time as we are attempting to 
do. Our project uses well developed 
speaker dependent voice recognit ion equip- 
ment with a small enough vocabulary to 
achieve usable accuracy rates. 
OTRER kK)RK 
Much of the applied work in natural 
language processing has concerned database 
query (Bronnenberg et al. C8\], CoddC9\], 
Harris\[17,18\], Hendrix\[22\], MylO- 
poUlOe\[27\], Plath\[29\], Thompson and Thomp- 
son\[32\], Weltz\[35\], and Woods et el. 
\[36\]). At least one such system is being 
marketed (namely INTELLECT \[18\]), while 
several others have been successful ly used 
in pilot studies. (Damerau\[10\], F.~ly and 
Wescourt\[12\], Hershman et el. \[24\], 
Krause\[25\], Tennant\[31\]). 
As descr ibed  in  th i s  paper ,  our  in i -  
t i a l  work with N-LC i nvo lved  programming  as 
an application area, while our more recent 
interest has shifted toward office 
domains. However, as Petrick\[2R\] 
observes, many of the same technical prob- 
lems arise regardless of application area. 
For the most part, the imperative sentence 
structures we are dealing with are simpler 
than the question forms recognized by the 
database systems cited above, whi le  our 
noun phrases tend to exhibit more ela- 
borate structures. Furthermore, whereas 
typical database sy-tems process each 
input separately, or perhaps seek to han- 
dle ellipsis by consult ing the immedlately 
preceding input, we build up a richer 
semantic context as a session proceeds to 
be used in handling matters such as focus 
and pronoun resolution. 
The most distinctive features of our 
present work are (a) the inclusion of 
voice input and output facilities, and (b) 
an attempt to deal with relatively "deep" 
relationships among domain objects. A 
more detailed discussion of the domain- 
independent mechanisms appears in Bier- 
mann\[5\], and as described in Ballard \[2\] 
the related LDC project being conducted in 
our laboratory is built around many of 
these techniques. Similar research pro- 
jects which are moving away from a fixed 
database setting include work by Haas and 
Hendrix\[16\], Reldorn\[20\], Hendrix and 
Lewis\[231, and Thompson and Thompson \[33\]. 
During the 197O's a number of speech 
understanding systems were developed under 
ARPA support (Lea \[26\], Reddy C30\], Walker 
\[34\], Woods \[37\]) and currently some sys- 
tems ace being built in other countries, 
for example \[19\]. Rowever, none of these 
t89 
r.Zl 
\[2\] 
\[3\] 
\[4\] 
\[s\] 
\[ 6\] 
\[71 
r87 
\[93 
\[i0\] 
REFERENCES 
B.W. Ballard, "Semantic and Pro- 
cedural Processing for a Natural 
Language Programming System," Ph.D. 
Dissertation, Report CS-1979-5, Dept. 
Of Computer Science, Duke University, 
Durham, NC, 1979. 
B.W. Ballard, "A Domain-Class 
Approach to Transportable Natural 
Language Processing," COgnition and 
Brain Theory, 5, pp. 269-~87, 1982. 
B.W. Ballard and A.W. Biermann, "Pro- 
gramming in Natural Language: NLC as 
Prototype," Proceedings of the 197g 
ACM National" Conference, "~to~, ,  
I-~J79. 
A.W. Biermann. "A Natural Language 
Processor for Office Automation," 
Proceedings of the 1982 Office Auto- 
marion Conir~re-'~'6e, San Franc{sco, 
~rai'l-~rnla, April, 1982. 
A.W. 8iermann, "Natural Language Pro- 
gramming," to appear in Computer Pro- 
~ m  Synthesis Methodologies (E~.  
ann and Guiho), Reide~, 1983. 
A.W. Biermann and S.W. Ballard, 
"Towards Natural Language Computa- 
tion," American Journal of Computa- 
tional L in@uis t l~ ,  vol. 6, No. 2, 
pp.--67-~T-86, 1980. 
A.W. Biermann, ~.W. Ballard, and A.H. 
Sigmon, "An Experimental Study of 
Natural Language Programming," to 
appear in International Journal of 
Man-Machine Studies, 1983. 
W. Bronnenberg, S. Landsbergen, R. 
Scha, and W. Schoenmaker, "PHLIQA-I, 
A Question-Answering System for 
Data-Base Consultat ion in Natural 
English," Philips Tech. Rev., 38, DD. 
229-239 an~- -~8~' -~97,~1979.  "" 
E.F. Codd, "Seven Steps to RENDEVOUS 
with the Casua' User," IBM Report 
J1333, 1974. 
F.J. Damerau, "Operating Statistics 
for the Transformational Question 
Answering System," American Journal 
of Computational Linpuist~cs, 
N-~. I, pp. 30-45, 1981. 
\[II\] H. Deas, M.Sc. Thesis, Dept. of Com- 
puter Science, Duke University, Dur- 
ham, N.C., November 1982. 
\[12\] D. Egly and K. Wescourt, "Cognitive 
Style, Categorizations, and Voca- 
tional Effects on Performance of REL 
Database Users," Joint Conference on 
Easier and More P r~ct ive  Use o-~ 
Comput i~ Systems, Ann Arbor,--~ch~-- 
Nan, May 1981. 
\ [13 \ ]  L. Fineman, "Prel iminary Results on 
the Voice Driven Information System 
Simulation Experiment," Report to IBM 
Corporation, Dept. of Computer Sci- 
ence, Duke University, Durham, N.C., 
1981. 
\[14\] P.K. Pink, "Conditionals in a Natural 
Language System" (Master's Thesis ), 
Report CS-1981-8, Duke University, 
Durham, N.C., 1981. 
\[153 R. Geist, D. Kraines, and P. Fink, 
"Natural Language Computing in a 
Linear Algebra Course," Proceedings 
of the National Educational Computing 
C'on e~ence ,  June, i982. 
\[16\] N. Haas and G. Hendrix, "An Approach 
to Acquir ing and Applying Knowledge," 
First National Conference on Artif i-  
c 1 - -~ Inte l  !igence, 1980. 
\[17\] L.R. Harris, "User Oriented Data Base 
Query with the ROBOT Natural Language 
Query System," International Journal 
of Man-Machine Studies, pp. 6~-~,  
Sept em~e r---'~. 
\[183 L. Harris, "The ROBOT System: 
Natural Language Processing Applied 
to Database  Query," Proceedings of 
the 1978 ACM National Conference, p~. 
\[19\] J.P. Haton and J.M. Pierrel, "Data 
• Structures and Organization of the 
MYRT ILLE II System, " Fourth 
T.I.C.P.R., Kyoto, Japan, 1978. - -  
\[20\] G. Heidorn, "Natural Language Dialo- 
gue for Managing an On-Line Calen- 
dar, " IBM ~esear ch Report RC7447, 
1978. 
\[21\] G.G. Hendri x, E.D. Sacerdot i, D. 
Sagalowicz, and J. Slocum, "Develop- 
ing a Natural Language Interface to 
Complex Data, " ACM Transactions on 
Database Systems,-~-~ol. 3, No. 2, pp-'? 
r0~:rr~, rvrs:--. 
\[22\] G.G. Henarix, "Human Engineering for 
A oplied Natural Language Processing," 
Fifth International Conference on 
Ar---{'~icial Intelli~ence, pp. 183-191-~, 
~9~7. 
\[23\] G. Hendrix and W. Lewis, "Transport- 
able Natural Language Interfaces to 
Databases," Annual Meeting of the 
Assoc. for Compu~io~~uT-6t i c -~,  
\[24\] R. Hershman, R. Kelly, and H. Miller, 
"User Performance with a Natural 
Language Query System for Command 
Control," NPRDC TR 79-7, Navy Person- 
nel Research and Development Center, 
San Diego, California, January 1979. 
\[25\] J. Krause, "Results of a User Study 
with the "User Specialty Language," 
System and Consequences for the 
Architecture of Natural Language 
Interfaces," Technical Report 
79.04.003, IBM Heidelberg Scientif ic 
Center, May 1979. 
\[26\] W.A. Lea (Ed.), Trends in Speech 
Recognition, Prentice---~l,'-\[982. 
\[27\] J. Mylopoulos, A. Borgida, P. C~hen, 
N. Roussopoulos, J. Tsotsos, and H. 
Wong, "TORUS - A Natural Language 
Understanding System for Data Manage- 
ment," Proceedings of the Fourth 
International Conference on Arti f i -  
cial Intelli~enc"e, 1975. 
\[28\] S.R. Petrick, "On Natural Language 
Based Computer Systems," IBM Journal 
of Research and Development, Vol. ~-~, 
~.  4, pp. 3~-335,  1976. 
\[29\] W.J. Plath, "REQUEST: A Natural 
Language Quest ion-Answering System," 
ISM Journal of Research and Develop- 
ment, Vol. 20, No. 4, pp. 326-335, 
19-97~. 
\[30\] D.R. Reddy, "Speech Recognit ion by 
Machine: A Review," Proceedings of 
the IEEE, Vol. 64, No. 4, pp. 50~ "/- 
\[31\] H. Tennant, "~xperience with the 
Evaluation of Natural Language Ques- 
tion Answerers," Working Paper 18, 
Advanced Automation Group, Coordi-  
nated Science Lab., U-iv. of Illi- 
nois, January 1979. 
\[32\] F.B. Thompson and B.H. ~ompson,  
"Practical Natural Language Process- 
ing: The REL System as Prototype," 
in Advances in Computers, Vol. 13 
(Eds. M. Rubino--~f and M.C. Yovits), 
Academic Press, New York, 1975. 
\[33\] F. Thompson and B. Thompson, "Shaft- 
ing to a Higher Gear in a Natural 
Language System," AFIPS Proc. of the 
National Computer Conf., Vol. 50, pp. 
6~7-662, 1981. 
190 
\[34\] D.E. Walker (ed.), Understandin~ Spo- 
ken Language, Elsevier North-Holland, 
Ne"'wYork, 1978. 
\[35\] D.L. Waltz° "An English Language 
Ouestion Answering System for a Large 
Relational Database," Communications 
of the ACM, Vol. 21, No. 7, pp. 526- 
\[36\] W.A. Woods, R.M. Kaplan, and B. 
Nash-Webber0 "The Lunar Sciences 
Natural Language Information System: 
Final RepOrt," RepOrt 2378, Bolt, 
Berenek, and Newman° Cambridge, HA., 
1972. 
\ [37\ ]  W.A. Woods° "Mot lvat lon  and Overv iew 
of  SPEECHLIS: An Exper lmenta l  P ro to -  
type  fo r  Speech Unders tand in  9 
Research , "  IEEE Transact ions  on 
Acoust i cs ,  Spee-e'c~, and Stqna l  Pr~:- ~; .  vo- r :~sp-~,  ~o.-:-~, pp.--'~- 
t9t 
