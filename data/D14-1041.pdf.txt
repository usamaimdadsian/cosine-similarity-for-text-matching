Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363–373,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Multi-Predicate Semantic Role Labeling
Haitong Yang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{htyang, cqzong}@nlpr.ia.ac.cn
Abstract
The current approaches to Semantic Role
Labeling (SRL) usually perform role clas-
sification for each predicate separately and
the interaction among individual predi-
cate’s role labeling is ignored if there is
more than one predicate in a sentence. In
this paper, we prove that different predi-
cates in a sentence could help each other
during SRL. In multi-predicate role label-
ing, there are mainly two key points: argu-
ment identification and role labeling of the
arguments shared by multiple predicates.
To address these issues, in the stage of
argument identification, we propose nov-
el predicate-related features which help re-
move many argument identification errors;
in the stage of argument classification, we
adopt a discriminative reranking approach
to perform role classification of the shared
arguments, in which a large set of glob-
al features are proposed. We conducted
experiments on two standard benchmarks:
Chinese PropBank and English PropBank.
The experimental results show that our
approach can significantly improve SRL
performance, especially in Chinese Prop-
Bank.
1 Introduction
Semantic Role Labeling (SRL) is a kind of shal-
low semantic parsing task and its goal is to rec-
ognize some related phrases and assign a joint
structure (WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW) to each predicate of a sen-
tence (Gildea and Jurafsky, 2002). Because of
the ability of encoding semantic information, SR-
L has been applied in many tasks of NLP, such as
question and answering (Narayanan and Haraba-
gir, 2004), information extraction (Surdeanu et
The justices will be forced to reconsider  the questions.
[      A1      ] [  Pred  ]
[      A0      ] [    Pred    ] [      A1      ]
Figure 1: A sentence from English PropBank,
with an argument shared by multiple predicates
al., 2003; Christensen et al., 2005), and machine
translation (Wu and Fung, 2009; Liu and Gildea,
2010; Xiong et al., 2012; Zhai et al., 2012).
Currently, an SRL system works as follows:
first identify argument candidates and then per-
form classification for each argument candidate.
However, this process only focuses on one inde-
pendent predicate without considering the internal
relations of multiple predicates in a sentence. Ac-
cording to our statistics, more than 80% sentences
in Propbank carry multiple predicates. One exam-
ple is shown in Figure 1, in which there are two
predicates ‘Force’ and ‘Reconsider’. Moreover,
the constituent ‘the justices’ is shared by the two
predicates and is labeled as A1 for ‘Force’ but as
A0 for ‘Reconsider’. We call this phenomenon of
the shared arguments Role Transition . Intuitive-
ly, all predicates in a sentence are closely related to
each other and the internal relations between them
would be helpful for SRL.
This paper has made deep investigation on
multi-predicate semantic role labeling. We think
there are mainly two key points: argument identi-
fication and role labeling of the arguments shared
by multiple predicates. We adopt different strate-
gies to address these two issues.
During argument identification, there are a large
number of identification errors caused by the poor
performance of auto syntax trees. However, many
of these errors can be removed, if we take other
predicates into consideration. To achieve this pur-
pose, we propose novel predicates-related features
which have been proved to be effective to recog-
363
nize many identification errors. After these fea-
tures added, the precision of argument identifica-
tion improves significantly by 1.6 points and 0.9
points in experiments on Chinese PropBank and
English PropBank respectively, with a slight loss
in recall.
Role labeling of the shared arguments is anoth-
er key point. The predicates and their shared argu-
ment could be considered as a joint structure, with
strong dependencies between the shared argumen-
t’s roles. If we consider linguistic basis for joint
modeling of the shared argument’s roles, there are
at least two types of information to be captured.
The first type of information is the compatibility
of Role Transition among the shared argument’s
roles. A noun phrase may be labeled as A0 for a
predicate and at the same time, it can be labeled
as A1 for another predicate. However, there are
few cases that a noun phrase is labeled as A0 for a
predicate and as AM-ADV for another predicate
at the same time. Secondly, joint modeling the
shared arguments could explore global informa-
tion. For example, in ‘The columbia mall is ex-
pected to open’, there are two predicates ‘expect’
and ‘open’ and a shared argument ‘the columbi-
a mall’. Because this shared argument is before
‘open’ and the predicate ‘open’ is in active voice,
a base classifier often incorrectly label this argu-
ment A0 for ‘open’. But if we observe that the ar-
gument is also an argument of ‘expect’, it should
be labeled as A1 for ‘expect’ and ‘open’.
Motivated by the above observations, we at-
tempt to jointly model the shared arguments’ roles.
Specifically, we utilize the discriminative rerank-
ing approach that has been successfully employed
in many NLP tasks. Typically, this method first
creates a list of n-best candidates from a base sys-
tem, and then reranks them with arbitrary features
(both local and global), which are either not com-
putable or are computationally intractable within
the base model.
We conducted experiments on Chinese Prop-
Bank and English PropBank. Results show that
compared with a state-of-the-art base model, the
accuracy of our joint model improves significant-
ly by 2.4 points and 1.5 points on Chinese Prop-
Bank and English PropBank respectively, which
suggests that there are substantial gains to be made
by jointly modeling the shared arguments of mul-
tiple predicates.
Our contributions can be summarized as fol-
lows:
• To the best of our knowledge, this is the first
work to investigate the mutual effect of mul-
tiple predicates’ semantic role labeling.
• We present a rich set of features for argument
identification and shared arguments’ classifi-
cation that yield promising performance.
• We evaluate our method on two standard
benchmarks: Chinese PropBank and English
PropBank. Our approach performs well in
both, which suggests its good universality.
The remainder of this paper is organized as fol-
lows. Section 2 gives an overview of our approach.
We discuss the mutual effect of multi-predicate’
argument identification and argument classifica-
tion in Section 3 and Section 4 respectively. The
experiments and results are presented in Section
5. Some discussion and analysis can be found in
Section 6. Section 7 discusses the related work-
s. Finally, the conclusion and future work are in
Section 8.
2 Approach Overview
As illustrated in Figure 2, our approach follows the
standard separation of the task of semantic role la-
beling into two phases: Argument Identification
and Argument Classification . We investigate the
effect of multiple predicates in Argument Identi-
fication and Argument Classification respectively.
Specifically, in the stage of Argument Identifica-
tion, we introduce new features related to predi-
cates which are effective to recognize many argu-
ment identification errors. In the stage of Argu-
ment Classification, we concentrate on the classi-
fication of the arguments shared by multiple pred-
icates. We first use a base model to generate n-
best candidates for the shared arguments and then
construct a joint model to rerank the n-best list, in
which a rich set of global features are proposed.
3 Argument Identification
In this section, we investigate multi-predicate’ mu-
tual effects in Argument Identification. Argument
Identification is to recognize the arguments from
all candidates of each predicate. Here, we use
the Maximum Entropy (ME) classifier to perform
binary classification. As a discriminative model,
ME can easily incorporate arbitrary features and
364
C andidates
Phase 1: 
Argument Identification Base 
Features
 New 
Features
Cl assifier
Refined Argument 
C andidates
Phase 2 : 
Argument Classification
Joint Model
B ase Model
N - Best list
Final Results
Is - Shared
Figure 2: The overview of our approach
achieve good performance. The model is formu-
lated as follows:
p(y|x) =
1
Z(x)
exp(
?
i
?
i
f
i
(x, y)) (1)
in which x is the input sample, y(0 or 1) is the out-
put label, f(x, y) are feature functions and Z(x)
is a normalization term as follows:
Z(x) =
?
y
exp(
?
i
?
i
f
i
(x, y))
3.1 Base Features
Xue (2008) took a critical look at the features used
in SRL and achieved good performance. So, we
use the same features in Xue (2008) as the base
features:
• Predicate lemma
• Path from node to predicate
• Head word
• Head word’s part-of-speech
• Verb class (Xue, 2008)
• Predicate and Head word combination
• Predicate and Phrase type combination
• Verb class and Head word combination
• Verb class and Phrase type combination
3.2 Additional Features
In the SRL community, it is widely recognized
that the overall performance of a system is large-
ly determined by the quality of syntactic parsers
(Gildea and Palmer, 2002), which is particularly
notable in the identification stage. Unfortunate-
ly, the state-of-the-art auto parsers fall short of the
demands of applications. Moreover, when there
are multiple predicates, or even multiple clauses
in a sentence, the problem of syntactic ambiguity
increases drastically (Kim et al., 2000). For ex-
ample, in Figure 3, there is a sentence with two
consecutive predicates ‘/’ (is) and ‘ Ñ’ (devel-
op). Compared with the gold tree, the auto tree is
less preferable, which makes the classifier easily
mistake ‘úQ’ (building) as an argument of ‘ 
Ñ’ (develop) with base features. But this identifi-
cation error can be removed if we note that there
is another predicate ‘/’ (is) before ‘ Ñ’ (devel-
IP
NP VP
V V NP
DNP NP
VC VP
N N D EG
IP
NP VP
VC NP
CP NP
IP DEC
V V NP
??
?
?? ??
?
????
??
??
?
?? ?
????
( a ) ( b)
?? ? ?? ?? ? ????
Building is an economic activity of developing Pudong .
Figure 3: An example from Chinese PropBank.
Tree (a) is the gold syntax tree and (b) is parsed by
a state of-the-art parser Berkeley parser. On tree
(b), ‘úQ’ (building) is mistaken as an argument
of ‘ Ñ’ (develop) with base features.
365
op). Similar examples with the pattern ‘NP +/ +
VV’ can be found in PropBank, in which the sub-
ject NP of the sentence is usually not an argument
of the latter predicate. Thus, ‘/’ (is) is an effec-
tive clue to detect this kind of identification error.
It is challenging to obtain a fully correct syntax
tree for a complex sentence with multiple predi-
cates. Therefore, base features that heavily rely
on syntax trees often fail in discriminating argu-
ments from candidates as demonstrated in Figure
3. However, by considering the elements of neigh-
boring predicates, we could capture useful clues
like in the above example and remove many iden-
tification errors. Below, we define novel predi-
caterelated features to encode these ‘clues’ to re-
fine candidates.
There are mainly five kinds of features as fol-
lows.
• Is the given predicate the nearest one?
This is a binary feature that indicates whether
the predicate is the nearest one to the candi-
date.
• Local adjunct
This is a binary feature designed for adjective
and adverbial phrases. Some adjunct phras-
es, such as ‘Å’ (only), have a limited sphere
of influence. If the candidate is ‘local’ but
the given predicate is not the nearest one, the
candidate is often not an argument for the
given predicate. To collect local adjuncts, we
traverse the whole training set to get the ini-
tial lexicon of adjuncts and refine it manually.
• Cut-Clause
This type of feature is a binary feature de-
signed to distinguish identification errors of
noun phrase candidates. If a noun phrase can-
didate is separated from the given predicate
by a clause consisting of a NP and VP, the
candidate is usually not the argument of the
given predicate.
• Different Relative Positions with Conjunc-
tions
This is a binary feature that describes whether
the candidate and the predicate are located in
different positions as separated by conjunc-
tions such as ‘F/’ (but). Conjunctions are
often used to concatenate two clauses, but the
first clause commonly describes one proposi-
tion and the second clause describes anoth-
er one. Thus, if the candidate and the given
predicate have different positions relative to
the conjunctions, the candidate is usually not
the argument of the given predicate.
• Consecutive Predicates Sequence
When multiple predicates appear in a sen-
tence consecutively, parse errors frequently
occurs due to the problems of syntactic am-
biguity as demonstrated in Figure 2. To in-
dicate such errors, sequence features of the
candidates and consecutive predicates are de-
fined specifically. For instance, for the candi-
date ‘úQ’ (building) of ‘ Ñ’ (develop),
the features are ‘cand-/- Ñ’ and ‘cand-
/-VV’, in which we use ‘cand’ to represent
the position of the candidate.
4 Argument Classification
In this section, we investigate multi-predicate’ mu-
tual effects in Argument Classification. Argument
Classification is to assign a label to each argumen-
t candidate recognized by the phase of Argument
Identification.
4.1 Base Model
A conventional method in Argument Classifica-
tion is to assign a label to each argument candidate
by a classifier independently. We call this kind of
method Base Model. In the base model, we still
adopt ME (1) as our classifier; all base features of
Argument Identification are contained (shown in
subsection 3.1). In addition, there are some other
features:
• Position: the relative position of the candi-
date argument compared to the predicate
• Subcat frame: the syntactic rule that expands
the parent of the verb
• The first and the last word of the candidate
• Phrase type: the syntactic tag of the candidate
argument
• Subcat frame+: the frame that consists of the
NPs (Xue, 2008).
366
4.2 Joint Model
As discussed briefly in Section 1, there are many
dependencies between the shared arguments’ la-
beling for different predicates, but the base model
completely ignores such useful information. To
incorporate these dependencies, we employ the
discriminative reranking method. Here, we first
establish a unified framework for reranking. For
an input x, the generic reranker selects the best
output y
?
among the set of candidates GEN(x)
according to the scoring function:
y
?
= argmax
y?GEN(x)
score(y) (2)
In our task, GEN(x) is a set of the n-best can-
didates generated from the base model. As usual,
we calculate the score of a candidate by the dot
product between a high dimensional feature and a
weight W:
score(y) = W · f(y) (3)
We estimate the weight W using the aver-
aged perceptron algorithm (Collins, 2002a) which
is well known for its fast speed and good per-
formance in similar large-parameter NLP tasks
(Huang, 2008). The training algorithm of the
generic averaged perceptron is shown in Table 1.
In line 5, the algorithm updates W with the differ-
ence (if any) between the feature representations
of the best scoring candidate and the gold candi-
date. We also use a refinement called “averaged
parameters” that the final weight vector W is the
average of weight vectors over T iterations and N
samples. This averaging effect has been shown to
reduce overfitting and produces more stable results
(Collins, 2002a).
Pseudocode: Averaged Structured Perceptron
1: Input: training data(x
t
, y
?
t
) for t = 1, ..., T ;
2: w¯
(0)
? 0; v ? 0; i? 0
3: for n in 1,...,N do
4: for t in 1, ..., T do
5: w¯
(i+1)
? update w¯
(i)
according to (x
t
, y
?
t
)
6: v ? v + w¯
i+1
7: i? i+ 1
8: w¯ ? v//(N ? T )
9: return w¯
Table 1: The perceptron training algorithm
4.3 Features for Joint Model
Here, we introduce features used in the joint mod-
el. For clear illustration, we describe these fea-
tures in the context of the example in Figure 1.
Role Transition (RT): a binary feature to in-
dicate whether the transitions among roles of the
candidate are reasonable. Because all roles are as-
signed to the same candidate, all role transitions
should be compatible. For instance, if an argu-
ment is labeled as AM-TMP for one predicate, it
cannot be labeled as AM-LOC for another pred-
icate. This feature is constructed by traversing
the training data to ascertain whether transitions
between all roles are reasonable. In Table 2, we
list some role transitions which are obtained from
the training data of experiments on Chinese Prop-
Bank.
Roles and Predicates’ Sequence (RPS): a
joint feature template that concatenates roles and
the given predicates. For the gold candidate
‘Arg1, Arg0’, the feature is ‘Arg1-force, Arg0-
reconsider’.
Roles and Predicates’ Sequence with Rela-
tive Orders (RPSWR): the template is similar to
the above one except that relative orders between
roles and predicates are added. If the shared argu-
ment is before the given predicate, the feature is
described as ‘Role-Predicate’; otherwise, the fea-
ture is ‘Predicate-Role’. And, if the predicate’s
voice is passive, the order is reversed. Thus, for
the gold candidate ‘Arg1, Arg0’, this feature is
‘force-Arg1, Arg0-reconsider’.
Roles and Phrase Type Sequence (RPTS)
Roles and Head Word Sequence (RHWS)
Roles and Head Word’s POS Sequence
(RHWPS)
These three features are utilized to explore the
shared argument’s relations with roles.
Time and Location Class (TLC): We find
there are much confusions between AM-TMP and
AM-LOC in the base model. To fix these errors,
we add two features: Time and Location Class.
For these features, we just collect phrases labeled
as AM-TMP and AM-LOC from the training da-
ta. When the argument belongs to Time or Loca-
tion Class, we add a sequence template consisting
of ‘Role-Time’ for Time Class or ‘Role-Location’
for Location Class. For the gold candidate ‘Arg1,
Arg0’, the feature is ‘Arg1-none, Arg0-none’ be-
cause ‘the justices’ belongs neither to Time Class
nor to Location Class.
367
Role Arg0 Arg1 Arg2 AM-LOC AM-TMP AM-ADV AM-MNR AM-TPC
Arg0 + + + + + + + +
Arg1 + + + + - + + +
Arg2 + + + + - - - +
AM-LOC + + + + - + - +
AM-TMP + - - - + + - -
AM-ADV + - + - + + - -
AM-MNR + + - - - - + -
AM-TPC + + + + + + - +
Table 2: Some role transitons from Chinese PropBank. “+” means reasonable role transition and “-”
means illegal.
5 Experiments
5.1 Experimental Setting
To evaluate the performance of our approach, we
have conducted on two standard benchmarks: Chi-
nese PropBank and English PropBank. The exper-
imental setting is as follows:
Chinese:
We use Chinese Proposition Bank 1.0. All data
are divided into three parts. 648 files (from cht-
b 081.fid to chtb 899.fid) are used as the training
set. 40 files (from chtb 041.fid to chtb 080.fid)
constitutes the development set. The test set con-
sists of 72 files (chtb 001.fid to chtb 040.fid and
chtb 900.fid to chtb 931.fid). This data setting is
the same as in (Xue, 2008; Sun et al., 2009). We
adopt Berkeley Parser
1
to carry out auto parsing
for SRL and the parser is retrained on the training
set. We used n =10 joint assignments for training
the joint model and testing.
English:
We choose English Propbank as the evaluation
corpus. According to the traditional partition, the
training set consists of the annotations in Sections
2 to 21, the development set is Section 24, and
the test set is Section 23. This data setting is the
same as in (Xue and Palmer, 2004; Toutanova et
al., 2005). We adopt Charniak Parser
2
to carry out
auto parsing for SRL and the parser is retrained on
the training set. We used n =10 joint assignments
for training the joint model and testing.
5.2 Experiment on Argument Identification
We first investigate the performance of our ap-
proach in Argument Identification.
For the task of Argument Identification (AI), we
1
http://code.google.com/p/berkeleyparser/
2
https://github.com/BLLIP/bllip-parser
adopt auto parser to produce auto parsing trees for
SRL. The results are shown in Table 3. We can
see that in the experiment of Chinese, the F1 score
reaches to 78.79% with base features. While after
additional predicates-related features are added,
the precision has improved by 1.6 points with s-
light loss in recall, which leads to the improve-
ment of 0.6 points in F1. The similar effect oc-
curred in the experiment of English. After addi-
tional features added in the identification module,
the precision is improved by about 0.9 points with
a slight loss in recall, leading to an improvement
of 0.3 points in F1. However, the improvemen-
t in English is slight smaller than in Chinese. We
think the main reason is that there are less parse er-
rors in English than in Chinese. All results demon-
strate that the novel predicted-related features are
effective in recognizing many identification errors
which are difficult to discriminate with base fea-
tures.
P(%) R(%) F
1
(%)
Ch
Base 84.36 73.90 78.79
+Additional 85.97 73.72 79.38*
En
Base 82.86 76.83 79.73
+Additional 83.75 76.69 80.06
Table 3: Comparison with Base Features in Ar-
gument Identification. Scores marked by “*” are
significantly better (p < 0.05) than base features.
5.3 Experiment on Argument Classification
5.3.1 Results
Errors produced in AI will influenced the evalu-
ation of Argument Classification (AC). So, to e-
valuate fairly we assume that the argument con-
stituents of a predicate are already known, and the
368
Num Acc(%)
Ch
Shared 2060 91.36
All 8462 92.77
En
Shared 2015 93.85
All 14061 92.30
Table 4: Performance of the Base Model in Argu-
ment Classification
Methods Acc(%)
Ch
Base 91.36
Joint 93.74*
En
Base 93.85
Joint 95.33*
Table 5: Comparison with Base Model on shared
arguments. Scores marked by “*” are significantly
better (p < 0.05) than base model.
task is only to assign the correct labels to the con-
stituents. The evaluation criterion is Accuracy.
The results of the base model are shown in Ta-
ble 4. We first note that in testing set, there are a
large number of shared arguments, which weigh-
s about one quarter of all arguments in Chinese
and 14% in English. Therefore, the fine process-
ing of these arguments is essential for argumen-
t classification. However, the base model cannot
handle these shared arguments so well in Chinese
that the accuracy of the shared arguments is lower
by about 1.4 points than the average value of all
arguments. Nevertheless, from Table 5 we can see
that our joint model’s accuracy on the shared argu-
ments reaches 93.74%, 2.4 points higher than the
base model in Chinese. Although the base mod-
el obtain good performance on shared arguments
of English, our joint model’s performance reach-
es 95.33%, 1.5 points higher than the base mod-
el. This indicates that even though the base model
is optimized to utilize a large set of features and
achieves the state-of-the-art performance, it is still
advantageous to model the joint information of the
shared arguments.
Another point to note is that our joint model in
resolving English SRL task is not so good as in
Chinese SRL. There are mainly two reasons. The
first reason is that the shared arguments occur less
in English than in Chinese so that training sam-
ples are insufficient for our discriminative model.
The second reason is the annotation of some in-
transitive verbs. In English PropBank, there is a
class of intransitive verbs such as “land” (known
as verbs of variable behavior), for which the ar-
gument can be tagged as either ARG0 or ARG1.
Here, we take examples from the guideline
3
of En-
glish PropBank to explain.
“A bullet (ARG1) landed at his feet”
“He (ARG0) landed”
In the above examples, the two arguments and
the predicate ‘land’ have the same relative order
and voice but the arguments have different label-
s for their respective predicates. In fact, accord-
ing to the intention of the annotator, ARG0 and
ARG1 are both correct. Unfortunately, in English
PropBank, there is only one gold label for each ar-
gument, which leads to much noise for our joint
model. Moreover, such situations are not rare in
the corpus.
5.3.2 Feature Performance
We investigate effects of the features of joint mod-
el to the performance and results are shown in Ta-
ble 6. Each row shows the improvement over the
baseline when that feature is used in the joint mod-
el. We can see that features proposed are beneficial
to the performance of the joint model. But some
features like ‘RPS’ and ‘RPSRO’ play a more im-
portant role.
Features Chinese English
base 91.36 93.85
RT 91.70 94.10
RPS 92.30 94.70
RPSRO 92.24 94.50
RPTS 91.80 94.18
RHWS 91.63 93.95
RHWPS 91.43 94.23
TCL 91.93 94.23
All 93.74 95.33
Table 6: Features performance in the Joint Model.
We use first letter of words to represent features.
5.4 SRL Results
We also conducted the complete experiment on the
auto parse trees. The results are shown in Table
7. In experiments on Chinese PropBank, we can
see that after novel predicate-related features are
added in the stage of Argument Identification, our
model outperforms the base model by 0.5 points
3
http://verbs.colorado.edu/propbank/EPB-
AnnotationGuidelines.pdf
369
F1
(%)
Chinese
Base 74.04
Base + AI 74.50
Base + AI + AC 75.31
English
Base 76.44
Base + AI 76.70
Base + AI + AC 77.00
Table 7: Results on auto parse trees. Base mean-
s the baseline system, +AI meaning predcates-
related features added in AI, + AC meaning joint
module added.
Methods F
1
(%)
Chinese
Xue(2008) 71.90
Sun et al.(2009) 74.12
Ours 75.31
English
Surdeanu and Turmo(2005) 76.46
Ours 77.00
Table 8: Comparison with Other Methods
in F1. Furthermore, after incorporating the joint
module, the performance goes up to 75.31%, 1.3
points higher than the base model. We obtain sim-
ilar observations in experiments on English Prop-
Bank, but due to reasons illustrated in Subsection
5.3, the performance of our method is slight better
than the base model.
We compare our method with others and the re-
sults are shown in Table 8. In Chinese, Xue (2008)
and Sun et al. (2009) are pioneer works in Chinese
SRL. Our approach outperforms these approaches
by about 3.4 and 1.9 F1 points respectively. In
English SRL, we compare out method with Sur-
deanu and Turmo (2005) which is best result ob-
tained with single parse tree as the input in CON-
LL 2005 SRL evaluation. Our approach is better
than their approach which ignores the relation of
multiple predicates’ SRL.
6 Discussion and Analysis
In this section, we discuss some case studies that
illustrate the advantages of our model. Some ex-
amples from our experiments are shown in Table
9. In example (1), the argument is a preposition-
al phrase ‘( nÊ ] t I¡ Y² ? ö’
(at the same time of compulsory education) and
shared by two predicates ‘?0’ (witness) and ‘i
'’ (expand). In the corpus, a prepositional phrase
is commonly labeled as ARGM-LOC and ARGM-
TMP. Thus, the base model labeled the argument
into these classes but one as ARGM-LOC, another
as ARGM-TMP. Unfortunately, ARGM-LOC for
‘?0’ (witness) is wrong while our joint model
outputs both correct answers, which benefits from
the role transition feature. From Table 1, we can
see that the role transition between ARGM-TMP
and ARGM-LOC is impossible, which lowers the
score of candidates containing both ARGM-LOC
and ARGM-TMP in the joint model. Thus, the
joint model is more likely to output the gold can-
didate.
In example (2), the argument is ‘wÉ Þ:
:’ (Hailar Airport) and shared by two predicates
‘iú’ (expand) and ‘:’ (become). Because of
the high similarity of the features in the base mod-
el, the argument for both predicates is classified
into the same class ARG0, but the label for ‘iú’
(expand) is wrong. Nevertheless, our joint mod-
el obtains both correct labels, which benefits from
the global features. After searching the training
data, we find some similar examples to this one,
such as ‘0Á ÐL Ì ò iú ó  l
Ì’ (The railway operation mileage is expanded to
120 kilometers), in which ‘0Á ÐL Ì’ (the
railway operation mileage) is labeled as ARG1 for
‘iú’ (expand) but ARG0 for ‘ó’ (to). We think
these samples provide evidence for our joint mod-
el while these information has not been captured
by the base model.
In example (3), the argument is ‘ý? å
¦? Ø ? ' Æâ’ (a large group with high
reputation) and shared by predicates ‘ÑU’ (de-
velop) and ‘:’ (become). Different from the
above cases in which only one label is wrong in
the base model, both labels for ‘ÑU’ (develop)
and ‘:’ (become) are misclassified by the base
model. However, our method still gets correct an-
swers for both predicates, which also benefits from
the global features.
7 Related work
Our work is related to semantic role labeling
and discriminative reranking. In this section, we
briefly review these two types of work.
On Semantic Role Labeling
Gildea and Jurafsky (2002) first presented a sys-
tem based on a statistical classifier which is trained
on a hand-annotated corpora FrameNet. In their
pioneering work, they used a gold or autoparsed
syntax tree as the input and then extracted vari-
ous lexical and syntactic features to identify the
370
Examples Base Ours
1. (nÊ]tI¡Y²?ö-I
LY²_?01ÑUÄ!­i'2
(At the same time of compulsory education,
secondary vocational education have achieved
1
significant development and constant expanding
2
)
ARGM-LOC |?0
ARGM-TMP |i'
ARGM-TMP |?0
ARGM-TMP |i'
2. wÉÞ::iú1:2 ýE*z/
(Hailar Airport had been expanded
1
and became
2
an international airport)
ARG0 |iú
ARG0 |:
ARG1 |iú
ARG0 |:
3. wÆâ? . 6eò?4mA¿C,
ÑU1:2 ý?å¦?Ø?'Æâ
(Haier Group’s sales revenue has exceeded six
billion yuan and it has developed
1
to be
2
a
large group with high reputation)
ARG1 |ÑU
ARG0 |:
ARG3 |ÑU
ARG1 |:
Table 9: Some examples in our experiments
semantic roles for a given predicate. After Gildea
and Jurafsky (2002), there have been a large num-
ber of works on automatic semantic role label-
ing. Based on a basic discriminative model, Pun-
yakanok et al. (2004) constructed an integer linear
programming architecture, in which the dependen-
cy relations among arguments are implied in the
constraint conditions. Toutanova et al. (2008) pro-
posed a joint model to explore relations of all ar-
guments of the same predicate. Unlike them, this
paper focus on mining relations of different pred-
icates’ semantic roles in one sentence. And, there
have been many extensions in machine learning
models (Moschitti et al., 2008), feature engineer-
ing (Xue and Palmer, 2004), and inference pro-
cedures (Toutanova et al., 2005; Punyakanok et
al., 2008; Zhuang and Zong, 2010a; Zhuang and
Zong, 2010b).
Sun and Jurafsky (2004) did the preliminary
work on Chinese SRL without employing any
large semantically annotated corpus of Chinese.
They just labeled the predicate-argument struc-
tures of ten specified verbs to a small collection
of Chinese sentences, and utilized Support Vec-
tor Machine to identify and classify the arguments.
They made the first attempt on Chinese SRL and
produced promising results. After the PropBank
(Xue and Palmer, 2003) was built, Xue and Palmer
(2004) and Xue (2008) took a critical look at fea-
tures of argument detection and argument classi-
fication. Unlike others’ using syntax trees as the
input of SRL, Sun et al. (2009) performed Chi-
nese semantic role labeling with shallow parsing.
Li et al. (2010) explored joint syntactic and se-
mantic parsing of Chinese to further improve the
performance of both syntactic parsing and SRL.
However, to the best of our knowledge, in
the literatures, there is no work related to multi-
predicate semantic role labeling.
On Discriminative Reranking
Discriminative reranking is a common approach
in the NLP community. Its general procedure is
that a base system first generates n-best candidates
and with the help of global features, we obtain
better performance through reranking the n-best
candidates. It has been shown to be effective for
various natural language processing tasks,such as
syntactic parsing (Collins, 2000; Collins, 2002b;
Collins and Koo, 2005; Charniak and Johnson,
2005; Huang, 2008), semantic parsing (Lu et al.,
2008; Ge and Mooney, 2006), part-of-speech tag-
ging (Collins, 2002a), named entity recognition
(Collins, 2002c), machine translation (Shen et al.,
2004) and surface realization in generation (Kon-
stas and Lapata, 2012).
8 Conclusion and Feature Work
This paper investigates the interaction effect a-
mong multi-predicate’s SRL. Our investigation
has shown that there is much interaction effec-
t of multi-predicate’s SRL both in Argument Iden-
tification and in Argument Classification. In the
stage of argument identification, we proposed nov-
el features related to predicates and successfully
removed many argument identification errors. In
the stage of argument classification, we concen-
trate on the classification of the arguments shared
by multiple predicates. Experiments have shown
371
that the base model often fails in classifying the
shared arguments. To perform the classification of
the shared arguments, we propose a joint model
and with the help of the global features, our join-
t model yields better performance than the base
model. To the best of our knowledge, this is the
first work of investigating the interaction effect of
multi-predicate’s SRL.
In the future, we will explore more effective fea-
tures for multi-predicate’s identification and clas-
sification. Since we adopt reranking approach in
the shared arguments’ classification, the perfor-
mance is limited by n-best list. Also, we would
like to explore whether there is another method to
resolve the problem.
Acknowledgments
We thank the three anonymous reviewers for their
helpful comments and suggestions. We would al-
so like to thank Nianwen Xue for help in base-
line system, and Tao Zhuang, Feifei Zhai, Lu Xi-
ang and Junjie Li for insightful discussions. The
research work has been partially funded by the
Natural Science Foundation of China under Grant
No.61333018 and the Hi-Tech Research and De-
velopment Program (“863” Program) of China un-
der Grant No.2012AA011101, and also the High
New Technology Research and Development Pro-
gram of Xinjiang Uyghur Autonomous Region un-
der Grant No.201312103 as well.
References
Collin F. Baker, Charles j. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of COLING-ACL 1998.
Janara Christensen, Mausam, Stephen Soderland and
Oren Etzioni. 2010. Semantic Role Labeling for
Open Information Extraction. In Proceedings of A-
CL 2010.
Eugene Charniak and Mark Johnson. 2005. Coarse
tofine n-best parsing and maxent discriminative r-
eranking. In Proceedings of ACL 1998.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):2569.
Michael Collins. 2000. Discriminative reranking for-
natural language parsing. In Proceedings of ICML
2000.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of
ACL 2002.
Michael Collins. 2002c. Ranking algorithms fornamed-
entity extraction: Boosting and the voted perceptron.
In Proceedings of ACL 2002.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimi-
native reranking for semantic parsing. In Proceed-
ings of COLING/ACL 2002.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling for semantic roles. In Computational Lin-
guistics, 28(3): 245-288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
ACL 2002.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL 2008.
Sung Dong Kim, Byoung-Tak Zhang and Yung Taek
Kim. 2000. Reducing Parsing Complexity by In-
traSentence Segmentation based on Maximum En-
tropy Model. In Proceedings of SIGDAT 2000.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of LREC 2002.
Junhui Li, Guodong Zhou and Hwee Tou Ng. 2010.
Joint Syntactic and Semantic Parsing of Chinese. In
Proceedings of ACL 2010.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of
COLING 2010.
Junhui Li, Guodong Zhou and Hwee Tou Ng. 2010.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke
S. Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of EMNLP 2008.
Alessandro Moschitti, Daniel Pighin and Roberto
Basili. 2008. Tree Kernels for Semantic Role Label-
ing. In Computational Linguistics, 34(2): 193-224.
Srini Narayanan and Sanda Harabagiu. 2004. Question
Answering based on Semantic Structures. In Pro-
ceedings of COLING 2004.
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING 2004.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of HLT/NAACL 2004.
372
Mihai Surdeanu, Sanda Harabagiu, John Williams
and Paul Aarseth. 2003. Using Predicate-Argument
Structures for Information Extraction. In Proceed-
ings of ACL 2003.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic Role
Labeling Using Complete Syntactic Analysis. In
Proceedings of CONLL 2005.
Weiwei Sun, Zhifang Sui, Meng Wang and Xin Wang.
2009. Chinese Semantic Role Labeling with Shallow
Parsing. In Proceedings of ACL 2009.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
totext generation via discriminative reranking. In
Proceedings of ACL 2012.
Kristina Toutanova, Aria Haghighi and Christopher
D.Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL 2005.
Kristina Toutanova, Aria Haghighi and Christopher
D.Manning. 2008. A Global Joint Model for Seman-
tic Role Labeling. In Computational Linguistics,
34(2): 161-191.
Dekai Wu and Pascale Fung. 2009. Can semantic role
labeling improve smt. In Proceedings of EAMT
2009.
Deyi Xiong, Min Zhang and Haizhou Li. 2012. Model-
ing the Translation of Predicate-Argument Structure
for SMT. In Proceedings of ACL 2012.
Nianwen Xue and Martha Palmer. 2003. Annotating
the Propositions in the Penn Chinese Treebank. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing..
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of EMNLP 2004.
Nianwen Xue. 2008. Labeling Chinese Predicates with
Semantic Roles. In Computational Linguistics,
34(2): 225-255.
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing
Zong. 2012. Machine Translation by Modeling
Predicate-Argument Structure Transformation. In
Proceedings of COLING 2012.
Tao Zhuang and Chengqing Zong. 2010a. A Minimum
Error Weighting Combination Strategy for Chinese
Semantic Role Labeling. In Proceedings of COL-
ING 2010.
Tao Zhuang and Chengqing Zong. 2010b. Joint Infer-
ence for Bilingual Semantic Role Labeling. In Pro-
ceedings of EMNLP 2010.
373
