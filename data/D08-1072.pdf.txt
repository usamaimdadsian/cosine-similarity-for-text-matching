Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689–697,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Online Methods for Multi-Domain Learning and Adaptation
Mark Dredze and Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104 USA
{mdredze,crammer}@cis.upenn.edu
Abstract
NLP tasks are often domain specific, yet sys-
tems can learn behaviors across multiple do-
mains. We develop a new multi-domain online
learning framework based on parameter com-
bination from multiple classifiers. Our algo-
rithms draw from multi-task learning and do-
main adaptation to adapt multiple source do-
main classifiers to a new target domain, learn
across multiple similar domains, and learn
across a large number of disparate domains.
We evaluate our algorithms on two popular
NLP domain adaptation tasks: sentiment clas-
sification and spam filtering.
1 Introduction
Statistical classifiers routinely process millions of
websites, emails, blogs and other text every day.
Variability across different data sources means that
training a single classifier obscures differences and
separate classifiers ignore similarities. Similarly,
adding new domains to existing systems requires
adapting existing classifiers.
We present new online algorithms for three multi-
domain learning scenarios: adapting existing classi-
fiers to new domains, learning across multiple simi-
lar domains and scaling systems to many disparate
domains. Multi-domain learning combines char-
acteristics of both multi-task learning and domain
adaptation and drawing from both areas, we de-
velop a multi-classifier parameter combination tech-
nique for confidence-weighted (CW) linear classi-
fiers (Dredze et al., 2008). We focus on online algo-
rithms that scale to large amounts of data.
Next, we describe multi-domain learning and re-
view the CW algorithm. We then consider our three
settings using multi-classifier parameter combina-
tion. We conclude with related work.
2 Multi-Domain Learning
In online multi-domain learning, each instance x is
drawn from a domain d specific distribution x ? Dd
over a vectors space RN and labeled with a domain
specific function fd with label y ? {?1,+1} (for
binary classification.) On round i the classifier re-
ceives instance xi and domain identifier di and pre-
dicts label yˆi ? {?1,+1}. It then receives the true
label yi ? {?1,+1} and updates its prediction rule.
As an example, consider a multi-user spam fil-
ter, which must give high quality predictions for
new users (without new user data), learn on multi-
ple users simultaneously and scale to thousands of
accounts. While a single classifier trained on all
users would generalize across users and extend to
new users, it would fail to learn user-specific prefer-
ences. Alternatively, separate classifiers would cap-
ture user-specific behaviors but would not general-
ize across users. The approach we take to solv-
ing multi-domain problems is to combine domain-
specific classifiers. In the adaptation setting, we
combine source domain classifiers for a new tar-
get domain. For learning across domains, we com-
bine domain-specific classifiers and a shared classi-
fier learned across all domains. For learning across
disparate domains we learn which domain-specific
and shared classifiers to combine.
Multi-domain learning combines properties of
both multi-task learning and domain adaptation. As
689
in multi-task learning, we consider domains that are
labeled with different classification functions. For
example, one user may enjoy some emails that an-
other user considers spam: differing in their classifi-
cation function. The goal of multi-task learning is to
generalize across tasks/domains (Dekel et al., 2006;
Evgeniou and Pontil, 2004). Furthermore, as in do-
main adaptation, some examples are draw from dif-
ferent distributions. For example, one user may re-
ceive emails about engineering while another about
art, differing in their distribution over features. Do-
main adaptation deals with these feature distribution
changes (Blitzer et al., 2007; Jiang and Zhai, 2007).
Our work combines these two areas by learning both
across distributions and behaviors or functions.
3 Confidence-Weighted Linear Classifiers
Confidence-weighted (CW) linear classification
(Dredze et al., 2008), a new online algorithm, main-
tains a probabilistic measure of parameter confi-
dence, which may be useful in combining parame-
ters from different domain distributions. We sum-
marize CW learning to familiarize the reader.
Parameter confidence is formalized by a Gaussian
distribution over weight vectors with mean µ ? RN
and diagonal covariance ? ? RN×N . The values
µj and ?j,j represent knowledge of and confidence
in the parameter for feature j. The smaller ?j,j ,
the more confidence we have in the mean parameter
value µj . In this work we consider diagonal covari-
ance matrices to scale to NLP data.
A model predicts the highest probability label,
arg max
y?{±1}
Prw?N (µ,?) [yi(w · xi) ? 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
score Si = w · xi parameterized by µ, ? and the
instance xi: Si ? N
(
µi, ?2i
)
, with mean µi = µ·xi
and variance ?2i = x
>
i ?xi.
The CW algorithm is inspired by the Passive Ag-
gressive (PA) update (Crammer et al., 2006) —
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the Kullback-
Leibler (KL) divergence over Gaussian distribu-
tions. It also replaces the minimal margin constraint
with a minimal probability constraint: with some
given probability ? ? (0.5, 1] a drawn classifier will
assign the correct label. This strategy yields the fol-
lowing objective solved on each round of learning:
min DKL (N (µ,?) ?N (µi,?i))
s.t. Pr [yi (w · xi) ? 0] ? ? ,
where (µi,?i) are the parameters on round i and
w ? N (µ,?). The constraint ensures that the re-
sulting parameters
(
µi+1,?i+1
)
will correctly clas-
sify xi with probability at least ?. For convenience
we write ? = ??1 (?), where ? is the cumula-
tive function of the normal distribution. The opti-
mization problem above is not convex, but a closed
form approximation of its solution has the follow-
ing additive form: µi+1 = µi + ?iyi?ixi and
??1i+1 = ?
?1
i + 2?i?xix
>
i for,
?i=
?(1+2?µi)+
?
(1+2?µi)
2?8?
(
µi???2i
)
4??2i
.
Each update changes the feature weights µ, and in-
creases confidence (variance ? always decreases).
We employ CW classifiers since they provide con-
fidence estimates, which are useful for classifier
combination. Additionally, since we require per-
parameter confidence estimates, other confidence
based classifiers are not suitable for this setting.
4 Multi-Classifier Parameter Combination
The basis of our approach to multi-domain learning
is to combine the parameters of CW classifiers from
separate domains while respecting parameter confi-
dence. A combination method takes M CW classi-
fiers each parameterized by its own mean and vari-
ance parameters {(µm,?m)}Mm=1 and produces a
single combined classifier (µc,?c). A simple tech-
nique would be to average the parameters of classi-
fiers into a new classifier. However, this ignores the
difference in feature distributions. Consider for ex-
ample that the weight associated with some word in
a source classifier has a value of 0. This could either
mean that the word is very rare or that it is neutral
for prediction (like the work “the”). The informa-
tion captured by the variance parameter allow us to
distinguish between the two cases: an high-variance
indicates a lack of confidence in the value of the
690
weight vectors because of small number of exam-
ples (first case), and vise-versa, small-variance indi-
cates that the value of the weight is based on plenty
of evidence. We favor combinations sensitive to this
distinction.
Since CW classifiers are Gaussian distributions,
we formalize classifier parameter combination as
finding a new distribution that minimizes the
weighted-divergence to a set of given distributions:
(µc,?c) = arg min
M?
m
D((µc,?c)||(µm,?m) ; bm) ,
where (since ? is diagonal),
D((µc,?c)||(µ,?) ; b) =
?N
f bfD((µ
c
f ,?
c
f,f )||(µf ,?f,f )) .
The (classifier specific) importance-weights bm ?
RN+ are used to weigh certain parameters of some
domains differently in the combination. When D is
the Euclidean distance (L2), we have,
D((µcf ,?
c
f,f )||(µf ,?f,f )) =
(µcf ? µf )
2 + (?cf,f ? ?f,f )
2 .
and we obtain:
µcf =
1
?M
m b
m
f
M?
m
bmf µ
m
f ,
?cf,f =
1
?
m?M b
m
f
M?
m
bmf ?
m
f,f . (1)
Note that this is a (weighted) average of parameters.
The other case we consider is when D is a weighted
KL divergence we obtain a weighting of µ by ??1:
µcf =
(
M?
m
(?mf,f )
?1bmf
)?1 M?
m
(?mf,f )
?1µmf b
m
f
(?c)?1 =
(
M
M?
m
bmf
)?1 M?
m
(?mf )
?1bf
m . (2)
While each parameter is weighed by its variance in
the KL, we can also explicitly encode this behavior
as bmf = a ? ?
m
f,f ? 0, where a is the initializa-
tion value for ?mf,f . We call this weighting “vari-
ance” as opposed to a uniform weighting of param-
eters (bmf = 1). We therefore have two combination
methods (L2 and KL) and two weighting methods
(uniform and variance).
5 Datasets
For evaluation we selected two domain adaptation
datasets: spam (Jiang and Zhai, 2007) and sentiment
(Blitzer et al., 2007). The spam data contains two
tasks, one with three users (task A) and one with 15
(task B). The goal is to classify an email (bag-of-
words) as either spam or ham (not-spam) and each
user may have slightly different preferences and fea-
tures. We used 700 and 100 training messages for
each user for task A and B respectively and 300 test
emails for each user.
The sentiment data contains product reviews from
Amazon for four product types: books, dvds, elec-
tronics and kitchen appliances and we extended this
with three additional domains: apparel, music and
videos. We follow Blitzer et. al. for feature ex-
traction. We created different datasets by modify-
ing the decision boundary using the ordinal rating
of each instance (1-5 stars) and excluding boundary
instances. We use four versions of this data:
• All - 7 domains, one per product type
• Books - 3 domains of books with the binary
decision boundary set to 2, 3 and 4 stars
• DVDs - Same as Books but with DVD reviews
• Books+DVDs - Combined Books and DVDs
The All dataset captures the typical domain adap-
tation scenario, where each domain has the same
decision function but different features. Books
and DVDs have the opposite problem: the same
features but different classification boundaries.
Books+DVDs combines both issues. Experiments
use 1500 training and 100 test instances per domain.
6 Multi-Domain Adaptation
We begin by examining the typical domain adapta-
tion scenario, but from an online perspective since
learning systems often must adapt to new users or
domains quickly and with no training data. For ex-
ample, a spam filter with separate classifiers trained
on each user must also classify mail for a new
user. Since other user’s training data may have been
deleted or be private, the existing classifiers must be
combined for the new user.
691
Train L2 KL
Target Domain All Src Target Best Src Avg Src Uniform Variance Uniform Variance
S
pa
m
user0 3.85 1.80 4.80 8.26 5.25 4.63 4.53 4.32
user1 3.57 3.17 4.28 6.91 4.53 3.80 4.23 3.83
user2 3.30 2.40 3.77 5.75 4.75 4.60 4.93 4.67
S
en
ti
m
en
t
apparel 12.32 12.02 14.12 21.15 14.03 13.18 13.50 13.48
books 16.85 18.95 22.95 25.76 19.58 18.63 19.53 19.05
dvd 13.65 17.40 17.30 21.89 15.53 13.73 14.48 14.15
kitchen 13.65 14.40 15.52 22.88 16.68 15.10 14.78 14.02
electronics 15.00 14.93 15.52 23.84 18.75 17.37 17.45 16.82
music 18.20 18.30 20.75 24.19 18.38 17.83 18.10 18.22
video 17.00 19.27 19.43 25.78 17.13 16.25 16.33 16.42
Table 1: Test error for multi-source adaptation on sentiment and spam data. Combining classifiers improves over
selecting a single classifier a priori (Avg Src).
We combine the existing user-specific classifiers
into a single new classifier for a new user. Since
nothing is known about the new user (their deci-
sion function), each source classifier may be useful.
However, feature similarity – possibly measured us-
ing unlabeled data – could be used to weigh source
domains. Specifically, we combine the parameters
of each classifier according to their confidence us-
ing the combination methods described above.
We evaluated the four combination strategies – L2
vs. KL, uniform vs. variance – on spam and sen-
timent data. For each evaluation, a single domain
was held out for testing while separate classifiers
were trained on each source domain, i.e. no target
training. Source classifiers are then combined and
the combined classifier is evaluated on the test data
(400 instances) of the target domain. Each classi-
fier was trained for 5 iterations over the training data
(to ensure convergence) and each experiment was
repeated using 10-fold cross validation. The CW
parameter ? was tuned on a single randomized run
for each experiment. We include several baselines:
training on target data to obtain an upper bound
on performance (Target), training on all source do-
mains together, a useful strategy if all source data is
maintained (All Src), selecting (with omniscience)
the best performing source classifier on target data
(Best Src), and the expected real world performance
of randomly selecting a source classifier (Avg Src).
While at least one source classifier achieved high
performance on the target domain (Best Src), the
correct source classifier cannot be selected without
target data and selecting a random source classifier
yields high error. In contrast, a combined classifier
almost always improved over the best source domain
classifier (table 1). That some of our results improve
over the best training scenario is likely caused by in-
creased training data from using multiple domains.
Increases over all available training data are very in-
teresting and may be due to a regularization effect of
training separate models.
The L2 methods performed best and KL improved
7 out of 10 combinations. Classifier parameter com-
bination can clearly yield good classifiers without
prior knowledge of the target domain.
7 Learning Across Domains
In addition to adapting to new domains, multi-
domain systems should learn common behaviors
across domains. Naively, we can assume that the
domains are either sufficiently similar to warrant
one classifier or different enough for separate clas-
sifiers. The reality is often more complex. Instead,
we maintain shared and domain-specific parameters
and combine them for learning and prediction.
Multi-task learning aims to learn common behav-
iors across related problems, a similar goal to multi-
domain learning. The primary difference is the na-
ture of the domains/tasks: in our setting each domain
is the same task but differs in the types of features in
addition to the decision function. A multi-task ap-
proach can be adapted to our setting by using our
classifier combination techniques.
692
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 3.88 8.75 23.7 25.11 23.26 16.57
Separate 5.46 14.53 22.22 21.64 21.23 21.89
Feature Splitting 4.16 8.93 15.65 16.20 14.60 17.45
MDR 4.09 9.18 15.65 15.12 13.76 17.45
MDR+L2 4.27 8.61 12.70 14.95 12.73 17.16
MDR+L2-Var 3.75 7.52 12.90 14.21 12.52 17.37
MDR+KL 4.32 9.22 13.51 13.81 13.32 17.20
MDR+KL-Var 4.02 8.70 14.93 14.03 14.22 18.40
Table 2: Online training error for learning across domains.
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 2.11 5.60 18.43 18.67 19.08 14.09
Separate 2.43 8.5 18.87 15.97 16.45 17.23
Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73
MDR 1.94 5.69 9.97 8.33 8.20 14.73
MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20
MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33
MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60
MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50
Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daume´ (2007).
We seek to learn domain specific parameters
guided by shared parameters. Dekel et al. (2006)
followed this approach for an online multi-task algo-
rithm, although they did not have shared parameters
and assumed that a training round comprised an ex-
ample from each task. Evgeniou and Pontil (2004)
achieved a similar goal by using shared parameters
for multi-task regularization. Specifically, they as-
sumed that the weight vector for problem d could be
represented aswc = wd+ws, wherewd are task spe-
cific parameters and ws are shared across all tasks.
In this framework, all tasks are close to some under-
lying meanws and each one deviates from this mean
by wd. Their SVM style multi-task objective mini-
mizes the loss ofwc and the norm ofwd andws, with
a tradeoff parameter allowing for domain deviance
from the mean. The simple domain adaptation al-
gorithm of feature splitting used by Daume´ (2007)
is a special case of this model where the norms are
equally weighted. An analogous CW objective is:
min
1
?1
DKL
(
N
(
µd,?d
)
?N
(
µdi ,?
d
i
))
+
1
?2
DKL (N (µs,?s) ?N (µsi ,?
s
i ))
s.t. Prw?N (µc,?c) [yi (w · xi) ? 0] ? ? . (3)
(
µd,?d
)
are the parameters for domain d, (µs,?s)
for the shared classifier and (µc,?c) for the com-
bination of the domain and shared classifiers. The
parameters are combined via (2) with only two ele-
ments summed - one for the shared parameters s and
the other for the domain parameters d . This captures
the intuition of Evgeniou and Pontil: updates en-
force the learning condition on the combined param-
eters and minimize parameter change. For conve-
nience, we rewrite ?2 = 2? 2?1, where ?1 ? [0, 1].
If classifiers are combined using the sum of the indi-
vidual weight vectors and ?1 = 0.5, this is identical
to feature splitting (Daume´) for CW classifiers.
The domain specific and shared classifiers can be
693
updated using the closed form solution to (3) as:
µs = µsi + ?2?yi?
cxi
(?s)?1 = (?si )
?1 + 2?2??xixTi
µd = µdi + ?1?yi?
c
ixi
(?d)?1 = (?di )
?1 + 2?1??xixTi
(4)
We call this objective Multi-Domain Regulariza-
tion (MDR). As before, the combined parameters
are produced by one of the combination methods.
On each round, the algorithm receives instance xi
and domain di for which it creates a combined clas-
sifier (µc,?c) using the shared (µs,?s) and domain
specific parameters
(
µd,?d
)
. A prediction is is-
sued using the standard linear classifier prediction
rule sign(µc · x) and updates follow (4). The ef-
fect is that features similar across domains quickly
converge in the shared classifier, sharing informa-
tion across domains. The combined classifier re-
flects shared and domain specific parameter confi-
dences: weights with low variance (i.e. greater con-
fidence) will contribute more.
We evaluate MDR on a single pass over a stream
of instances from multiple domains, simulating a
real world setting. Parameters ?1 and ? are iter-
atively optimized on a single randomized run for
each dataset. All experiments use 10-fold CV. In ad-
dition to evaluating the four combination methods
with MDR, we evaluate the performance of a sin-
gle classifier trained on all domains (Single), a sep-
arate classifier trained on each domain (Separate),
Feature Splitting (Daume´) and feature splitting with
optimized ?1 (MDR). Table 3 shows results on test
data and table 2 shows online training error.
In this setting, L2 combinations prove best on 5
of 6 datasets, with the variance weighted combina-
tion doing the best. MDR (optimizing ?1) slightly
improves over feature splitting, and the combination
methods improve in every case. Our best result is
statistically significant compared to Feature Split-
ting using McNemar’s test (p = .001) for Task B,
Books, DVD, Books+DVD. While a single or sepa-
rate classifiers have a different effect on each dataset,
MDR gives the best performance overall.
8 Learning in Many Domains
So far we have considered settings with a small
number of similar domains. While this is typical
of multi-task problems, real world settings present
many domains which do not all share the same be-
haviors. Online algorithms scale to numerous ex-
amples and we desire the same behavior for numer-
ous domains. Consider a spam filter used by a large
email provider, which filters billions of emails for
millions of users. Suppose that spammers control
many accounts and maliciously label spam as legiti-
mate. Alternatively, subsets of users may share pref-
erences. Since behaviors are not consistent across
domains, shared parameters cannot be learned. We
seek algorithms robust to this behavior.
Since subsets of users share behaviors, these can
be learned using our MDR framework. For example,
discovering spammer and legitimate mail accounts
would enable intra-group learning. The challenge is
the online discovery of these subsets while learning
model parameters. We augment the MDR frame-
work to additionally learn this mapping.
We begin by generalizing MDR to include k
shared classifiers instead of a single set of shared pa-
rameters. Each set of shared parameters represents
a different subset of domains. If the corresponding
shared parameters are known for a domain, we could
use the same objective (3) and update (4) as before.
If there are many fewer shared parameters than do-
mains (k  D), we can benefit from multi-domain
learning. Next, we augment the learning algorithm
to learn a mapping between the domains and shared
classifiers. Intuitively, a domain should be mapped
to shared parameters that correctly classify that do-
main. A common technique for learning such ex-
perts in the Weighted Majority algorithm (Little-
stone and Warmuth, 1994), which weighs a mixture
of experts (classifiers). However, since we require a
hard assignment — pick a single shared parameter
set s — rather than a mixture, the algorithm reduces
to picking the classifier s with the fewest mistakes
in predicting domain d. This requires tracking the
number of mistakes made by each shared classifier
on each domain once a label is revealed. For learn-
ing, the shared classifier with the fewest mistakes
for a domain is selected for an MDR update. Clas-
sifier ties are broken randomly. While we experi-
694
Figure 1: Learning across many domains - spam (left) and sentiment (right) - with MDR using k shared classifiers.
Figure 2: Learning across many domains - spam (left) and sentiment (right) - with no domain specific parameters.
mented with more complex techniques, this simple
method worked well in practice. When a new do-
main is added to the system, it takes fewer exam-
ples to learn which shared classifier to use instead of
learning a new model from scratch.
While this approach adds another free parameter
(k) that can be set using development data, we ob-
serve that k can instead be fixed to a large constant.
Since only a single shared classifier is updated each
round, the algorithm will favor selecting a previ-
ously used classifier as opposed to a new one, using
as many classifiers as needed but not scaling up to k.
This may not be optimal, but it is a simple.
To evaluate a larger number of domains, we cre-
ated many varying domains using spam and senti-
ment data. For spam, 6 email users were created by
splitting the 3 task A users into 2 users, and flipping
the label of one of these users (a malicious user),
yielding 400 train and 100 test emails per user. For
sentiment, the book domain was split into 3 groups
with binary boundaries at a rating of 2, 3 or 4. Each
of these groups was split into 8 groups of which half
had their labels flipped, creating 24 domains. The
same procedure was repeated for DVD reviews but
for a decision boundary of 3, 6 groups were created,
and for a boundary of 2 and 4, 3 groups were created
with 1 and 2 domains flipped respectively, resulting
in 12 DVD domains and 36 total domains with var-
ious decision boundaries, features, and inverted de-
cision functions. Each domain used 300 train and
100 test instances. 10-fold cross validation with one
training iteration was used to train models on these
695
two datasets. Parameters were optimized as before.
Experiments were repeated for various settings of
k. Since L2 performed well before, we evaluated
MDR+L2 and MDR+L2-Var.
The results are shown in figure 1. For both spam
and sentiment adding additional shared parameters
beyond the single shared classifier significantly re-
duces error, with further reductions as k increases.
This yields a 45% error reduction for spam and a
38% reduction for sentiment over the best baseline.
While each task has an optimal k (about 5 for spam,
2 for sentiment), larger values still achieve low error,
indicating the flexibility of using large k values.
While adding parameters clearly helps for many
domains, it may be impractical to keep domain-
specific classifiers for thousands or millions of do-
mains. In this case, we could eliminate the domain-
specific classifiers and rely on the k shared clas-
sifiers only, learning the domain to classifier map-
ping. We compare this approach using the best result
from MDR above, again varying k. Figure 2 shows
that losing domain-specific parameters hurts perfor-
mance, but is still an improvement over baseline
methods. Additionally, we can expect better perfor-
mance as the number of similar domains increases.
This may be an attractive alternative to keeping a
very large number of parameters.
9 Related Work
Multi-domain learning intersects two areas of re-
search: domain adaptation and multi-task learning.
In domain adaptation, a classifier trained for a source
domain is transfered to a target domain using either
unlabeled or a small amount of labeled target data.
Blitzer et al. (2007) used structural correspondence
learning to train a classifier on source data with
new features induced from target unlabeled data. In
a complimentary approach, Jiang and Zhai (2007)
weighed training instances based on their similarity
to unlabeled target domain data. Several approaches
utilize source data for training on a limited number
of target labels, including feature splitting (Daume´,
2007) and adding the source classifier’s prediction
as a feature (Chelba and Acero, 2004). Others have
considered transfer learning, in which an existing
domain is used to improve learning in a new do-
main, such as constructing priors (Raina et al., 2006;
Marx et al., 2008) and learning parameter functions
for text classification from related data (Do and Ng,
2006). These methods largely require batch learn-
ing, unlabeled target data, or available source data
at adaptation. In contrast, our algorithms operate
purely online and can be applied when no target data
is available.
Multi-task algorithms, also known as inductive
transfer, learn a set of related problems simultane-
ously (Caruana, 1997). The most relevant approach
is that of Regularized Multi-Task Learning (Evge-
niou and Pontil, 2004), which we use to motivate
our online algorithm. Dekel et al. (2006) gave a sim-
ilar online approach but did not use shared parame-
ters and assumed multiple instances for each round.
We generalize this work to both include an arbi-
trary classifier combination and many shared classi-
fiers. Some multi-task work has also considered the
grouping of tasks similar to our learning of domain
subgroups (Thrun and O’Sullivan, 1998; Bakker and
Heskes, 2003).
There are many techniques for combining the out-
put of multiple classifiers for ensemble learning or
mixture of experts. Kittler et al. (Mar 1998) provide
a theoretical framework for combining classifiers.
Some empirical work has considered adding versus
multiplying classifier output (Tax et al., 2000), using
local accuracy estimates for combination (Woods et
al., 1997), and applications to NLP tasks (Florian et
al., 2003). However, these papers consider combin-
ing classifier output for prediction. In contrast, we
consider parameter combination for both prediction
and learning.
10 Conclusion
We have explored several multi-domain learning
settings using CW classifiers and a combination
method. Our approach creates a better classifier for
a new target domain than selecting a random source
classifier a prior, reduces learning error on multiple
domains compared to baseline approaches, can han-
dle many disparate domains by using many shared
classifiers, and scales to a very large number of do-
mains with a small performance reduction. These
scenarios are realistic for NLP systems in the wild.
This work also raises some questions about learning
on large numbers of disparate domains: can a hi-
696
erarchical online clustering yield a better represen-
tation than just selecting between k shared parame-
ters? Additionally, how can prior knowledge about
domain similarity be included into the combination
methods? We plan to explore these questions in fu-
ture work.
Acknowledgements This material is based upon
work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. FA8750-07-D-0185.
References
B. Bakker and T. Heskes. 2003. Task clustering and gat-
ing for bayesian multi–task learning. Journal of Ma-
chine Learning Research, 4:83–99.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41–75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
max- imum entropy classifier: Little data can help a
lot. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Hal Daume´. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Ofer Dekel, Philip M. Long, and Yoram Singer. 2006.
Online multitask learning. In Conference on Learning
Theory (COLT).
Chuong B. Do and Andrew Ng. 2006. Transfer learning
for text classification. In Advances in Neural Informa-
tion Processing Systems (NIPS).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Conference on
Knowledge Discovery and Data Mining (KDD).
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Conference on Computational
Natural Language Learning (CONLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Association for
Computational Linguistics (ACL).
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas. Mar
1998. On combining classifiers. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(3):226–239.
N. Littlestone and M. K. Warmuth. 1994. The weighted
majority algorithm. Information and Computation,
108:212–261.
Zvika Marx, Michael T. Rosenstein, Thomas G. Diet-
terich, and Leslie Pack Kaelbling. 2008. Two algo-
rithms for transfer learning. In Inductive Transfer: 10
years later.
Rajat Raina, Andrew Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In International Conference on Machine Learn-
ing (ICML).
David M. J. Tax, Martijn van Breukelen, Robert P. W.
Duina, and Josef Kittler. 2000. Combining multiple
classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475–1485, September.
S. Thrun and J. O’Sullivan. 1998. Clustering learning
tasks and the selective cross–task transfer of knowl-
edge. In S. Thrun and L.Y. Pratt, editors, Learning To
Learn. Kluwer Academic Publishers.
Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin
Bowyer. 1997. Combination of multiple classifiers
using local accuracy estimates. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 19(4):405–
410.
697
