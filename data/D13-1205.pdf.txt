Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996–2006,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Cross-Lingual Discriminative Learning of Sequence Models
with Posterior Regularization
Kuzman Ganchev
Google Research
76 9th Avenue
New York, NY 10011
kuzman@google.com
Dipanjan Das
Google Research
76 9th Avenue
New York, NY 10011
dipanjand@google.com
Abstract
We present a framework for cross-lingual
transfer of sequence information from a
resource-rich source language to a resource-
impoverished target language that incorporates
soft constraints via posterior regularization. To
this end, we use automatically word aligned
bitext between the source and target language
pair, and learn a discriminative conditional ran-
dom field model on the target side. Our poste-
rior regularization constraints are derived from
simple intuitions about the task at hand and
from cross-lingual alignment information. We
show improvements over strong baselines for
two tasks: part-of-speech tagging and named-
entity segmentation.
1 Introduction
Supervised systems for NLP tasks are available for
a handful of languages. These systems achieve high
accuracy for many applications; a variety of robust
algorithms to train them from labeled data have been
developed. Here, we focus on learning sequence mod-
els for the languages that lack annotated resources.
For a given resource-poor target language of inter-
est, we assume that parallel data with a resource-rich
source language exists. With the help of this bitext
and a supervised system in the source language, we
infer constraints over the label distribution in the tar-
get language, and train a discriminative model using
posterior regularization (Ganchev et al., 2010).
Cross-lingual learning of structured prediction
models via parallel data has been applied for several
natural language processing problems, including part-
of-speech (POS) tagging (Yarowsky and Ngai, 2001),
syntactic parsing (Hwa et al., 2005) and named-entity
recognition (Kim et al., 2012). These methods are
useful in several ways. First, they help in fast proto-
typing of natural language systems for new languages
that do not boast human annotations. Second, the
output of such systems could be used to bootstrap
more extensive human annotation projects (Vlachos,
2006). Finally, they are significantly more accurate
than purely unsupervised systems (McDonald et al.,
2011; Das and Petrov, 2011).
Recently, Ta¨ckstro¨m et al. (2013) presented a tech-
nique for coupling token constraints derived from pro-
jected cross-lingual information and type constraints
derived from noisy tag dictionaries to learn POS tag-
gers. Although this technique resulted in state-of-
the-art weakly supervised taggers, the authors used a
heuristic to combine the aforementioned two sources
of constraints: the dictionary constraints pruned the
tagger’s search space, and the intersected token-level
projections were treated as hard observations. On
the other hand, Ganchev et al. (2009) presented a
framework for learning weakly-supervised systems
(in their case, dependency parsers) that incorporated
alignment-based information too, but used the cross-
lingual information only as soft constraints, via poste-
rior regularization. The advantage of this framework
lay in the fact that the projections were only trusted
to a certain degree, determined by a strength hyper-
parameter, which unfortunately the authors did not
have an elegant way to tune. In this paper, we ex-
ploit the better aspects of these two lines of work:
first, we extend the framework of Ta¨ckstro¨m et al.
by treating the alignment-based projections only as
soft constraints (see §3.4); second, we choose the
constraint strength by utilizing the tag ambiguity of
tokens for a given resource-poor language (see §6.1).
Other than validating our framework on part-of-
speech tagging, we experiment on named-entity seg-
mentation in a cross-lingual framework. For this
1996
task, we present a novel method to perform high-
precision phrase-level entity transfer (§5.2.2); we
also provide ways to balance precision and recall
with posterior regularization (§6.2) by incorporating
intuitive soft constraints during learning. We mea-
sure performance on standard benchmark datasets for
both of these tasks, and report improvements over
state-of-the-art baselines.
2 Prior Work
Cross-lingual projection methods can be classified
by their use of two very broad ideas. The first idea
utilizes parallel data to create full or partial annota-
tions in the low-resource language and trains from
this data. This was popularized by Yarowsky and
Ngai (2001) who applied this to POS tagging and
shallow parsing. It was later applied to parsing (Hwa
et al., 2005) and named entity recognition (Kim et
al., 2012). The second idea, first proposed by Ze-
man and Resnik (2008) and applied more broadly
by McDonald et al. (2011), is to train a model on
a resource-rich language and apply it to a resource-
poor language directly. The disparity between the
languages is mitigated by the choice of features. In
addition to cross-lingual projection, purely unsuper-
vised methods have been explored but with limited
success (Christodoulopoulos et al., 2010). Here, we
resort to cross-lingual projection and incorporate the
first idea; we also follow Li et al. (2012) and use
Wiktionary to further constrain the POS tagging task.
Our learning setup is similar to that of Ganchev et
al. (2009), who also use posterior regularization but
focus on dependency parsing alone. Our work differs
with respect to the tasks, the learning algorithm and
also in that we use corpus-wide constraints, while
Ganchev et al. use one constraint per sentence. For
the part-of-speech tagging task, our approach is sim-
ilar to that of Ta¨ckstro¨m et al. (2013), who use an
almost identical learning setup but only make use of
hard constraints. By relaxing these constraints, we
allow the model to identify and ignore inconsistently
labeled parts of sentences, and achieve better results
using identical training and test data.
3 Approach
We give an overview of our approach, and present the
details of our model used for cross-lingual learning.
Algorithm 1 Cross-Lingual Learning with Posterior
Regularization
Require: Parallel source and target language data
De and Df , source language model (M)e, task-
specific target language constraints C.
Ensure: ?f , a set of target language parameters.
1: De?f ? word-align-bitext(De,Df )
2: D?e ? label-supervised(De)
3: D?f ? project-and-filter-labels(De?f , D?e)
4: ?f ? learn-posterior-constrained(D?f , C)
5: Return ?f
3.1 General Overview
The general overview of our framework is provided
in Algorithm 1. The process of learning parame-
ters for a target language for a given task involves
four subtasks. First, we run word alignment over a
large corpus of parallel data between the resource-
rich source language and the resource-impoverished
target language (see §4.3). In the second step, we
use a supervised model to label the source side of
the parallel data (see §5.1.1 and §5.2.1). The third
step involves a task-specific word-alignment filter-
ing step; this step involves heuristics for which we
use cues from prior state-of-the-art (Das and Petrov,
2011; Ta¨ckstro¨m et al., 2013, see §5.1.2) and also
introduce some novel ones for the NE segmentation
problem (see §5.2.2). In the fourth step, we train a
linear chain conditional random field (Lafferty et al.,
2001, CRF henceforth) using posterior regularization.
In the next subsection, we turn to a brief summary of
this final step of estimating parameters of a discrimi-
native model with posterior regularization.
3.2 Learning with Posterior Regularization
In this work, we utilize discriminative CRF mod-
els, and use posterior regularization (PR) to optimize
their parameters. As a framework, posterior regular-
ization is described in detail by Ganchev et al. (2010).
However in our work, we adopt a different optimiza-
tion technique; in what follows, we summarize the
optimization algorithm in the context of CRF models.
Let x be an input sentence with a set of possible
labelings Y(x) and let y ? Y(x) be a particular la-
beling for sentence x. We use bold capital letters
X = {x1 . . .xn} and Y = {y1 . . .yn} to denote
1997
a corpus of sentences and labelings for the corpus
respectively. A CRF models the probability distri-
bution over possible labels for a sentence p?(y|x)
as:
p?(y | x) ? exp(? · f(x,y)) (1)
where ? are the model parameters and f(.) is a fea-
ture function. The model examines sentences in iso-
lation, and the probability of a particular labeling for
a corpus is defined as a product over the individual
sentences:
p?(Y | X) =
?
(x,y)?(X,Y)
p?(y | x). (2)
Traditionally, CRF models have been trained to op-
timize the regularized log-likelihood of the training
data
max
?
L(?) = max
?
log(p?(Y | X))? ? ||?|| (3)
In our setting, we do not have a fully labeled cor-
pus, but we have constraints on the distribution of
labels. For example, we may know that a particular
token could be labeled only by a label inventory li-
censed by a dictionary, or that a labeling projected
from a source language is usually (but not always)
correct. We define these constraints in terms of fea-
ture expectations. Let q(Y) be a distribution over all
possible labelings of our corpus Y(X). Let Q be a
set of distributions defined by:
Q = {q(Y) : Eq[?(X,Y)] ? b}, (4)
where ? is a constraint feature function and b is a vec-
tor of non-negative values that serve as upper bounds
to the expectations of every constraint feature. The
vector b is used to encode our prior knowledge about
desirable distributions q(Y). Note that the constraint
features ? are not related to the model features f . The
model features, together with the model parameters ?
define the CRF model; the model features need to be
computed at inference time for prediction. By con-
trast, the constraint features and their corresponding
constraint values are used to define our training ob-
jective function (and are only used during learning).
The PR objective with no labeled data is defined with
respect to Q as:
PR: max
?
JQ(?) =
max
?
?KL(Q?p?(Y | X))? ? ||?|| (5)
where KL(Q||p) = minq?QKL(q||p) is the
KL-divergence (Kullback and Leibler, 1951)
from a set to a point. Note that as we add more
constraints, Q becomes a smaller set. In the
limit, Q = {q(Y) : q(Yˆ) = 1} contains just one
distribution concentrated on a single labeling Yˆ.
In this limit, posterior regularization degenerates
into the convex log-likelihood objective normally
used for supervised data JQ(?) = L(?). However,
in the general case, the PR objective JQ is not
necessarily convex. Prior work, including that of
Ganchev et al. propose an algorithm similar to
Expectation-Maximization (Dempster et al., 1977,
EM henceforth) to optimize JQ, but we follow Liang
et al. (2009) in using a schochastic update-based
algorithm described below.
Note: To make it easier to reason about constraint
values b, we scale constraint features ?(X,Y) to lie
in [0, 1] by computing maxY ?(X,Y) for the corpus
to which ? is applied.
3.3 Optimization
The optimization procedure proposed by
Ganchev et al. is similar to the EM algorithm,
and computes the minimization minq?QKL(q||p)
at each step, using its dual form; this minimization is
convex, so there is no duality gap. They show that
the optimal primal variables q?(Y) are related to the
optimal dual variables ?? by:
q?(Y) =
p?(Y|X)e??
?·?(X,Y)
Z(??)
. (6)
where Z(??) is the normalizer. The dual problem is
given by:
max
??0
?b · ?? logZ(?). (7)
Substituting Eq. 7 into the objective in Eq. 5, we get
the saddle-point problem:
max
?
min
??0
b · ?+ log
?
Y
p?(Y|X)e
???·?(X,Y)
? ? ||?|| . (8)
To optimize the above objective function, we need to
compute partial derivatives with respect to both ? and
?. First, to compute the partial derivatives of Eq. 8
1998
with respect to ?, we need to find expectations of the
model features f given the current distribution p? and
the constraint distribution q. To perform tractable
inference, a linear-chain CRF model assumes that
the feature function factorizes according to smaller
parts; in particular the factorization uses the follow-
ing structure:
f(x,y) =
?
i
f(x, yi, yi?1) (9)
where i ranges over the tokens in the sentence. This
factorization allows us to efficiently compute expec-
tations over the labels yi and label-pairs (yi, yi+1).
To compute the partial gradient of Eq. 8 with respect
to ?, we need to find the expectations of the con-
straint features ?. In order to be tractable here too,
we ensure that ? also factorize according to the same
structure as f . Therefore, the gradient computation
w.r.t. ? turns out to be straightforward.
For all the experiments in this paper, we optimize
Eq. 8 using stochastic projected gradient. For each
training sentence, we compute the gradient of ? and
? with respect to Eq. 8, take a gradient step in each
one, and truncate the negative entries in ? to zero.
We use a step size of 1 for all experiments.1
3.4 Relationship with Ta¨ckstro¨m et al. (2013)
In this subsection, we focus briefly on the relationship
between this work and the work of Ta¨ckstro¨m et al.
(2013), who focused on constrained learning of POS
taggers. Ta¨ckstro¨m et al. define constrained lattices
and train by optimizing marginal conditional log-
likelihood. In our notation, they define their objective
as:
max
?
log
?
Y?Y?(X)
p?(Y|X)? ???? (10)
where Y?(X) are the constrained lattices of label se-
quences that agree with both a dictionary and cross-
lingually projected POS tags for each sentence of
the training corpus. Let us define a constraint fea-
ture ?(X,Y) which counts the number of tags in Y
which are outside the constraint set Y?(X) and require
?(X,Y) ? 0. Note that,
arg min
q
KL(q||p?(Y|X)) s. t. ?(X,Y) ? 0
1Note that we did not implement regularization of ? in the
stochastic optimizer, hence our PR objective (Eq. 8) was unregu-
larized; however, the baseline models use `2 regularization.
gives the same distribution as Eq. 10. Given this
equivalence, it is easy to see that the gradient of
Eq. 5 with respect to ? is the same as that of Eq. 10.
By using such constrained lattices, Ta¨ckstro¨m et al.
avoid maintaining a parameter for the constraint, but
lose the ability to relax the constraint value and al-
low some probability mass outside the pruned lat-
tice. Their paper also differs from ours in that they
use L-BFGS (Liu and Nocedal, 1989), while we use
an online optimization procedure. Since the objec-
tives are non-convex, the two optimization techniques
could lead to different local optima even when the
constraint is not relaxed (b = 0).
4 Tasks and Data
In this section, we focus on the nature of the two tasks
that we attempt to solve, describe the source language
datasets we use to train our supervised models for
transfer, the target language datasets on which we
evaluate our models and the parallel data we use for
cross-lingual transfer.
4.1 Part-of-Speech Tagging
First, we focus on the task of part-of-speech tagging.
Following previous work on cross-lingual POS tag-
ging (Das and Petrov, 2011; Ta¨ckstro¨m et al., 2013),
we adopt the POS tags of Petrov et al. (2012), ver-
sion 1.03;2 we use the October 2012 version of Wik-
tionary3 as our tag dictionary.
After pruning the search space with the dictionary,
we place soft constraints derived by projecting POS
tags across word alignments. The alignments are fil-
tered for confidence (see §5.1.2), but we also filter
any projected tags that are not licensed by the dictio-
nary. The example in Figure 1 illustrates why this
dictionary filtering step is important. Consider the
English-Spanish phrase pair from Figure 1, which
we observed in our training data. Our supervised tag-
ger correctly tags Asian with the ADJ tag as shown
in the figure. Asian is aligned to the Spanish word
Asia, which should be tagged NOUN. Because the
Spanish Wiktionary only allows the NOUN tag for
Asia, we do not project the ADJ tag from the English
word Asian. By contrast, we do project the NOUN
tag from the English word sponges to the Spanish
2http://code.google.com/p/universal-pos-tags
3http://meta.wikimedia.org/wiki/Wiktionary
1999
of    [ Asian ]        sponges
de   las   esponjas   de   Asia
ADP
ADJ NOUN
MISC
Figure 1: An English (top) – Spanish (bottom) phrase pair
from our parallel data. The correct POS tags and NER
annotations are shown for the English phrase. Word align-
ments are shown as links between English and Spanish
words.
word esponjas because this tag is in our dictionary
for the latter word.
For all our POS experiments, we evaluate on sev-
enteen target languages. Fifteen of these languages
were part of the experiments conducted by Ta¨ckstro¨m
et al. (2013); we add Arabic and Hungarian to the set.
The first column of Table 1 lists all seventeen lan-
guages using their two-letter abbreviation codes from
the ISO 639-1 standard. The evaluation datasets cor-
respond to the test sets from the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al., 2007). For French we use the treebank
of Abeille´ et al. (2003). English serves as our source
language and we use the Penn Treebank (Marcus et
al., 1993, with tags mapped to the universal tags) to
train our supervised source-side model.
4.2 Named-Entity Segmentation
Second, we investigate the task of named-entity seg-
mentation. The goal of this task is to identify the
boundaries of named-entities for a given language
without classifying them by type. This is the un-
labeled version of named-entity recognition, and is
more amenable to cross-lingual supervision. To un-
derstand why that is, consider again the example
from Figure 1. The English supervised NE tagger
correctly identifies Asian as a named entity of type
MISC (miscellaneous). The word-alignments sug-
gest we should transfer this annotation to the Spanish
word Asia which is also an entity. However, this
should be labeled LOC (location) according to the
CoNLL annotation guidelines (Tjong Kim Sang and
De Meulder, 2003). Because syntactic variations
of this kind are common, it makes cross-lingual de-
tection of NE boundaries as well as types hard.4 In
this paper, we focus on named-entity segmentation
alone, consider the full NER task out of scope. We
use English as a source language and train a super-
vised English named-entity tagger with the labels in
place, using the CoNLL 2003 shared task data (Tjong
Kim Sang and De Meulder, 2003). We project the
spans using the maximal-span heuristic (Yarowsky
and Ngai, 2001). We project into Dutch, German and
Spanish and evaluate on the standard CoNLL 2002
and 2003 shared task data sets (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003).
4.3 Parallel Data
For both tasks we use parallel data gathered automat-
ically from the web using the method of Uszkoreit
et al. (2010), as well as data from Europarl (Koehn,
2005) and the UN parallel corpus (UN, 2006), for
languages covered by the latter two corpora. The
parallel sentences are word aligned with the aligner
of DeNero and Macherey (2011). The size of the
parallel corpus is larger than we need for our tasks,
so we follow Ta¨ckstro¨m et al. (2013) in sampling
500k tokens for POS tagging and 10k sentences for
named-entity segmentation (see §5.1.2 and §5.2.2).
5 Experimental Details
In this section, we provide details about task-
specific implementations of the supervised source-
side model and the word-alignment filtering tech-
niques (steps 2 and 3 in Algorithm 1 respectively);
we also briefly describe the setup of the cross-lingual
experiments for each task.
5.1 Part-of-Speech Tagging
We first focus on the experimental setup for the POS
tagging task. When describing feature sets we refer to
features conjoined with just a single tag as emission
features and with consecutive tag pairs as transition
features.
4We tried using English and German gazetteers from the
CoNLL 2002 and 2003 shared tasks as a label dictionary similar
to the way we use Wiktionary for POS tagging. This did not work
well because the CoNLL gazetteers do not have good coverage
on our parallel datasets, which we use for training.
2000
5.1.1 Supervised Source-Side Model
We tag the English side of our parallel data with
a supervised first-order linear-chain CRF POS tag-
ger. We use standard features for tagging. Our emis-
sion features are a bias feature, the current word,
its suffixes up to length 3, its capitalization shape,
whether it contains a hyphen, digit or punctuation
and its cluster identity. Our transition features are a
bias feature and the cluster identities of each word
in the transition. For the cluster-based features, we
use monolingual word clusters induced with the ex-
change algorithm of Uszkoreit and Brants (2008),
which implements the same objective as Brown et al.
(1992); these clusters have shown improvements for
sequence labeling tasks (Turian et al., 2010; Ta¨ck-
stro¨m et al., 2012). We set the number of clusters to
256 for both the source side tagger and all the other
languages. On Section 23 of the WSJ section of the
Penn Treebank, the source side tagger achieves an
accuracy of 96.2%.
5.1.2 Word Alignment Filtering
Following Ta¨ckstro¨m et al. (2013), we tag the En-
glish side of our parallel data using the source-side
POS tagger, intersect the word alignments and filter
alignments with confidence below 0.95. We sam-
ple 500,000 tokens of target side sentences for each
language, and use this as training data for learning
weakly-supervised taggers.
5.1.3 Setup for Cross-Lingual Experiments
Following Ta¨ckstro¨m et al. (2013) we use a re-
duced feature set for the cross-lingual models. The
emission features are the same as the supervised
model but without the punctuation feature,5 and we
use only the bias transition feature. Because this
limits the ability of the model to use context, we
also experiment with an extended feature set that
has transition features for the clusters of each word
in the transition, and their suffixes up to length 3.
We refer to the extended-feature models as “BASE+”
and “PR+” to distinguish them from the models with
fewer features, labeled “BASE” and “PR”.
We train BASE and BASE+ using L-BFGS with
an `2 regularization weight of 1 for 100 iterations to
reproduce the setup used by Ta¨ckstro¨m et al. (2013).
5The dictionary licenses punctuations, only by the ‘.’ tag.
We have only one constraint feature in our poste-
rior regularization models that fires for the unpruned
projected tags on words xi. This feature controls
how often our model trusts a projected tag; we ex-
plain how its strength is chosen in §6.1. The PR and
PR+ models are trained using the stochastic gradient
method described in §3.3.
5.2 Named-Entity Segmentation
In this subsection, we turn to the experimental details
of the named-entity segmentation system.
5.2.1 Supervised Source-Side Model
To train our supervised source-side NER model,
we implemented a linear-chain first order CRF model.
Our feature set was inspired by the model of Kazama
and Torisawa (2007, §6.1); we used all the local fea-
tures from their model except the gazetteer features,
and added cluster emission features for offsets in the
range [-2, 2] and transition features for offsets in the
range [-1, 1] as well as a sentence-start feature. We
use automatic POS tags for all the experiments.
We use a BIO encoding of the four NER labels
(PER, LOC, ORG and MISC). We also experi-
mented with omitting the NE labels from the tagger,
still with a BIO encoding for segments, but the results
were worse on average than what we report in Table 2.
We train the source-side model on the CoNLL 2003
English training set with log-loss using L-BFGS for
100 iterations with `2 regularization weight of 0.1.
The model gets 90.9% and 87.5% labeled F1 on the
CoNLL development and test sets respectively.6
5.2.2 Word-Alignment Filtering
Projecting named entities across languages can
be error prone for several reasons. Mistakes intro-
duced by the automatic word aligner is one of them.
Word alignment errors are particularly problematic
for entity mentions because of the garbage collector
effect (Brown et al., 1993); due to differences in the
word order between languages, a few alignment er-
rors can result in many errors in the other language.
Additionally, entities can occur on just one side of
the bitext.7 Another source of error is the automatic
6These performance values would place us among the top
three competitors of the CoNLL 2003 shared task.
7For example, “It’s all Greek to me.” in one language and “I
don’t understand it.” in another.
2001
labeling on the source side, which is inaccurate if the
parallel corpus is out of domain. To mitigate these
errors, we aggressively filter the training data for this
task. We discard sentence pairs where more than
30% of the source language tokens are unaligned,
where any source entities are unaligned or where
any source entities are more than 4 tokens long. We
also compute a confidence score over entity anno-
tations as the minimum posterior over the tags that
comprise the entity and discard sentence pairs that
have an entity with confidence below 0.9. Finally,
we discard any sentences that contain no projected
entities. These filtering steps allow us to keep 7.4%,
9.7% and 10.4% of the aligned sentence pairs for Ger-
man, Spanish and Dutch, respectively, resulting in
very high-precision named-entity projections (see Ta-
ble 2). For comparison, we also perform experiments
without this filtering step.
5.2.3 Setup for Cross-Lingual Experiments
We use a CRF with the same feature set and BIO
encoding for the cross-lingual models as the source-
side NER model. We compare our approach (“PR”
in Table 2) to a baseline (“BASE” in Table 2) which
treats the projected annotations as fully observed.
The PR model treats the projected NE spans of a
sentence as observed, and allows all labels on the
remaining tokens. Since the “O” tag is never seen, an
unconstrained model would learn to never predict it.
We add two features that fire when the current word
is tagged “O”: a bias feature and a feature that fires
when the automatic POS tag is a proper noun. We set
upQ so the desired expectations are at least 0.98 and
at most 0.1 for these constraint features respectively.
6 Results
In this section, we turn to our experimental results;
first, we focus on POS tagging and then turn to the
NE segmentation task.
6.1 Part-of-Speech Tagging
Constraint Strength: As discussed in §4.1, it is
important to filter out projected annotations not li-
censed by Wiktionary. Thus, the quality of weakly-
supervised POS taggers learned from projections
is closely correlated with the coverage of the Wik-
tionary. To quantify the effect of Wiktionary cover-
age, we counted the expected number of possible tags
 0.7
 0.8
 0.9
 1
 0.1  0.2  0.3  0.4
optim
al b
1/TpT
Figure 2: Correlation between optimal constraint value b
and dictionary pruning efficiency. Each blue square is a
language, the green line is a linear approximation of the
data.
per token (TpT) for our unlabeled corpora. Specif-
ically, for each token, we counted the number of
tags licensed by the dictionary, or all tags for word
forms not in the dictionary. For each language, we
also ran our system with constraint strengths in {0.7,
0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.98, 1.00}, and com-
puted the optimal constraint strength from this set.
We found that the best constraint strength is closely
correlated with the average number of tags available
for each token. Figure 2 shows the best constraint
strength as a function of the inverse of the number of
unpruned tags per token. As observed in the figure,
the relationship between the optimal strength and
1/TpT is roughly linear. Figure 2 also shows a linear
approximation to the data plotted. When applying
this technique to a new language, we would not be
able to estimate the optimal constraint strength, but
we could use the linear approximation and knowl-
edge of 1/TpT to estimate it. For our experiments
below, we perform this estimation for each language
using the linear approximation computed from the
remaining languages.
Results: The results for our part-of-speech tagging
experiments are in Table 1. We compare our results
to BASE, which corresponds to reruns of the best
model of Ta¨ckstro¨m et al. (2013, Column 9 of Ta-
ble 2), and closely aligns with the numbers reported
by the authors. We see in Table 1 that for both fea-
ture sets (i.e., with and without the ‘+’ extension),
our estimated constraint strength is usually better
than using a constraint strength of 1. The results
in the column labeled PR are better than BASE for
12 out of 17 languages, and the results for PR+ are
2002
BASE BASE+ PR PR+
ar 37.84 44.96 ? 49.04? 50.10?
bg 88.04 87.93 88.02 88.42?
cs 79.67 80.01 ? 80.20? 80.68?
da 88.14 87.92 88.24? 87.90
de 90.32 89.97 90.41? 90.29
el 90.03 89.03 90.63? 90.24?
es 86.99 86.81 87.20? 87.21?
fr 87.07 87.53 ? 87.44? 87.48?
hu 82.05 82.05 82.14? 83.13?
it 89.48 89.89 ? 89.52 89.72?
ja 80.63 78.54 80.02 79.68
nl 85.89 85.77 85.59 85.98?
pt 90.93 91.60 ? 91.48? 91.56?
sl 82.46 82.08 83.16? 83.49?
sv 89.06 88.72 89.25? 88.77
tr 64.39 65.74 ? 63.88 66.47?
zh 73.98 72.82 74.51? 68.43
Avg 81.59 81.85 ? 82.40? 82.33?
-zh-ar 85.01 84.91 85.15? 85.40?
Table 1: POS tagging results. BASE represents the best
model of Ta¨ckstro¨m et al. (2013). PR is a system with
the same features but with relaxed constraints. BASE+
and PR+ add additional model features (see §5.2.3). ? in-
dicates improvements over the previous state of the art
(BASE), and bold values indicate the best score for a lan-
guage. “Avg” indicates averaged results for all 17 lan-
guages, while “-zh-ar” shows averaged results without
Chinese and Arabic.
better than BASE+ for 13 out of 17 languages. Ad-
ditionally, adding features does not tend to help the
baseline model to a large extent (the wins are for
6 languages), but does tend to help the PR model
(for 11 languages); however, there is a large drop in
performance for Chinese.
Error Analysis: Here, we analyze the nature of
improvements that the PR models get. For the lan-
guages where PR results in large improvements, it
stems from the ability to allow the sentential con-
text to sometimes override the tag projected via the
parallel data. For example, the Czech word se can
either be a reflexive pronoun (such as ourselves in
English) or translate to the preposition with. The
pronominal sense comprises about 95% of occur-
rences in the Czech annotations, but it would not
appear in an English translation. For example, the
phrase “pod?´vali jsme se” translates to “we looked”,
and the word jsme would typically be aligned to we;
se, which serves as a reflexive pronoun here, remains
unaligned. Consequently, in our data, over 7000 oc-
currences of se appear, but only 17 instances have a
tag projection that is not filtered by Wiktionary. Since
the remaining are tagged with the preposition tag, the
hard-constrained baseline always tags se as a prepo-
sition. By contrast, the soft-constrained PR model
predicts the pronominal sense in cases where the con-
text is most indicative of a pronoun – 38% of the time.
It still mistags many of the pronominal cases where
the contextual evidence is not strong enough. We get
very similar behavior with the Hungarian word hogy
which can translate to the conjunction that (as in “I
see that you are here”) or the adverb how.
We found that the drastic drop in performance for
Chinese under the PR+ model is due to the possessive
marker “?” which serves exclusively as a particle
in the test data. Wiktionary also allows the noun and
adverb tags. The adverbial use is actually a different
token (?? ? really, truly) containing the same
character. Because the cross-lingual training data is
based on machine-learned alignments, 99.4% of the
training examples of? have no annotations, and only
0.6% have the particle annotation projected from the
English ’s possessive marker. If we remove the noun
and adverb senses from the Wiktionary performance
of PR+ improves to 72.87%. Alternatively, we could
add another constraint to prefer closed-class words
over open-class words when both are licensed by the
dictionary. When we add such a constraint to Chinese
with a constraint value of 0.95, we recover most of the
loss (68.43? 72.94); however, we do not report this
specific change to the Chinese experimental setup in
Table 1 to maintain generality.
6.2 Named-Entity Segmentation
Results: Table 2 shows the results for the named en-
tity segmentation experiments. First, we observe that
the word alignment filtering step (§5.2.2) improves
results for all three languages by significant margins,
for both the BASE and PR models. Both with and
without filtering, we observe that the baseline mod-
els are very strongly biased towards precision. The
filtering step tends to help with recall more than pre-
cison for both models. By having a soft constraint via
PR and allowing some segmentations to fall outside
of the transferred one, we get an increase in recall,
2003
No Filtering Filtering (§5.2.2)
Lang Metric BASE PR BASE PR
Prec 74.29 73.85 75.36 76.47
de Recall 41.69 54.50 54.71 64.61
F1 53.41 62.71 63.39 70.04
Prec 74.53 62.10 82.50 70.22
es Recall 56.39 78.33 67.27 81.10
F1 64.20 69.28 74.11 75.27
Prec 81.90 75.12 86.39 76.09
nl Recall 50.54 76.11 65.45 79.11
F1 62.51 75.61 74.47 77.57
Above: dev, below: test
Prec 73.23 71.67 69.90 70.94
de Recall 39.70 51.81 52.52 61.42
F1 51.49 60.14 59.97 65.84
Prec 75.38 65.40 83.50 73.68
es Recall 56.00 80.30 67.55 83.31
F1 64.26 72.09 74.68 78.20
Prec 79.45 73.55 86.01 77.05
nl Recall 47.45 75.37 65.16 80.11
F1 59.42 74.45 74.14 78.55
Table 2: Result for the named-entity segmentation exper-
iments. The highest score in each category is shown in
bold. Note that “No Filtering” still discards sentences with
no projected entities.
and in turn an improved F1 score. On average the
PR model improves F-score by 3.6% on the develop-
ment set and 4.6% on the test set over the baseline
(when filtering is used). Note that because we focus
on named entity segmentation, our results are not
directly comparable to those of Ta¨ckstro¨m (2012),
who train a de-lexicalized named entity recognizer
on one language and apply it to other languages.
Error Analysis: In order to get a sense for the types
of errors made by the baseline which are corrected
by the PR model, we collected statistics about the
most frequent errors in the segments extracted by
the baseline and by our model. We divided the er-
rors into missing segments, extraneous segments and
overlapping segments.
From Table 2, it is clear that the most common
errors for the baseline models are missing entities.
From our analysis of the CoNLL development data,
we found that the entities that occur with little context
(such as the location and publisher of an item) at the
onset of news articles are most frequently missed. For
German, dpa (Deutsche Presse-Agentur) and Reuter
are the two most common missing segmentations;
the Spanish counterparts are Gobierno (Government)
and Barcelona, while for Dutch they are De Morgen
and Brussel. While filtering parallel sentences and
using a soft constraint both increase recall, even our
strongest model does not get enough information to
predict these entities, and they continue to be major
sources of error. By contrast, the names mentioned
in context are the ones that are most frequently added
to the analysis when PR is used. In a sense this
is desirable, since a machine-learned named-entity
segmentation system is most useful for the long tail
of entity mentions.
If we filter the training data and use the PR model
to further increase recall, precision errors tend to
become relatively more frequent (this trend is ob-
servable in Table 2). For German, the most frequent
precision error is Mark referring to the Deutsche
Mark. For Spanish, the most frequent precision er-
rors are due to boundary errors. The Spanish an-
notation guidelines include enclosing quotes as part
of the entity name, and failing to include them ac-
counts for just under 1% of the precision errors of
the PR system that uses filtering. The second most
frequent error is failing to segment Inter de Mila´n.
The model segments out either Inter or Mila´n or both
by themselves depending on context.
7 Conclusions
In this paper, we presented a framework for cross-
lingual transfer of sequence information from a
resource-rich source language to a resource-poor tar-
get language. Our framework incorporates soft con-
straints while training with projected information
via posterior regularization. We presented the effi-
cacy of our framework on two very useful natural
language tasks: POS tagging and named-entity seg-
mentation. The soft constraints used in our work
model intuitions about a given task. For the POS
tagging problem, we designed constraints that also
incorporate projected token-level information, and
presented a principled method for choosing the extent
to which this information should be trusted within
the PR framework. This approach generalizes the
state of the art in cross-lingual projection work in
the context of POS tagging, and improves upon it.
2004
Across seventeen languages, our models outperform
the previous state of the art by an average of 0.8%
(greater than 4% error reduction), and outperforms
it on twelve out of seventeen languages. For named-
entity segmentation, our model results in 3.6% and
4.6% absolute improvements in F1-score on our de-
velopment and test sets respectively, when averaged
across three languages.
Acknowledgments
We would like to thank Ryan McDonald, Fernando
Pereira, Slav Petrov and Oscar Ta¨ckstro¨m for numer-
ous discussions on this topic and providing detailed
feedback on early drafts of this paper. We are also
grateful to the four anonymous reviewers for their
valuable comments.
References
Anne Abeille´, Lionel Cle´ment, and Franc¸ois Toussenel.
2003. Building a Treebank for French. In A. Abeille´,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the Work-
shop on Human Language Technology.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1–38.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Kuzman Ganchev, Joa˜o Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learn-
ing Research, 11:2001–2049.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311–325.
Jun’ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012.
Multilingual named entity recognition using parallel
data and metadata from wikipedia. In Proceedings of
ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Solomon Kullback and Richard A. Leibler. 1951. On
information and sufficiency. Annals of Mathematical
Statistics, 22:49–86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa˜o Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential families.
In Proceedings of ICML.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313–330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku¨bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Oscar Ta¨ckstro¨m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
2005
Oscar Ta¨ckstro¨m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. Trans-
actions of the Association for Computational Linguis-
tics, 1:1–12.
Oscar Ta¨ckstro¨m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recogni-
tion. In Proceedings of the NAACL-HLT Workshop on
the Induction of Linguistic Structure.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Andreas Vlachos. 2006. Active annotation. Proceedings
of EACL.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP Workshop: NLP for Less Privi-
leged Languages.
2006
