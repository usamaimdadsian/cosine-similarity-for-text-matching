Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 857–868,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Soft Dependency Constraints for Reordering
in Hierarchical Phrase-Based Translation
Yang Gao, Philipp Koehn and Alexandra Birch
School of Informatics
University of Edinburgh
Edinburgh, UK, EH8 9AB
yanggao1119@gmail.com, pkoehn@inf.ed.ac.uk, a.birch@ed.ac.uk
Abstract
Long-distance reordering remains one of the
biggest challenges facing machine translation.
We derive soft constraints from the source de-
pendency parsing to directly address the re-
ordering problem for the hierarchical phrase-
based model. Our approach significantly im-
proves Chinese–English machine translation
on a large-scale task by 0.84 BLEU points
on average. Moreover, when we switch the
tuning function from BLEU to the LRscore
which promotes reordering, we observe total
improvements of 1.21 BLEU, 1.30 LRscore
and 3.36 TER over the baseline. On aver-
age our approach improves reordering preci-
sion and recall by 6.9 and 0.3 absolute points,
respectively, and is found to be especially ef-
fective for long-distance reodering.
1 Introduction
Reordering, especially movement over longer dis-
tances, continues to be a hard problem in statistical
machine translation. It motivates much of the re-
cent work on tree-based translation models, such as
the hierarchical phrase-based model (Chiang, 2007)
which extends the phrase-based model (Koehn et al.,
2003) by allowing the so-called hierarchical phrases
containing subphrases.
The hierarchical phrase-based model captures the
recursiveness of language without relying on syntac-
tic annotation, and promises better reordering than
the phrase-based model. However, Birch et al.
(2009) find that although the hierarchical phrase-
based model outperforms the phrase-based model in
terms of medium-range reordering, it does equally
poorly in long-distance reordering due to constraints
to guarantee efficiency.
Syntax-based models that use phrase structure
constituent labels as non-terminals in their transfer
rules, exemplified by that of Galley et al. (2004),
produce smarter and syntactically motivated re-
ordering. However, when working with off-the-shelf
tools for parsing and alignment, this approach may
impose harsh limits on rule extraction and requires
serious efforts of optimization (Wang et al., 2010).
An alternative approach is to augment the general
hierarchical phrase-based model with soft syntactic
constraints. Here, we derive three word-based, com-
plementary constraints from the source dependency
parsing, including:
• A dependency orientation feature, trained with
maximum entropy on the word-aligned par-
allel data, which directly models the head-
dependent orientation for source words;
• An integer-valued cohesion penalty that com-
plements the dependency orientation feature,
and fires when a word is not translated with its
head. It measures derivation well-formedness
and is used to indirectly help reordering;
• An auxiliary unaligned penalty feature that mit-
igates search error given the other two features.
We achieve significant improvements in terms of
the overall translation quality and reordering behav-
ior. To our knowledge we are the first to use the
source dependency parsing to target the reordering
problem for hierarchical phrase-based MT.
857
??   ?   ?  ??      ?    ??   ?   ??   ??    ??  .pobj dobj
prep
 Australia  is          with  North Korea    have       dipl. rels.  that      few           countries    one of    .     Aozhou    shi        yu     Beihan             you        bangjiao    de        shaoshu    guojia         zhiyi      .    
 Australia is one of the few countries that have diplomatic relations with North Korea. 
top
pobj
cpm nummodnn
attrpunct
rcmod
 
Figure 1: Example dependency parsing generated by the Stanford Parser. The Chinese source sentence and its English
translation come from (Chiang, 2007).
2 Three Soft Dependency Constraints
Our features are based on the source dependency
parsing, as shown in Figure 1. The basic unit of de-
pendency parsing is a triple consisting of the depen-
dent word, the head word and the dependency rela-
tion that connects them. For example, in Figure 1,
an arrow labelled prep goes from the word yu (En-
glish with) to the word you (English have), showing
that yu is a prepositional modifier of you.
We use the Stanford Parser1 to generate depen-
dency parsing, which automatically extracts de-
pendency relations from phrase structure parsing
(de Marneffe et al., 2006).
2.1 Dependency Orientation
Based on the assumption that constituents generally
move as a whole (Quirk et al., 2005), we decompose
the sentence reordering probability into the reorder-
ing probability for each aligned source word with re-
spect to its head, excluding the root word at the top
of the dependency hierarchy which does not have a
head word. Similarly, Hayashi et al. (2010) also take
a word-based reordering approach for HPBMT, but
they model all possible pairwise orientation from
the source side as a general linear ordering prob-
lem (Tromble and Eisner, 2009).
To be more specific, we have a maximum entropy
orientation classifier that predicts the probability of
a source word being translated in a monotone or re-
versed manner with respect to its head. For example,
1http://nlp.stanford.edu/software/lex-parser.shtml
(a) (b)
pob poj pod pob poj pod
rob rob
roj roj
rod rod
roe roe
iheadidep idepihead
Figure 2: Word alignments to illustrate orientation clas-
sification. In (a), monotone (M); in (b), reversed (R).
given the alignment in Figure 2(a), with the align-
ment points (idep, jdep) for the source dependent
word and (ihead, jhead) for the source head word,
we define two orientation classes as:
c =
{
R if (jdep ? jhead)(idep ? ihead) < 0
M otherwise
(1)
When a source head or dependent word is aligned
to multiple target words, as shown in Figure 2(b),
we always take the first target word for orientation
classification.
The orientation classifier is trained on the large
word-aligned parallel corpus. Various features can
potentially be used, based on the source and target
context as well as syntactic and semantic analysis.
The orientation probability is evaluated in the fol-
lowing log-linear equation, where f is the source
context, d is the source dependency parsing, e? is
the target context produced so far, a? is the align-
ment produced so far and c is the orientation class:
858
Word p(M) p(R)
Aozhou 0.81 0.19
shi NA NA
yu 0.45 0.55
Beihan 0.88 0.12
you 0.12 0.88
bangjiao 0.83 0.17
de 0.58 0.42
shaoshu 0.30 0.70
guojia 0.19 0.81
zhiyi 0.85 0.15
. 1.00 0.00
Table 1: The dependency orientation probabilities for
words of the Figure 1 sentence, in both monotone and
reversed cases.
p(c|f, d, e?, a?) =
exp(
?N
n=1 ?nhn(f, d, e?, a?, c))?
c??{M,R} exp(
?N
n=1 ?nhn(f, d, e?, a?, c?))
(2)
Currently, we only use two kinds of features: (1)
the concatenation of the source dependent word with
the dependency relation and (2) the concatenation of
the source head word with the dependency relation.
So for the word yu (English with) in Figure 1, we
extract these features for orientation classification:
prep DEP yu and prep HEAD you.
We define the dependency orientation feature
score for a translation hypothesis as the sum of the
log orientation probabilities for each source word.
This score is used as one feature in the log-linear
formulation of the hierarchical phrase-based model.
Table 1 shows the dependency orientation proba-
bilities for all words in the Figure 1 sentence. Most
interestingly, the orientation probabilities for you
(English have) strongly support global reordering of
one of the few countries with the relative clause that
have diplomatic relations with North Korea. We find
that it is a general trend for long-distance reordering
to gain stronger support, since it is often correlated
with prominent reordering patterns (such as relative
clause and preposition) as well as lexical evidences
(such as “... zhiyi” (English “one of ...”)) for which
the reversed orientation takes up the majority of the
training cases.
Consider the following rules (both terminals and
nonterminals are coindexed):
X ? (yu1 Beihan2 you3 bangjiao4,
have3 dipl.4 rels.4 with1 North2 Korea2)
(3)
X ? (yu1 Beihan2 you3 bangjiao4,
with1 North2 Korea2 have3 dipl.4 rels.4)
(4)
According to Table 1, the hypothesis that applies
Rule 3 receives a probability of 0.55 for yu getting
reversed with its head you, as well as 0.88 and 0.83
for translating Beihan and bangjiao in a monotone
manner with respect to their heads. Rule 4 is associ-
ated with probabilities 0.45, 0.88 and 0.83 for mono-
tone translation of yu, Beihan and bangjiao. Thus
our dependency orientation feature is able to trace
the difference in ordering the PP with North Korea
(as underlined) and the VP have dipl. rels. down to
the orientation of the preposition yu (English with)
with respect to its head you (English have), and pro-
mote Rule 3 which has the right word order.
The word you (English have) cannot be scored
in Rules 3 or 4, since its head word zhiyi (English
one of) is not covered. In this case, we say that
the word you is unresolved. We carry an unre-
solved word along in the derivation process until we
reach a terminator hypothesis which translates the
head word. Then the resulting dependency orien-
tation score is added to the terminator hypothesis.
This means that the dependency orientation feature
is “stateless”, i.e., hypotheses that cover the same
source span with the same orientation information
will receive the same feature score, regardless of the
derivation history. Therefore, Derivation 5 in the fol-
lowing will have the same dependency orientation
score as Derivation (Rule) 3, and Derivation 6 will
score the same as Derivation (Rule) 4.
5.1 X ? (yu1, with1)
5.2 X ? (Beihan1, North1 Korea1)
5.3 X ? (X1 X2, X1 X2)
5.4 X ? (X1 you2 bangjiao3,
have2 dipl.3 rels.3 X1)
(5)
859
6.1 X ? (Beihan1 you2, North1 Korea1 has2)
6.2 X ? (X1 bangjiao2, X1 dipl.2 rels.2)
6.3 X ? (yu1 X2, with1 X2)
(6)
2.2 Cohesion Penalty
When the dependency orientation for a word is
temporarily unavailable (“unresolved”), a cohesion
penalty fires. Cohesion penalty counts the total oc-
currences of unresolved words for a translation hy-
pothesis, which involve newly encountered unre-
solved words as well as old unresolved words car-
ried on from the derivation history. Therefore, the
cohesion penalty is “stateful”, i.e., an unresolved
word is repeatedly penalized until it gets resolved.
Under this definition, the most cohesive derivation
translates the entire sentence with one rule, where
every word is locally resolved. The least cohe-
sive derivation translates each word individually and
glues word translations together. Consulting Fig-
ure 1, the cohesion penalty in Derivation 5 is 4, since
the word yu (English with) is unresolved twice (in
5.1 and 5.3), and both Beihan (English North Ko-
rea) and you (English have) are unresolved once (in
5.2 and 5.4, respectively); the cohesion penalty in
Derivation 6 is 5: 2 from Beihan (English North
Korea) (in 6.1 and 6.2) and 3 from you (English
have). As a result, Derivation 5 gets promoted,
which echoes with human intuition since Deriva-
tion 5 translates syntactic constituents. To sum
up, our cohesion penalty provides an integer-valued
measure of derivation well-formedness in the hierar-
chical phrase-based MT. Same as dependency orien-
tation, the cohesion penalty is not applicable to the
root word of the sentence.
We propose the cohesion penalty in order to fur-
ther improve reordering, especially in long-distance
cases, since a well-formed derivation at an earlier
stage makes it more likely to explore hierarchical
rules that perform more reliable reordering. In this
respect, the cohesion penalty can be seen as an aid
to the glue rule penalty and as an alternative to
constituency-based constraints.
Specifically, the glue rule penalty (Chiang, 2007)
promotes hierarchical rules. Hierarchical rules
whose lexical evidence helps resolve words locally
will also be favored by our cohesion penalty feature.
However, ignorant of the syntactic structure, the
glue rule penalty may penalize a reasonably cohe-
sive derivation such as Derivation 5 and at the same
time promote a less cohesive hierarchical transla-
tion, such as Derivation 6.
Compared with constituency constraints based on
the phrase structure, our cohesion penalty derived
from the binary dependency parsing has two differ-
ent characteristics.
First, our cohesion penalty is by nature more tol-
erant to some meaningful noncontituent translations.
For example, constituency constraints in (Chiang,
2005; Marton and Resnik, 2008; Chiang et al., 2009)
would penalize Rule 7 below which is useful for
German–English translation (Koehn et al., 2003),
and Rule 8 which can be applied to the Figure 1
sentence. Fuzzy constituency constraints can solve
this problem with a combination of product cate-
gories and slash categories (Chiang, 2010). Yet
our cohesion penalty by nature admits these trans-
lations as cohesive (with no extra cost from es and
Aozhou since both are locally resolved). Admittedly,
our current implementation of the cohesion penalty
is blind to some other meaningful nonconstituent
collocations, such as neighbouring siblings of a
common uncovered head (regulated as the “floating
structure” in (Shen et al., 2008)). A concrete exam-
ple is Rule 9 which is useful for the Figure 1 sen-
tence. To address this problem, another feature can
be defined in the same manner to capture how each
head word is translated with its children.
X ? (es1 gibt2, there1 is2) (7)
X ? (Aozhou1 shi2, Australia1 is2) (8)
X ? (shaoshu1 guojia2, few1 countries2) (9)
Second, our cohesion penalty can be by na-
ture more discriminative. Compared with the
constituency constraints, the cohesion penalty is
integer-valued, and can be made sensitive to the
depth of each word in the dependency hierarchy (see
Section 2.4). Inspired by (Marton and Resnik, 2008;
Chiang et al., 2009), the cohesion penalty could
also be made sensitive to the dependency relation
of each word. However, this drastically increases
the number of features and requires a tuning algo-
rithm which scales better to high-dimensional model
spaces, such as MIRA (Watanabe et al., 2007; Chi-
ang et al., 2008).
860
pobj
 
?
pobj
dobjprep
 Aus t top
cpm
nummod nn
attr punct
rcmod
?
?? .
?
??
?
??
?? ??
??
 Aus r 
aliwh t 
aliwh r 
aliwh N 
aliwh o 
aliwh K 
Figure 3: Using 2 bins for the dependency parse tree of
the Figure 1 sentence.
2.3 Unaligned Penalty
The dependency orientation and cohesion penalty
cannot be applied to unaligned source words. This
may lead to search error, such as dropping (i.e., un-
aligning) key content words that are important for
lexical translation and reordering. The problem is
mitigated by an unaligned penalty applicable to all
words in the dependency hierarchy.
2.4 Grouping Words into Bins
Having defined dependency orientation, cohesion
penalty and unaligned penalty, we section the source
dependency tree uniformly by depth, group words at
different depths into bins and only add the feature
scores of a word into its respective bin. In this way
one feature is split into several sub-features and each
can be trained discriminatively by MERT.
There are two motivations for binning. The pri-
mary motivation is to distinguish long-distance re-
ordering which is still problematic for the hiero-
style model, since local reorderings generally op-
erate at low levels of the tree while high tree lev-
els tend to take more care of long-distance reorder-
ing. Parsing accuracy is another concern, yet its
impact on feature performance is intricate and our
MaxEnt-trained dependency orientation feature also
buffers against odd parsing. Using bins, we simply
let the tuning process decide how much to trust fea-
ture scores coming from different levels of parsing.
We experiment with 1, 2 and 3 bins. An example
of binning for the Figure 1 sentence can be found in
Figure 3. With 2 bins (hereafter “bin-2”), words at
Depth 1 and 2 are grouped into Bin 1, and words at
Depth 3, 4, 5 are grouped into Bin 2. As a simple
approach, binning does not take into account how
the tree levels spread out.
3 Experiments
3.1 General Settings
We used a parallel training corpus with 2.1 mil-
lion Chinese–English sentence pairs, aligned by
GIZA++. The Chinese side was parsed by the Stan-
ford Parser. Then we extracted 33.8 million exam-
ples from the parsed Chinese side to discriminatively
train 1.1 million features (using the MegaM soft-
ware2) for dependency orientation classification.
We trained three 5-gram language models with
modified Kneser-Ney smoothing (Kneser and Ney,
1995): one on the English half of the parallel cor-
pus, one on the Xinhua part of the Gigaword corpus,
one on the AFP part, and interpolated them for best
fit to the tuning set (Schwenk and Koehn, 2008).
We used NIST MT06 evaluation data (1664 lines)
as our tuning set, and tested on NIST MT02 (878
lines), MT05 (1082 lines) and MT08 (1357 lines).
Our baseline system was the Moses implemen-
tation of the hierarchical phrase-based model with
standard settings (Hoang et al., 2009). When only
1 bin was used, 3 additional features were added to
the baseline, one each from the soft dependency con-
straints. When we used 2 or 3 bins, the additional
feature counts doubled or tripled. We preserved ter-
minal alignment alongside nonterminal alignment
during the rule extraction and output word align-
ments together with translated strings. Since the fea-
tures we currently define are based entirely on the
source side, we used preprocessing to speed up de-
coding of our feature-augmented model. All experi-
ments were tuned with MERT (Och, 2003).
3.2 Using BLEU as the Tuning Metric
As a standard practice, we first used BLEU (Pap-
ineni et al., 2002) as the objective function for tun-
ing. Table 2 shows the results of the baseline model
as well as our complete feature-augmented model
with different bin numbers. With the “bin-2” setting,
we get substantial improvement of up to 1.03 BLEU
points (on MT02 data), and 0.84 BLEU points on
average. Using more than one bin (i.e., differentiat-
ing tree depths) is generally beneficial, although the
2http://www.umiacs.umd.edu/?hal/megam/index.html
861
Setting BLEU / LRscore / TERMT02 MT05 MT08 Average
baseline 34.01 / 41.85 / 68.93 32.23 / 40.50 / 68.15 28.09 / 37.17 / 66.82 31.44 / 39.84 / 67.97
bin-2 35.04 / 43.07 / 65.58 33.18 / 41.62 / 65.59 28.63 / 38.12 / 65.36 32.28 / 40.94 / 65.51
baseline-lr 34.23 / 42.06 / 68.08 32.28 / 40.61 / 67.61 27.99 / 37.27 / 66.98 31.50 / 39.98 / 67.56
bin-2-lr 35.42 / 43.25 / 64.82 33.44 / 41.80 / 64.88 29.10 / 38.38 / 64.14 32.65 / 41.14 / 64.61
Table 4: Results for the baseline model and the complete feature-augmented model with 2 bins (“bin-2”), using BLEU
and LRscore (“-lr”) as the tuning function. The BLEU scores of “bin-2” and “bin-2-lr” are significantly better than
baseline (p < 0.05), computed by paired bootstrap resampling (Koehn, 2004).
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
bin-1 34.20 32.13 28.41 31.58(+.14)
bin-2 35.04 33.18 28.63 32.28(+.84)
bin-3 34.35 32.79 28.37 31.84(+.40)
Table 2: Results of the baseline model as well as our
complete feature-augmented model with 1, 2 and 3 bins.
BLEU is the tuning function.
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
dep 34.26 32.58 28.07 31.64(+.20)
dep+coP 34.47 32.81 28.61 31.96(+.52)
dep+coP+unP 35.04 33.18 28.63 32.28(+.84)
Table 3: Contributions of the three soft dependency con-
straints, with the “bin-2” setting
problem of overfitting sets in when we use 3 bins
(with slightly higher tuning BLEU, not shown here).
We also studied the effect of adding features in-
crementally onto the baseline with the “bin-2” set-
ting, as shown in Table 3. On average, all three fea-
tures seem to have similar contributions.
3.3 Using LRscore as the Tuning Metric
Since our features are proposed to address the re-
ordering problem and BLEU is not sensitive enough
to reordering (especially in long-distance cases), we
have also tried tuning with a metric that highlights
reordering, i.e., the LRscore (Birch and Osborne,
2010). LRscore is a linear interpolation of a lexi-
cal metric and a reordering metric. We interpolated
BLEU (as the lexical metric) with the Kendall’s
tau permutation distance (as the reordering metric).
The Kendall’s tau permutation distance measures the
relative word order difference between the transla-
tion output and the reference(s) and is particularly
sensitive to long-distance reordering. Testing re-
sults in terms of BLEU, LRscore and TER (Snover
et al., 2006) are shown in Table 4. Tuned with
the LRscore, our feature-augmented model achieves
further average improvements (compare “bin-2” and
“bin-2-lr”) of 0.20 LRscore as well as 0.37 BLEU
and 0.90 TER. Note that while the BLEU increase
can largely be seen as a projection of the LRscore
increase back into its lexical component, the consis-
tent TER drop confirms that our improvement is not
metric-specific3. Altogether the final improvement
is 1.21 BLEU, 1.30 LRscore and 3.36 TER on aver-
age over the baseline.
However, an important question is how our fea-
tures affect short, medium and long-distance re-
orderings. In the next section, we conduct quanti-
tative analysis on reordering precision and recall, as
well as qualitative analysis on translation examples.
4 Analysis
4.1 Precision and Recall of Reordering
The key to obtaining precision and recall for reorder-
ing is to investigate whether reorderings in the refer-
ences are reproduced in the translations. We calcu-
late precision as the number of reproduced reorder-
ings divided by the total number of reorderings in
the translation, and recall as the number of repro-
duced reorderings divided by the number of reorder-
3One of our reviewers points out that according to the in-
ductive learning theory, it is counter-intuitive to improve on
BLEU and TER if we optimize by the LRscore. Yet we do
observe some other papers reporting increased TER or other
metric scores when BLEU is used for tuning (Carpuat and Wu,
2007; Shen et al., 2008), suggesting that MT evaluation might
be too complicated to be characterized just with inductive learn-
ing. Similar results based on extensive experiments can also be
found in (Birch and Osborne, 2011).
862
Setting MT02 MT05 MT08 Average
baseline 37.0 35.3 35.6 36.0
bin-2 42.7 40.8 38.7 40.7 (+4.7)
baseline-lr 37.3 35.0 34.2 35.5 (-0.5)
bin-2-lr 44.1 42.0 42.5 42.9 (+6.9)
Table 5: Overall precision for the test sets.
Setting MT02 MT05 MT08 Average
baseline 37.5 36.2 33.2 35.6
bin-2 36.8 35.9 31.8 34.8 (-0.8)
baseline-lr 37.0 35.6 32.2 34.9 (-0.7)
bin-2-lr 37.7 36.7 33.2 35.9 (+0.3)
Table 6: Overall recall for the test sets.
ings in the reference. Then we average the precision
and recall over all four reference translations.
Details of measuring reproduced reordering can
be found in Birch et al. (2008). An important dif-
ference in this work is in handling many-to-one and
one-to-many alignments, as we only retain the first
word alignment for any source or target word which
has multiple alignments. This is consistent with our
treatment in dependency orientation classification,
and results in more reorderings being extracted.
From Table 5 we can see that our features im-
prove precision by an average of 4.7 absolute points
when BLEU is used for tuning (“bin-2”). Switch-
ing from BLEU to the LRscore (“bin-2-lr”), we gain
2.2 points more and have a total improvement of 6.9
absolute points on average. This is a novel and im-
portant finding as we directly show that the quality
of reordering has been improved.
From Table 6, we observe a small but consistent
increase in recall with the “bin-2-lr” setting, averag-
ing 0.3 absolute points. However, the drop of recall
with the “bin-2” setting (by an average 0.8 points
from the baseline) is unexpected. It seems that when
applying our features alone, we are trading a small
drop in recall for a large gain in precision.
In Figure 4 we break down the precision and re-
call statistics in MT08 by the reordering width on
the source side. We find that our features con-
sistently help precision over all word ranges, with
more substantial improvement in the medium and
long word ranges. When recall is concerned, our
model does not help for short ranges of up to Width
4, but improves consistently for longer distance re-
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Prec
ision
0.0
0.1
0.2
0.3
0.4
0.5
0.6
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Reca
ll
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 4: Precision and recall breakdown for the source-
side reordering width 2-15 for the NIST MT08 dataset.
orderings. Once again, it seems that the feature-
augmented model is able to benefit from tuning with
a metric that is more sensitive to reordering, as the
performance of “bin-2-lr” is the best in all reorder-
ing statistics.
4.2 Translation Examples
We observe a number of outputs with improved
word order and more cohesive derivation, as the one
in Figure 5. The baseline translation is fragmented
and requires more glue rule applications. Specifi-
cally, it fails to translate the boxed area as a whole
into “the relations between the palestinian national
authority (pna) and the european union (eu)”. The
key dependency orientation that controls the global
reordering is between the prepositional modifier dui
(English to) and its head word, the verb gandao (En-
glish feel). The baseline system translates dui (En-
glish to) as “of the” and misorders the sentence. In
contrast, the feature-augmented model “bin-2” cap-
863
 AustraliwshhhhhhhhNhhhoKeishhhhhhvAs
ethtwdhiepdhtspdhNhhhoKKeihhhhhtr .stwhhhhfchhhhhhhhhhhhKdt.dda
 AhhhhhhhnApdalhhhzwsysea
BebditsaseahgetbjhoAtwrmst 
e hhhhpsazAhhhhAeabshhhyslrA vdhhhhhlAea s
!fhhhhmdbetsrai
????    , ??? ? ?  ??  ??  ?? ?   ??    ?? ?  ?? ??  ??     . 
leaverhhpea shhhhhhhhhhj
"ddbhhhhhhhhietsi"eutsrahhj
pobp
baseline: [at the same time , abbas] [of the] [palestinian [national authority ( pna )]] [and] [is satisfied
with the [relations] [between [the european union ( eu )]]] [.]
bin-2: [at the same time , abbas] [expressed satisfaction with [the relations between the [palestinian
[national authority ( pna ) [and the european union ( eu )]]]] .]
Figure 5: Example translations from the NIST MT08 set, output by the baseline model and “bin-2” model. The “-lr”
version outputs are quite similar and not shown here. Translation outputs are in lower case.
tures the boxed area as a whole and uses Rule 10 to
perform the right global reordering.
X ? (dui1 X2 gandao3 manyi4 .5 ,
expressed3 satisfaction4 with1 X2 .5 ) (10)
5 Related Work
In recent years, there has been a growing body of re-
search on using dependency for statistical machine
translation. Some directly encodes dependency in
the translation model (Ding and Palmer, 2005; Quirk
et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi
and Liu, 2010), while others use dependency as a
soft constraint (Cherry, 2008; Bach et al., 2009a,b;
Chang et al., 2009). Among them, Shen et al. (2008)
report that just filtering the phrase table by the so-
called well-formed target dependency structure does
not help, yet adding a target dependency language
model improves performance significantly. Our in-
tuitive interpretation is that the target dependency
language model capitalizes on two characteristics of
the dependency structure: it is based on words and it
directly connects head and child. Therefore, the tar-
get dependency language model makes good use of
the dependency representation as well as the target
side training data.
We follow the second line of research, and derive
three word-based soft constraints from the source
dependency parsing. Note that although we reuse
the word “cohesion” to name one of the constraints,
our work is different from (Cherry, 2008; Bach
et al., 2009a,b) which have successfully defined an-
other cohesion constraint from the source depen-
dency structure, with the aim of improving reorder-
ing in phrase-based MT.
To take a glance, Cherry (2008) and Bach et al.
(2009b) define cohesion as translating a source de-
pendency subtree contiguously into the target side
without interruption (span or subtree overlapping),
following Fox (2002). This span-based cohesion
constraint has a different criterion from our word-
based cohesion penalty and often leads to opposite
conclusions. Bach et al. (2009a) also use cohesion to
correlate with the lexicalized reordering model (Till-
man, 2004; Koehn et al., 2005), whereas we define
an orthogonal dependency orientation feature to ex-
plicitly model head-dependent reordering.
The fundamental difference, however, is rooted
in the translation model. Their span-based cohe-
sion constraint is implemented as an “interruption
check” to encourage finishing a subtree before trans-
lating something else. This check is very effective
for phrase-based decoding which searches over an
entire space within the distortion limit in order to
advance a hypothesis. In fact, it constrains reorder-
ing for the phrase-based model, as Cherry finds that
the cohesion constraint is used “primarily to prevent
distortion” and to provide “an intelligent estimate as
to when source order must be respected” (Cherry,
2008). However, since the hierarchical phrase-
based model already conducts principled reorder-
ing search with rules through the more constrained
chart-decoding, ill-formed derivations exhibit them-
selves more often as nonconstituent translation than
interrupted translation as defined in (Cherry, 2008;
Bach et al., 2009a,b) (They do have a non-empty in-
tersection, but neither subsumes the other). There-
864
fore, our cohesion penalty is better suited for the hi-
erarchical phrase-based model.
To discourage nonconstituent translation, Chiang
(2005) has proposed a constituency feature to exam-
ine whether a source rule span matches the source
constituent as defined by phrase structure parsing.
Finer-grained constituency constraints significantly
improve hierarchical phrase-based MT when ap-
plied on the source side (Marton and Resnik, 2008;
Chiang et al., 2009), or on the target side in a
more tolerant fashion (Zollmann and Venugopal,
2006). Using both source and target syntax, but
relaxing on rule extraction and substitution enables
HPBMT to produce more well-formed and syntac-
tically richer derivations (Chiang, 2010). Softening
constituency matching with latent syntactic distribu-
tions proves to be helpful (Huang et al., 2010). Com-
pared to constituency-based approaches, our cohe-
sion penalty based on the dependency structure nat-
urally supports constituent translations as well as
some nonconstituent translations, if not all of them
(as discussed in Section 2.2).
Our dependency orientation feature is similar to
the order model within dependency treelet trans-
lation (Quirk et al., 2005). Yet instead of a
head-relative position number for each modifier
word, we simply predict the head-dependent ori-
entation which is either monotone or reversed.
Our coarser-grained approach is more robust from
a machine learning perspective, yet still captures
prominent and long-distance reordering patterns ob-
served in Chinese–English (Wang et al., 2007),
German–English (Collins et al., 2005), Japanese–
English (Katz-Brown and Collins, 2008) and trans-
lation from English to a group of SOV lan-
guages (Xu et al., 2009). Not committed to spe-
cific language pairs, we learn orientation classifi-
cation from the word-aligned parallel data through
maximum entropy training as Zens and Ney (2006)
and Chang et al. (2009) for phrase-based translation
and Xiong et al. (2006) for the BTG model (Wu,
1996). While Chang et al. (2009) also make use
of source dependency, their orientation classifica-
tion concerns two subsequent phrase pairs in the left-
to-right phrase-based decoding (as apposed to each
dependent word and its head) and is therefore less
linguistically-motivated.
6 Conclusion
We have derived three novel features from the source
dependency structure for hierarchical phrase-based
MT. They work as a whole to capitalize on two char-
acteristics of the dependency representation: it is di-
rectly based on words and it directly connects head
and child. The effectiveness of our approach has
been demonstrated by a final average improvement
of 1.21 BLEU, 1.30 LRscore and 3.36 TER. On av-
erage we improve reordering precision and recall by
6.9 and 0.3 absolute points, respectively, over the
baseline. Moreover, our approach is found to be es-
pecially effective for long-distance reodering.
As mentioned in Section 2.2, the cohesion penalty
can be extended to also account for how a head
word is translated with its children so that we are
not biased towards one form of cohesive noncon-
stituent translation. All our features can be made
sensitive to the dependency relations or even words.
This fine-grainedness is especially desirable when
we want to reward words for being unaligned or un-
resolved, such as punctuations and function words
in certain context. Word alignment quality is crucial
for the performance of our features as well as the
LRscore which uses word alignment to compute the
permutation distance. As an alternative to GIZA++,
we would like to experiment with syntactically in-
formed aligners that better handle function words
which often exhibit high alignment ambiguity due
to low cross-lingual correspondence.
Finally, since our soft dependency constraints
promote reordering without increasing model com-
plexity, further gains can be achieved when combin-
ing our approach with orthogonal studies to improve
the quantity and quality of hierarchical (reordering)
rules, such as relaxing hierarchical rule extraction
constraints (Setiawan and Resnik, 2010) and selec-
tively lexicalizing rules with function words (Seti-
awan et al., 2009).
Acknowledgments
We would like to thank Miles Osborne, Adam
Lopez, Barry Haddow, Hieu Hoang, Philip Williams
and Michael Auli in the Edinburgh SMT group
as well as Kevin Knight, David Chiang and An-
drew Dai for inspiring discussions. We appreci-
ate Pichuan Chang, Huihsin Tseng, Richard Zens,
Matthew Snover and Nguyen Bach for helping us
865
understand their brilliant work. Many thanks to the
anonymous reviewers for their insightful comments
and suggestions. This work was supported in part
by the EuroMatrixPlus project funded by the Euro-
pean Commission (7th Framework Programme) and
in part under the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-C-0022.
References
Bach, N., Gao, Q., and Vogel, S. (2009a). Source-
side dependency tree reordering models with sub-
tree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada. International Association for
Machine Translation.
Bach, N., Vogel, S., and Cherry, C. (2009b). Cohesive
constraints in a beam search phrase-based decoder. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 1–4, Boul-
der, Colorado.
Birch, A., Blunsom, P., and Osborne, M. (2009). A quan-
titative analysis of reordering phenomena. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 197–205, Athens, Greece.
Birch, A. and Osborne, M. (2010). LRscore for evaluat-
ing lexical and reordering quality in MT. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 327–332, Upp-
sala, Sweden.
Birch, A. and Osborne, M. (2011). Reordering metrics
for mt. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Techologies, pages 1027–1035, Port-
land, Oregon, USA.
Birch, A., Osborne, M., and Koehn, P. (2008). Predict-
ing success in machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 745–754, Honolulu,
Hawaii.
Carpuat, M. and Wu, D. (2007). Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61–72, Prague, Czech Republic.
Chang, P.-C., Tseng, H., Jurafsky, D., andManning, C. D.
(2009). Discriminative reordering with Chinese gram-
matical relations features. In Proceedings of the Third
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST-3) at NAACL HLT 2009, pages 51–59,
Boulder, Colorado.
Cherry, C. (2008). Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of ACL-
08: HLT, pages 72–80, Columbus, Ohio.
Chiang, D. (2005). A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’05, pages 263–270, Strouds-
burg, PA, USA.
Chiang, D. (2007). Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
Chiang, D. (2010). Learning to translate with source and
target syntax. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1443–1452, Uppsala, Sweden.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001
new features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218–226, Boulder, Colorado.
Chiang, D., Marton, Y., and Resnik, P. (2008). Online
large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 224–233, Honolulu, Hawaii.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’05), pages
531–540, Ann Arbor, Michigan.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC-06.
Ding, Y. and Palmer, M. (2005). Machine translation
using probabilistic synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 541–548, Ann Arbor, Michigan.
Fox, H. (2002). Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 304–3111.
Galley, M., Hopkins, M., Knight, K., and Marcu, D.
(2004). What’s in a translation rule? In HLT-NAACL
2004: Main Proceedings, pages 273–280, Boston,
Massachusetts, USA.
866
Hayashi, K., Tsukada, H., Sudoh, K., Duh, K., and Ya-
mamoto, S. (2010). Hierarchical phrase-based ma-
chine translation with word-based reordering model.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
439–446, Beijing, China.
Hoang, H., Koehn, P., and Lopez, A. (2009). A unified
framework for phrase-based, hierarchical, and syntax-
based statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 152–159, Tokyo, Japan.
Huang, Z., Cmejrek, M., and Zhou, B. (2010). Soft syn-
tactic constraints for hierarchical phrase-based trans-
lation using latent syntactic distributions. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 138–147, Cam-
bridge, MA.
Katz-Brown, J. and Collins, M. (2008). Syntactic reorder-
ing in preprocessing for japanese-to-english transla-
tion: Mit system description for ntcir-7 patent transla-
tion task. In Proceedings of NTCIR-7 Workshop Meet-
ing, Tokyo, Japan.
Kneser, R. and Ney, H. (1995). Improved backing-off
for m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, pages 181–184.
Koehn, P. (2004). Statistical significance tests for ma-
chine translation evaluation. In Lin, D. and Wu, D.,
editors, Proceedings of EMNLP 2004, pages 388–395,
Barcelona, Spain. Association for Computational Lin-
guistics.
Koehn, P., Axelrod, A., Birch, A., Callison-burch, C., Os-
borne, M., and Talbot, D. (2005). Edinburgh system
description for the 2005 iwslt speech translation eval-
uation. In Proceedings of IWSLT2005.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal phrase based translation. In Proceedings of the
Joint Conference on Human Language Technologies
and the Annual Meeting of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL).
Marton, Y. and Resnik, P. (2008). Soft syntactic con-
straints for hierarchical phrased-based translation. In
Proceedings of ACL-08: HLT, pages 1003–1011,
Columbus, Ohio.
Mi, H. and Liu, Q. (2010). Constituency to dependency
translation with forests. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1433–1442, Uppsala, Sweden.
Och, F. J. (2003). Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation of
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.
Quirk, C., Menezes, A., and Cherry, C. (2005). De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 271–279, Ann Arbor, Michigan.
Schwenk, H. and Koehn, P. (2008). Large and diverse
language models for statistical machine translation. In
Proceedings of International Joint Conference on Nat-
ural Language Processing.
Setiawan, H., Kan, M. Y., Li, H., and Resnik, P. (2009).
Topological ordering of function words in hierarchical
phrase-based translation. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 324–332, Sun-
tec, Singapore.
Setiawan, H. and Resnik, P. (2010). Generalizing hierar-
chical phrase-based translation using rules with adja-
cent nonterminals. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 349–352, Los Angeles, California.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pages 577–585, Columbus,
Ohio.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of As-
sociation for Machine Translation in the Americas.
Tillman, C. (2004). A unigram orientation model for
statistical machine translation. In HLT-NAACL 2004:
Short Papers, pages 101–104, Boston, Massachusetts,
USA.
Tromble, R. and Eisner, J. (2009). Learning linear order-
ing problems for better translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1007–1016, Singapore.
Wang, C., Collins, M., and Koehn, P. (2007). Chinese
syntactic reordering for statistical machine translation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
867
Computational Natural Language Learning (EMNLP-
CoNLL), pages 737–745, Prague, Czech Republic.
Wang, W., May, J., Knight, K., and Marcu, D. (2010).
Re-structuring, re-labeling, and re-aligning for syntax-
based machine translation. Computational Linguistics,
36(2).
Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki, H.
(2007). Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764–773,
Prague, Czech Republic.
Wu, D. (1996). A polynomial-time algorithm for statis-
tical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152–158, Santa Cruz, California,
USA.
Xiong, D., Liu, Q., and Lin, S. (2006). Maximum en-
tropy based phrase reordering model for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 521–528, Sydney, Australia.
Xiong, D., Liu, Q., and Lin, S. (2007). A dependency
treelet string correspondence model for statistical ma-
chine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 40–47,
Prague, Czech Republic.
Xu, P., Kang, J., Ringgaard, M., and Och, F. (2009). Us-
ing a dependency parser to improve smt for subject-
object-verb languages. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 245–253, Boulder,
Colorado.
Zens, R. and Ney, H. (2006). Discriminative reordering
models for statistical machine translation. In Proceed-
ings on the Workshop on Statistical Machine Transla-
tion, pages 55–63, New York City.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138–141, New York City.
868
