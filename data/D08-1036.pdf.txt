Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344–352,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
A comparison of Bayesian estimators for
unsupervised Hidden Markov Model POS taggers
Jianfeng Gao
Microsoft Research
Redmond, WA, USA
jfgao@microsoft.com
Mark Johnson
Brown Univeristy
Providence, RI, USA
Mark?Johnson@Brown.edu
Abstract
There is growing interest in applying Bayesian
techniques to NLP problems. There are a
number of different estimators for Bayesian
models, and it is useful to know what kinds of
tasks each does well on. This paper compares
a variety of different Bayesian estimators for
Hidden Markov Model POS taggers with var-
ious numbers of hidden states on data sets of
different sizes. Recent papers have given con-
tradictory results when comparing Bayesian
estimators to Expectation Maximization (EM)
for unsupervised HMM POS tagging, and we
show that the difference in reported results is
largely due to differences in the size of the
training data and the number of states in the
HMM. We invesigate a variety of samplers for
HMMs, including some that these earlier pa-
pers did not study. We find that all of Gibbs
samplers do well with small data sets and few
states, and that Variational Bayes does well
on large data sets and is competitive with the
Gibbs samplers. In terms of times of conver-
gence, we find that Variational Bayes was the
fastest of all the estimators, especially on large
data sets, and that explicit Gibbs sampler (both
pointwise and sentence-blocked) were gener-
ally faster than their collapsed counterparts on
large data sets.
1 Introduction
Probabilistic models now play a central role in com-
putational linguistics. These models define a prob-
ability distribution P(x) over structures or analyses
x. For example, in the part-of-speech (POS) tag-
ging application described in this paper, which in-
volves predicting the part-of-speech tag ti of each
word wi in the sentence w = (w1, . . . , wn), the
structure x = (w, t) consists of the words w in a
sentence together with their corresponding parts-of-
speech t = (t1, . . . , tn).
In general the probabilistic models used in com-
putational linguistics have adjustable parameters ?
which determine the distribution P(x | ?). In this
paper we focus on bitag Hidden Markov Models
(HMMs). Since our goal here is to compare algo-
rithms rather than achieve the best performance, we
keep the models simple by ignoring morphology and
capitalization (two very strong cues in English) and
treat each word as an atomic entity. This means that
the model parameters ? consist of the HMM state-
to-state transition probabilities and the state-to-word
emission probabilities.
In virtually all statistical approaches the parame-
ters ? are chosen or estimated on the basis of training
data d. This paper studies unsupervised estimation,
so d = w = (w1, . . . , wn) consists of a sequence
of words wi containing all of the words of training
corpus appended into a single string, as explained
below.
Maximum Likelihood (ML) is the most common
estimation method in computational linguistics. A
Maximum Likelihood estimator sets the parameters
to the value ?ˆ that makes the likelihood Ld of the
data d as large as possible:
Ld(?) = P(d | ?)
?ˆ = argmax
?
Ld(?)
In this paper we use the Inside-Outside algo-
rithm, which is a specialized form of Expectation-
344
Maximization, to find HMM parameters which (at
least locally) maximize the likelihood function Ld.
Recently there is increasing interest in Bayesian
methods in computational linguistics, and the pri-
mary goal of this paper is to compare the perfor-
mance of various Bayesian estimators with each
other and with EM.
A Bayesian approach uses Bayes theorem to fac-
torize the posterior distribution P(? | d) into the
likelihood P(d | ?) and the prior P(?).
P(? | d) ? P(d | ?) P(?)
Priors can be useful because they can express pref-
erences for certain types of models. To take an
example from our POS-tagging application, most
words belong to relatively few parts-of-speech (e.g.,
most words belong to a single POS, and while there
are some words which are both nouns and verbs,
very few are prepositions and adjectives as well).
One might express this using a prior which prefers
HMMs in which the state-to-word emissions are
sparse, i.e., each state emits few words. An appro-
priate Dirichlet prior can express this preference.
While it is possible to use Bayesian inference to
find a single model, such as the Maximum A Pos-
teriori or MAP value of ? which maximizes the
posterior P(? | d), this is not necessarily the best
approach (Bishop, 2006; MacKay, 2003). Instead,
rather than commiting to a single value for the pa-
rameters ? many Bayesians often prefer to work
with the full posterior distribution P(? | d), as this
naturally reflects the uncertainty in ?’s value.
In all but the simplest models there is no known
closed form for the posterior distribution. However,
the Bayesian literature describes a number of meth-
ods for approximating the posterior P(? | d). Monte
Carlo sampling methods and Variational Bayes are
two kinds of approximate inference methods that
have been applied to Bayesian inference of unsu-
pervised HMM POS taggers (Goldwater and Grif-
fiths, 2007; Johnson, 2007). These methods can also
be used to approximate other distributions that are
important to us, such as the conditional distribution
P(t | w) of POS tags (i.e., HMM hidden states) t
given words w.
This recent literature reports contradictory results
about these Bayesian inference methods. John-
son (2007) compared two Bayesian inference algo-
rithms, Variational Bayes and what we call here a
point-wise collapsed Gibbs sampler, and found that
Variational Bayes produced the best solution, and
that the Gibbs sampler was extremely slow to con-
verge and produced a worse solution than EM. On
the other hand, Goldwater and Griffiths (2007) re-
ported that the same kind of Gibbs sampler produced
much better results than EM on their unsupervised
POS tagging task. One of the primary motivations
for this paper was to understand and resolve the dif-
ference in these results. We replicate the results of
both papers and show that the difference in their re-
sults stems from differences in the sizes of the train-
ing data and numbers of states in their models.
It turns out that the Gibbs sampler used in these
earlier papers is not the only kind of sampler for
HMMs. This paper compares the performance of
four different kinds of Gibbs samplers, Variational
Bayes and Expectation Maximization on unsuper-
vised POS tagging problems of various sizes. Our
goal here is to try to learn how the performance of
these different estimators varies as we change the
number of hidden states in the HMMs and the size
of the training data.
In theory, the Gibbs samplers produce streams
of samples that eventually converge on the true
posterior distribution, while the Variational Bayes
(VB) estimator only produces an approximation to
the posterior. However, as the size of the training
data distribution increases the likelihood function
and therefore the posterior distribution becomes in-
creasingly peaked, so one would expect this varia-
tional approximation to become increasingly accu-
rate. Further the Gibbs samplers used in this paper
should exhibit reduced mobility as the size of train-
ing data increases, so as the size of the training data
increases eventually the Variational Bayes estimator
should prove to be superior.
However the two point-wise Gibbs samplers in-
vestigated here, which resample the label of each
word conditioned on the labels of its neighbours
(amongst other things) only require O(m) steps per
sample (where m is the number of HMM states),
while EM, VB and the sentence-blocked Gibbs sam-
plers require O(m2) steps per sample. Thus for
HMMs with many states it is possible to perform one
or two orders of magnitude more iterations of the
345
point-wise Gibbs samplers in the same run-time as
the other samplers, so it is plausible that they would
yield better results.
2 Inference for HMMs
There are a number of excellent textbook presen-
tations of Hidden Markov Models (Jelinek, 1997;
Manning and Schu¨tze, 1999), so we do not present
them in detail here. Conceptually, a Hidden Markov
Model uses a Markov model to generate the se-
quence of states t = (t1, . . . , tn) (which will be in-
terpreted as POS tags), and then generates each word
wi conditioned on the corresponding state ti.
We insert endmarkers at the beginning and end
of the corpus and between sentence boundaries,
and constrain the estimators to associate endmarkers
with a special HMM state that never appears else-
where in the corpus (we ignore these endmarkers
during evaluation). This means that we can formally
treat the training corpus as one long string, yet each
sentence can be processed independently by a first-
order HMM.
In more detail, the HMM is specified by a pair of
multinomials ?t and ?t associated with each state t,
where ?t specifies the distribution over states t? fol-
lowing t and ?t specifies the distribution over words
w given state t.
ti | ti?1 = t ? Multi(?t)
wi | ti = t ? Multi(?t)
(1)
The Bayesian model we consider here puts a fixed
uniform Dirichlet prior on these multinomials. Be-
cause Dirichlets are conjugate to multinomials, this
greatly simplifies inference.
?t | ? ? Dir(?)
?t | ?? ? Dir(??)
A multinomial ? is distributed according to the
Dirichlet distribution Dir(?) iff:
P(? | ?) ?
m
?
j=1
??j?1j
In our experiments we set ? and ?? to the uniform
values (i.e., all components have the same value ? or
??), but it is possible to estimate these as well (Gold-
water and Griffiths, 2007). Informally, ? controls
the sparsity of the state-to-state transition probabil-
ities while ?? controls the sparsity of the state-to-
word emission probabilities. As ?? approaches zero
the prior strongly prefers models in which each state
emits as few words as possible, capturing the intu-
ition that most word types only belong to one POS
mentioned earlier.
2.1 Expectation Maximization
Expectation-Maximization is a procedure that iter-
atively re-estimates the model parameters (?,?),
converging on a local maximum of the likelihood.
Specifically, if the parameter estimate at iteration `
is (?(`),?(`)), then the re-estimated parameters at it-
eration `+ 1 are:
?(`+1)t?|t = E[nt?,t]/E[nt] (2)
?(`+1)w|t = E[n
?
w,t]/E[nt]
where n?w,t is the number of times word w occurs
with state t, nt?,t is the number of times state t? fol-
lows t and nt is the number of occurences of state t;
all expectations are taken with respect to the model
(?(`),?(`)).
The experiments below used the Forward-
Backward algorithm (Jelinek, 1997), which is a dy-
namic programming algorithm for calculating the
likelihood and the expectations in (2) in O(nm2)
time, where n is the number of words in the train-
ing corpus and m is the number of HMM states.
2.2 Variational Bayes
Variational Bayesian inference attempts to find a
function Q(t,?,?) that minimizes an upper bound
(3) to the negative log likelihood.
? log P(w)
= ? log
?
Q(t,?,?)P(w, t,?,?)Q(t, ?, ?) dt d? d?
? ?
?
Q(t,?,?) log P(w, t,?,?)Q(t,?,?) dt d? d? (3)
The upper bound (3) is called the Variational Free
Energy. We make a “mean-field” assumption that
the posterior can be well approximated by a factor-
ized model Q in which the state sequence t does not
covary with the model parameters ?,?:
P(t,?,? | w) ? Q(t,?,?) = Q1(t)Q2(?,?)
346
P(ti|w, t?i, ?, ??) ?
(n?wi,ti + ??
nti + m???
) (nti,ti?1 + ?
nti?1 + m?
) (nti+1,ti + I(ti?1 = ti = ti+1) + ?
nti + I(ti?1 = ti) + m?
)
Figure 1: The conditional distribution for state ti used in the pointwise collapsed Gibbs sampler, which conditions on
all states t?i except ti (i.e., the counts n do not include ti). Here m? is the size of the vocabulary, m is the number of
HMM states and I(·) is the indicator function (i.e., equal to one if its argument is true and zero otherwise),
The calculus of variations is used to minimize the
KL divergence between the desired posterior distri-
bution and the factorized approximation. It turns
out that if the likelihood and conjugate prior be-
long to exponential families then the optimal Q1 and
Q2 do too, and there is an EM-like iterative pro-
cedure that finds locally-optimal model parameters
(Bishop, 2006).
This procedure is especially attractive for HMM
inference, since it involves only a minor modifica-
tion to the M-step of the Forward-Backward algo-
rithm. MacKay (1997) and Beal (2003) describe
Variational Bayesian (VB) inference for HMMs. In
general, the E-step for VB inference for HMMs is
the same as in EM, while the M-step is as follows:
?˜(`+1)t?|t = f(E[nt?,t] + ?)/f(E[nt] +m?) (4)
?˜(`+1)w|t = f(E[n
?
w,t] + ??)/f(E[nt] + m???)
f(v) = exp(?(v))
where m? and m are the number of word types and
states respectively, ? is the digamma function and
the remaining quantities are as in (2). This means
that a single iteration can be performed in O(nm2)
time, just as for the EM algorithm.
2.3 MCMC sampling algorithms
The goal of Markov Chain Monte Carlo (MCMC)
algorithms is to produce a stream of samples from
the posterior distribution P(t | w,?). Besag (2004)
provides a tutorial on MCMC techniques for HMM
inference.
A Gibbs sampler is a simple kind of MCMC
algorithm that is well-suited to sampling high-
dimensional spaces. A Gibbs sampler for P(z)
where z = (z1, . . . , zn) proceeds by sampling and
updating each zi in turn from P(zi | z?i), where
z?i = (z1, . . . , zi?1, zi+1, . . . , zn), i.e., all of the
z except zi (Geman and Geman, 1984; Robert and
Casella, 2004).
We evaluate four different Gibbs samplers in this
paper, which vary along two dimensions. First, the
sampler can either be pointwise or blocked. A point-
wise sampler resamples a single state ti (labeling a
single word wi) at each step, while a blocked sam-
pler resamples the labels for all of the words in a
sentence at a single step using a dynamic program-
ming algorithm based on the Forward-Backward al-
gorithm. (In principle it is possible to use block
sizes other than the sentence, but we did not explore
this here). A pointwise sampler requires O(nm)
time per iteration, while a blocked sampler requires
O(nm2) time per iteration, where m is the number
of HMM states and n is the length of the training
corpus.
Second, the sampler can either be explicit or col-
lapsed. An explicit sampler represents and sam-
ples the HMM parameters ? and ? in addition to
the states t, while in a collapsed sampler the HMM
parameters are integrated out, and only the states t
are sampled. The difference between explicit and
collapsed samplers corresponds exactly to the dif-
ference between the two PCFG sampling algorithms
presented in Johnson et al. (2007).
An iteration of the pointwise explicit Gibbs sam-
pler consists of resampling ? and ? given the state-
to-state transition counts n and state-to-word emis-
sion counts n? using (5), and then resampling each
state ti given the corresponding word wi and the
neighboring states ti?1 and ti+1 using (6).
?t | nt,? ? Dir(nt +?)
?t | n?t,?? ? Dir(n?t +??)
(5)
P(ti | wi, t?i,?,?) ? ?ti|ti?1?wi|ti?ti+1|ti (6)
The Dirichlet distributions in (5) are non-uniform;
nt is the vector of state-to-state transition counts in
t leaving state t in the current state vector t, while
347
n?t is the vector of state-to-word emission counts for
state t. See Johnson et al. (2007) for a more detailed
explanation, as well as an algorithm for sampling
from the Dirichlet distributions in (5).
The samplers that Goldwater and Griffiths (2007)
and Johnson (2007) describe are pointwise collapsed
Gibbs samplers. Figure 1 gives the sampling distri-
bution for this sampler. As Johnson et al. (2007)
explains, samples of the HMM parameters ? and ?
can be obtained using (5) if required.
The blocked Gibbs samplers differ from the point-
wise Gibbs samplers in that they resample the POS
tags for an entire sentence at a time. Besag (2004)
describes the well-known dynamic programming
algorithm (based on the Forward-Backward algo-
rithm) for sampling a state sequence t given the
words w and the transition and emission probabil-
ities ? and ?.
At each iteration the explicit blocked Gibbs sam-
pler resamples ? and ? using (5), just as the explicit
pointwise sampler does. Then it uses the new HMM
parameters to resample the states t for the training
corpus using the algorithm just mentioned. This can
be done in parallel for each sentence in the training
corpus.
The collapsed blocked Gibbs sampler is a
straight-forward application of the Metropolis-
within-Gibbs approach proposed by Johnson et al.
(2007) for PCFGs, so we only sketch it here. We
iterate through the sentences of the training data, re-
sampling the states for each sentence conditioned
on the state-to-state transition counts n and state-
to-word emission counts n? for the other sentences
in the corpus. This is done by first computing the
parameters ?? and ?? of a proposal HMM using (7).
??t?|t =
nt?,t + ?
nt + m?
(7)
??w|t =
n?w,t + ??
nt + m??
Then we use the dynamic programming sampler de-
scribed above to produce a proposal state sequence
t? for the words in the sentence. Finally, we use
a Metropolis-Hastings accept-reject step to decide
whether to update the current state sequence for the
sentence with the proposal t?, or whether to keep the
current state sequence. In practice, with all but the
very smallest training corpora the acceptance rate is
very high; the acceptance rate for all of our collapsed
blocked Gibbs samplers was over 99%.
3 Evaluation
The previous section described six different unsu-
pervised estimators for HMMs. In this section
we compare their performance for English part-of-
speech tagging. One of the difficulties in evalu-
ating unsupervised taggers such as these is map-
ping the system’s states to the gold-standard parts-
of-speech. Goldwater and Griffiths (2007) proposed
an information-theoretic measure known as the Vari-
ation of Information (VI) described by Meila? (2003)
as an evaluation of an unsupervised tagging. How-
ever as Goldwater (p.c.) points out, this may not be
an ideal evaluation measure; e.g., a tagger which as-
signs all words the same single part-of-speech tag
does disturbingly well under Variation of Informa-
tion, suggesting that a poor tagger may score well
under VI.
In order to avoid this problem we focus here on
evaluation measures that construct an explicit map-
ping between the gold-standard part-of-speech tags
and the HMM’s states. Perhaps the most straight-
forward approach is to map each HMM state to the
part-of-speech tag it co-occurs with most frequently,
and use this mapping to map each HMM state se-
quence t to a sequence of part-of-speech tags. But as
Clark (2003) observes, this approach has several de-
fects. If a system is permitted to posit an unbounded
number of states (which is not the case here) it can
achieve a perfect score on by assigning each word
token its own unique state.
We can partially address this by cross-validation.
We divide the corpus into two equal parts, and from
the first part we extract a mapping from HMM states
to the parts-of-speech they co-occur with most fre-
quently, and use that mapping to map the states of
the second part of the corpus to parts-of-speech. We
call the accuracy of the resulting tagging the cross-
validation accuracy.
Finally, following Haghighi and Klein (2006) and
Johnson (2007) we can instead insist that at most
one HMM state can be mapped to any part-of-speech
tag. Following these authors, we used a greedy algo-
rithm to associate states with POS tags; the accuracy
of the resulting tagging is called the greedy 1-to-1
348
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165
VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599
GSe,p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811
GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032
GSc,p 0.49910? 0.45028 0.42785 0.43652 0.39182 0.39164
GSc,b 0.49486? 0.46193 0.41162 0.42278 0.38497 0.36793
Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators.
The column heading indicates the size of the corpus and the number of HMM states. In the Gibbs sampler (GS) results
the subscript “e” indicates that the parameters ? and ? were explicitly sampled while the subscript “c” indicates that
they were integrated out, and the subscript “p” indicates pointwise sampling, while “b” indicates sentence-blocked
sampling. Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but was
still slowly improving.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.62115 0.64651 0.44135 0.56215 0.28576 0.46669
VB 0.60484 0.63652 0.48427 0.36458 0.35946 0.36926
GSe,p 0.64190 0.63057 0.53571 0.46986 0.41620 0.37165
GSe,b 0.65953 0.65606 0.57918 0.48975 0.47228 0.37311
GSc,p 0.61391? 0.67414 0.65285 0.65012 0.58153 0.62254
GSc,b 0.60551? 0.65516 0.62167 0.58271 0.55006 0.58728
Figure 3: Average cross-validation accuracy of state sequences produced by HMMs estimated by the various estima-
tors. The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 4.47555 3.86326 6.16499 4.55681 7.72465 5.42815
VB 4.27911 3.44029 5.00509 3.19670 4.80778 3.14557
GSe,p 4.24919 3.53024 4.30457 3.23082 4.24368 3.17076
GSe,b 4.04123 3.46179 4.22590 3.20276 4.29474 3.10609
GSc,p 4.03886? 3.52185 4.21259 3.17586 4.30928 3.18273
GSc,b 4.11272? 3.61516 4.36595 3.23630 4.32096 3.17780
Figure 4: Average Variation of Information between the state sequences produced by HMMs estimated by the various
estimators and the gold tags (smaller is better). The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 558 346 648 351 142 125
VB 473 123 337 24 183 20
GSe,p 2863 382 3709 63 2500 177
GSe,b 3846 286 5169 154 4856 139
GSc,p ? 34325 44864 40088 45285 43208
GSc,b ? 6948 7502 7782 7342 7985
Figure 5: Average number of iterations until the negative logarithm of the posterior probability (or likelihood) changes
by less than 0.5% (smaller is better) per at least 2,000 iterations. No annealing was used.
349
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
–
lo
g
po
st
er
io
r
pr
o
ba
bi
lit
y
50000400003000020000100000
8.1e+06
8.05e+06
8e+06
7.95e+06
7.9e+06
7.85e+06
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
G
re
ed
y
1-
to
-
1
ac
cu
ra
cy
50000400003000020000100000
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42
0.4
Figure 6: Variation in (a) negative log likelihood and (b) 1-to-1 accuracy as a function of running time on a 3GHz
dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states. Each iteration took
approximately 96 sec. for the collapsed blocked sampler, 7.5 sec. for the collapsed pointwise sampler, 25 sec. for the
explicit blocked sampler and 4.4 sec. for the explicit pointwise sampler.
350
accuracy.
The studies presented by Goldwater and Griffiths
(2007) and Johnson (2007) differed in the number of
states that they used. Goldwater and Griffiths (2007)
evaluated against the reduced tag set of 17 tags de-
veloped by Smith and Eisner (2005), while Johnson
(2007) evaluated against the full Penn Treebank tag
set. We ran all our estimators in both conditions here
(thanks to Noah Smith for supplying us with his tag
set).
Also, the studies differed in the size of the corpora
used. The largest corpus that Goldwater and Grif-
fiths (2007) studied contained 96,000 words, while
Johnson (2007) used all of the 1,173,766 words
in the full Penn WSJ treebank. For that reason
we ran all our estimators on corpora containing
24,000 words and 120,000 words as well as the full
treebank.
We ran each estimator with the eight different
combinations of values for the hyperparameters ?
and ?? listed below, which include the optimal
values for the hyperparameters found by Johnson
(2007), and report results for the best combination
for each estimator below 1.
? ??
1 1
1 0.5
0.5 1
0.5 0.5
0.1 0.1
0.1 0.0001
0.0001 0.1
0.0001 0.0001
Further, we ran each setting of each estimator at
least 10 times (from randomly jittered initial start-
ing points) for at least 1,000 iterations, as Johnson
(2007) showed that some estimators require many it-
erations to converge. The results of our experiments
are summarized in Figures 2–5.
1We found that on some data sets the results are sensitive to
the values of the hyperparameters. So, there is a bit uncertainty
in our comparison results because it is possible that the values
we tried were good for one estimator and bad for others. Un-
fortunately, we do not know any efficient way of searching the
optimal hyperparameters in a much wider and more fine-grained
space. We leave it to future work.
4 Conclusion and future work
As might be expected, our evaluation measures dis-
agree somewhat, but the following broad tendancies
seem clear. On small data sets all of the Bayesian
estimators strongly outperform EM (and, to a lesser
extent, VB) with respect to all of our evaluation
measures, confirming the results reported in Gold-
water and Griffiths (2007). This is perhaps not too
surprising, as the Bayesian prior plays a compara-
tively stronger role with a smaller training corpus
(which makes the likelihood term smaller) and the
approximation used by Variational Bayes is likely to
be less accurate on smaller data sets.
But on larger data sets, which Goldwater et al did
not study, the results are much less clear, and depend
on which evaluation measure is used. Expectation
Maximization does surprisingly well on larger data
sets and is competitive with the Bayesian estimators
at least in terms of cross-validation accuracy, con-
firming the results reported by Johnson (2007).
Variational Bayes converges faster than all of the
other estimators we examined here. We found that
the speed of convergence of our samplers depends
to a large degree upon the values of the hyperparam-
eters ? and ??, with larger values leading to much
faster convergence. This is not surprising, as the ?
and ?? specify how likely the samplers are to con-
sider novel tags, and therefore directly influence the
sampler’s mobility. However, in our experiments the
best results are obtained in most settings with small
values for ? and ??, usually between 0.1 and 0.0001.
In terms of time to convergence, on larger data
sets we found that the blocked samplers were gen-
erally faster than the pointwise samplers, and that
the explicit samplers (which represented and sam-
pled ? and ?) were faster than the collapsed sam-
plers, largely because the time saved in not com-
puting probabilities on the fly overwhelmed the time
spent resampling the parameters.
Of course these experiments only scratch the sur-
face of what is possible. Figure 6 shows that
pointwise-samplers initially converge faster, but are
overtaken later by the blocked samplers. Inspired
by this, one can devise hybrid strategies that inter-
leave blocked and pointwise sampling; these might
perform better than both the blocked and pointwise
samplers described here.
351
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience unit, University College
London.
Julian Besag. 2004. An introduction to Markov Chain
Monte Carlo methods. In Mark Johnson, Sanjeev P.
Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-
itors, Mathematical Foundations of Speech and Lan-
guage Processing, pages 247–270. Springer, New
York.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
59–66. Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721–741.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744–751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320–327, New York
City, USA, June. Association for Computational Lin-
guistics.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139–146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296–305.
David J.C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labora-
tory, Cambridge.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Chris Manning and Hinrich Schu¨tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho¨lkopf and Man-
fred K. Warmuth, editors, COLT 2003: The Sixteenth
Annual Conference on Learning Theory, volume 2777
of Lecture Notes in Computer Science, pages 173–187.
Springer.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 354–362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
352
