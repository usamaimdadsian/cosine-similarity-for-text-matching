Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Simple Effective Decipherment via Combinatorial Optimization
Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, klein}@cs.berkeley.edu
Abstract
We present a simple objective function that
when optimized yields accurate solutions to
both decipherment and cognate pair identifica-
tion problems. The objective simultaneously
scores a matching between two alphabets and
a matching between two lexicons, each in a
different language. We introduce a simple
coordinate descent procedure that efficiently
finds effective solutions to the resulting com-
binatorial optimization problem. Our system
requires only a list of words in both languages
as input, yet it competes with and surpasses
several state-of-the-art systems that are both
substantially more complex and make use of
more information.
1 Introduction
Decipherment induces a correspondence between
the words in an unknown language and the words
in a known language. We focus on the setting where
a close correspondence between the alphabets of the
two languages exists, but is unknown. Given only
two lists of words, the lexicons of both languages,
we attempt to induce the correspondence between
alphabets and identify the cognates pairs present in
the lexicons. The system we propose accomplishes
this by defining a simple combinatorial optimiza-
tion problem that is a function of both the alphabet
and cognate matchings, and then induces correspon-
dences by optimizing the objective using a block co-
ordinate descent procedure.
There is a range of past work that has var-
iously investigated cognate detection (Kondrak,
2001; Bouchard-Coˆte´ et al., 2007; Bouchard-Coˆte´
et al., 2009; Hall and Klein, 2010), character-level
decipherment (Knight and Yamada, 1999; Knight
et al., 2006; Snyder et al., 2010; Ravi and Knight,
2011), and bilingual lexicon induction (Koehn and
Knight, 2002; Haghighi et al., 2008). We consider
a common element, which is a model wherein there
are character-level correspondences and word-level
correspondences, with the word matching parame-
terized by the character one. This approach sub-
sumes a range of past tasks, though of course past
work has specialized in interesting ways.
Past work has emphasized the modeling as-
pect, where here we use a parametrically simplistic
model, but instead emphasize inference.
2 Decipherment as Two-Level
Optimization
Our method represents two matchings, one at the al-
phabet level and one at the lexicon level. A vector of
variables x specifies a matching between alphabets.
For each character i in the source alphabet and each
character j in the target alphabet we define an indi-
cator variable xij that is on if and only if character i
is mapped to character j. Similarly, a vector y rep-
resents a matching between lexicons. For word u in
the source lexicon and word v in the target lexicon,
the indicator variable yuv denotes that u maps to v.
Note that the matchings need not be one-to-one.
We define an objective function on the matching
variables as follows. Let EDITDIST(u, v;x) denote
the edit distance between source word u and target
word v given alphabet matching x. Let the length
of word u be lu and the length of word w be lw.
This edit distance depends on x in the following
way. Insertions and deletions always cost a constant
.1 Substitutions also cost  unless the characters
are matched in x, in which case the substitution is
1In practice we set  = 1lu+lv . lu + lv is the maximumnumber of edit operations between words u and v. This nor-
malization insures that edit distances are between 0 and 1 for
all pairs of words.
313
free. Now, the objective that we will minimize can
be stated simply: ?u
?
v yuv · EDITDIST(u, v;x),
the sum of the edit distances between the matched
words, where the edit distance function is parame-
terized by the alphabet matching.
Without restrictions on the matchings x and y
this objective can always be driven to zero by either
mapping all characters to all characters, or matching
none of the words. It is thus necessary to restrict
the matchings in some way. Let I be the size of
the source alphabet and J be the size of the target
alphabet. We allow the alphabet matching x to
be many-to-many but require that each character
participate in no more than two mappings and that
the total number of mappings be max(I, J), a
constraint we refer to as restricted-many-to-many.
The requirements can be encoded with the following
linear constraints on x:
?i
?
j
xij ? 2
?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
The lexicon matching y is required to be ? -one-to-
one. By this we mean that y is an at-most-one-to-one
matching that covers proportion ? of the smaller of
the two lexicons. Let U be the size of the source
lexicon and V be this size of the target lexicon.
This requirement can be encoded with the following
linear constraints:
?u
?
v
yuv ? 1
?v
?
u
yuv ? 1
?
u
?
v
yuv = ? min(U, V )
Now we are ready to define the full optimization
problem. The first formulation is called the Implicit
Matching Objective since includes an implicit
minimization over edit alignments inside the com-
putation of EDITDIST.
(1) Implicit Matching Objective:
min
x,y
?
u
?
v
yuv · EDITDIST(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
In order to get a better handle on the shape of the
objective and to develop an efficient optimization
procedure we decompose each edit distance compu-
tation and re-formulate the optimization problem in
Section 2.2.
2.1 Example
Figure 1 presents both an example matching prob-
lem and a diagram of the variables and objective.
Here, the source lexicon consists of the English
words (cat, bat, cart, rat, cab), and
the source alphabet consists of the characters (a,
b, c, r, t). The target alphabet is (0, 1,
2, 3). We have used digits as symbols in the target
alphabet to make it clear that we treat the alphabets
as disjoint. We have no prior knowledge about any
correspondence between alphabets, or between lexi-
cons.
The target lexicon consists of the words (23,
1233, 120, 323, 023). The bipartite graphs
show a specific setting of the matching variables.
The bold edges correspond to the xij and yuv that
are one. The matchings shown achieve an edit dis-
tance of zero between all matched word pairs ex-
cept for the pair (cat, 23). The best edit align-
ment for this pair is also diagrammed. Here, ‘a’
is aligned to ‘2’, ‘t’ is aligned to ‘3’, and ‘c’ is
deleted and therefore aligned to the null position ‘#’.
Only the initial deletion has a non-zero cost since
all other alignments correspond to substitutions be-
tween characters that are matched in x.
2.2 Explicit Objective
Computing EDITDIST(u, v;x) requires running a
dynamic program because of the unknown edit
alignments; here we define those alignments z ex-
plicitly, which makes the EDITDIST(u, v;x) easy to
write explicitly at the cost of more variables. How-
ever, by writing the objective in an explicit form that
refers to these edit variables, we are able to describe
a efficient block coordinate descent procedure that
can be used for optimization.
EDITDIST(u, v;x) is computed by minimizing
over the set of monotonic alignments between the
characters of the source word u and the characters
of the target word v. Let un be the character at the
nth position of the source word u, and similarly for
314
a
b
c
r
t
0
1
2
3
Alphabet Matching
Lexicon Matching
xij
cat
bat
rat
cart
cab
1233
120
323
023
23
yuv
Edit Distance
c
a
t
2
3
# # EditDist(u, v;x) =
min
x,y
?
u
?
v
yuv · EditDist(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
Matching Problem
Insertion
? ?
?
Substitution
Deletion
? ·
??
n
?
m
(1? xunvm)zuv,nm
+
?
n
zuv,n# +
?
m
zuv,#m
?s.t.
minzuv
zuv is monotonic
zuv,nm
Figure 1: An example problem displaying source and target lexicons and alphabets, along with specific matchings.
The variables involved in the optimization problem are diagrammed. x are the alphabet matching indicator variables,
y are the lexicon matching indicator variables, and z are the edit alignment indicator variables. The index u refers to
a word in the source lexicon, v refers to word in the target lexicon, i refers to a character in the source alphabet, and
j refers to a character in the target alphabet. n and m refer to positions in source and target words respectively. The
matching objective function is also shown.
vm. Let zuv be the vector of alignment variables
for the edit distance computation between source
word u and target word v, where entry zuv,nm
indicates whether the character at position n of
source word u is aligned to the character at position
m of target word v. Additionally, define variables
zuv,n# and zuv,#m denoting null alignments, which
will be used to keep track of insertions and deletions.
EDITDIST(u, v;x) =
min
zuv
 ·
(
SUB(zuv, x) + DEL(zuv) + INS(zuv)
)
s.t. zuv is monotonic
We define SUB(zuv, x) to be the number of sub-
stitutions between characters that are not matched
in x, DEL(zuv) to be the number of deletions, and
INS(zuv) to be the number of insertions.
SUB(zuv, x) =
?
n,m
(1? xunvm)zuv,nm
DEL(zuv) =
?
n
zuv,n#
INS(zuv) =
?
m
zuv,#m
Notice that the variable zuv,nm being turned on in-
dicates the substitute operation, while a zuv,n# or
zuv,#m being turned on indicates an insert or delete
operation. These variables are digrammed in Fig-
ure 1. The requirement that zuv be a monotonic
alignment can be expressed using linear constraints,
but in our optimization procedure (described in Sec-
tion 3) these constraints need not be explicitly rep-
resented.
Now we can substitute the explicit edit distance
equation into the implicit matching objective (1).
315
Noticing that the mins and sums commute, we arrive
at the explicit form of the matching optimization
problem.
(2) Explicit Matching Objective:
min
x,y,z
[?
u,v
yuv ·  ·
(SUB(zuv, x) + DEL(zuv) + INS(zuv))
]
s.t. x is restricted-many-to-many
y is ? -one-to-one
?uv zuv is monotonic
The implicit and explicit optimizations are the same,
apart from the fact that the explicit optimization now
explicitly represents the edit alignment variables z.
Let the explicit matching objective (2) be denoted
as J(x, y, z). The relaxation of the explicit problem
with 0-1 constraints removed has integer solutions,2
however the objective J(x, y, z) is non-convex. We
thus turn to a block coordinate descent method in the
next section in order to find local optima.
3 Optimization Method
We now state a block coordinate descent procedure
to find local optima of J(x, y, z) under the con-
straints on x, y, and z. This procedure alternates
between updating y and z to their exact joint optima
when x is held fixed, and updating x to its exact op-
timum when y and z are held fixed.
The psuedocode for the procedure is given in Al-
gorithm 1. Note that the function EDITDIST returns
both the min edit distance euv and the argmin edit
alignments zuv. Also note that cij is as defined in
Section 3.2.
3.1 Lexicon Matching Update
Let x, the alphabet matching variable, be fixed. We
consider the problem of optimizing J(x, y, z) over
the lexicon matching variable y and and the edit
alignments z under the constraint that y is ? -one-
to-one and each zuv is monotonic.
2This can be shown by observing that optimizing x when y
and z are held fixed yields integer solutions (shown in Section
3.2), and similarly for the optimization of y and z when x is
fixed (shown in Section 3.1). Thus, every local optimum with
respect to these block coordinate updates has integer solutions.
The global optimum must be one of these local optima.
Algorithm 1 Block Coordinate Descent
Randomly initialize alphabet matching x.
repeat
for all u, v do
(euv, zuv)? EDITDIST(u, v;x)
end for
[Hungarian]
y ? argminy ? -one-to-one
[?
u,v yuveuv
]
[Solve LP]
x? argmaxx restr.-many-to-many
[?
i,j xijcij
]
until convergence
Notice that y simply picks out which edit distance
problems affect the objective. The zuv in each of
these edit distance problems can be optimized in-
dependently. zuv that do not have yuv active have
no effect on the objective, and zuv with yuv active
can be optimized using the standard edit distance dy-
namic program. Thus, in a first step we compute the
U · V edit distances euv and best monotonic align-
ment variables zuv between all pairs of source and
target words usingU ·V calls to the standard edit dis-
tance dynamic program. Altogether, this takes time
O
(
(
?
u lu) · (
?
v lv)
).
Now, in a second step we compute the least
weighted ? -one-to-one matching y under the
weights euv. This can be accomplished in time
O(max(U, V )3) using the Hungarian algorithm
(Kuhn, 1955). These two steps produce y and z that
exactly achieve the optimum value of J(x, y, z) for
the given value of x.
3.2 Alphabet Matching Update
Let y and z, the lexicon matching variables and the
edit alignments, be fixed. Now, we find the optimal
alphabet matching variables x subject to the con-
straint that x is restricted-many-to-many.
It makes sense that to optimize J(x, y, z) with re-
spect to x we should prioritize mappings xij that
would mitigate the largest substitution costs in the
active edit distance problems. Indeed, with a little
algebra it can be shown that solving a maximum
weighted matching problem with weights cij that
count potential substitution costs gives the correct
update for x. In particular, cij is the total cost of
substitution edits in the active edit alignment prob-
316
lems that would result if source character i were not
mapped to target character j in the alphabet match-
ing x. This can be written as:
cij =
?
u,v
?
n,m s.t. un=i,vm=j
 · yuv · zuv,nm
If x were constrained to be one-to-one, we
could again apply the Hungarian algorithm, this
time to find a maximum weighted matching under
the weights cij . Since we have instead allowed
restricted-many-to-many alphabet matchings we
turn to linear programming for optimizing x. We
can state the update problem as the following linear
program (LP), which is guaranteed to have integer
solutions:
min
x
?
ij
xijcij
s.t. ?i
?
j
xij ? 2, ?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
In experiments we used the GNU Linear Program-
ming Toolkit (GLPK) to solve the LP and update
the alphabet matching x. This update yields match-
ing variables x that achieve the optimum value of
J(x, y, z) for fixed y and z.
3.3 Random Restarts
In practice we found that the block coordinate de-
scent procedure can get stuck at poor local optima.
To find better optima, we run the coordinate descent
procedure multiple times, initialized each time with
a random alphabet matching. We choose the local
optimum with the best objective value across all ini-
tializations. This approach yielded substantial im-
provements in achieved objective value.
4 Experiments
We compare our system to three different state-of-
the-art systems on three different data sets. We set
up experiments that allow for as direct a comparison
as possible. In some cases it must be pointed out
that the past system’s goals are different from our
own, and we will be comparing in a different way
than the respective work was intended. The three
systems make use of additional, or slightly different,
sources of information.
4.1 Phonetic Cognate Lexicons
The first data set we evaluate on consists of 583
triples of phonetic transcriptions of cognates in
Spanish, Portuguese, and Italian. The data set was
introduced by Bouchard-Coˆte´ et al. (2007). For a
given pair of languages the task is to determine the
mapping between lexicons that correctly maps each
source word to its cognate in the target lexicon. We
refer to this task and data set as ROMANCE.
Hall and Klein (2010) presented a state-of-the-
art system for the task of cognate identification and
evaluated on this data set. Their model explicitly
represents parameters for phonetic change between
languages and their parents in a phylogenetic tree.
They estimate parameters and infer the pairs of cog-
nates present in all three languages jointly, while we
consider each pair of languages in turn.
Their model has similarities with our own in that
it learns correspondences between the alphabets of
pairs of languages. However, their correspondences
are probabilistic and implicit while ours are hard and
explicit. Their model also differs from our own in
a key way. Notice that the phonetic alphabets for
the three languages are actually the same. Since
phonetic change occurs gradually across languages
a helpful prior on the correspondence is to favor the
identity. Their model makes use of such a prior.
Our model, on the other hand, is unaware of any
prior correspondence between alphabets and does
not make use of this additional information about
phonetic change.
Hall and Klein (2010) also evaluate their model
on lexicons that do not have a perfect cognate map-
ping. This scenario, where not every word in one
language has a cognate in another, is more realistic.
They produced a data set with this property by prun-
ing words from the ROMANCE data set until only
about 75% of the words in each source lexicon have
cognates in each target lexicon. We refer to this task
and data set as PARTIALROMANCE.
4.2 Lexicons Extracted from Corpora
Next, we evaluate our model on a noisier data set.
Here the lexicons in source and target languages
are extracted from corpora by taking the top 2,000
words in each corpus. In particular, we used the En-
glish and Spanish sides of the Europarl parallel cor-
317
pus (Koehn, 2005). To make this set up more real-
istic (though fairly comparable), we insured that the
corpora were non-parallel by using the first 50K sen-
tences on the English side and the second 50K sen-
tences on the Spanish side. To generate a gold cog-
nate matching we used the intersected HMM align-
ment model of Liang et al. (2008) to align the full
parallel corpus. From this alignment we extracted a
translation lexicon by adding an entry for each word
pair with the property that the English word was
aligned to the Spanish in over 10% of the alignments
involving the English word. To reduce this transla-
tion lexicon down to a cognate matching we went
through the translation lexicon by hand and removed
any pair of words that we judged to not be cognates.
The resulting gold matching contains cognate map-
pings in the English lexicon for 1,026 of the words
in the Spanish lexicon. This means that only about
50% of the words in English lexicon have cognates
in the Spanish lexicon. We evaluate on this data set
by computing precision and recall for the number of
English words that are mapped to a correct cognate.
We refer to this task and data set as EUROPARL.
On this data set, we compare against the state-of-
the-art orthographic system presented in Haghighi
et al. (2008). Haghighi et al. (2008) presents sev-
eral systems that are designed to extract transla-
tion lexicons for non-parallel corpora by learning
a correspondence between their monolingual lexi-
cons. Since our system specializes in matching cog-
nates and does not take into account additional infor-
mation from corpus statistics, we compare against
the version of their system that only takes into ac-
count orthographic features and is thus is best suited
for cognate detection. Their system requires a small
seed of correct cognate pairs. From this seed the sys-
tem learns a projection using canonical correlation
analysis (CCA) into a canonical feature space that
allows feature vectors from source words and target
words to be compared. Once in this canonical space,
similarity metrics can be computed and words can be
matched using a bipartite matching algorithm. The
process is iterative, adding cognate pairs to the seed
lexicon gradually and each time re-computing a re-
fined projection. Our system makes no use of a seed
lexicon whatsoever.
Both our system and the system of Haghighi et
al. (2008) must solve bipartite matching problems
between the two lexicons. For this data set, the lexi-
cons are large enough that finding the exact solution
can be slow. Thus, in all experiments on this data
set, we instead use a greedy competitive linking al-
gorithm that runs in time O(U2V 2log(UV )).
Again, for this dataset it is reasonable to expect
that many characters will map to themselves in the
best alphabet matching. The alphabets are not iden-
tical, but are far from disjoint. Neither our system,
nor that of Haghighi et al. (2008) make use of this
expectation. As far as both systems are concerned,
the alphabets are disjoint.
4.3 Decipherment
Finally, we evaluate our model on a data set where
a main goal is to decipher an unknown correspon-
dence between alphabets. We attempt to learn a
mapping from the alphabet of the ancient Semitic
language Ugaritic to the alphabet of Hebrew, and
at the same time learn a matching between Hebrew
words in a Hebrew lexicon and their cognates in a
Ugaritic lexicon. This task is related to the task at-
tempted by Snyder et al. (2010). The data set con-
sists of a Ugaritic lexicon of 2,214 words, each of
which has a Hebrew cognate, the lexicon of their
2,214 Hebrew cognates, and a gold cognate dictio-
nary for evaluation. We refer to this task and data set
as UGARITIC.
The non-parameteric Bayesian system of Snyder
et al. (2010) assumes that the morphology of He-
brew is known, making use of an inventory of suf-
fixes, prefixes, and stems derived from the words
in the Hebrew bible. It attempts to learn a corre-
spondence between the morphology of Ugaritic and
that of Hebrew while reconstructing cognates for
Ugaritic words. This is a slightly different goal than
that of our system, which learns a correspondence
between lexicons. Snyder et al. (2010) run their
system on a set 7,386 Ugaritic words, the same set
that we extracted our 2,214 Ugaritic words with He-
brew cognates from. We evaluate the accuracy of the
lexicon matching produced by our system on these
2,214 Ugaritic words, and so do they, measuring the
number of correctly reconstructed cognates.
By restricting the source and target lexicons to
sets of cognates we have made the task easier. This
was necessary, however, because the Ugaritic and
Hebrew corpora used by Snyder et al. (2010) are not
318
Model ? Accuracy
Hall and Klein (2010) – 90.3
MATCHER 1.0 90.1
Table 1: Results on ROMANCE data set. Our system is
labeled MATCHER. We compare against the phylogenetic
cognate detection system of Hall and Klein (2010). We
show the pairwise cognate accuracy across all pairs of
languages from the following set: Spanish, Portuguese,
and Italian.
comparable: only a small proportion of the words
in the Ugaritic lexicon have cognates in the lexicon
composed of the most frequent Hebrew words.
Here, the alphabets really are disjoint. The sym-
bols in both languages look nothing alike. There is
no obvious prior expectation about how the alpha-
bets will be matched. We evaluate against a well-
established correspondence between the alphabets
of Ugaritic and Hebrew. The Ugaritic alphabet con-
tains 30 characters, the Hebrew alphabet contains 22
characters, and the gold matching contains 33 en-
tries. We evaluate the learned alphabet matching by
counting the number of recovered entries from the
gold matching.
Due to the size of the source and target lexicons,
we again use the greedy competitive linking algo-
rithm in place of the exact Hungarian algorithm in
experiments on this data set.
5 Results
We present results on all four datasets ROMANCE,
PARTIALROMANCE, EUROPARL, and UGARITIC.
On the ROMANCE and PARTIALROMANCE data sets
we compare against the numbers published by Hall
and Klein (2010). We ran an implementation of
the orthographic system presented by Haghighi et
al. (2008) on our EUROPARL data set. We com-
pare against the numbers published by Snyder et al.
(2010) on the UGARITIC data set. We refer to our
system as MATCHER in result tables and discussion.
5.1 ROMANCE
The results of running our system, MATCHER, on
the ROMANCE data set are shown in Table 1. We
recover 88.9% of the correct cognate mappings on
the pair Spanish and Italian, 85.7% on Italian and
Portuguese, and 95.6% on Spanish and Portuguese.
Model ? Precision Recall F1
Hall and Klein (2010) – 66.9 82.0 73.6
MATCHER 0.25 99.7 34.0 50.7
0.50 93.8 60.2 73.3
0.75 81.1 78.0 79.5
Table 2: Results on PARTIALROMANCE data set. Our
system is labeled MATCHER. We compare against the
phylogenetic cognate detection system of Hall and Klein
(2010). We show the pairwise cognate precision, recall,
and F1 across all pairs of languages from the following
set: Spanish, Portuguese, and Italian. Note that approx-
imately 75% of the source words in each of the source
lexicons have cognates in each of the target lexicons.
Our average accuracy across all pairs of languages
is 90.1%. The phylogenetic system of Hall and
Klein (2010) achieves an average accuracy of 90.3%
across all pairs of languages. Our system achieves
accuracy comparable to that of the phylogenetic sys-
tem, despite the fact that the phylogenetic system is
substantially more complex and makes use of an in-
formed prior on alphabet correspondences.
The alphabet matching learned by our system is
interesting to analyze. For the pairing of Span-
ish and Portuguese it recovers phonetic correspon-
dences that are well known. Our system learns the
correct cognate pairing of Spanish /bino/ to Por-
tuguese /vinu/. This pair exemplifies two com-
mon phonetic correspondences for Spanish and Por-
tuguese: the Spanish /o/ often transforms to a /u/ in
Portuguese, and Spanish /b/ often transforms to /v/
in Portuguese. Our system, which allows many-to-
many alphabet correspondences, correctly identifies
the mappings /o/? /u/ and /b/? /v/ as well as the
identity mappings /o/? /o/ and /b/? /b/ which are
also common.
5.2 PARTIALROMANCE
In Table 2 we present the results of running our sys-
tem on the PARTIALROMANCE data set. In this data
set, only approximately 75% of the source words in
each of the source lexicons have cognates in each of
the target lexicons. The parameter ? trades off pre-
cision and recall. We show results for three different
settings of ? : 0.25, 0.5, and 0.75.
Our system achieves an average precision across
language pairs of 99.7% at an average recall of
34.0%. For the pairs Italian – Portuguese, and Span-
319
Model Seed ? Precision Recall F1
Haghighi et al. (2008) 20 0.1 72.0 14.0 23.5
20 0.25 63.6 31.0 41.7
20 0.5 44.8 43.7 44.2
50 0.1 90.5 17.6 29.5
50 0.25 75.4 36.7 49.4
50 0.5 56.4 55.0 55.7
MATCHER 0 0.1 93.5 18.2 30.5
0 0.25 83.2 40.5 54.5
0 0.5 56.5 55.1 55.8
Table 3: Results on EUROPARL data set. Our system
is labeled MATCHER. We compare against the bilingual
lexicon induction system of Haghighi et al. (2008). We
show the cognate precision, recall, and F1 for the pair of
languages English and Spanish using lexicons extracted
from corpora. Note that approximately 50% of the words
in the English lexicon have cognates in the Spanish lexi-
con.
ish – Portuguese, our system achieves prefect preci-
sion at recalls of 32.2% and 38.1% respectively. The
best average F1 achieved by our system is 79.5%,
which surpasses the average F1 of 73.6 achieved by
the phylogenetic system of Hall and Klein (2010).
The phylogenetic system observes the phyloge-
netic tree of ancestry for the three languages and
explicitly models cognate evolution and survival in
a ‘survival’ tree. One might expect the phyloge-
netic system to achieve better results on this data set
where part of the task is identifying which words do
not have cognates. It is surprising that our model
does so well given its simplicity.
5.3 EUROPARL
Table 3 presents results for our system on the EU-
ROPARL data set across three different settings of ? :
0.1, 0.25, and 0.5. We compare against the ortho-
graphic system presented by Haghighi et al. (2008),
across the same three settings of ? , and with two dif-
ferent sizes of seed lexicon: 20 and 50. In this data
set, only approximately 50% of the source words
have cognates in the target lexicon.
Our system achieves a precision of 93.5% at a re-
call of 18.2%, and a best F1 of 55.0%. Using a seed
matching of 50 word pairs, the orthographic sys-
tem of Haghighi et al. (2008) achieves a best F1 of
55.7%. Using a seed matching of 20 word pairs,
it achieves a best F1 of 44.2%. Our system out-
performs the orthographic system even though the
orthographic system makes use of important addi-
Model ? Lexicon Acc. Alphabet Acc.
Snyder et al. (2010) – 60.4* 29/33*
MATCHER 1.0 90.4 28/33
Table 4: Results on UGARITIC data set. Our system is la-
beled MATCHER. We compare against the decipherment
system of Snyder et al. (2010). *Note that results for this
system are on a somewhat different task. In particular, the
MATCHER system assumes the inventories of cognates in
both Hebrew and Ugaritic are known, while the system
of Snyder et al. (2010) reconstructs cognates assuming
only that the morphology of Hebrew is known, which is a
harder task. We show cognate pair identification accuracy
and alphabet matching accuracy for Ugaritic and Hebrew.
tional information: a seed matching of correct cog-
nate pairs. The results show that as the size of
this seed is decreased, the performance of the ortho-
graphic system degrades.
5.4 UGARITIC
In Table 4 we present results on the UGARITIC data
set. We evaluate both accuracy of the lexicon match-
ing learned by our system, and the accuracy of the
alphabet matching. Our system achieves a lexicon
accuracy of 90.4% while correctly identifying 28 out
the 33 gold character mappings.
We also present the results for the decipherment
model of Snyder et al. (2010) in Table 4. Note that
while the evaluation data sets for our two models
are the same, the tasks are very different. In par-
ticular, our system assumes the inventories of cog-
nates in both Hebrew and Ugaritic are known, while
the system of Snyder et al. (2010) reconstructs cog-
nates assuming only that the morphology of Hebrew
is known, which is a harder task. Even so, the re-
sults show that our system is effective at decipher-
ment when semantically similar lexicons are avail-
able.
6 Conclusion
We have presented a simple combinatorial model
that simultaneously incorporates both a matching
between alphabets and a matching between lexicons.
Our system is effective at both the tasks of cognate
identification and alphabet decipherment, requiring
only lists of words in both languages as input.
320
References
A. Bouchard-Coˆte´, P. Liang, T.L. Griffiths, and D. Klein.
2007. A probabilistic approach to diachronic phonol-
ogy. In Proc. of EMNLP.
A. Bouchard-Coˆte´, T.L. Griffiths, and D. Klein.
2009. Improved reconstruction of protolanguage word
forms. In Proc. of NAACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL.
D. Hall and D. Klein. 2010. Finding cognate groups
using phylogenies. In Proc. of ACL.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of
ACL Workshop on Unsupervised Learning in Natural
Language Processing.
K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems. In
Proc. of COLING/ACL.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proc. of ACL
workshop on Unsupervised lexical acquisition.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
G. Kondrak. 2001. Identifying Cognates by Phonetic and
Semantic Similarity. In NAACL.
H.W. Kuhn. 1955. The Hungarian method for the assign-
ment problem. Naval research logistics quarterly.
P. Liang, D. Klein, and M.I. Jordan. 2008. Agreement-
based learning. Proc. of NIPS.
S. Ravi and K. Knight. 2011. Bayesian inference for Zo-
diac and other homophonic ciphers. In Proc. of ACL.
B. Snyder, R. Barzilay, and K. Knight. 2010. A statisti-
cal model for lost language decipherment. In Proc. of
ACL.
321
