Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 136–144,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Ranking Reader Emotions Using Pairwise Loss Minimization and  
Emotional Distribution Regression 
 
 
Kevin Hsin-Yih Lin and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 
{f93141, hhchen}@csie.ntu.edu.tw 
 
 
 
 
 
 
Abstract 
This paper presents two approaches to ranking 
reader emotions of documents. Past studies 
assign a document to a single emotion cate-
gory, so their methods cannot be applied di-
rectly to the emotion ranking problem. 
Furthermore, whereas previous research ana-
lyzes emotions from the writer’s perspective, 
this work examines readers’ emotional states. 
The first approach proposed in this paper 
minimizes pairwise ranking errors. In the sec-
ond approach, regression is used to model 
emotional distributions. Experiment results 
show that the regression method is more ef-
fective at identifying the most popular emo-
tion, but the pairwise loss minimization 
method produces ranked lists of emotions that 
have better correlations with the correct lists. 
1 Introduction 
Emotion analysis is an increasingly popular re-
search topic due to the emergence of large-scale 
emotion data on the web. Previous work primarily 
studies emotional contents of texts from the 
writer's perspective, where it is typically assumed 
that a writer expresses only a single emotion in a 
document. Unfortunately, this premise does not 
hold when analyzing a document from the reader's 
perspective, because readers rarely agree unani-
mously on the emotion that a document instills. 
Figure 1 illustrates this phenomenon. In the figure,  
 
0%
10%
20%
30%
40%
H
ea
rtw
ar
m
in
g
H
ap
py
S
ad
S
ur
pr
is
in
g
A
ng
ry
B
or
in
g
A
w
es
om
e
U
se
fu
l
Emotion
%
 o
f R
ea
de
rs
 
Figure 1. Emotional responses of 626 people after read-
ing a Yahoo! News article about an Iranian refugee 
mother and her two children who finally reunited with 
their family in the March of 2007 after been stranded in 
a Moscow airport for 10 months due to false passports. 
 
readers’ responses are distributed among different 
emotion categories. In fact, none of the emotions in 
Figure 1 has a majority (i.e., more than 50%) of the 
votes. Intuitively, it is better to provide a ranking 
of emotions according to their popularity rather 
than associating a single reader emotion with a 
document. As a result, current writer-emotion 
analysis techniques for classifying a document into 
a single emotion category are not suitable for ana-
lyzing reader emotions. New methods capable of 
ranking emotions are required.  
Reader-emotion analysis has potential applica-
tions that differ from those of writer-emotion 
analysis. For example, by integrating emotion 
ranking into information retrieval, users will be 
able to retrieve documents that contain relevant 
contents and at the same time produce desired feel-
ings. In addition, reader-emotion analysis can as-
sist writers in foreseeing how their work will 
influence readers emotionally.  
136
In this paper, we present two approaches to 
ranking reader emotions. The first approach is in-
spired by the success of the pairwise loss minimi-
zation framework used in information retrieval to 
rank documents. Along a similar line, we devise a 
novel scheme to minimize the number of incor-
rectly-ordered emotion pairs in a document. In the 
second approach, regression is used to model 
reader-emotion distributions directly. Experiment 
results show that the regression method is more 
effective at identifying the most popular emotion, 
but the pairwise loss minimization method pro-
duces ordered lists of emotions that have better 
correlations with the correct lists. 
The rest of this paper is organized as follows. 
Section 2 describes related work. In Section 3, de-
tails about the two proposed approaches are pro-
vided. Section 4 introduces the corpus and Section 
5 presents how features are extracted from the cor-
pus. Section 6 shows the experiment procedures 
and results. Section 7 concludes the paper. 
2 Related Work 
Only a few studies in the past deal with the reader 
aspect of emotion analysis. For example, Lin et al. 
(2007; 2008) classify documents into reader-
emotion categories. Most previous work focuses 
on the writer’s perspective. Pang et al. (2002) de-
sign an algorithm to determine whether a docu-
ment’s author expresses a positive or negative 
sentiment. They discover that using Support Vec-
tor Machines (SVM) with word unigram features 
results in the best performance. Since then, more 
work has been done to find features better than 
unigrams. In (Hu et al., 2005), word sentiment in-
formation is exploited to achieve better classifica-
tion accuracy. 
Experiments have been done to extract emo-
tional information from texts at granularities finer 
than documents. Wiebe (2000) investigates the 
subjectivity of words, whereas Aman and Szpako-
wicz (2007) manually label phrases with emotional 
categories. In 2007, the SemEval-2007 workshop 
organized a task on the unsupervised annotation of 
news headlines with emotions (Strapparava and 
Mihalcea, 2007). 
As for the task of ranking, many machine-
learning algorithms have been proposed in infor-
mation retrieval. These techniques generate rank-
ing functions which predict the relevance of a 
document. One class of algorithms minimizes the 
errors resulting from ordering document pairs in-
correctly. Examples include (Joachims, 2002), 
(Freund et al., 2003) and (Qin et al., 2007). In par-
ticular, the training phase of the Joachims’ Rank-
ing SVM (Joachims, 2002) is formulated as the 
following SVM optimization problem: 
 
min ?+ kjiCkji ,,T21,, ?? www,  
subject to: 
 
0  : 
1)),(),((  
:|),(),,(
,,
,,
T
,,
????
?????
>??
kji
kjijkik
jkikjkik
kji
dqdq
ssVdqdq
?
?w     (1) 
 
where V is the training corpus, ?(qk, di) is the fea-
ture vector of document di with respect to query qk, 
sk,i is the relevance score of di with respect to qk, w 
is a weight vector, C is the SVM cost parameter, 
and ?i,j,k are slack variables. The set of constraints 
at (1) means that document pairwise orders should 
be preserved. 
Unfortunately, the above scheme for exploiting 
pairwise order information cannot be applied di-
rectly to the emotion ranking task, because the task 
requires us to rank emotions within a document 
rather than provide a ranking of documents. In par-
ticular, the definitions of ?(qk,di), ?(qk,dj), sk,i and 
sk,j do not apply to emotion ranking. In the next 
section, we will show how the pairwise loss mini-
mization concept is adapted for emotion ranking. 
3 Ranking Reader Emotions 
In this section, we provide the formal description 
of the reader-emotion ranking problem. Then we 
describe the pairwise loss minimization (PLM) 
approach and the emotional distribution regression 
(EDR) approach to ranking emotions. 
3.1 Problem Specification 
The reader emotion ranking problem is defined as 
follows. Let D = {d1, d2, …, dN} be the document 
space, and E = {e1, e2, …, eM} be the emotion 
space. Let fi : E ? ? be the emotional probability 
function of di?D. That is, fi(ej) outputs the fraction 
of readers who experience emotion ej after reading 
document di. Our goal is to find a function r : D ? 
EM such that r(di) = (e?(1), e?(2), …, e?(M)) where ? is 
137
Input: Set of emotion ordered pairs P 
1.  G ? a graph with emotions as vertices and no edge 
2.  while (P ? ?) 
3.    remove (ej,ek) with the highest confidence from P 
4.    if adding edge (ej,ek) to G produces a loop 
5.      then add (ek,ej) to G 
6.    else add (ej,ek) to G 
7.  return topological sort of G 
a permutation on {1, 2, …, M}, and fi(e?(1)) ? fi(e?(2)) 
? … ? fi(e?(M)). 
3.2 Pairwise Loss Minimization 
As explained in Section 2, the information retrieval 
framework for exploiting pairwise order informa-
tion cannot be applied directly to the emotion rank-
ing problem. Hence, we introduce a novel 
formulation of the emotion ranking problem into 
an SVM optimization problem with constraints 
based on pairwise loss minimization. 
Algorithm 1. Merge Pairwise Orders. 
 
We now describe how we rank the emotions of a 
previously unseen document using the M(M – 1)/2 
pairwise ranking functions gjk created during the 
training phase. First, all of the pairwise ranking 
functions are applied to the unseen document, 
which generates the relative orders of every pair of 
emotions. These pairwise orders need to be com-
bined together to produce a ranked list of all the 
emotions. Algorithm 1 does exactly this.  
Whereas Ranking SVM generates only a single 
ranking function, our method creates a pairwise 
ranking function gjk : D ? ? for each pair of emo-
tions ej and ek, aiming at satisfying the maximum 
number of the inequalities: 
 In Algorithm 1, the confidence of an emotion 
ordered pair at Line 3 is the probability value re-
turned by a LIBSVM classifier for predicting the 
order. LIBSVM’s method for generating this prob-
ability is described in (Wu et al., 2003). Lines 4 
and 5 resolve the problem of conflicting emotion 
ordered pairs forming a loop in the ordering of 
emotions. The ordered list of emotions returned by 
Algorithm 1 at Line 7 is the final output of the 
PLM method. 
?di?D | fi(ej) > fi(ek) : gjk(di) > 0 
?di?D | fi(ej) < fi(ek) : gjk(di) < 0 
 
In other words, we want to minimize the number of 
incorrectly-ordered emotion pairs. We further re-
quire gjk(di) to have the linear form wT?(di) + b, 
where w is a weight vector, b is a constant, and 
?(di) is the feature vector of di. Details of feature 
extraction will be presented in Section 5. 
As Joachims (2002) points out, the above type 
of problem is NP-Hard. However, an approximate 
solution to finding gik can be obtained by solving 
the following SVM optimization problem: 
3.3 Emotional Distribution Regression 
In the second approach to ranking emotions, we 
use regression to model fi directly. A regression 
function hj : D ? ? is generated for each ej?E by 
learning from the examples (?(di), fi(ej)) for all 
documents di in the training corpus. 
 
min ?+ ib Ci ??, www T21,  
subject to: 
0  : 
1))((:)()(|
1)(:)()(|
T
T
??
??+??<??
??+?>??
i
iikijii
iikijii
i
bdefefQd
bdefefQd
?
?
?
w
w
 
The regression framework we adopt is Support 
Vector Regression (SVR), which is a regression 
analysis technique based on SVM (Schölkopf et al., 
2000). We require hj to have the form wT?(di) + b. 
Finding hj is equivalent to solving the following 
optimization problem: 
 
where C is the SVM cost parameter, ?i are slack 
variables, and Q is the training corpus. We assume 
each document di?Q is labeled with fi(ej) for every 
emotion ej?E. 
 
min )( 2,1,T2
1
,, 2,1, iib Cii ????, ++ ?www  
subject to: 
When formulated as an SVM optimization prob-
lem, finding gjk is equivalent to training an SVM 
classifier for classifying a document into the ej or 
ek category. Hence, we use LIBSVM, which is an 
SVM implementation, to obtain the solution.1 
0 , : 
)())((
))(()(
: 
2,1,
2,
T
1,
T
??
???+?
??+??
??
ii
ijii
iiji
i
i
efbd
bdef
Qd
??
??
??
w
w
 
                                                          
 1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
138
02000
4000
6000
8000
[2
0%
,3
0%
)
[3
0%
,4
0%
)
[4
0%
,5
0%
)
[5
0%
,6
0%
)
[6
0%
,7
0%
)
[7
0%
,8
0%
)
[8
0%
,9
0%
)
[9
0%
,1
00
%
]
Percentage of Votes Received by Most Popular Emotion
N
um
be
r o
f N
ew
s 
A
rt
ic
le
s
 
Figure 2. News articles in the entire corpus grouped by 
the percentage of votes received by the most popular 
emotion. 
 
where C is the cost parameter, ? is the maximum 
difference between the predicted and actual values 
we wish to maintain, ?i,1 and ?i,2 are slack variables, 
and Q is the training corpus. To solve the above 
optimization problem, we use SVMlight’s SVR im-
plementation.2 
When ranking the emotions of a previously un-
seen document dk, we sort the emotions ej?E in 
descending order of hj(dk). 
4 Constructing the Corpus 
The training and test corpora used in this study 
comprise Chinese news articles from Yahoo! Kimo 
News3, which allows a user to cast a vote for one 
of eight emotions to express how a news article 
makes her feel. Each Yahoo! news article contains 
a list of eight emotions at the bottom of the web-
page. A reader may select one of the emotions and 
click on a submit button to submit the emotion. As 
with many websites which collect user responses, 
such as the Internet Movie Database, users are not 
forced to submit their responses. After submitting a 
response, the user can view a distribution of emo-
tions indicating how other readers feel about the 
same article. Figure 1 shows the voting results of a 
Yahoo! news article. 
The eight available emotions are happy, sad, 
angry, surprising, boring, heartwarming, awesome, 
and useful. Useful is not a true emotion. Rather, it 
means that a news article contains practical infor-
mation. The value fi(ej) is derived by normalizing 
the number of votes for emotion ej in document di 
by the total number votes in di. 
The entire corpus consists of 37,416 news arti-
cles dating from January 24, 2007 to August 7, 
2007. News articles prior to June 1, 2007 form the 
training corpus (25,975 articles), and the remaining 
ones form the test corpus (11,441 articles). We 
collect articles a week after their publication dates 
to ensure that the vote counts have stabilized. 
                                                          
                                                          2 http://svmlight.joachims.org/ 
3 http://tw.news.yahoo.com 
As mentioned earlier, readers rarely agree 
unanimously on the emotion of a document. Figure 
2 illustrates this. In 41% of all the news articles in 
the entire corpus, the most popular emotion re-
ceives less than 60% of the votes. 
5 Extracting Features 
After obtaining news articles, the next step is to 
determine how to convert them into feature vectors 
for SVM and SVR. That is, we want to instantiate 
?. For this purpose, three types of features are ex-
tracted. 
The first feature type consists of Chinese charac-
ter bigrams, which are taken from the headline and 
content of each news article. The presence of a bi-
gram is indicated by a binary feature value. 
Chinese words form the second type of features. 
Unlike English words, consecutive Chinese words 
in a sentence are not separated by spaces. To deal 
with this problem, we utilize Stanford NLP 
Group’s Chinese word segmenter to split a sen-
tence into words.4 As in the case of bigrams, bi-
nary feature values are used. 
We use character bigram features in addition to 
word features to increase the coverage of Chinese 
words. A Chinese word is formed by one or more 
contiguous Chinese characters. As mentioned ear-
lier, Chinese words in a sentence are not separated 
by any boundary symbol (e.g., a space), so a Chi-
nese word segmentation tool is always required to 
extract words from a sentence. However, a word 
segmenter may identify word boundaries errone-
ously, resulting in the loss of correct Chinese 
words. This problem is particularly severe if there 
are a lot of out-of-vocabulary words in a dataset. In 
Chinese, around 70% of all Chinese words are 
Chinese character bigrams (Chen et al., 1997). 
Thus, using Chinese character bigrams as features 
will allow us to identify a lot of Chinese words, 
which when combined with the words extracted by 
the word segmenter, will give us a wider coverage 
of Chinese words. 
The third feature type is extracted from news 
metadata. A news article’s metadata are its news 
4 http://nlp.stanford.edu/software/segmenter.shtml 
139
NDCG@k is used because ACC@k has the dis-
advantage of not taking emotional distributions 
into account. Take Figure 1 as an example. In the 
figure, heartwarming and happy have 31.3% and 
30.7% of the votes, respectively. Since the two 
percentages are very close, it is reasonable to say 
that predicting happy as the first item in a ranked 
list may also be acceptable. However, doing so 
would be completely incorrect according to 
ACC@k. In contrast, NDCG@k would consider it 
to be partially correct, and the extent of correctness 
depends on how much heartwarming and happy’s 
percentages of votes differ. To be exact, if happy is 
predicted as the first item, then the corresponding 
NDCG@1 would be 30.7% / 31.3% = 0.98. 
category, agency, hour of publication, reporter, and 
event location. Examples of news categories in-
clude sports and political. Again, we use binary 
feature values. News metadata are used because 
they may contain implicit emotional information. 
6 Experiments 
The experiments are designed to achieve the fol-
lowing four goals: (i) to compare the ranking per-
formance of different methods, (ii) to analyze the 
pairwise ranking quality of PLM, (iii) to analyze 
the distribution estimation quality of EDR, and (iv) 
to compare the ranking performance of different 
feature sets. The Yahoo! News training and test 
corpora presented in Section 4 are used in all ex-
periments. 
The third metric is SACC@k, or set accuracy at 
k. It is a variant of ACC@k. According to 
SACC@k, a predicted ranked list is correct if the 
set of its first k items is the same as the true ranked 
list’s set of first k items. In effect, SACC@k evalu-
ates a ranking method’s ability to place the top k 
most important items in the first k positions. 
6.1 Evaluation Metrics for Ranking 
We employ three metrics as indicators of ranking 
quality: ACC@k, NDCG@k and SACC@k. 
ACC@k stands for accuracy at position k. Ac-
cording to ACC@k, a predicted ranked list is cor-
rect if the list’s first k items are identical (i.e., same 
items in the same order) to the true ranked list’s 
first k items. If two emotions in a list have the 
same number of votes, then their positions are in-
terchangeable. ACC@k is computed by dividing 
the number of correctly-predicted instances by the 
total number of instances. 
6.2 Tuning SVM and SVR Parameters 
SVM and SVR are employed in PLM and EDR, 
respectively. Both SVM and SVR have the adjust-
able C cost parameter, and SVR has an additional ? 
parameter. To estimate the optimal C value for a 
combination of SVM and features, we perform 4-
fold cross-validation on the Yahoo! News training 
corpus, and select the C value which results in the 
highest binary classification accuracy during cross-
validation. The same procedure is used to estimate 
the best C and ? values for a combination of SVR 
and features. The C-? pair which results in the 
lowest mean squared error during cross-validation 
is chosen. The candidate C values for both SVM 
and SVR are 2-10, 2-9, …, 2-6. The candidate ? val-
ues for SVR are 10-2 and 10-1. All cross-validations 
are performed solely on the training data. The test 
data are not used to tune the parameters. Also, 
SVM and SVR allow users to specify the type of 
kernel to use. Linear kernel is selected for both 
SVM and SVR. 
NDCG@k, or normalized discounted cumulative 
gain at position k (Järvelin and Kekäläinen, 2002), 
is a metric frequently used in information retrieval 
to judge the quality of a ranked list when multiple 
levels of relevance are considered. This metric is 
defined as 
? = += ki ik irelzk 1 2 )1(log@NDCG  
 
where reli is the relevance score of the predicted 
item at position i, and zk is a normalizing factor 
which ensures that a correct ranked list has an 
NDCG@k value of 1. In the emotion ranking prob-
lem, reli is the percentage of reader votes received 
by the emotion at position i. Note that the log2(i+1) 
value in the denominator is a discount factor which 
decreases the weights of items ranked later in a list. 
NDCG@k has the range [0, 1], where 1 is the best. 
In the experiment results, NDCG@k values are 
averaged over all instances in the test corpus. 
6.3 Nearest Neighbor Baseline 
The nearest neighbor (NN) method is used as the 
baseline. The ranked emotion list of a news article 
in the test corpus is predicted as follows. First, the
140
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
0.5
0.6
0.7
0.8
0.9
1.0
1 2 3 4 5 6 7 8
NDCG@
N
D
C
G
 V
al
ue
NN
EDR
PLM
 
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8
SACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
Figure 3. ACC@k Figure 4. NDCG@k Figure 5. SACC@k 
 
 
0%
20%
40%
60%
80%
100%
1 2 3 4 5 6 7 8
ACC@
%
 o
f T
es
t I
ns
ta
nc
es
Both
Incorrect
Only PLM
Correct
Only EDR
Correct
Both
Correct
 
Figure 6. Performance of PLM and EDR. 
 
test news article is compared to every training 
news article using cosine similarity, which is de-
fined as  
||||
||
),(cos
ii
ji
ji DD
DD
dd ×
?=  
 
where di and dj are two news articles, and Di and Dj 
are sets of Chinese character bigrams in di and dj, 
respectively. The ranked emotion list of the train-
ing article having the highest cosine similarity with 
the test article is used as the predicted ranked list. 
6.4 Comparison of Methods 
Figures 3 to 5 show the performance of different 
ranking methods on the test corpus. For both PLM 
and EDR, all of the bigram, word, and news meta-
data features are used. 
In Figure 3, EDR’s ACC@1 (0.751) is higher 
than those of PLM and NN, and the differences are 
statistically significant with p-value < 0.01. So, 
EDR is the best method at predicting the most 
popular emotion. However, PLM has the best 
ACC@k for k ? 2, and the differences from the 
other two methods are all significant with p-value 
< 0.01. This means that PLM’s predicted ranked 
lists better resemble the true ranked lists.  
Figure 3 displays a sharp decrease in ACC@k 
values as k increases. This trend indicates the hard-
ness of predicting a ranked list correctly. Looking 
from a different angle, the ranking task under the 
ACC@k metric is equivalent to the classification 
of news articles into one of 8!/(8 – k)! classes, 
where we regard each unique emotion sequence of 
length k as a class. In fact, computing ACC@8 for 
a ranking method is the same as evaluating the 
method’s ability to classify a news article into one 
of 8! = 40,320 classes. So, producing a completely-
correct ranked list is a difficult task. 
In Figure 4, all of PLM and EDR’s NDCG@k 
improvements over NN are statistically significant 
with p-value < 0.01. For some values of k, the dif-
ference in NDCG@k between PLM and EDR is 
not significant. The high NDCG@k values (i.e., 
greater than 0.8) of PLM and EDR imply that al-
though it is difficult for PLM and EDR to generate 
completely-correct ranked lists, these two methods 
are effective at placing highly popular emotions to 
the beginning of ranked lists. 
In Figure 5, PLM outperforms the other two 
methods for 2 ? k ? 7, and the differences are all 
statistically significant with p-value < 0.01. For 
small values of k (e.g., 2 ? k ? 3), PLM’s higher 
SACC@k values mean that PLM is better at plac-
ing the highly popular emotions in the top posi-
tions of a ranked list. 
To further compare PLM and EDR, we examine 
their performance on individual test instances. Fig-
ure 6 shows the percentage of test instances where 
both PLM and EDR give incorrect lists, only PLM 
gives correct lists, only EDR gives ranked lists, 
and both methods give correct lists. The “Only 
PLM Correct” and “Only EDR Correct” categories 
are nonzero, so neither PLM nor EDR is always 
better than the other. 
In summary, EDR is the best at predicting the 
most popular emotion according to ACC@1, 
NDCG@1 and SACC@1. However, PLM gener-
ates ranked lists that better resemble the correct 
ranked lists according to ACC@k and SACC@k 
141
Method Average ?b Average p-value
PLM 0.584 0.068
EDR 0.474 0.114
NN 0.392 0.155
Table 1. Kendall’s ?b statistics. 
 
 He Su Sa Us Ha Bo An
Aw 0.80  0.75  0.78  0.77  0.82  0.76 0.79 
He  0.79  0.81  0.78  0.81  0.89 0.81 
Su   0.82  0.78  0.80  0.82 0.82 
Sa    0.78  0.80  0.84 0.82 
Us     0.82  0.91 0.82 
Ha      0.83 0.79 
Bo      0.80 
Table 2. Classification accuracies of SVM pairwise 
emotion classifiers on the test corpus. He = heartwarm-
ing, Su = surprising, Sa = sad, Us = useful, Ha = happy, 
Bo = boring, and An = angry. 
 
0.53
0.58
0.63
0.68
0.73
0.75 0.8 0.85 0.9
Accuracy of Pairwise Emotion Classification
A
ve
ra
ge
 D
is
cr
im
in
at
io
n
V
al
ue
 o
f E
m
ot
io
n 
P
ai
r
 
Figure 7. Accuracy of pairwise emotion classification 
and the corresponding average discrimination value. 
 
for k ? 2. Further analysis shows that neither 
method is always better than the other. 
6.5 Pairwise Ranking Quality of PLM 
In this subsection, we evaluate the performance of 
PLM in predicting pairwise orders. 
We first examine the quality of ranked lists gen-
erated by PLM in terms of pairwise orders. To do 
this, we use Kendall’s ?b correlation coefficient, 
which is a statistical measure for determining the 
correlation between two ranked lists when there 
may be ties between two items in a list (Liebetrau, 
1983). The value of ?b is determined based on the 
number of concordant pairwise orders and the 
number of discordant pairwise orders between two 
ranked lists. Therefore, this measure is appropriate 
for evaluating the effectiveness of PLM at predict-
ing pairwise orders correctly. ?b has the range [-1, 
1], where 1 means a perfect positive correlation, 
and -1 means two lists are the reverse of each other. 
When computing ?b of two ranked lists, we also 
calculate a p-value to indicate whether the correla-
tion is statistically significant. 
We compute ?b statistics between a predicted 
ranked list and the corresponding true ranked list. 
Table 1 shows the results. In Table 1, numbers in 
the “Average ?b” and “Average p-value” columns 
are averaged over all test instances. The statistics 
for EDR and NN are also included for comparison. 
From the table, we see that PLM has the highest 
average ?b value and the lowest average p-value, so 
PLM is better at preserving pairwise orders than 
EDR and NN methods. This observation verifies 
that PLM’s minimization of pairwise loss leads to 
better prediction of pairwise orders.  
We now look at the individual performance of 
the 28 pairwise emotion rankers gjk. As mentioned 
in Section 3.2, each pairwise emotion ranker gjk is 
equivalent to a binary classifier for classifying a 
document into the ej or ek category. So, we look at 
their classification accuracies in Table 2. In the 
table, accuracy ranges from 0.75 for the awesome-
surprising pair to 0.91 for the useful-boring pair. 
From the psychological perspective, the rela-
tively low accuracy of the awesome-surprising pair 
is expected, because awesome is surprising in a 
positive sense. So, readers should have a hard time 
distinguishing between these two emotions. And 
the SVM classifier, which models reader responses, 
should also find it difficult to discern these two 
emotions. Based on this observation, we suspect 
that the pairwise classification performance actu-
ally reflects the underlying emotional ambiguity 
experienced by readers. To verify this, we quantify 
the degree of ambiguity between two emotions, 
and compare the result to pairwise classification 
accuracy. 
To quantify emotional ambiguity, we introduce 
the concept of discrimination value between two 
emotions ej and ek in a document di, which is de-
fined as follows: 
 
)()(
)()(
kiji
kiji
efef
efef
+
?
 
 
where fi is the emotional probability function de-
fined in Section 3.1. Intuitively, the larger the dis-
crimination value is, the smaller the degree of 
ambiguity between two emotions is. 
Figure 7 shows the relationship between pair-
wise classification accuracy and the average dis-
crimination value of the corresponding emotion 
142
0.00
0.02
0.04
0.06
0.08
A
w
es
om
e
H
ea
rtw
ar
m
in
g
S
ur
pr
is
in
g
S
ad
U
se
fu
l
H
ap
py
B
or
in
g
A
ng
ry
Emotion
M
ea
n 
S
qu
ar
ed
 E
rr
or
NN
EDR
 
Figure 8. Mean squared error of NN and EDR for esti-
mating the emotional distributions of the test corpus. 
 
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
Metadata
Words
Bigrams
All
 
Figure 9. PLM performance using different features. 
 
pair. The general pattern is that as accuracy in-
creases, the discrimination value also increases. To 
provide concrete evidence, we use Pearson’s prod-
uct-moment correlation coefficient, which has the 
range of [-1, 1], where 1 means a perfect positive 
correlation (Moore, 2006). The coefficient for the 
data in Figure 7 is 0.726 with p-value < 0.01. Thus, 
pairwise emotion classification accuracy reflects 
the emotional ambiguity experienced by readers. 
In summary, PLM’s pairwise loss minimization 
leads to better pairwise order predictions than EDR 
and NN. Also, the pairwise classification results 
reveal the inherent ambiguity between emotions. 
6.6 Distribution Estimation Quality of EDR 
In this subsection, we evaluate EDR’s performance 
in estimating the emotional probability function fi. 
With the prior knowledge that a news article’s fi 
values sum to 1 over all emotions, and fi is between 
0 and 1, we adjust EDR’s fi predictions to produce 
proper distributions. It is done as follows. A pre-
dicted fi value greater than 1 or less than 0 is set to 
1 and 0, respectively. Then the predicted fi values 
are normalized to sum to 1 over all emotions.  
NN’s distribution estimation performance is in-
cluded for comparison. For NN, the predicted fi 
values of a test article are taken from the emotional 
distribution of the most similar training article.  
Figure 8 shows the mean squared error of EDR 
and NN for predicting fi. In the figure, the error 
generated by EDR is less than those by NN, and all 
the differences are statistically significant with p-
value < 0.01. Thus, EDR’s use of regression leads 
to better estimation of fi than the NN. 
6.7 Comparison of Features 
Figure 9 shows each of the three feature type’s 
ACC@k for predicting test instances’ ranked lists 
when PLM is used. The feature comparison graph 
for EDR is not shown, because it exhibits a very 
similar trend as PLM. For both PLM and EDR, 
bigrams are better than words, which are in turn 
better than news metadata. In Figure 9, the combi-
nation of all three feature sets achieves the best 
performance. For both PLM and EDR, the im-
provements in ACC@k of using all features over 
words and metadata are all significant with p-value 
< 0.01, and the improvements over bigrams are 
significant for k ? 2. Hence, in general, it is better 
to use all three feature types together. 
7 Conclusions and Future Work 
This paper presents two methods to ranking reader 
emotions. The PLM method minimizes pairwise 
loss, and the EDR method estimates emotional dis-
tribution through regression. Experiments with 
significant tests show that EDR is better at predict-
ing the most popular emotion, but PLM produces 
ranked lists that have higher correlation with the 
correct lists. We further verify that PLM has better 
pairwise ranking performance than the other two 
methods, and EDR has better distribution estima-
tion performance than NN. 
As for future work, there are several directions 
we can pursue. An observation is that PLM ex-
ploits pairwise order information, whereas EDR 
exploits emotional distribution information. We 
plan to combine these two methods together. An-
other research direction is to improve EDR by 
finding better features. We would also like to inte-
grate emotion ranking into information retrieval. 
Acknowledgments 
We are grateful to the Computer and Information 
Networking Center, National Taiwan University, 
for the support of high-performance computing 
facilities. The research in this paper was partially 
supported by National Science Council, Taiwan, 
under the contract NSC 96-2628-E-002-240-MY3. 
143
References  
Saima Aman and Stan Szpakowicz. 2007. Identifying 
Expressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196-205. Springer, Plze?, CZ. 
Aitao Chen, Jianzhang He, Liangjie Xu, Frederic Gey, 
and Jason Meggs. 1997. Chinese Text Retrieval 
wihtout using a Dictionary. In Proceedings of 20th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
42-49. Association for Computing Machinery, Phila-
delphia, US. 
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and 
Yoram Singer. 2003. An Efficient Boosting Algorithm 
for Combining Preferences. Journal of Machine 
Learning Research, 4, 933-969. 
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, 
and Ruzhan Lu. 2005. A New Method for Sentiment 
Classification in Text Retrieval. In Proceedings of 
2nd International Joint Conference on Natural Lan-
guage Processing, 1-9. Jeju Island, KR. 
Kalervo Järvelin and Jaana Kekäläinen. Cumulative 
Gain-based Evaluation of IR Techniques. 2002. 
ACM Transactions on Information Systems, 20(4), 
422-446. 
Thorsten Joachims. 2002. Optimizing Search Engines 
using Clickthrough Data. In Proceedings of 8th 
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. Association for 
Computing Machinery, Edmonton, CA. 
Albert M. Liebetrau. 1983. Measures of Association. 
Sage Publications, Newbury Park, US. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2007. What Emotions do News Articles Trigger in 
their Readers? In Proceedings of 30th ACM SIGIR 
Conference, 733-734. Association for Computing 
Machinery, Amsterdam, NL. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2008. Emotion Classification of Online News Articles 
from the Reader’s Perspective. In Proceedings of In-
ternational Conference on Web Intelligence. Institute 
of Electrical and Electronics Engineers, Sydney, AU. 
David Moore. 2006. The Basic Practice of Statistics. 
W.H. Freeman and Company, New York, US. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification Using 
Machine Learning Techniques. In Proceedings of 
2002 Conference on Empirical Methods in Natural 
Language Processing, 79-86. Association for Com-
putational Linguistics, Philadelphia, US. 
Tao Qin, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-
Sheng Wang, and Hang Li. 2007. Ranking with Mul-
tiple Hyperplanes. In Proceedings of 30th ACM 
SIGIR Conference, 279-286. Association for Com-
puting Machinery, Amsterdam, NL. 
Bernhard Schölkopf, Alex J. Smola, Robert C. William-
son, and Peter L. Barlett. 2000. New Support Vector 
Algorithms. Neural Computation, 12(5), 1207-1245. 
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of 4th 
International Workshop on Semantic Evaluations. 
Prague, CZ. 
Janyce M. Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. In Proceedings of 17th Conference of 
the American Association for Artificial Intelligence, 
735-740. AAAI Press, Austin, US. 
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. Prob-
ability Estimates for Multi-class Classification by 
Pairwise Coupling. 2004. Journal of Machine Learn-
ing Research, 5, 975-1005. 
144
