Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 557–565,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Beyond Parallel Data: Joint Word Alignment and Decipherment
Improves Machine Translation
Qing Dou , Ashish Vaswani, and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,avaswani,knight}@isi.edu
Abstract
Inspired by previous work, where decipher-
ment is used to improve machine translation,
we propose a new idea to combine word align-
ment and decipherment into a single learning
process. We use EM to estimate the model pa-
rameters, not only to maximize the probabil-
ity of parallel corpus, but also the monolingual
corpus. We apply our approach to improve
Malagasy-English machine translation, where
only a small amount of parallel data is avail-
able. In our experiments, we observe gains of
0.9 to 2.1 Bleu over a strong baseline.
1 Introduction
State-of-the-art machine translation (MT) systems ap-
ply statistical techniques to learn translation rules au-
tomatically from parallel data. However, this reliance
on parallel data seriously limits the scope of MT ap-
plication in the real world, as for many languages and
domains, there is not enough parallel data to train a de-
cent quality MT system.
However, compared with parallel data, there are
much larger amounts of non parallel data. The abil-
ity to learn a translation lexicon or even build a ma-
chine translation system using monolingual data helps
address the problems of insufficient parallel data. Ravi
and Knight (2011) are among the first to learn a full
MT system using only non parallel data through deci-
pherment. However, the performance of such systems
is much lower compared with those trained with par-
allel data. In another work, Klementiev et al. (2012)
show that, given a phrase table, it is possible to esti-
mate parameters for a phrase-based MT system from
non parallel data.
Given that we often have some parallel data, it is
more practical to improve a translation system trained
on parallel data by using additional non parallel data.
Rapp (1995) shows that with a seed lexicon, it is possi-
ble to induce new word level translations from non par-
allel data. Motivated by the idea that a translation lexi-
con induced from non parallel data can be used to trans-
late out of vocabulary words (OOV), a variety of prior
research has tried to build a translation lexicon from
non parallel or comparable data (Fung and Yee, 1998;
Koehn and Knight, 2002; Haghighi et al., 2008; Garera
Figure 1: Combine word alignment and decipherment
into a single learning process.
et al., 2009; Bergsma and Van Durme, 2011; Daum´e
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a; Irvine et al.,
2013).
Lately, there has been increasing interest in learn-
ing translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al., 2012; Dou and Knight,
2013). Decipherment views one language as a cipher
for another and learns a translation lexicon that pro-
duces fluent text in the target (plaintext) language. Pre-
vious work has shown that decipherment not only helps
find translations for OOVs (Dou and Knight, 2012), but
also improves translations of observed words (Dou and
Knight, 2013).
We find that previous work using monolingual or
comparable data to improve quality of machine transla-
tion separates two learning tasks: first, translation rules
are learned from parallel data, and then the information
learned from parallel data is used to bootstrap learning
with non parallel data. Inspired by approaches where
joint inference reduces the problems of error propaga-
tion and improves system performance, we combine
the two separate learning processes into a single one,
as shown in Figure 1. The contributions of this work
are:
557
• We propose a new objective function for word
alignment that combines the process of word
alignment and decipherment into a single learning
task.
• In experiments, we find that the joint process out-
performs the previous pipeline approach, and ob-
serve Bleu gains of 0.9 and 2.1 on two different
test sets.
• We release 15.3 million tokens of monolingual
Malagasy data from the web, as well as a small
Malagasy dependency tree bank containing 20k
tokens.
2 Joint Word Alignment and
Decipherment
2.1 A New Objective Function
In previous work that uses monolingual data to im-
prove machine translation, a seed translation lexicon
learned from parallel data is used to find new transla-
tions through either word vector based approaches or
decipherment. In return, selection of a seed lexicon
needs to be careful as using a poor quality seed lexi-
con could hurt the downstream process. Evidence from
a number of previous work shows that a joint inference
process leads to better performance in both tasks (Jiang
et al., 2008; Zhang and Clark, 2008).
In the presence of parallel and monolingual data, we
would like the alignment and decipherment models to
benefit from each other. Since the decipherment and
word alignment models contain word-to-word transla-
tion probabilities t( f | e), having them share these pa-
rameters during learning will allow us to pool infor-
mation from both data types. This leads us to de-
velop a new objective function that takes both learn-
ing processes into account. Given our parallel data,
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), and monolingual
data F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, we seek to maximize
the likelihood of both. Our new objective function is
defined as:
F
joint
=
M
?
m=1
log P(F
m
| E
m
) + ?
N
?
n=1
log P(F
n
mono
) (1)
The goal of training is to learn the parameters that
maximize this objective, that is
?
?
= arg max
?
F
joint
(2)
In the next two sections, we describe the word align-
ment and decipherment models, and present how they
are combined to perform joint optimization.
2.2 Word Alignment
Given a source sentence F = f
1
, . . . , f
j
, . . . , f
J
and a
target sentence E = e
1
, . . . , e
i
, . . . , e
I
, word alignment
models describe the generative process employed to
produce the French sentence from the English sentence
through alignments a = a
1
, . . . , a
j
, . . . , a
J
.
The IBM models 1-2 (Brown et al., 1993) and the
HMM word alignment model (Vogel et al., 1996) use
two sets of parameters, distortion probabilities and
translation probabilities, to define the joint probabil-
ity of a target sentence and alignment given a source
sentence.
P(F, a | E) =
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
). (3)
These alignment models share the same translation
probabilities t( f
j
| e
a
j
), but differ in their treatment of
the distortion probabilities d(a
j
| a
j?1
, j). Brown et
al. (1993) introduce more advanced models for word
alignment, such as Model 3 and Model 4, which use
more parameters to describe the generative process. We
do not go into details of those models here and the
reader is referred to the paper describing them.
Under the Model 1-2 and HMM alignment models,
the probability of target sentence given source sentence
is:
P(F | E) =
?
a
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
).
Let ? denote all the parameters of the word align-
ment model. Given a corpus of sentence pairs
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), the standard ap-
proach for training is to learn the maximum likelihood
estimate of the parameters, that is,
?
?
= arg max
?
M
?
m=1
log P(F
m
| E
m
)
= arg max
?
log
?
?
?
?
?
?
?
?
a
P(F
m
, a | E
m
)
?
?
?
?
?
?
?
.
We typically use the EM algorithm (Dempster et al.,
1977), to carry out this optimization.
2.3 Decipherment
Given a corpus of N foreign text sequences (cipher-
text), F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, decipherment finds
word-to-word translations that best describe the cipher-
text.
Knight et al. (2006) are the first to study several natu-
ral language decipherment problems with unsupervised
learning. Since then, there has been increasing interest
in improving decipherment techniques and its applica-
tion to machine translation (Ravi and Knight, 2011;
558
Dou and Knight, 2012; Nuhn et al., 2012; Dou and
Knight, 2013; Nuhn et al., 2013).
In order to speed up decipherment, Dou and Knight
(2012) suggest that a frequency list of bigrams might
contain enough information for decipherment. Accord-
ing to them, a monolingual ciphertext bigram F
mono
is
generated through the following generative story:
• Generate a sequence of two plaintext tokens e
1
e
2
with probability P(e
1
e
2
) given by a language
model built from large numbers of plaintext bi-
grams.
• Substitute e
1
with f
1
and e
2
with f
2
with probabil-
ity t( f
1
|e
1
) · t( f
2
|e
2
).
The probability of any cipher bigram F is:
P(F
mono
) =
?
e
1
e
2
P(e
1
e
2
) · t( f
1
|e
1
) · t( f
2
|e
2
) (4)
And the probability of the corpus is:
P(corpus) =
N
?
n=1
P(F
n
mono
) (5)
Given a plaintext bigram language model, the goal is
to manipulate t( f |e) to maximize P(corpus). Theoret-
ically, one can directly apply EM to solve the problem
(Knight et al., 2006). However, EM has time complex-
ity O(N ·V
2
e
) and space complexity O(V
f
·V
e
), where V
f
,
V
e
are the sizes of ciphertext and plaintext vocabularies
respectively, and N is the number of cipher bigrams.
There have been previous attempts to make decipher-
ment faster. Ravi and Knight (2011) apply Bayesian
learning to reduce the space complexity. However,
Bayesian decipherment is still very slow with Gibbs
sampling (Geman and Geman, 1987). Dou and Knight
(2012) make sampling faster by introducing slice sam-
pling (Neal, 2000) to Bayesian decipherment. Besides
Bayesian decipherment, Nuhn et al. (2013) show that
beam search can be used to solve a very large 1:1 word
substitution cipher. In subsection 2.4.1, we describe
our approach that uses slice sampling to compute ex-
pected counts for decipherment in the EM algorithm.
2.4 Joint Optimization
We now describe our EM approach to learn the param-
eters that maximize F
joint
(equation 2), where the dis-
tortion probabilities, d(a
j
| a
j?1
, j) in the word align-
ment model are only learned from parallel data, and
the translation probabilities, t( f | e) are learned using
both parallel and non parallel data. The E step and M
step are illustrated in Figure 2.
Our algorithm starts with EM learning only on par-
allel data for a few iterations. When the joint inference
starts, we first compute expected counts from parallel
data and non parallel data using parameter values from
the last M step separately. Then, we add the expected
counts from both parallel data and non parallel data to-
gether with different weights for the two. Finally we
Figure 2: Joint Word Alignment and Decipherment
with EM
renormalize the translation table and distortion table to
update parameters in the new M step.
The E step for parallel part can be computed effi-
ciently using the forward-backward algorithm (Vogel et
al., 1996). However, as we pointed out in Section 2.3,
the E step for the non parallel part has a time com-
plexity of O(V
2
) with the forward-backward algorithm,
where V is the size of English vocabulary, and is usu-
ally very large. Previous work has tried to make de-
cipherment scalable (Ravi and Knight, 2011; Dou and
Knight, 2012; Nuhn et al., 2013; Ravi, 2013). How-
ever, all of them are designed for decipherment with ei-
ther Bayesian inference or beam search. In contrast, we
need an algorithm to make EM decipherment scalable.
To overcome this problem, we modify the slice sam-
pling (Neal, 2000) approach used by Dou and Knight
(2012) to compute expected counts from non parallel
data needed for the EM algorithm.
2.4.1 Draw Samples with Slice Sampling
To start the sampling process, we initialize the first
sample by performing approximate Viterbi decoding
using results from the last EM iteration. For each for-
eign dependency bigram f
1
, f
2
, we find the top 50 can-
didates for f
1
and f
2
ranked by t(e| f ), and find the En-
glish sequence e
1
, e
2
that maximizes t(e
1
| f
1
) · t(e
2
| f
2
) ·
P(e
1
, e
2
).
Suppose the derivation probability for current sam-
ple e current is P(e current), we use slice sampling to
draw a new sample in two steps:
• Select a threshold T uniformly between 0 and
P(e current).
• Draw a new sample e new uniformly from a pool
559
of candidates: {e new|P(e new) > T }.
The first step is straightforward to implement. How-
ever, it is not trivial to implement the second step. We
adapt the idea from Dou and Knight (2012) for EM
learning.
Suppose our current sample e current contains En-
glish tokens e
i?1
, e
i
, and e
i+1
at position i ? 1, i, and
i+1 respectively, and f
i
be the foreign token at position
i. Using point-wise sampling, we draw a new sample
by changing token e
i
to a new token e
?
. Since the rest
of the sample remains the same, only the probability of
the trigram P(e
i?1
e
?
e
i+1
) (The probability is given by a
bigram language model.), and the channel model prob-
ability t( f
i
|e
?
) change. Therefore, the probability of a
sample is simplified as shown Equation 6.
P(e
i?1
e
?
e
i+1
) · t( f
i
|e
?
) (6)
Remember that in slice sampling, a new sample is
drawn in two steps. For the first step, we choose a
threshold T uniformly between 0 and P(e
i?1
e
i
e
i+1
) ·
t( f
i
|e
i
). We divide the second step into two cases based
on the observation that two types of samples are more
likely to have a probability higher than T (Dou and
Knight, 2012): (1) those whose trigram probability is
high, and (2) those whose channel model probability is
high. To find candidates that have high trigram proba-
bility, Dou and Knight (2012) build a top k sorted lists
ranked by P(e
i?1
e
?
e
i+1
), which can be pre-computed
off-line. Then, they test if the last item e
k
in the list
satisfies the following inequality:
P(e
i?1
e
k
e
i+1
) · c < T (7)
where c is a small constant and is set to prior in their
work. In contrast, we choose c empirically as we do
not have a prior in our model. When the inequality in
Equation 7 is satisfied, a sample is drawn in the fol-
lowing way: Let set A = {e
?
|e
i?1
e
?
e
i+1
· c > T } and
set B = {e
?
|t( f
i
|e
?
) > c}. Then we only need to sample
e
?
uniformly from A ? B until P(e
i?1
e
?
e
i+1
) · t( f
i
|e
?
) is
greater than T . It is easy to prove that all other candi-
dates that are not in the sorted list and with t( f
i
|e
?
) ? c
have a upper bound probability: P(e
i?1
e
k
e
i+1
)·c. There-
fore, they do not need to be considered.
Second, when the last item e
k
in the list does not
meet the condition in Equation 7, we keep drawing
samples e
?
randomly until its probability is greater than
the threshold T .
As we mentioned before, the choice of the small con-
stant c is empirical. A large c reduces the number of
items in set B, but makes the condition P(e
i?1
e
k
e
i+1
) ·
c < T less likely to satisfy, which slows down the sam-
pling. On the contrary, a small c increases the number
of items in set B significantly as EM does not encour-
age a sparse distribution, which also slows down the
sampling. In our experiments, we set c to 0.001 based
on the speed of decipherment. Furthermore, to reduce
the size of set B, we rank all the candidate translations
Spanish English
Parallel 10.3k 9.9k
Non Parallel 80 million 400 million
Table 1: Size of parallel and non parallel data for word
alignment experiments (Measured in number of tokens)
of f
i
by t(e
?
| f
i
), then we add maximum the first 1000
candidates whose t( f
i
|e
?
) >= c into set B. For the rest
of the candidates, we set t( f
i
|e
?
) to a value smaller than
c (0.00001 in experiments).
2.4.2 Compute Expected Counts from Samples
With the ability to draw samples efficiently for deci-
pherment using EM, we now describe how to compute
expected counts from those samples. Let f
1
, f
2
be a
specific ciphertext bigram, N be the number of sam-
ples we want to use to compute expected counts, and
e
1
, e
2
be one of the N samples. The expected counts
for pairs ( f
1
, e
1
) and ( f
2
, e
2
) are computed as:
? ·
count( f
1
, f
2
)
N
where count( f
1
, f
2
) is count of the bigram, and ? is the
weight for non parallel data as shown in Equation 1.
Expected counts collected for f
1
, f
2
are accumulated
from each of its N samples. Finally, we collect ex-
pected counts using the same approach from each for-
eign bigram.
3 Word Alignment Experiments
In this section, we show that joint word alignment and
decipherment improves the quality of word alignment.
We choose to evaluate word alignment performance
for Spanish and English as manual gold alignments
are available. In experiments, our approach improves
alignment F score by as much as 8 points.
3.1 Experiment Setup
As shown in Table 1, we work with a small amount of
parallel, manually aligned Spanish-English data (Lam-
bert et al., 2005), and a much larger amount of mono-
lingual data.
The parallel data is extracted from Europarl, which
consists of articles from European parliament plenary
sessions. The monolingual data comes from English
and Spanish versions of Gigaword corpra containing
news articles from different news agencies.
We view Spanish as a cipher of English, and follow
the approach proposed by Dou and Knight (2013) to
extract dependency bigrams from parsed Spanish and
English monolingual data for decipherment. We only
keep bigrams where both tokens appear in the paral-
lel data. Then, we perform Spanish to English (En-
glish generating Spanish) word alignment and Span-
ish to English decipherment simultaneously with the
method discussed in section 2.
560
3.1.1 Results
We align all 500 sentences in the parallel corpus, and
tune the decipherment weight (?) for Model 1 and
HMM using the last 100 sentences. The best weights
are 0.1 for Model 1, and 0.005 for HMM. We start with
Model 1 with only parallel data for 5 iterations, and
switch to the joint process for another 5 iterations with
Model 1 and 5 more iterations of HMM. In the end, we
use the first 100 sentence pairs of the corpus for evalu-
ation.
Figure 3 compares the learning curve of alignment
F-score between EM without decipherment (baseline)
and our joint word alignment and decipherment. From
the learning curve, we find that at the 6th iteration, 2
iterations after we start the joint process, alignment F-
score is improved from 34 to 43, and this improvement
is held through the rest of the Model 1 iterations. The
alignment model switches to HMM from the 11th iter-
ation, and at the 12th iteration, we see a sudden jump
in F-score for both the baseline and the joint approach.
We see consistent improvement of F-score till the end
of HMM iterations.
4 Improving Low Density Languages
Machine Translation with Joint Word
Alignment and Decipherment
In the previous section, we show that the joint word
alignment and decipherment process improves quality
of word alignment significantly for Spanish and En-
glish. In this section, we test our approach in a more
challenging setting: improving the quality of machine
translation in a real low density language setting.
In this task, our goal is to build a system to trans-
late Malagasy news into English. We have a small
amount of parallel data, and larger amounts of mono-
lingual data collected from online websites. We build a
dependency parser for Malagasy to parse the monolin-
gual data to perform dependency based decipherment
(Dou and Knight, 2013). In the end, we perform joint
word alignment and decipherment, and show that the
joint learning process improves Bleu scores by up to
2.1 points over a phrase-based MT baseline.
4.1 The Malagasy Language
Malagasy is the official language of Madagascar. It has
around 18 million native speakers. Although Mada-
gascar is an African country, Malagasy belongs to the
Malayo-Polynesian branch of the Austronesian lan-
guage family. Malagasy and English have very dif-
ferent word orders. First of all, in contrast to En-
glish, which has a subject-verb-object (SVO) word or-
der, Malagasy has a verb-object-subject (VOS) word
order. Besides that, Malagasy is a typical head ini-
tial language: Determiners precede nouns, while other
modifiers and relative clauses follow nouns (e.g. ny
“the” boky “book” mena “red”). The significant dif-
ferences in word order pose great challenges for both
Source Malagasy English
Parallel
Global Voices 2.0 million 1.8 million
Web News 2.2k 2.1k
Non Parallel
Gigaword N/A 2.4 billion
allAfrica N/A 396 million
Local News 15.3 million N/A
Table 2: Size of Malagasy and English data used in our
experiments (Measured in number of tokens)
machine translation and decipherment.
4.2 Data
Table 2 shows the data available to us in our experi-
ments. The majority of parallel text comes from Global
Voices
1
(GV). The website contains international news
translated into different foreign languages. Besides
that, we also have a very small amount of parallel text
containing local web news, with English translations
provided by native speakers at the University of Texas,
Austin. The Malagasy side of this small parallel corpus
also has syntactical annotation, which is used to train a
very basic Malagasy part of speech tagger and depen-
dency parser.
We also have much larger amounts of non paral-
lel data for both languages. For Malagasy, we spent
two months manually collecting 15.3 million tokens of
news text from local news websites in Madagascar.
2
We have released this data for future research use. For
English, we have 2.4 billion tokens from the Gigaword
corpus. Since the Malagasy monolingual data is col-
lected from local websites, it is reasonable to argue that
those data contain significant amount of information re-
lated to Africa. Therefore, we also collect 396 million
tokens of African news in English from allAfrica.com.
4.3 Building A Dependency Parser for Malagasy
Since Malagasy and English have very different word
orders, we decide to apply dependency based decipher-
ment for the two languages as suggested by Dou and
Knight (2013). To extract dependency relations, we
need to parse monolingual data in Malagasy and En-
glish. For English, there are already many good parsers
available. In our experiments, we use Turbo parser
(Martins et al., 2013) trained on the English Penn Tree-
bank (Marcus et al., 1993) to parse all our English
monolingual data. However, there is no existing good
parser for Malagasy.
The quality of a dependency parser depends on the
amount of training data available. State-of-the-art En-
glish parsers are built from Penn Treebank, which con-
tains over 1 million tokens of annotated syntactical
1
globalvoicesonline.org
2
aoraha.com, gazetiko.com, inovaovao.com,
expressmada.com, lakroa.com
561
Figure 3: Learning curve showing our joint word alignment and decipherment approach improves word alignment
quality over the traditional EM without decipherment (Model 1: Iteration 1 to 10, HMM: Iteration 11 to 15)
trees. In contrast, the available data for training a Mala-
gasy parser is rather limited, with only 168 sentences,
and 2.8k tokens, as shown in Table 2. At the very be-
ginning, we use the last 120 sentences as training data
to train a part of speech (POS) tagger using a toolkit
provided by Garrette et al. (2013) and a dependency
parser with the Turbo parser. We test the performance
of the parser on the first 48 sentences and obtain 72.4%
accuracy.
One obvious way to improve tagging and parsing ac-
curacy is to get more annotated data. We find more data
with only part of speech tags containing 465 sentences
and 10k tokens released by (Garrette et al., 2013), and
add this data as extra training data for POS tagger.
Also, we download an online dictionary that contains
POS tags for over 60k Malagasy word types from mala-
gasyword.org. The dictionary is very helpful for tag-
ging words never seen in the training data.
It is natural to think that creation of annotated data
for training a POS tagger and a parser requires large
amounts of efforts from annotators who understand the
language well. However, we find that through the help
of parallel data and dictionaries, we are able to create
more annotated data by ourselves to improve tagging
and parsing accuracy. This idea is inspired by previ-
ous work that tries to learn a semi-supervised parser
by projecting dependency relations from one language
(with good dependency parsers) to another (Yarowsky
and Ngai, 2001; Ganchev et al., 2009). However, we
find those automatic approaches do not work well for
Malagasy.
To further expand our Malagasy training data, we
first use a POS tagger and parser with poor perfor-
mance to parse 788 sentences (20k tokens) on the
Malagasy side of the parallel corpus from Global
Voices. Then, we correct both the dependency links
and POS tags based on information from dictionaries
3
and the English translation of the parsed sentence. We
spent 3 months to manually project English dependen-
cies to Malagasy and eventually improve test set pars-
ing accuracy from 72.4% to 80.0%. We also make this
data available for future research use.
4.4 Machine Translation Experiments
In this section, we present the data used for our MT
experiments, and compare three different systems to
justify our joint word alignment and decipherment ap-
proach.
4.4.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT system,
PBMT, using Moses (Koehn et al., 2007). PBMT has 3
models: a translation model, a distortion model, and
a language model. We train the other models using
half of the Global Voices parallel data (the rest is re-
served for development and testing), and build a 5-
gram language model using 834 million tokens from
AFP section of English Gigaword, 396 million tokens
from allAfrica, and the English part of the parallel cor-
pus for training. For alignment, we run 10 iterations
of Model 1, and 5 iterations of HMM. We did not run
Model 3 and Model 4 as we see no improvements in
Bleu scores from running those models. We do word
3
an online dictionary from malagasyword.org, as well as
a lexicon learned from the parallel data
562
alignment in two directions and use grow-diag-final-
and heuristic to obtain final alignment. During decod-
ing, we use 8 standard features in Moses to score a can-
didate translation: direct and inverse translation proba-
bilities, direct and inverse lexical weighting, a language
model score, a distortion score, phrase penalty, and
word penalty. The weights for the features are learned
on the tuning data using minimum error rate training
(MERT) (Och, 2003).
To compare with previous decipherment approach to
improve machine translation, we build a second base-
line system. We follow the work by Dou and Knight
(2013) to decipher Malagasy into English, and build a
translation lexicon T
decipher
from decipherment. To im-
prove machine translation, we simply use T
decipher
as
an additional parallel corpus. First, we filter T
decipher
by keeping only translation pairs ( f , e), where f is ob-
served in the Spanish part and e is observed in the En-
glish part of the parallel corpus. Then we append all
the Spanish and English words in the filtered T
decipher
to the end of Spanish part and English part of the paral-
lel corpus respectively. The training and tuning process
is the same as the baseline machine translation system
PBMT. We call this system Decipher-Pipeline.
4.4.2 Joint Word Alignment and Decipherment
for Machine Translation
When deciphering Malagasy to English, we extract
Malagasy dependency bigrams using all available
Malagasy monolingual data plus the Malagasy part of
the Global Voices parallel data, and extract English
dependency bigrams using 834 million tokens from
English Gigaword, and 396 million tokens from al-
lAfrica news to build an English dependency language
model. In the other direction, we extract English de-
pendency bigrams from English part of the entire paral-
lel corpus plus 9.7 million tokens from allAfrica news
4
, and use 17.3 million tokens Malagasy monolingual
data (15.3 million from the web and 2.0 million from
Global Voices) to build a Malagasy dependency lan-
guage model. We require that all dependency bigrams
only contain words observed in the parallel data used
to train the baseline MT system.
During learning, we run Model 1 without decipher-
ment for 5 iterations. Then we perform joint word
alignment and decipherment for another 5 iterations
with Model 1 and 5 iterations with HMM. We tune
decipherment weights (?) for Model 1 and HMM us-
ing grid search against Bleu score on a development
set. In the end, we only extract rules from one di-
rection P(English|Malagasy), where the decipherment
weights for Model 1 and HMM are 0.5 and 0.005 re-
spectively. We chose this because we did not find any
benefits to tune the weights on each direction, and then
use grow-diag-final-end heuristic to form final align-
ments. We call this system Decipher-Joint.
4
We do not find further Bleu gains by using more English
monolingual data.
Parallel
Malagasy English
Train (GV) 0.9 million 0.8 million
Tune (GV) 22.2k 20.2k
Test (GV) 23k 21k
Test (Web) 2.2k 2.1k
Non Parallel
Malagasy English
Gigaword N/A 834 million
Web 15.3 million 396 million
Table 3: Size of training, tuning, and testing data in
number of tokens (GV: Global Voices)
4.5 Results
We tune each system three times with MERT and
choose the best weights based on Bleu scores on tuning
set.
Table 4 shows that while using a translation lexicon
learnt from decipherment does not improve the quality
of machine translation significantly, the joint approach
improves Bleu score by 0.9 and 2.1 on Global Voices
test set and web news test set respectively. The results
show that the parsing quality correlates with gains in
Bleu scores. Scores in the brackets in the last row of
the table are achieved using a dependency parser with
72.4% attachment accuracy, while scores outside the
brackets are obtained using a dependency parser with
80.0% attachment accuracy.
We analyze the results and find the gain mainly
comes from two parts. First, adding expected counts
from non parallel data makes the distribution of trans-
lation probabilities sparser in word alignment models.
The probabilities of translation pairs favored by both
parallel data and decipherment becomes higher. This
gain is consistent with previous observation where a
sparse prior is applied to EM to help improve word
alignment and machine translation (Vaswani et al.,
2012). Second, expected counts from decipherment
also help discover new translation pairs in the paral-
lel data for low frequency words, where those words
are either aligned to NULL or wrong translations in the
baseline.
5 Conclusion and Future Work
We propose a new objective function for word align-
ment to combine the process of word alignment and
decipherment into a single task. In, experiments, we
find that the joint process performs better than previous
pipeline approach, and observe Bleu gains of 0.9 and
2.1 point on Global Voices and local web news test sets,
respectively. Finally, our research leads to the release
of 15.3 million tokens of monolingual Malagasy data
from the web as well as a small Malagasy dependency
tree bank containing 20k tokens.
Given the positive results we obtain by using the
joint approach to improve word alignment, we are in-
563
Decipherment System Tune (GV) Test (GV) Test (Web)
None PBMT (Baseline) 18.5 17.1 7.7
Separate Decipher-Pipeline 18.5 17.4 7.7
Joint Decipher-Joint 18.9 (18.7) 18.0 (17.7) 9.8 (8.5)
Table 4: Decipher-Pipeline does not show significant improvement over the baseline system. In contrast, Decipher-
Joint using joint word alignment and decipherment approach achieves a Bleu gain of 0.9 and 2.1 on the Global
Voices test set and the web news test set, respectively. The results in brackets are obtained using a parser trained
with only 120 sentences. (GV: Global Voices)
spired to apply this approach to help find translations
for out of vocabulary words, and to explore other pos-
sible ways to improve machine translation with deci-
pherment.
Acknowledgments
This work was supported by NSF Grant 0904684 and
ARO grant W911NF-10-1-0533. The authors would
like to thank David Chiang, Malte Nuhn, Victoria Fos-
sum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and
Hui Zhang (in no particular order) for their comments
and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of the
Twenty-Second International Joint Conference on
Artificial Intelligence - Volume Three. AAAI Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19:263–311.
Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1–
38.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1. Association for Com-
putational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. In Readings in computer vi-
sion: issues, problems, principles, and paradigms.
Morgan Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguis-
tics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion. Association for Computational Linguistics, Au-
gust.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
564
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ann Irvine, Chris Quirk, and Hal Daume III. 2013.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT. Association for Com-
putational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Association for Computational Linguistics.
Patrik Lambert, Adri´a De Gispert, Rafael Banchs, and
Jos´e B. Mari˜no. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267–285.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast third-order non-
projective Turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd an-
nual meeting of Association for Computational Lin-
guistics. Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: Unsupervised word alignment with the l0-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2. Associa-
tion for Computational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language Technologies. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, Columbus,
Ohio. Association for Computational Linguistics.
565
