Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
Deriving lexical and syntactic expectation-based measures
for psycholinguistic modeling via incremental top-down parsing
Brian Roark
†
Asaf Bachrach
‡
Carlos Cardenas
?
and Christophe Pallier
‡
†
Center for Spoken Language Understanding, Oregon Health & Science University
‡
INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France
?
MIT
roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org
Abstract
A number of recent publications have
made use of the incremental output of
stochastic parsers to derive measures of
high utility for psycholinguistic modeling,
following the work of Hale (2001; 2003;
2006). In this paper, we present novel
methods for calculating separate lexical
and syntactic surprisal measures from a
single incremental parser using a lexical-
ized PCFG. We also present an approx-
imation to entropy measures that would
otherwise be intractable to calculate for a
grammar of that size. Empirical results
demonstrate the utility of our methods in
predicting human reading times.
1 Introduction
Assessment of linguistic complexity has played
an important role in psycholinguistics and neu-
rolinguistics for a long time, from the use of
mean length of utterance and related scores in
child language development (Klee and Fitzgerald,
1985), to complexity scores related to reading dif-
ficulty in human sentence processing studies (Yn-
gve, 1960; Frazier, 1985; Gibson, 1998). Opera-
tionally, such linguistic complexity scores are de-
rived via deterministic manual (human) annotation
and scoring algorithms of language samples. Nat-
ural language processing has been employed to
automate the extraction of such measures (Sagae
et al., 2005; Roark et al., 2007), which can have
high utility in terms of reduction of time required
to annotate and score samples. More interest-
ingly, however, novel data driven methods are be-
ing increasingly employed in this sphere, yield-
ing language sample characterizations that require
NLP in their derivation. For example, scores
derived from variously estimated language mod-
els have been used to evaluate and classify lan-
guage samples associated with neurodevelopmen-
tal or neurodegenerative disorders (Roark et al.,
2007; Solorio and Liu, 2008; Gabani et al., 2009),
as well as within general studies of human sen-
tence processing (Hale, 2001; 2003; 2006). These
scores cannot feasibly be derived by hand, but
rather rely on large-scale statistical models and
structured inference algorithms to be derived. This
is quickly becoming an important application of
NLP, making possible new methods in the study
of human language processing in both typical and
impaired populations.
The use of broad-coverage parsing for psy-
cholinguistic modeling has become very popular
recently. Hale (2001) suggested a measure (sur-
prisal) derived from an Earley (1970) parser us-
ing a probabilistic context-free grammar (PCFG)
for psycholinguistic modeling; and in later work
(Hale, 2003; 2006) he suggested an alternate
parser-derived measure (entropy reduction) that
may also account for some human sentence pro-
cessing performance. Recent work continues to
advocate surprisal in particular as a very use-
ful measure for predicting processing difficulty
(Boston et al., 2008a; Boston et al., 2008b; Dem-
berg and Keller, 2008; Levy, 2008), and the mea-
sure has been derived using a variety of incre-
mental (left-to-right) parsing strategies, includ-
ing an Earley parser (Boston et al., 2008a), the
Roark (2001) incremental top-down parser (Dem-
berg and Keller, 2008), and an n-best version of
the Nivre et al. (2007) incremental dependency
parser (Boston et al., 2008a; 2008b). Deriving
such measures by hand, even for a relatively lim-
ited set of stimuli, is not feasible, hence parsing
plays a critical role in this developing psycholin-
guistic enterprise.
There is no single measure that can account for
all of the factors influencing human sentence pro-
cessing performance, and some of the most recent
work on using parser-derived measures for psy-
cholinguistic modeling has looked to try to de-
rive multiple, complementary measures. One of
324
the key distinctions being looked at is syntactic
versus lexical expectations (Gibson, 2006). For
example, in Demberg and Keller (2008), trials
were run deriving surprisal from the Roark (2001)
parser under two different conditions: fully lex-
icalized parsing, and fully unlexicalized parsing
(to pre-terminal part-of-speech tags). Boston et
al. (2008a) capture a similar distinction by mak-
ing use of an unlexicalized PCFG within an Ear-
ley parser and a fully lexicalized unlabeled depen-
dency parser (Nivre et al., 2007). As Demberg and
Keller (2008) point out, fully unlexicalized gram-
mars ignore important lexico-syntactic informa-
tion when deriving the “syntactic” expectations,
such as subcategorization preferences of particular
verbs, which are generally accepted to impact syn-
tactic expectations in human sentence processing
(Garnsey et al., 1997). Demberg and Keller argue,
based on their results, for unlexicalized surprisal
instead of lexicalized surprisal. Here we present a
novel method for deriving separate syntactic and
lexical surprisal measures from a fully lexicalized
incremental parser, to allow for rich probabilistic
grammars to be used to derive either measure, and
demonstrate the utility of this method versus that
of Demberg and Keller in empirical trials.
The use of large-scale lexicalized grammars
presents a problem for using an Earley parser to
derive surprisal or for the calculation of entropy as
Hale (2003; 2006) defines it, because both meth-
ods require matrix inversion of a matrix with di-
mensionality the size of the non-terminal set. With
very large lexicalized PCFGs, the size of the non-
terminal set is too large for tractable matrix in-
version. The use of an incremental, beam-search
parser provides a tractable approximation to both
measures. Incremental top-down and left-corner
parsers have been shown to effectively (and effi-
ciently) make use of non-local features from the
left-context to yield very high accuracy syntactic
parses (Roark, 2001; Henderson, 2003; Collins
and Roark, 2004), and we will use such rich mod-
els to derive our scores.
In addition to teasing apart syntactic and lexical
surprisal (defined explicitly in §3), we present an
approximation to the full entropy that Hale (2003;
2006) used to define the entropy reduction hypoth-
esis. Such an entropy measure is derived via a pre-
dictive step, advancing the parses independently
of the input, as described in §3.3. We also present
syntactic and lexical alternatives for this measure,
and demonstrate the utility of making such a dis-
tinction for entropy as well as surprisal.
The purpose of this paper is threefold. First,
to present a careful and well-motivated decompo-
sition of lexical and syntactic expectation-based
measures from a given lexicalized PCFG. Sec-
ond, to explicitly document methods for calculat-
ing these and other measures from a specific in-
cremental parser. And finally, to present some em-
pirical validation of the novel measures from real
reading time trials. We modified the Roark (2001)
parser to calculate the discussed measures
1
, and
the empirical results in §4 show several things,
including: 1) using a fully lexicalized parser to
calculate syntactic surprisal and entropy provides
higher predictive utility for reading times than
these measures calculated via unlexicalized pars-
ing (as in Demberg and Keller); and 2) syntactic
entropy is a useful predictor of reading time.
2 Notation and preliminaries
A probabilistic context-free grammar (PCFG)
G = (V, T, S
†
, P, ?) consists of a set of non-
terminal variables V ; a set of terminal items
(words) T ; a special start non-terminal S
†
? V ;
a set of rule productions P of the form A ? ?
for A ? V , ? ? (V ? T )
?
; and a function ?
that assigns probabilities to each rule in P such
that for any given non-terminal symbol X ? V ,
?
?
?(X ? ?) = 1.
For a given rule A ? ? ? P , let the func-
tion RHS return the right-hand side of the rule, i.e.,
RHS(A ? ?) = ?. Without loss of generality, we
will assume that for every rule A ? ? ? P , one
of two cases holds: either RHS(A ? ?) ? T or
RHS(A ? ?) ? V
?
. That is, the right-hand side
sequences consist of either (1) exactly one termi-
nal item, or (2) zero or more non-terminals.
Let W ? T
n
be a terminal string of length n,
i.e., W = W
1
. . .W
n
and |W | = n. Let W [i, j]
denote the substring beginning at word W
i
and
ending at word W
j
of the string. Then W
|W |
is the
last word in the string, and W [1, |W |] is the string
as a whole. Adjacent strings represent concate-
nation, i.e., W [1, i]W [i+1, j] = W [1, j]. Thus
W [1, i]w represents the string where W
i+1
= w.
We can define a “derives” relation (denoted?
G
for a given PCFG G) as follows: ?A? ?
G
???
if and only if A ? ? ? P . A string W ? T
?
is in the language of a grammar G if and only
if S
†
+
?
G
W , i.e., a sequence of one or more
derivation steps yields the string from the start
1
The parser version will be made publicly available.
325
non-terminal. A leftmost derivation begins with
S
†
and each derivation step replaces the leftmost
non-terminal A in the yield with some ? such that
A ? ? ? P . For a leftmost derivation S
†
?
?
G
?,
where ? ? (V ? T )
?
, the sequence of deriva-
tion steps that yield ? can be represented as a
tree, with the start symbol S
†
at the root, and the
“yield” sequence ? at the leaves of the tree. A
complete tree has only terminal items in the yield,
i.e., ? ? T
?
; a partial tree has some non-terminal
items in the yield. With a leftmost derivation, the
yield ? = ?? partitions into an initial sequence
of terminals ? ? T
?
followed by a sequence of
non-terminals ? ? V
?
. For a complete derivation,
? = ; for a partial derivation ? ? V
+
, i.e., one or
more non-terminals. Let T (G,W [1, i]) be the set
of complete trees with W [1, i] as the yield of the
tree, given PCFG G.
A leftmost derivation D consists of a sequence
of |D| steps. Let D
i
represent the i
th
step in
the derivation D, and D[i, j] represent the subse-
quence of steps in D beginning with D
i
and end-
ing with D
j
. Note that D
|D|
is the last step in
the derivation, and D[1, |D|] is the derivation as
a whole. Each step D
i
in the derivation is a rule
in G, i.e., D
i
? P for all i. The probability of the
derivation and the corresponding tree is:
?(D) =
m
?
i=1
?(D
i
) (1)
Let D(G,W [1, i]) be the set of all possible left-
most derivations D (with respect to G) such that
RHS(D
|D|
) = W
i
. These are the set of partial left-
most derivations whose last step used a production
with terminal W
i
on the right-hand side. The pre-
fix probability of W [1, i] with respect to G is
PrefixProb
G
(W [1, i]) =
?
D?D(G,W [1,i])
?(D) (2)
From this prefix probability, we can calculate the
conditional probability of each word w ? T in the
terminal vocabulary, given the preceding sequence
W [1, i] as follows:
P
G
(w | W [1, i]) =
PrefixProb
G
(W [1, i]w)
P
w
?
?T
PrefixProb
G
(W [1, i]w
?
)
=
PrefixProb
G
(W [1, i]w)
PrefixProb
G
(W [1, i])
(3)
This, in fact, is precisely the conditional proba-
bility that is used for language modeling for such
applications as speech recognition and machine
translation, which was the motivation for various
syntactic language modeling approaches (Jelinek
and Lafferty, 1991; Stolcke, 1995; Chelba and Je-
linek, 1998; Roark, 2001).
As with language modeling, it is important to
model the end of the string as well, usually with
an explicit end symbol, e.g., </s>. For a string
W [1, i], we can calculate its prefix probability as
shown above. To calculate its complete probabil-
ity, we must sum the probabilities over the set of
complete trees T (G,W [1, i]). In such a way, we
can calculate the conditional probability of ending
the string with </s> given W [1, i] as follows:
P
G
(</s> | W [1, i]) =
?
D?T (G,W [1,i])
?(D)
PrefixProb
G
(W [1, i])
(4)
2.1 Incremental top-down parsing
In this section, we review relevant details of
the Roark (2001) incremental top-down parser,
as configured for use here. As presented in
Roark (2004), the probabilities in the PCFG are
smoothed so that the parser is guaranteed not to
fail due to garden pathing, despite following a
beam search strategy. Hence there is always a non-
zero prefix probability as defined in Eq. 2.
The parser follows a top-down leftmost deriva-
tion strategy. The grammar is factored so that ev-
ery production has either a single terminal item on
the right-hand side or is of the form A ? B A-B,
where A,B ? V and the factored A-B category
can expand to any sequence of children categories
of A that can follow B. This factorization of n-
ary productions continues to nullary factored pro-
ductions, i.e., the end of the original production
A ? B
1
. . . B
n
is signaled with an empty produc-
tion A-B
1
-. . . -B
n
? .
The parser maintains a set of possible connected
derivations, weighted via the PCFG. It uses a beam
search, whereby the highest scoring derivations
are worked on first, and derivations that fall out-
side of the beam are discarded. The reader is re-
ferred to Roark (2001; 2004) for specifics about
the beam search.
The model conditions the probability of each
production on features extracted from the par-
tial tree, including non-local node labels such as
parents, grandparents and siblings from the left-
context, as well as c-commanding lexical items.
Hence this is a lexicalized grammar, though the
incremental nature precludes a general head-first
strategy, rather one that looks to the left-context
for c-commanding lexical items.
To avoid some of the early prediction of struc-
ture, the version of the Roark parser that we used
326
performs an additional grammar transformation
beyond the simple factorization already described
– a selective left-corner transform of left-recursive
productions (Johnson and Roark, 2000). In the
transformed structure, slash categories are used to
avoid predicting left-recursive structure until some
explicit indication of modification is present, e.g.,
a preposition.
The final step in parsing, following the last word
in the string, is to “complete” all non-terminals
in the yield of the tree. All of these open non-
terminals are composite factored categories, such
as S-NP-VP, which are “completed” by rewriting
to . The probability of these  productions is what
allows for the calculation of the conditional prob-
ability of ending the string, shown in Eq. 4.
One final note about the size of the non-terminal
set and the intractability of exact inference for
such a scenario. The non-terminal set not only
includes the original atomic non-terminals of the
grammar, but also any categories created by gram-
mar factorization (S-NP) or the left-corner trans-
form (NP/NP). Additionally, however, to remain
context-free, the non-terminal set must include
categories that incorporate non-local features used
by the statistical model into their label, includ-
ing parents, grandparents and sibling categories in
the left-context, as well as c-commanding lexical
heads. These non-local features must be made lo-
cal by encoding them in the non-terminal labels,
leading to a very large non-terminal set and in-
tractable exact inference. Heavy smoothing is re-
quired when estimating the resulting PCFG. The
benefit of such a non-terminal set is a rich model,
which enables a more peaked statistical distribu-
tion around high quality syntactic structures and
thus more effective pruning of the search space.
The fully connected left-context produced by top-
down derivation strategies provides very rich fea-
tures for the stochastic parsing models. See Roark
(2001; 2004) for discussion of these issues.
We now turn to measures that can be derived
from the parser which may be of use for psycholin-
guistic modeling.
3 Parser and grammar derived measures
3.1 Surprisal
The surprisal at word W
i
is the negative log prob-
ability of W
i
given the preceding words. Using
prefix probabilities, this can be calculated as:
S
G
(W
i
) = ? log
PrefixProb
G
(W [1, i])
PrefixProb
G
(W [1, i? 1])
(5)
Substituting equation 2 into this, we get
S
G
(W
i
) = ? log
?
D?D(G,W [1,i])
?(D)
?
D?D(G,W [1,i?1])
?(D)
(6)
If we are using a beam-search parser, some of the
derivations are pruned away. Let B(G,W [1, i]) ?
D(G,W [1, i]) be the set of derivations in the
beam. Then the surprisal can be approximated as
S
G
(W
i
) ? ? log
?
D?B(G,W [1,i])
?(D)
?
D?B(G,W [1,i?1])
?(D)
(7)
Any pruning in the beam search will result in a de-
ficient probability distribution, i.e., a distribution
that sums to less than 1. Roark’s thesis (2001)
showed that the amount of probability mass lost
for this particular approach is very low, hence this
provides a very tight bound on the actual surprisal
given the model.
3.2 Lexical and Syntactic surprisal
High surprisal scores result when the prefix proba-
bility at word W
i
is low relative to the prefix prob-
ability at word W
i?1
. Sometimes this is due to the
identity of W
i
, i.e., it is a surprising word given
the context. Other times, it may not be the lexical
identity of the word so much as the syntactic struc-
ture that must be created to integrate the word into
the derivations. One would like to tease surprisal
apart into “syntactic surprisal” versus “lexical sur-
prisal”, which would capture this intuition of the
lexical versus syntactic dimensions to the score.
Our solution to this has the beneficial property of
producing two scores whose sum equals the origi-
nal surprisal score.
The original surprisal score is calculated via
sets of partial derivations at the point when each
word W
i
is integrated into the syntactic structure,
D(G,W [1, i]). We then calculate the ratio from
point to point in sequence. To tease apart the lexi-
cal and syntactic surprisal, we will consider sets of
partial derivations immediately before each word
W
i
is integrated into the syntactic structure, i.e.,
D[1, |D|?1] for D ? D(G,W [1, i]). Recall that
the last derivation move for every derivation in the
set is from the POS-tag to the lexical item. Hence
the sequence of derivation moves that excludes the
last one includes all structure except the word W
i
.
Then the syntactic surprisal is calculated as:
SynS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D[1, |D|?1])
P
D?D(G,W [1,i?1])
?(D)
(8)
327
and the lexical surprisal is calculated as:
LexS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D)
P
D?D(G,W [1,i])
?(D[1, |D|?1])
(9)
Note that the numerator of SynS
G
(W
i
) is the de-
nominator of LexS
G
(W
i
), hence they sum to form
total surprisal S
G
(W
i
). As with total surprisal,
these measures can be defined either for the full
set D(G,W [1, i]) or for a pruned beam of deriva-
tions B(G,W [1, i]) ? D(G,W [1, i]).
Finally, we replicated the Demberg and Keller
(2008) “unlexicalized” surprisal by replacing ev-
ery lexical item in the training corpus with its
POS-tag, and then parsing the POS-tags of the lan-
guage samples rather than the words. This differs
from our syntactic surprisal by having no lexical
conditioning events for rule probabilities, and by
having no ambiguity about the POS-tag of the lex-
ical items in the string. We will refer to the result-
ing surprisal measure as “POS surprisal” to distin-
guish it from our syntactic surprisal measure.
3.3 Entropy
Entropy scores of the sort advocated by Hale
(2003; 2006) involve calculation over the set of
complete derivations consistent with the set of par-
tial derivations. Hale performs this calculation
efficiently via matrix inversion, which explains
the use of relatively small-scale grammars with
tractably sized non-terminal sets. Such methods
are not tractable for the kinds of richly condi-
tioned, large-scale PCFGs that we advocate using
here. At each word in the string, the Roark (2001)
top-down parser provides access to the weighted
set of partial analyses in the beam; the set of com-
plete derivations consistent with these is not im-
mediately accessible, hence additional work is re-
quired to calculate such measures.
Let H(D) be the entropy over a set of deriva-
tions D, calculated as follows:
H(D) = ?
X
D?D
?(D)
P
D
?
?D
?(D
?
)
log
?(D)
P
D
?
?D
?(D
?
)
(10)
If the set of derivations D = D(G,W [1, i])
is a set of partial derivations for string W [1, i],
then H(D) is a measure of uncertainty over the
partial derivations, i.e., the uncertainty regarding
the correct analysis of what has already been pro-
cessed. This can be calculated directly from the
existing parser operations. If the set of derivations
are the complete derivations consistent with the set
of partial derivations – complete derivations that
could occur over the set of possible continuations
of the string – then this is a measure of the un-
certainty about what is yet to come. We would
like measures that can capture this distinction be-
tween (a) uncertainty of what has already been
processed (“current ambiguity”) versus (b) uncer-
tainty of what is yet to be processed (“predictive
entropy”). In addition, as with surprisal, we would
like to tease apart the syntactic uncertainty versus
lexical uncertainty.
To calculate the predictive entropy after word
sequence W [1, i], we modify the parser as fol-
lows: the parser extends the set of partial deriva-
tions to include all possible next words (the entire
vocabulary plus </s>), and calculates the entropy
over that set. This measure is calculated from just
one additional word beyond the current word, and
hence is an approximation to Hale’s conditional
entropy of grammatical continuations, which is
over complete derivations. We will denote this as
H
1
G
(W [1, i]) and calculate it as follows:
H
1
G
(W [1, i]) = H(
?
w?T?{</s>}
D(G,W [1, i]w)) (11)
This is performing a predictive step that the base-
line parser does not perform, extending the parses
to all possible next words.
Unlike surprisal, entropy does not decompose
straightforwardly into syntactic and lexical com-
ponents that sum to the original composite mea-
sure. To tease apart entropy due to syntactic un-
certainty versus that due to lexical uncertainty, we
can define the set of derivations up to the pre-
terminal (POS-tag) non-terminals as follows. Let
S(D) = {D[1, |D|?1] : D ? D}, i.e., the set of
derivations achieved by removing the last step of
all derivations inD. Then we can calculate a “syn-
tactic” H
1
G
as follows:
SynH
1
G
(W [1, i]) = H(
[
w?T?{</s>}
S(D(G,W [1, i]w))) (12)
Finally, “lexical” H
1
G
is defined in terms of the
conditional probabilities derived from prefix prob-
abilities as defined in Eq. 3.
LexH
1
G
(W [1, i]) =
?
X
w?T?{</s>}
P
G
(w | W [1, i]) logP
G
(w | W [1, i]) (13)
As a practical matter, these values are calculated
within the Roark parser as follows. A “dummy”
word is created that can be assigned every POS-
tag, and the parser extends from the current state to
this dummy word. (The beam threshold is greatly
328
expanded to allow for many possible extensions.)
Then every word in the vocabulary is substituted
for the word, and the appropriate probabilities cal-
culated over the beam. Finally, the actual next
word is substituted, the beam threshold is reduced
to the actual working threshold, and the requisite
number of analyses are advanced to continue pars-
ing the string. This represents a significant amount
of additional work for the parser – particularly for
vocabulary sizes that we currently use, on the or-
der of tens of thousands of words.
As with surprisal, we can calculate an “unlex-
icalized” version of the measure by training and
parsing just to POS-tags. We will refer to this sort
of entropy as “POS entropy”.
4 Empirical validation
4.1 Subjects and stimuli
In order to test the psycholinguistic relevance of
the different measures produced by the parser, we
conducted a word by word reading experiment.
23 native speakers of English read 4 short texts
(mean length: 883.5 words, 49.25 sentences). The
texts were the written versions of narratives used
in a parallel fMRI experiment making use of the
same parser derived measures and whose results
will be published in a different paper (Bachrach et
al., 2009). The narratives contained a high density
of syntactically complex structures (in the form of
sentential embeddings, relative clauses and other
non-local dependencies) but were constructed so
as to appear highly natural. The modified version
of the Roark parser, trained on the Brown Cor-
pus section of the Penn Treebank (Marcus et al.,
1993), was used to parse the different narratives
and produce the word by word measures.
4.2 Procedure
Each narrative was presented line by line (cer-
tain sentences required more than one line) on a
computer screen (Dell Optiplex 755 running Win-
dows XP Professional) using Linger 2.88
2
. Each
line contained 11.5 words on average. Each word
would appear in its relative position on the screen.
The subject would then be required to push a key-
board button to advance to the next word. The
original word would then disappear and the fol-
lowing word appear in the subsequent position on
the screen. After certain sentences a comprehen-
sion question would appear on the screen (10 per
narrative). This was done in order to encourage
2
http://tedlab.mit.edu/?dr/Linger/readme.html
subjects to pay attention and to provide data for a
post-hoc evaluation of comprehension. After each
narrative, subjects were instructed to take a short
break (2 minutes on average).
4.3 Data analysis
The log (base 10) of the reaction times were ana-
lyzed using a linear mixed effects regression anal-
ysis implemented in the language R (Bates et al.,
2008). Reaction times longer than 1500 ms and
shorter than 150 ms (raw) were excluded from the
analysis (4.8% of total data). Since button press la-
tencies inferior to 150 ms must have been planned
prior to the presentation of the word, we consid-
ered that they could not reflect stimulus driven ef-
fects. Data from the first and last words on each
line were discarded.
The combined data from the 4 narratives was
first modeled using a model which included or-
der of word in the narrative
3
, word length, parser-
derived lexical surprisal, unigram frequency, bi-
gram probability, syntactic surprisal, lexical en-
tropy, syntactic entropy and mean number of
parser derivation steps as numeric regressors. We
also included the unlexicalized POS variants of
syntactic surprisal and entropy, along the lines of
Demberg and Keller (2008), as detailed in § 3.
Table 1 presents the correlations between these
mean-centered measures.
In addition, we modeled word class
(open/closed) as a categorical factor in order
to assess interaction between class and the vari-
ables of interest, since such an interaction has
been observed in the case of frequency (Bradley,
1983). Finally, the random effect part of the
model included intercepts for subjects, words and
sentences. We report significant effects at the
threshold p < .05.
Given the presence of significant interactions
between lexical class (open/closed) and a number
of the variables of interests, we decided to split
the data set into open and closed class words and
model these separately (linear mixed effects with
the same numeric variables as in the full model).
In order to evaluate the usefulness of splitting
total surprisal into lexical and syntactic compo-
nents we compared, using a likelihood ratio test,
a model where lexical and syntactic surprisal are
modeled as distinct regressors to a model where a
single regressor equal to their sum (total surprisal)
3
This is a regressor to control for the trend of subjects to
read faster later in the narrative.
329
Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLen
Syntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18
Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29
Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03
Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64
Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72
Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56
POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02
POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11
Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24
Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00
Table 1: Correlations between (mean-centered) predictors. Note that unigram frequencies were represented as logs, other
scores as negative logs, hence the sign of the correlations.
was included. If the larger model provides a sig-
nificantly better fit than the smaller model, this
provides evidence that distinguishing between lex-
ical and syntactic contributions to surprisal is rel-
evant. Since total entropy is not a sum of syntactic
and lexical entropy, an analogous test would not
be valid in that case.
4.4 Results
All subjects successfully answered the com-
prehension questions (92.8% correct responses,
S.D.=5.1). In the full model, we observed signifi-
cant main effects of word class as well as of lexical
surprisal, bigram probability, unigram frequency,
syntactic entropy, POS entropy and of order in the
narrative. Syntactic surprisal, lexical entropy and
number of steps had no significant effect. Word
length also had no significant main effect but inter-
acted significantly with word class (open/closed).
Word class also interacted significantly with lexi-
cal surprisal, unigram frequency and syntactic sur-
prisal.
The presence of these interactions led us to
construct models restricted to open and closed
class items respectively. The estimated parame-
ters are reported in Table 2. Reading time for open
class words showed significant effects of unigram
frequency, syntactic surprisal, syntactic entropy,
POS entropy and order within the narrative. The
positive effect of length approached significance.
Reading time for closed class words exhibited sig-
nificant effects of lexical surprisal, bigram prob-
ability, syntactic entropy and order in the narra-
tive. Length had a non-significant negative effect,
thus explaining the interaction observed in the full
model.
The models with separate lexical and syntac-
tic surprisal performed better than models includ-
ing combined surprisal. For open class words, the
Akaike’s information criterion (AIC) was -54810
for the combined model and -54819 for the inde-
pendent model (likelihood ratio test comparing the
Estimate Std. Error t-value
Open-class
(Intercept) 2.40×10
+00
2.39×10
?02
100.4*
Lexical Surprisal -1.99×10
?04
7.28×10
?04
-0.3
Word Length 8.97×10
?04
4.62×10
?04
1.9
Bigram 4.18×10
?04
5.27×10
?04
0.8
Unigram Freq -2.43×10
?03
1.20×10
?03
-2.0*
Derivation Steps -1.17×10
?03
9.02×10
?04
-1.3
Syntactic Entropy 2.55×10
?03
6.19×10
?04
4.1*
Lexical Entropy 3.96×10
?04
6.68×10
?04
0.6
Syntactic Surprisal 3.28×10
?03
9.71×10
?04
3.4*
Order in narrative -1.43×10
?05
4.34×10
?06
-3.3*
POS Surprisal -6.84×10
?04
8.11×10
?04
-0.8
POS Entropy 1.47×10
?03
6.05×10
?04
2.4*
Closed-class
(Intercept) 2.42×10
+00
2.32×10
?02
104.3*
Lexical Surprisal 2.02×10
?03
7.84×10
?04
2.6*
Word Length -1.87×10
?03
1.13×10
?03
-1.7
Bigram 1.19×10
?03
4.94×10
?04
2.4*
Unigram Freq 1.69×10
?03
2.67×10
?03
0.6
Derivation Steps 3.01×10
?04
5.09×10
?04
0.6
Syntactic Entropy 3.15×10
?03
5.05×10
?04
6.2*
Lexical Entropy 1.83×10
?04
8.63×10
?04
0.2
Syntactic Surprisal 3.00×10
?04
8.35×10
?04
0.4
Order in narrative -1.33×10
?05
3.99×10
?06
-3.3*
POS Surprisal -6.46×10
?04
6.81×10
?04
-0.9
POS Entropy 6.63×10
?04
5.04×10
?04
1.3
Table 2: Estimated effects from mixed effects models on
open and closed items (stars denote significance at p<.05)
two, nested, models: ?
2
(1)=10.7, p<.001). For
closed class items, combined model’s AIC was -
61467 and full model’s AIC was -61469 (likeli-
hood ratio test: ?
2
(1)=3.54, p=0.06).
4.5 Discussion
Our results demonstrate the relevance of model-
ing psycholinguistic processes using an incremen-
tal probabilistic parser, and the utility of the novel
measures presented here. Of particular interest
are: the significant effects of our syntactic en-
tropy measure; the independent contributions of
lexical surprisal, bigram probability and unigram
frequency; and the differences between the pre-
dictions of the lexicalized parsing model and the
unlexicalized (POS) parsing model.
The effect of entropy, or uncertainty regarding
330
the upcoming input independent of the surprise
of that input, has been observed in non-linguistic
tasks (Hyman, 1953; Bestmann et al., 2008) but
to our knowledge has not been quantified before
in the context of sentence processing. The use-
fulness of computational modeling is particularly
evident in the case of entropy given the absence of
any subjective procedure for its evaluation
4
. The
results argue in favor of a predictive parsing archi-
tecture (Van Berkum et al., 2005). The approach
to entropy here differs from the one described in
Hale (2006) in a couple of ways. First, as dis-
cussed above, the calculation procedure is differ-
ent – we focus on extending the derivations with
just one word, rather than to all possible complete
derivations. Second, and most importantly, Hale
emphasizes entropy reduction (or the gain in in-
formation, given an input, regarding the rest of the
sentence) as the correlate of cognitive cost while
here we are interested in the amount of entropy it-
self (and not the size of change).
Interestingly, we observed only an effect of syn-
tactic entropy, not lexical entropy. Recent ERP
work has demonstrated that subjects do form spe-
cific lexical predictions in the context of sentence
processing (Van Berkum et al., 2005; DeLong et
al., 2005) and so we suspect that the absence of
lexical entropy effect might be partly due to sparse
data. Lexical surprisal and entropy were calcu-
lated using the internal state of a parser trained
on the relatively small Brown corpus. Lexical en-
tropy showed no significant effect while lexical
surprisal affected only closed class words. This
pattern of results might be due to the sparseness
of the relevant information in such a small corpus
(e.g., verb/object preferences) and the relevance of
extra-textual dimensions (world knowledge, con-
textual information) to lexical-specific prediction.
Closed class words are both more frequent (and
hence better sampled) and are less sensitive to
world knowledge, yet are often determined by the
grammatical context.
Demberg and Keller (2008) made use of the
same parsing architecture used here to compute a
syntactic surprisal measure, but used an unlexical-
ized parser (down to POS-tags rather than words)
for this score. Their “lexicalized” surprisal is
equivalent to our total surprisal (lexical surprisal
+ syntactic surprisal), while their POS surprisal is
4
The Cloze procedure (Taylor, 1953) is one way to derive
probabilities that could be used to calculate entropy, though
this procedure is usually conducted with lexical elicitation,
which would make syntactic entropy calculations difficult.
derived from a completely different model. In con-
trast, our approach achieves lexical and syntactic
measures from the same model. In order to eval-
uate the difference between the two approaches
we added unlexicalized POS surprisal calculated
along the lines of that paper to our model, along
with an unlexicalized POS entropy from the same
model. We found no effect of unlexicalized POS
surprisal
5
and a significant (but relatively small)
effect of unlexicalized POS entropy. While syn-
tactic surprisal was correlated with POS surprisal
(see Table 1) and syntactic entropy correlated with
POS entropy, the fact that our syntactic measures
still had a significant effect suggests that lexical
information contributes towards the formation of
syntactic expectations.
While the effect of surprisal calculated by an
incremental top down parser has been already
demonstrated (Demberg and Keller, 2008), our re-
sults argue for a distinction between the effect
of lexical surprisal and that of syntactic surprisal
without requiring unlexicalized parsing of the sort
that Demberg and Keller advocate. It is important
to keep in mind that this distinction between types
of prediction (and as a consequence, prediction er-
ror) is not equivalent to the one drawn in the tradi-
tional cognitive science modularity debate, which
has focused on the source of these predictions. We
found a positive effect of syntactic surprisal in the
case of open class words. The absence of an effect
for closed class words remains to be explained.
We quantified word specific surprisal using 3
sources: the parser’s internal state (lexical sur-
prisal); probability given the preceding word (neg-
ative log bigram probability); and the unigram fre-
quency of the word in a large corpus
6
. As can
be observed in Table 1, these three measures are
highly correlated
7
. This is the consequence of the
smoothing in the estimation procedure but also re-
lates to a more general fact about language use:
overall, more frequent words are also words more
expected to appear in a specific context (Anderson
and Schooler, 1991). Despite these strong corre-
lations, the three measures produced independent
5
We also ran the model including unlexicalized POS sur-
prisal without our syntactic surprisal or syntactic entropy, and
in this condition the unlexicalized POS surprisal measure had
a nearly significant effect (t = 1.85), which is consistent with
the results in Boston et al. (2008a) and Demberg and Keller
(2008).
6
The unigram frequencies came from the HAL corpus
(Lund and Burgess, 1996). All other statistical models were
estimated from the Brown Corpus.
7
Unigram frequencies were represented as logs, the others
as negative logs, hence the sign of the correlations.
331
effects. Unigram frequency had a significant effect
for open class words while bigram probability and
lexical surprisal each had an effect on reading time
of closed class items. Bigram probability has been
often found to affect reading time using eye move-
ment measures. This is the first study to demon-
strate an additional effect of contextual surprisal
given the preceding sentential context (lexical sur-
prisal). Demberg and Keller found no effect for
surprisal once bigram and unigram probabilities
were included in the model but, importantly, they
did not distinguish lexical and syntactic surprisal,
rather “lexicalized” and “unlexicalized” surprisal.
5 Summary
We have presented novel methods for teasing apart
syntactic and lexical surprisal from a fully lexi-
calized parser, as well as for extending the oper-
ation of a predictive parser to capture novel en-
tropy measures that are also shown to be rele-
vant to psycholinguistic modeling. Such auto-
matic methods provide psycholinguistically rele-
vant measures that are intractable to calculate by
hand. The empirical validation presented here
demonstrated that the new measures – particularly
syntactic entropy and syntactic surprisal – have
high utility for modeling human reading time data.
Our approach to calculating syntactic surprisal,
based on fully lexicalized parsing, provided sig-
nificant effects, while the POS-tag based (unlexi-
calized) surprisal – of the sort used in Boston et
al. (2008a) and Demberg and Keller (2008) – did
not provide a significant effect in our trials. Fur-
ther, we showed an effect of lexical surprisal for
closed class words even when combined with uni-
gram and bigram probabilities in the same model.
This work contributes to the important, develop-
ing enterprise of leveraging data-driven NLP ap-
proaches to derive new measures of high utility for
psycholinguistic and neuropsychological studies.
Acknowledgments
Thanks to Michael Collins, John Hale and Shravan
Vasishth for valuable discussions about this work.
This research was supported in part by NSF Grant
#BCS-0826654. Any opinions, findings, conclu-
sions or recommendations expressed in this publi-
cation are those of the authors and do not neces-
sarily reflect the views of the NSF.
References
J.R. Anderson and L.J. Schooler. 1991. Reflections of
the environment in memory. Psychological Science,
2(6):396–408.
A. Bachrach, B. Roark, A. Marantz, S. Whitfield-
Gabrieli, C. Cardenas, and J.D.E. Gabrieli. 2009.
Incremental prediction in naturalistic language pro-
cessing: An fMRI study. In preparation.
D. Bates, M. Maechler, and B. Dai, 2008. lme4: Linear
mixed-effects models using S4 classes. R package
version 0.999375-20.
S. Bestmann, L.M. Harrison, F. Blankenburg, R.B.
Mars, P. Haggard, and K.J. Friston. 2008. Influence
of uncertainty and surprise on human corticospinal
excitability during preparation for action. Current
Biology, 18:775–780.
M. Ferrara Boston, J.T. Hale, R. Kliegl, U. Patil, and
S. Vasishth. 2008a. Parsing costs as predictors
of reading difficulty: An evaluation using the Pots-
dam sentence corpus. Journal of Eye Movement Re-
search, 2(1):1–12.
M. Ferrara Boston, J.T. Hale, R. Kliegl, and S. Va-
sishth. 2008b. Surprising parser actions and read-
ing difficulty. In Proceedings of ACL-08:HLT, Short
Papers, pages 5–8.
D.C. Bradley. 1983. Computational Distinctions of
Vocabulary Type. Indiana University Linguistics
Club, Bloomington.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In Proceedings of
ACL-COLING, pages 225–231.
M.J. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of
ACL, pages 111–118.
K.A. DeLong, T.P. Urbach, and M. Kutas. 2005. Prob-
abilistic word pre-activation during language com-
prehension inferred from electrical brain activity.
Nature Neuroscience, 8(8):1117–1121.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451–455.
L. Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen, and A.M. Zwicky, editors,
Natural Language Parsing. Cambridge University
Press, Cambridge, UK.
K. Gabani, M. Sherman, T. Solorio, and Y. Liu.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of NAACL-HLT.
S.M. Garnsey, N.J. Pearlmutter, E. Myers, and M.A.
Lotocky. 1997. The contributions of verb bias and
plausibility to the comprehension of temporarily am-
biguous sentences. Journal of Memory and Lan-
guage, 37(1):58–93.
332
E. Gibson. 1998. Linguistic complexity: locality of
syntactic dependencies. Cognition, 68(1):1–76.
E. Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic
category ambiguity. Journal of Memory and Lan-
guage, 54(3):363–388.
J.T. Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
J.T. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.
J.T. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643–672.
J. Henderson. 2003. Inducing history representations
for broad coverage statistical parsing. In Proceed-
ings of HLT-NAACL, pages 24–31.
R. Hyman. 1953. Stimulus information as a determi-
nant of reaction time. Journal of Experimental Psy-
chology: General, 45(3):188–96.
F. Jelinek and J. Lafferty. 1991. Computation of
the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315–323.
M. Johnson and B. Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner
transform and factoring. In Proceedings of COL-
ING, pages 355–361.
T. Klee and M.D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length
of utterance in morphemes. Journal of Child Lan-
guage, 12:251–269.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28:203–208.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95–135.
B. Roark, M. Mitchell, and K. Hollingshead. 2007.
Syntactic complexity measures for detecting mild
cognitive impairment. In Proceedings of BioNLP
Workshop at ACL, pages 1–8.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1–24.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Au-
tomatic measurement of syntactic development in
child langugage. In Proceedings of ACL, pages 197–
204.
T. Solorio and Y. Liu. 2008. Using language models
to identify language impairment in Spanish-English
bilingual children. In Proceedings of BioNLP Work-
shop at ACL, pages 116–117.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):165–202.
W.L. Taylor. 1953. Cloze procedure: A new tool
for measuring readability. Journalism Quarterly,
30:415–433.
J.J.A. Van Berkum, C.M. Brown, P. Zwitserlood,
V.Kooijman, and P. Hagoort. 2005. Anticipat-
ing upcoming words in discourse: Evidence from
ERPs and reading times. Learning and Memory,
31(3):443–467.
V.H. Yngve. 1960. A model and an hypothesis for lan-
guage structure. Proceedings of the American Philo-
sophical Society, 104:444–466.
333
