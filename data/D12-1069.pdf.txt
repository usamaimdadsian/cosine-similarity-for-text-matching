Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 754–765, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Weakly Supervised Training of Semantic Parsers
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present a method for training a semantic
parser using only a knowledge base and an un-
labeled text corpus, without any individually
annotated sentences. Our key observation is
that multiple forms of weak supervision can be
combined to train an accurate semantic parser:
semantic supervision from a knowledge base,
and syntactic supervision from dependency-
parsed sentences. We apply our approach
to train a semantic parser that uses 77 rela-
tions from Freebase in its knowledge repre-
sentation. This semantic parser extracts in-
stances of binary relations with state-of-the-
art accuracy, while simultaneously recovering
much richer semantic structures, such as con-
junctions of multiple relations with partially
shared arguments. We demonstrate recovery
of this richer structure by extracting logical
forms from natural language queries against
Freebase. On this task, the trained semantic
parser achieves 80% precision and 56% recall,
despite never having seen an annotated logical
form.
1 Introduction
Semantic parsing converts natural language state-
ments into logical forms in a meaning repre-
sentation language. For example, the phrase
“town in California” might be represented as
?x.CITY(x) ? LOCATEDIN(x,CALIFORNIA), where
CITY, LOCATEDIN and CALIFORNIA are predicates
and entities from a knowledge base. The expressiv-
ity and utility of semantic parsing is derived from
this meaning representation, which is essentially a
program that is directly executable by a computer.
In this sense, broad coverage semantic parsing is the
goal of natural language understanding.
Unfortunately, due to data annotation constraints,
modern semantic parsers only operate in narrow do-
mains. The best performing semantic parsers are
trained using extensive manual annotation: typi-
cally, a number of sentences must be annotated with
their desired logical form. Although other forms of
supervision exist (Clarke et al., 2010; Liang et al.,
2011), these methods similarly require annotations
for individual sentences. More automated training
methods are required to produce semantic parsers
with richer meaning representations.
This paper presents an algorithm for training a se-
mantic parser without per-sentence annotations. In-
stead, our approach exploits two easily-obtainable
sources of supervision: a large knowledge base and
(automatically) dependency-parsed sentences. The
semantic parser is trained to identify relation in-
stances from the knowledge base while simulta-
neously producing parses that syntactically agree
with the dependency parses. Combining these two
sources of supervision allows us to train an accurate
semantic parser for any knowledge base without an-
notated training data.
We demonstrate our approach by training a Com-
binatory Categorial Grammar (CCG) (Steedman,
1996) that parses sentences into logical forms con-
taining any of 77 relations from Freebase. Our
training data consists of relation instances from
Freebase and automatically dependency-parsed sen-
tences from a web corpus. The trained semantic
parser extracts binary relations with state-of-the-art
performance, while recovering considerably richer
semantic structure. We demonstrate recovery of this
semantic structure using natural language queries
754
town
N : ?x.CITY(x)
Lex
in
(N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Lex California
N : ?x.x = CALIFORNIA
Lex
N\N : ?g.?x.?y.y = CALIFORNIA ? g(x) ? LOCATEDIN(x, y)
>
N : ?x.?y.y = CALIFORNIA ? CITY(x) ? LOCATEDIN(x, y)
<
Figure 1: An example parse of “town in California” using the example CCG lexicon. The first stage in parsing
retrieves a category from each word from the lexicon, represented by the “Lex” entries. The second stage applies CCG
combination rules, in this case both forms of function application, to combine these categories into a semantic parse.
against Freebase. Our weakly-supervised semantic
parser predicts the correct logical form for 56% of
queries, despite never seeing a labeled logical form.
This paper is structured as follows. We first pro-
vide some background information on CCG and the
structure of a knowledge base in Section 2. Section
3 formulates the weakly supervised training prob-
lem for semantic parsers and presents our algorithm.
Section 4 describes how we applied our algorithm to
construct a semantic parser for Freebase, and Sec-
tion 5 presents our results. We conclude with related
work and discussion.
2 Background
2.1 Combinatory Categorial Grammar
Combinatory Categorial grammar (CCG) is a lin-
guistic formalism that represents both the syntax and
semantics of language (Steedman, 1996). CCG is a
lexicalized formalism that encodes all grammatical
information in a lexicon ?. This lexicon contains
syntactic and semantic categories for each word. A
lexicon may include entries such as:
town := N : ?x.CITY(x)
California := N : ?x.x = CALIFORNIA
in := (N\N)/N : ?f.?g.?x.
?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Each entry of the lexicon w := s : l maps a word or
short phrase w to a syntactic category s and a logical
form l. Syntactic categories s may be atomic (N ) or
complex (N\N ). Logical forms l are lambda calcu-
lus expressions constructed using predicates from a
knowledge base. These logical forms combine dur-
ing parsing to form a complete logical form for the
parsed text.
Parses are constructed by combining adjacent cat-
egories using several combination rules, such as for-
ward (>) and backward (<) application:
X/Y : f Y : g =? X : f(g) (>)
Y : g X\Y : f =? X : f(g) (<)
These rules mean that the complex categoryX/Y
(X\Y ) behaves like a function which accepts an ar-
gument of type Y on its right (left) and returns a
value of type X . Parsing amounts to sequentially
applying these two rules, as shown in Figure 1. The
result of parsing is an ordered pair, containing both
a syntactic parse tree and an associated logical form.
We refer to such an ordered pair as a semantic parse,
or by using the letter `.
Given a lexicon, there may be multiple seman-
tic parses ` for a given phrase w. Like context-free
grammars (CFGs), CCGs can be extended to repre-
sent a probability distribution over parses P (`|w; ?)
where ? is a parameter vector.
2.2 Knowledge Base
The main input to our system is a propositional
knowledge base K = (E,R,C,?), containing
entities E, categories C, relations R and relation
instances ?. Categories and relations are pred-
icates which operate on entities and return truth
values; categories c ? C are one-place predi-
cates (CITY(e)) and relations r ? R are two-
place predicates (LOCATEDIN(e1, e2)). Entities e ?
E represent real-world entities and have a set of
known text names. For example, CALIFORNIA
is an entity whose text names include “Califor-
nia” and “CA.” Relation instances r(e1, e2) ? ?
are facts asserted by the knowledge base, such
as LOCATEDIN(SACRAMENTO,CALIFORNIA). Ex-
amples of such knowledge bases include Freebase
(Bollacker et al., 2008), NELL (Carlson et al.,
2010), and YAGO (Suchanek et al., 2007).
The knowledge base influences the semantic
parser in two ways. First, CCG logical forms are
constructed by combining categories, relations and
entities from the knowledge base with logical con-
nectives; hence, the predicates in the knowledge
base determine the expressivity of the parser’s se-
mantic representation. Second, the known relation
755
instances r(e1, e2) ? ? are used as weak supervi-
sion to train the semantic parser.
3 Weakly Supervised Semantic Parsing
We define weakly supervised semantic parsing as
the following learning problem.
Input:
1. A knowledge base K = (E,R,C,?), as de-
fined above.
2. A corpus of dependency-parsed sentences S.
3. A CCG lexicon ? that produces logical forms
containing predicates from K. Section 4.1 de-
scribes an approach to generate this lexicon.
4. A procedure for identifying mentions of enti-
ties from K in sentences from S. (e.g., simple
string matching).
Output:
1. Parameters ? for the CCG that produce correct
semantic parses ` for sentences s ? S.
This problem is ill-posed without additional as-
sumptions: since the correct logical form for a sen-
tence is never observed, there is no a priori reason
to prefer one semantic parse to another. Our train-
ing algorithm makes two assumptions about correct
semantic parses, which are encoded as weak super-
vision constraints. These constraints make learning
possible by adding an inductive bias:
1. Every relation instance r(e1, e2) ? ? is ex-
pressed by at least one sentence in S (Riedel
et al., 2010; Hoffmann et al., 2011).
2. The correct semantic parse of a sentence s con-
tains a subset of the syntactic dependencies
contained in a dependency parse of s.
Our weakly supervised training uses these con-
straints as a proxy for labeled semantic parses. The
training algorithm has two steps. First, the algo-
rithm constructs a graphical model that contains
both the semantic parser and constant factors en-
coding the above two constraints. This graphical
model is then used to estimate parameters ? for the
semantic parser, essentially optimizing ? to produce
parses that satisfy the weak supervision constraints.
If our assumptions are correct and sufficiently con-
strain the parameter space, then this procedure will
identify parameters for an accurate semantic parser.
3.1 Encoding the Weak Supervision
Constraints
The first step of training constructs a graphical
model containing the semantic parser and two weak
supervision constraints. However, the first weak su-
pervision constraint couples the semantic parses for
every sentence s ? S. Such coupling would result in
an undesirably large graphical model. We therefore
modify this constraint to enforce that every relation
r(e1, e2) is expressed at least once in S(e1,e2) ? S,
the subset of sentences which mention both e1 and
e2. These mentions are detected using the provided
mention-identification procedure.
Figure 2 depicts the graphical model constructed
for training. The semantic constraint couples the ex-
tractions for all sentences S(e1,e2), so the graphical
model is instantiated once per (e1, e2) tuple. The
model has 4 types of random variables and values:
Si = si represents a sentence, Li = `i represents
a semantic parse, Zi = zi represents the satisfac-
tion of the syntactic constraint and Yr = yr repre-
sents the truth value of relation r. Si, Li and Zi are
replicated once for each sentence s ? S(e1,e2), while
Yr is replicated once for each relation type r in the
knowledge base (all r ? R).
For each entity pair (e1, e2), this graphical model
defines a conditional distribution over L,Y,Z given
S. This distribution factorizes as:
p(Y = y,Z = z,L = `|S = s; ?) =
1
Zs
?
r
?(yr, `)
?
i
?(zi, `i, si)?(si, `i; ?)
The factorization contains three replicated fac-
tors. ? represents the semantic parser, which is
parametrized by ? and produces a semantic parse
`i for each sentence si. ? and ? are deterministic
factors representing the two weak supervision con-
straints. We now describe each factor in more detail.
Semantic Parser
The factor ? represents the semantic parser, which
is a log-linear probabilistic CCG using the input lex-
icon ?. Given a sentence s and parameters ?, the
parser defines an unnormalized probability distribu-
tion over semantic parses `, each of which includes
both a syntactic CCG parse tree and logical form.
756
YlocatedIn
?
Y
acquired
?
Y
capitalOf
?
L
1
S
1
?
Z
1
?
L
2
S
2
?
Z
2
?
Figure 2: Factor graph containing the semantic parser
? and weak supervision constraints ? and ?, instanti-
ated for an (e1, e2) tuple occurring in 2 sentences S1 and
S2, with corresponding semantic parses L1 and L2. The
knowledge base contains 3 relations, represented by the
Y variables.
Let f(`, s) represent a feature function mapping se-
mantic parses to vectors of feature values1. The fac-
tor ? is then defined as:
?(s, `; ?) = exp{?T f(`, s)}
If the features f(`, s) factorize according to the
structure of the CCG parse tree, it is possible to
perform exact inference using a CKY-style dynamic
programming algorithm. However, other aspects of
the graphical model preclude exact inference, so we
perform approximate inference using beam search.
Inference is explained in more detail in Section 3.2.
Semantic Constraint
The semantic constraint states that, given an entity
tuple (e1, e2), every relation instance r(e1, e2) ? ?
must be expressed somewhere in S(e1,e2). Further-
more, no semantic parse can express a relation in-
stance which is not in the knowledge base. This con-
straint is identical to the multiple deterministic-OR
constraint used by Hoffmann et al. (2011) to train a
sentential relation extractor.
The graphical model contains a semantic con-
straint factor ? and one binary variable Yr for each
relation r in the knowledge base. Yr represents
whether r(e1, e2) is expressed by any sentence in
S(e1,e2). The ? factor determines whether each se-
mantic parse in ` extracts a relation between e1 and
e2. It then aggregates these sentence-level extrac-
tions using a deterministic OR: if any sentence ex-
tracts r(e1, e2) then Yr = 1. Otherwise, Yr = 0.
1Section 4.3 describes the features used by our semantic
parser for Freebase.
?(Yr, `) =
1 if Yr = 1 ? ?i.EXTRACTS(`i, r, e1, e2)
1 if Yr = 0 ? 6 ?i.EXTRACTS(`i, r, e1, e2)
0 otherwise
The EXTRACTS function determines the relation
instances that are asserted by a semantic parse `.
EXTRACTS(`, r, e1, e2) is true if ` asserts the rela-
tion r(e1, e2) and false otherwise. This function es-
sentially converts the semantic parser into a senten-
tial relation extractor, and its implementation may
depend on the types of logical connectives included
in the lexicon ?. Logical forms in our Freebase se-
mantic parser consist of conjunctions of predicates
from the knowledge base; we therefore define EX-
TRACTS(`, r, e1, e2) as true if `’s logical form con-
tains the clauses r(x, y), x = e1 and y = e2.
Syntactic Constraint
A problem with the semantic constraint is that it
admits a large number of ungrammatical parses. The
syntactic constraint penalizes ungrammatical parses
by encouraging the semantic parser to produce parse
trees that agree with a dependency parse of the same
sentence. Specifically, the syntactic constraint re-
quires the predicate-argument structure of the CCG
parse to agree with the predicate-argument structure
of the dependency parse.
Agreement is defined as a function of each CCG
rule application in `. In the parse tree `, each rule
application combines two subtrees, `h and `c, into a
single tree spanning a larger portion of the sentence.
A rule application is consistent with a dependency
parse t if the head words of `h and `c have a depen-
dency edge between them in t. AGREE(`, t) is true
if and only if every rule application in ` is consistent
with t. This syntactic constraint is encoded in the
graphical model by the ? factors and Z variables:
?(z, `, s) = 1 if z = AGREE(`,DEPPARSE(s))
0 otherwise
3.2 Parameter Estimation
To train the model, a single training example is con-
structed for every tuple of entities (e1, e2). The in-
put to the model is s = S(e1,e2), the set of sentences
757
containing e1 and e2. The weak supervision vari-
ables, y, z, are the output of the model. y is con-
structed by setting yr = 1 if r(e1, e2) ? ?, and 0
otherwise. This setting trains the semantic parser to
extract every true relation instance between (e1, e2)
from some sentence in S(e1,e2), while simultane-
ously avoiding incorrect instances. Finally, z = 1,
to encourage agreement between the semantic and
dependency parses. The training data for the model
is therefore a collection, {(sj , yj , zj)}nj=1, where j
indexes entity tuples (e1, e2).
Training optimizes the semantic parser parame-
ters ? to predict Y = yj ,Z = zj given S = sj . The
parameters ? are estimated by running the structured
perceptron algorithm (Collins, 2002) on the training
data defined above. The structured perceptron al-
gorithm iteratively applies a simple update rule for
each example (sj , yj , zj) in the training data:
`predicted ? arg max
`
max
y,z
p(`, y, z|sj ; ?t)
`actual ? arg max
`
p(`|yj , zj , sj ; ?t)
?
t+1 ? ?t +
?
i
f(`
actual
i , si)
?
?
i
f(`
predicted
i , si)
Each iteration of training requires solving two
maximization problems. The first maximization,
max`,y,z p(`, y, z|s; ?t), is straightforward because y
and z are deterministic functions of `. Therefore,
it is solved by finding the maximum probability as-
signment `, then choosing values for y and z that
satisfy the weak supervision constraints.
The second maximization, max` p(`|y, z, s; ?t), is
more challenging. When y and z are given, the infer-
ence procedure must restrict its search to the parses
` which satisfy these weak supervision constraints.
The original formulation of the ? factors permitted
tractable inference (Hoffmann et al., 2011), but the
EXTRACTS function and the ? factors preclude ef-
ficient inference. We approximate this maximiza-
tion using beam search over CCG parses `. For each
sentence s, we perform a beam search to produce
k = 300 possible semantic parses. We then check
the value of ? for each generated parse and elimi-
nate parses which do not satisfy this syntactic con-
straint. Finally, we apply EXTRACTS to each parse,
then use the greedy approximate inference proce-
dure from Hoffmann et al. (2011) for the ? factors.
4 Building a Grammar for Freebase
We apply the training algorithm from the previous
section to produce a semantic parser for a subset of
Freebase. This section describes details of the gram-
mar we construct for this task, including the con-
struction of the lexicon ?, some extensions to the
CCG parser, and the features used during training.
In this section, we assume access to a knowledge
base K = (E,C,R,?), a corpus of dependency-
parsed sentences S and a procedure for identifying
mentions of entities in sentences.
4.1 Constructing the Lexicon ?
The first step in constructing the semantic parser
is defining a lexicon ?. We construct ? by ap-
plying simple dependency-parse-based heuristics to
sentences in the training corpus. The resulting lex-
icon ? captures a variety of linguistic phenomena,
including verbs, common nouns (“city”), noun com-
pounds (“California city”) and prepositional modi-
fiers (“city in California”).
The first step in lexicon construction is to use the
mention identification procedure to identify all men-
tions of entities in the sentences S. This process
results in (e1, e2, s) triples, consisting of sentences
with two entity mentions. The dependency path be-
tween e1 and e2 in s is then matched against the de-
pendency parse patterns in Table 1. Each matched
pattern adds one or more lexical entries to ?
Each pattern in Table 1 has a corresponding lexi-
cal category template, which is a CCG lexical cate-
gory containing parameters e, c and r that are chosen
at initialization time. Given the triple (e1, e2, s), re-
lations r are chosen such that r(e1, e2) ? ?, and
categories c are chosen such that c(e1) ? ? or
c(e2) ? ?. The template is then instantiated with
every combination of these e, c and r values.
After instantiating lexical categories for each sen-
tence in S, we prune infrequent lexical categories to
improve parser efficiency. This pruning step is re-
quired because the common noun pattern generates
a large number of lexical categories, the majority
of which are incorrect. Therefore, we eliminate all
common noun categories instantiated by fewer than
758
Part of
Dependency Parse Pattern Lexical Category Template
Speech
Proper (name of entity e) w :=N : ?x.x = e
Noun Sacramento Sacramento :=N : ?x.x = SACRAMENTO
Common e1
SBJ
===? [is, are, was, ...]
OBJ
?=== w w :=N : ?x.c(x)
Noun Sacramento is the capital capital :=N : ?x.CITY(x)
Noun e1
NMOD
?===== e2 Type change N : ?x.c(x) to N |N : ?f.?x.?y.c(x) ? f(y) ? r(x, y)
Modifier Sacramento, California N : ?x.CITY(x) to N |N : ?f.?x.?y.CITY(x) ? f(y) ? LOCATEDIN(x, y)
Preposition
e1
NMOD
?===== w
PMOD
?===== e2 w := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? r(x, y)
Sacramento in California in := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? VB*
ADV
?=== w
PMOD
?===== e2 w := PP/N : ?f.?x.f(x)
Sacramento is located in California in := PP/N : ?f.?x.f(x)
Verb
e1
SBJ
===? w*
OBJ
?=== e2 w* := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento governs California governs := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento is located in California is located := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
NMOD
?===== w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? r(x, y)
Sacramento located in California located := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Forms of
(none) w* := (S\N)/N : ?f.?g.?x.g(x) ? f(x)
“to be”
Table 1: Dependency parse patterns used to instantiate lexical categories for the semantic parser lexicon ?. Each
pattern is followed by an example phrase that instantiates it. An * indicates a position that may be filled by multiple
consecutive words in the sentence. e1 and e2 are the entities identified in the sentence, r represents a relation where
r(e1, e2), and c represents a category where c(e1). Each template may be instantiated with multiple values for the
variables e, c, r.
5 sentences in S. The other rules are less fertile, so
we do not need to prune their output.
In addition to these categories, the grammar in-
cludes type-changing rules from N to N |N . These
rules capture noun compounds by allowing nouns to
become functions from nouns to nouns. There are
several such type-changing rules since the resulting
category includes a hidden relation r between the
noun and its modifier (see Table 1). As with lexical
categories, the set of type changing rules included
in the grammar is determined by matching depen-
dency parse patterns to the training data. Similar
rules for noun compounds are used in other CCG
parsers (Clark and Curran, 2007).
The instantiated lexicon represents the semantics
of words and phrases as conjunctions of predicates
from the knowledge base, possibly including exis-
tentially quantified variables and ? expressions. The
syntactic types N and PP are semantically rep-
resented as functions from entities to truth values
(e.g., ?x.CITY(x)), while sentences S are statements
with no ? terms, such as ?x, y.x = CALIFORNIA ?
CITY(y) ? LOCATEDIN(x, y). Variables in the seman-
tic representation (x, y) range over entities from the
knowledge base. Intuitively, the N and PP cate-
gories represent sets of entities, while sentences rep-
resent assertions about the world.
4.2 Extensions to CCG
The semantic parser is trained using sentences from
a web corpus, which contains many out-of-domain
words. As a consequence, many of the words en-
countered during training cannot be represented us-
ing the vocabulary of predicates from the knowl-
edge base. To handle these extraneous words, we
allow the CCG parser to skip words while parsing
a sentence. During parsing, the parser first decides
whether to retrieve a lexical category for each word
in the sentence. The sentence is then parsed as if
only the retrieved lexical categories existed.
4.3 Features
The features f(`, s) for our probabilistic CCG con-
tain two sets of features. The first set contains lexi-
cal features, which count the number of times each
lexical entry is used in `. The second set contains
rule application features, which count the number
of times each combination rule is applied to each
possible set of arguments. An argument is defined
by its syntactic and semantic category, and in some
cases by the lexical entry which created it. We lex-
759
icalize arguments for prepositional phrases PP and
common nouns (initialized by the second rule in Ta-
ble 1). This lexicalization allows the parser to dis-
tinguish between prepositional phrases headed by
different prepositions, as well as between different
common nouns. All other types are distinguished
solely by syntactic and semantic category.
5 Evaluation
In this section, we evaluate the performance of
a semantic parser for Freebase, trained using our
weakly-supervised algorithm. Empirical compari-
son is somewhat difficult because the most compara-
ble previous work – weakly-supervised relation ex-
traction – uses a shallower semantic representation.
Our evaluation therefore has two components: (1) a
binary relation extraction task, to demonstrate that
the trained semantic parser extracts instances of bi-
nary relations with performance comparable to other
state-of-the-art systems, and (2) a natural language
database query task, to demonstrate the parser’s abil-
ity to extract more complex logical forms than bi-
nary relation instances, such as logical expressions
involving conjunctions of multiple categories and re-
lations with partially shared arguments.
5.1 Corpus Construction
Our experiments use a subset of 77 relations2 from
Freebase3 as the knowledge base and a corpus of
web sentences. We constructed the sentence corpus
by first sampling sentences from a web crawl and
parsing them with MaltParser (Nivre et al., 2006).
Long sentences tended to have noisy parses while
also rarely expressing relations, so we discarded
sentences longer than 10 words. Entities were iden-
tified by performing a simple string match between
canonical entity names in Freebase and proper noun
phrases identified by the parser. In cases where a
single noun phrase matched multiple entities, we se-
lected the entity participating in the most relations.
The resulting corpus contains 2.5 million (e1, e2, s)
triples, from which we reserved 10% for validation
and 10% for testing. The validation set was used
to estimate performance during algorithm develop-
2These relations are defined by a set of MQL queries and
potentially traverse multiple relation links.
3http://www.freebase.com
Relation Name
Relation
Sentences
Instances
CITYLOCATEDINSTATE 2951 13422
CITYLOCATEDINCOUNTRY 1696 7904
CITYOFPERSONBIRTH 397 440
COMPANIESHEADQUARTEREDHERE 326 432
MUSICARTISTMUSICIAN 251 291
CITYUNIVERSITIES 239 338
CITYCAPITALOFCOUNTRY 123 2529
HASHUSBAND 103 367
PARENTOFPERSON 85 356
HASSPOUSE 81 461
Table 2: Occurrence statistics for the 10 most frequent
relations in the training data. “Relation Instances” shows
the number of entity tuples (e1, e2) that appear as positive
examples for each relation, and “Sentences” shows the
total number of sentences in which these tuples appear.
ment, while the test set was used to generate the fi-
nal experimental results. All triples for each (e1, e2)
tuple were placed in the same set.
Approximately 1% of the resulting (e1, e2, s)
triples are positive examples, meaning there exists
some relation r where r(e1, e2) ? ?4. To improve
training efficiency and prediction performance, we
subsample 5% of the negative examples for training,
producing a training set of 125k sentences with 27k
positive examples. The validation and test sets retain
the original positive/negative ratio. Table 2 shows
some statistics of the most frequent relations in the
test set.
5.2 Relation Extraction
The first experiment measures the semantic parser’s
ability to extract relations from sentences in our web
corpus. We compare our semantic parser to MUL-
TIR (Hoffmann et al., 2011), which is a state-of-
the-art weakly supervised relation extractor. This
method uses the same weak supervision constraint
and parameter estimation procedure, but replaces the
semantic parser by a linear classifier. The features
for this classifier include the dependency path be-
tween the entity mentions, the type of each mention,
and the intervening context (Mintz et al., 2009).
Both the semantic parser and MULTIR were
trained by running 5 iterations of the structured per-
4Note that the positive/negative ratio was much lower with-
out the length filter or entity disambiguation, which is partly
why filtering was performed.
760
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 3: Aggregate precision as a function of recall, for
MULTIR (Hoffman et al., 2011) and our three semantic
parser variants.
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 600 1200 1800 2400 3000
0
0.2
0.4
0.6
0.8
1.0
Figure 4: Sentential precision as a function of the ex-
pected number of correct extractions for MULTIR (Hoff-
man et al., 2011) and our three semantic parser variants.
ceptron algorithm5. At test time, both models pre-
dicted a relation r ? R or NONE for each (e1, e2, s)
triple in the test set. The parser parses the sen-
tence without considering the entities marked in the
sentence, then applies the EXTRACTS function de-
fined in Section 3.1 to identify a relation between e1
and e2. We compare three versions of the semantic
parser: PARSE, which is the basic semantic parser,
PARSE+DEP which additionally observes the cor-
rect dependency parse at test time, and PARSE-DEP
which is trained without the syntactic constraint.
Note that MULTIR uses the sentence’s dependency
parse to construct its feature vector.
Our evaluation considers two performance mea-
sures: aggregate and sentential precision/recall. Ag-
gregate precision takes the union of all extracted re-
lation instances r(e1, e2) from the test corpus and
compares these instances to Freebase. To pro-
5The structured perceptron algorithm does not converge to a
parameter estimate, and we empirically found that performance
did not improve beyond 5 iterations.
MULTIR
PARSE+DEP
PARSE
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 5: Aggregate precision as a function of recall,
ignoring the two most frequent relations, CITYLOCATE-
DINSTATE and CITYLOCATEDINCOUNTRY.
duce a precision/recall curve, each extracted in-
stance r(e1, e2) is assigned the maximum score over
all sentences which extracted it. This metric is easy
to compute, but may be inaccurate due to inaccura-
cies and missing relations in Freebase.
Sentential precision computes the precision of ex-
tractions on individual (e1, e2, s) tuples. This met-
ric is evaluated by manually sampling and evaluat-
ing 100 test sentences from which a relation was ex-
tracted per model. Unfortunately, it is difficult to
compute recall for this metric, since the true number
of sentences expressing relations is unknown. We
instead report precision as a function of the expected
number of correct extractions, which is directly pro-
portional to recall.
Figure 3 displays aggregate precision/recall and
Figure 4 displays sentential precision/recall for all
4 models. Generally, PARSE behaves like MUL-
TIR with somewhat lower recall. In the sentential
evaluation, PARSE+DEP outperforms both PARSE
and MULTIR. The difference between PARSE+DEP’s
aggregate and sentential precision stems from the
fact that PARSE+DEP extracts each relation instance
from more sentences than either MULTIR or PARSE.
PARSE-DEP has the worst performance in both eval-
uations, suggesting the importance of syntactic su-
pervision. Precision in the aggregate experiment is
low partially due to examples with incorrect entity
disambiguation.
We found that the skewed distribution of relation
types hides interesting differences between the mod-
els. Therefore, we include Figure 5 comparing our
syntactically-supervised parsers to MULTIR, ignor-
ing the two most frequent relations (which together
761
make up over half of all relation instances). Both
PARSE and PARSE+DEP are considerably more pre-
cise than MULTIR on these less frequent relations
because their compositional meaning representation
shares parameter strength between relations. For
example, the semantic parsers learn that “in” often
combines with a city to form a prepositional phrase;
the parsers can apply this knowledge to identify city
arguments of any relation. However, MULTIR is ca-
pable of higher recall, since its dependency parse
features can represent syntactic dependencies that
cannot be represented by our semantic parsers. This
limitation is a consequence of our heuristic lexicon
initialization procedure, and could be rectified by a
more flexible initialization procedure.
5.3 Natural Language Database Queries
The second experiment measures our trained
parser’s ability to correctly translate natural lan-
guage queries into logical queries against Freebase.
To avoid biasing the evaluation, we constructed
a test corpus of natural language queries in a data-
driven fashion. We searched the test data for sen-
tences with two related entities separated by an “is
a” expression. The portion of the sentence before the
“is a” expression was discarded and the remainder
retained as a candidate query. For example “Jesse is
an author from Austin, Texas,” was converted into
the candidate query “author from Austin, Texas.”
Each candidate query was then annotated with a log-
ical form using categories and relations from the
knowledge base; candidate queries without satisfac-
tory logical forms were discarded. We annotated 50
validation and 50 test queries in this fashion. The
validation set was used to estimate performance dur-
ing algorithm development and the test set was used
to generate the final results. Example queries with
their annotated logical forms are shown in Table 3.
Table 4 displays the results of the query evalua-
tion. For this evaluation, we forced the parser to in-
clude every word of the query in the parse. Precision
is the percentage of successfully parsed queries for
which the correct logical form was predicted. Re-
call is the percentage of all queries for which the
correct logical form was predicted. This evalua-
tion demonstrates that the semantic parser success-
fully interprets common nouns and identifies mul-
tiple relations with shared arguments. The perfor-
Example Query Logical Form
capital of Russia ?x.CITYCAPITALOFCOUNTRY(x, RUSSIA)
wife of Abraham ?x.HASHUSBAND(x,ABRAHAM)
vocalist from ?x.MUSICIAN(x)?
London, England PERSONBORNIN(x, LONDON)?
CITYINCOUNTRY(LONDON, ENGLAND)
home of ?x.HEADQUARTERS(CONOCOPHILLIPS, x)
ConocoPhillips ?CITYINCOUNTRY(x, CANADA)
in Canada
Table 3: Example natural language queries and their cor-
rect annotated logical form.
Precision Recall
PARSE 0.80 0.56
PARSE-DEP 0.45 0.32
Table 4: Precision and recall for predicting logical forms
of natural language queries against Freebase. The table
compares PARSE, trained with syntactic supervision to
PARSE-DEP, trained without syntactic supervision.
mance difference between PARSE and PARSE-DEP
also demonstrates the benefit of including syntactic
supervision.
Examining the system output, we find two ma-
jor sources of error. The first is missing lexical cat-
egories for uncommon words (e.g., “ex-guitarist”),
which negatively impact recall by making some
queries unparsable. The second is difficulty distin-
guishing between relations with similar type signa-
tures, such as CITYLOCATEDINCOUNTRY and CITY-
CAPITALOFCOUNTRY.
6 Related Work
There are many approaches to supervised seman-
tic parsing, including inductive logic programming
(Zelle and Mooney, 1996), probabilistic and syn-
chronous grammars (Ge and Mooney, 2005; Wong
and Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008), and automatically learned transformation
rules (Kate et al., 2005). This work most closely
follows the work on semantic parsing using CCG
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Kwiatkowski et al., 2010). These su-
pervised systems are all trained with annotated sen-
tence/logical form pairs; hence these approaches are
labor intensive and do not scale to broad domains
with large numbers of predicates.
Several recent papers have attempted to reduce
the amount of human supervision required to train
762
a semantic parser. One line of work eliminates the
need for an annotated logical form, instead using
only the correct answer for a database query (Liang
et al., 2011) or even a binary correct/incorrect sig-
nal (Clarke et al., 2010). This type of feedback may
be easier to obtain than full logical forms, but still
requires individually annotated sentences. Other ap-
proaches are completely unsupervised, but do not tie
the language to an existing meaning representation
(Poon and Domingos, 2009). It is also possible to
self-train a semantic parser without any labeled data
(Goldwasser et al., 2011). However, this approach
does not perform as well as more supervised ap-
proaches, since the parser’s self-training predictions
are not constrained by the correct logical form.
Recent research has produced several weakly su-
pervised relation extractors (Craven and Kumlien,
1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel
et al., 2010; Hoffmann et al., 2011). These sys-
tems scale up to hundreds of predicates, but have
much shallower semantic representations than se-
mantic parsers. For example, these systems can-
not be directly used to respond to natural language
queries. This work extends weakly supervised rela-
tion extraction to produce richer semantic structure,
using only slightly more supervision in the form of
dependency parses.
7 Discussion
This paper presents a method for training a seman-
tic parser using only a knowledge base and a cor-
pus of unlabeled sentences. Our key observation is
that multiple forms of weak supervision can be com-
bined to train an accurate semantic parser: semantic
supervision from a knowledge base of facts, and syn-
tactic supervision in the form of a standard depen-
dency parser. We presented an algorithm for train-
ing a semantic parser in the form of a probabilistic
Combinatory Categorial Grammar, using these two
types of weak supervision. We used this algorithm
to train a semantic parser for an ontology of 77 Free-
base predicates, using Freebase itself as the weak se-
mantic supervision.
Experimental results show that our trained se-
mantic parser extracts binary relations as well as
a state-of-the-art weakly supervised relation extrac-
tor (Hoffmann et al., 2011). Further experiments
tested our trained parser’s ability to extract more
complex meanings from sentences, including logi-
cal forms involving conjunctions of multiple relation
and category predicates with shared arguments (e.g.,
?x.MUSICIAN(x) ? PERSONBORNIN(x, LONDON) ?
CITYINCOUNTRY(LONDON, ENGLAND)). To test this
capability, we applied the trained parser to natural
language queries against Freebase. The semantic
parser correctly interpreted 56% of these queries,
despite the broad domain and never having seen an
annotated logical form. Together, these two experi-
mental analyses suggest that the combination of syn-
tactic and semantic weak supervision is indeed a suf-
ficient basis for training semantic parsers for a di-
verse range of corpora and predicate ontologies.
One limitation of our method is the reliance on
hand-built dependency parse patterns for lexicon ini-
tialization. Although these patterns capture a va-
riety of linguistic phenomena, they require manual
engineering and may miss important relations. An
area for future work is developing an automated
way to produce this lexicon, perhaps by extend-
ing the recent work on automatic lexicon generation
(Kwiatkowski et al., 2010) to the weakly supervised
setting. Such an algorithm seems especially impor-
tant if one wishes to model phenomena such as ad-
jectives, which are difficult to initialize heuristically
without generating large numbers of lexical entries.
An elegant aspect of semantic parsing is that it is
easily extensible to include more complex linguis-
tic phenomena, such as quantification and events
(multi-argument relations). In the future, we plan
to increase the expressivity of our parser’s mean-
ing representation to capture more linguistic and se-
mantic phenomena. In this fashion, we can make
progress toward broad coverage semantic parsing,
and thus natural language understanding.
Acknowledgments
This research has been supported in part by DARPA
under contract number FA8750-09-C-0179, and by a
grant from Google. Additionally, we thank Yahoo!
for use of their M45 cluster. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Justin Betteridge for collecting the
Freebase relations, Jamie Callan and colleagues for
the web crawl, and Thomas Kollar and Matt Gardner
for helpful comments on earlier drafts of this paper.
763
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247–1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings, The Twentieth National Con-
ference on Artificial Intelligence and the Seventeenth
Innovative Applications of Artificial Intelligence Con-
ference.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the 2010 European
conference on Machine learning and Knowledge Dis-
covery in Databases.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international confer-
ence on World Wide Web, WWW ’07, pages 697–706,
New York, NY, USA. ACM.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
764
Annual Meeting of the Association for Computational
Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the thirteenth national
conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: structured clas-
sification with probabilistic categorial grammars. In
UAI ’05, Proceedings of the 21st Conference in Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
765
