Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Selecting Sentences for Answering Complex Questions
Yllias Chali
University of Lethbridge
4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4
chali@cs.uleth.ca
Shafiq R. Joty
University of British Columbia
2366 Main Mall
Vancouver, B.C. Canada V6T 1Z4
rjoty@cs.ubc.ca
Abstract
Complex questions that require inferencing
and synthesizing information from multiple
documents can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. In this paper, we have experimented
with one empirical and two unsupervised
statistical machine learning techniques: k-
means and Expectation Maximization (EM),
for computing relative importance of the sen-
tences. However, the performance of these ap-
proaches depends entirely on the feature set
used and the weighting of these features. We
extracted different kinds of features (i.e. lex-
ical, lexical semantic, cosine similarity, ba-
sic element, tree kernel based syntactic and
shallow-semantic) for each of the document
sentences in order to measure its importance
and relevancy to the user query. We used a
local search technique to learn the weights of
the features. For all our methods of generating
summaries, we have shown the effects of syn-
tactic and shallow-semantic features over the
bag of words (BOW) features.
1 Introduction
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named enti-
ties (persons, organizations, locations, dates, etc.)
from documents. For example, the question: “De-
scribe the after-effects of cyclone Sidr-Nov 2007 in
Bangladesh” requires inferencing and synthesizing
information from multiple documents. This infor-
mation synthesis in NLP can be seen as a kind of
topic-oriented, informative multi-document summa-
rization, where the goal is to produce a single text as
a compressed version of a set of documents with a
minimum loss of relevant information.
In this paper, we experimented with one em-
pirical and two well-known unsupervised statisti-
cal machine learning techniques: k-means and EM
and evaluated their performance in generating topic-
oriented summaries. However, the performance of
these approaches depends entirely on the feature set
used and the weighting of these features. We ex-
tracted different kinds of features (i.e. lexical, lexi-
cal semantic, cosine similarity, basic element, tree
kernel based syntactic and shallow-semantic) for
each of the document sentences in order to measure
its importance and relevancy to the user query. We
have used a gradient descent local search technique
to learn the weights of the features. Traditionally,
information extraction techniques are based on the
BOW approach augmented by language modeling.
But when the task requires the use of more com-
plex semantics, the approaches based on only BOW
are often inadequate to perform fine-level textual
analysis. Some improvements on BOW are given
by the use of dependency trees and syntactic parse
trees (Hirao et al., 2004), (Punyakanok et al., 2004),
(Zhang and Lee, 2003), but these, too are not ade-
quate when dealing with complex questions whose
answers are expressed by long and articulated sen-
tences or even paragraphs. Shallow semantic rep-
resentations, bearing a more compact information,
could prevent the sparseness of deep structural ap-
proaches and the weakness of BOW models (Mos-
chitti et al., 2007). Attempting an application of
304
syntactic and semantic information to complex QA
hence seems natural, as pinpointing the answer to a
question relies on a deep understanding of the se-
mantics of both. In more complex tasks such as
computing the relatedness between the query sen-
tences and the document sentences in order to gen-
erate query-focused summaries (or answers to com-
plex questions), to our knowledge no study uses tree
kernel functions to encode syntactic/semantic infor-
mation. For all our methods of generating sum-
maries (i.e. empirical, k-means and EM), we have
shown the effects of syntactic and shallow-semantic
features over the BOW features.
This paper is organized as follows: Section 2 fo-
cuses on the related work, Section 3 describes how
the features are extracted, Section 4 discusses the
scoring approaches, Section 5 discusses how we re-
move the redundant sentences before adding them
to the summary, Section 6 describes our experimen-
tal study. We conclude and give future directions in
Section 7.
2 Related Work
Researchers all over the world working on query-
based summarization are trying different direc-
tions to see which methods provide the best re-
sults. The LexRank method addressed in (Erkan
and Radev, 2004) was very successful in generic
multi-document summarization. A topic-sensitive
LexRank is proposed in (Otterbacher et al., 2005).
As in LexRank, the set of sentences in a document
cluster is represented as a graph, where nodes are
sentences and links between the nodes are induced
by a similarity relation between the sentences. Then
the system ranked the sentences according to a ran-
dom walk model defined in terms of both the inter-
sentence similarities and the similarities of the sen-
tences to the topic description or question.
The summarization methods based on lexical
chain first extract the nouns, compound nouns and
named entities as candidate words (Li et al., 2007).
Then using WordNet, the systems find the semantic
similarity between the nouns and compound nouns.
After that, lexical chains are built in two steps: 1)
Building single document strong chains while dis-
ambiguating the senses of the words and, 2) build-
ing multi-chain by merging the strongest chains of
the single documents into one chain. The systems
rank sentences using a formula that involves a) the
lexical chain, b) keywords from query and c) named
entities.
(Harabagiu et al., 2006) introduce a new paradigm
for processing complex questions that relies on a
combination of (a) question decompositions; (b) fac-
toid QA techniques; and (c) Multi-Document Sum-
marization (MDS) techniques. The question decom-
position procedure operates on a Marcov chain, by
following a random walk with mixture model on a
bipartite graph of relations established between con-
cepts related to the topic of a complex question and
subquestions derived from topic-relevant passages
that manifest these relations. Decomposed questions
are then submitted to a state-of-the-art QA system
in order to retrieve a set of passages that can later be
merged into a comprehensive answer by a MDS sys-
tem. They show that question decompositions using
this method can significantly enhance the relevance
and comprehensiveness of summary-length answers
to complex questions.
There are approaches that are based on probabilis-
tic models (Pingali et al., 2007) (Toutanova et al.,
2007). (Pingali et al., 2007) rank the sentences based
on a mixture model where each component of the
model is a statistical model:
Score(s) = ?×QIScore(s)+(1??)×QFocus(s,Q)
Where, Score(s) is the score for sentence s. Query-
independent score (QIScore) and query-dependent score
(QFocus) are calculated based on probabilistic models.
(Toutanova et al., 2007) learns a log-linear sentence rank-
ing model by maximizing three metrics of sentence good-
ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)
Model Frequency. The scoring function is learned by fit-
ting weights for a set of feature functions of sentences
in the document set and is trained to optimize a sentence
pair-wise ranking criterion. The scoring function is fur-
ther adapted to apply to summaries rather than sentences
and to take into account redundancy among sentences.
There are approaches in “Recognizing Textual Entail-
ment”, “Sentence Alignment” and “Question Answering”
that use syntactic and/or semantic information in order to
measure the similarity between two textual units. (Mac-
Cartney et al., 2006) use typed dependency graphs (same
as dependency trees) to represent the text and the hypoth-
esis. Then they try to find a good partial alignment be-
tween the typed dependency graphs representing the hy-
pothesis and the text in a search space of O((m + 1)n)
305
where hypothesis graph contains n nodes and a text graph
contains m nodes. (Hirao et al., 2004) represent the sen-
tences using Dependency Tree Path (DTP) to incorporate
syntactic information. They apply String Subsequence
Kernel (SSK) to measure the similarity between the DTPs
of two sentences. They also introduce Extended String
Subsequence Kernel (ESK) to incorporate semantics in
DTPs. (Kouylekov and Magnini, 2005) use the tree edit
distance algorithms on the dependency trees of the text
and the hypothesis to recognize the textual entailment.
According to this approach, a text T entails a hypothesis
H if there exists a sequence of transformations (i.e. dele-
tion, insertion and substitution) applied to T such that
we can obtain H with an overall cost below a certain
threshold. (Punyakanok et al., 2004) represent the ques-
tion and the sentence containing answer with their depen-
dency trees. They add semantic information (i.e. named
entity, synonyms and other related words) in the depen-
dency trees. They apply the approximate tree matching
in order to decide how similar any given pair of trees are.
They also use the edit distance as the matching criteria in
the approximate tree matching. All these methods show
the improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-
dency tree to extract the BEs (i.e. head-modifier-relation)
and ranked the BEs based on their log-likelihood ratios.
For syntactic feature, we extracted the syntactic trees for
the sentence as well as for the query using the Charniak
parser and measured the similarity between the two trees
using the tree kernel function. We used the ASSERT se-
mantic role labeler system to parse the sentence as well
as the query semantically and used the shallow seman-
tic tree kernel to measure the similarity between the two
shallow-semantic trees.
3 Feature Extraction
The sentences in the document collection are analyzed
in various levels and each of the document-sentences is
represented as a vector of feature-values. The features
can be divided into several categories:
3.1 Lexical Features
3.1.1 N-gram Overlap
N-gram overlap measures the overlapping word se-
quences between the candidate sentence and the query
sentence. With the view to measure the N-gram
(N=1,2,3,4) overlap scores, a query pool and a sentence
pool are created. In order to create the query (or sentence)
pool, we took the query (or document) sentence and cre-
ated a set of related sentences by replacing its important
words1 by their first-sense synonyms. For example given
1hence forth important words are the nouns, verbs, adverbs
and adjectives
a stemmed document-sentence: “John write a poem”, the
sentence pool contains: “John compose a poem”, “John
write a verse form” along with the given sentence. We
measured the recall based n-gram scores for a sentence P
using the following formula:
n-gramScore(P) = maxi(maxj N-gram(si, qj))
N-gram(S,Q) =
?
gramn?S
Countmatch(gramn)
?
gramn?S
Count(gramn)
Where, n stands for the length of the n-gram (n =
1, 2, 3, 4) and Countmatch (gramn) is the number
of n-grams co-occurring in the query and the candi-
date sentence, qj is the jth sentence in the query
pool and si is the ith sentence in the sentence pool
of sentence P .
3.1.2 LCS, WLCS and Skip-Bigram
A sequence W = [w1, w2, ..., wn] is a subse-
quence of another sequence X = [x1, x2, ..., xm], if
there exists a strict increasing sequence [i1, i2, ..., ik]
of indices of X such that for all j =
1, 2, ..., k we have xij = wj . Given two sequences,
S1 and S2, the Longest Common Subsequence
(LCS) of S1 and S2 is a common subsequence with
maximum length. The longer the LCS of two sen-
tences is, the more similar the two sentences are.
The basic LCS has a problem that it does not dif-
ferentiate LCSes of different spatial relations within
their embedding sequences (Lin, 2004). To improve
the basic LCS method, we can remember the length
of consecutive matches encountered so far to a reg-
ular two dimensional dynamic program table com-
puting LCS. We call this weighted LCS (WLCS)
and use k to indicate the length of the current con-
secutive matches ending at words xi and yj . Given
two sentences X and Y, the WLCS score of X and
Y can be computed using the similar dynamic pro-
gramming procedure as stated in (Lin, 2004). We
computed the LCS and WLCS-based F-measure fol-
lowing (Lin, 2004) using both the query pool and the
sentence pool as in the previous section.
Skip-bigram is any pair of words in their sentence
order, allowing for arbitrary gaps. Skip-bigram mea-
sures the overlap of skip-bigrams between a candi-
date sentence and a query sentence. Following (Lin,
2004), we computed the skip bi-gram score using
both the sentence pool and the query pool.
306
3.1.3 Head and Head Related-words Overlap
The number of head words common in between
two sentences can indicate how much they are rel-
evant to each other. In order to extract the heads
from the sentence (or query), the sentence (or query)
is parsed by Minipar 2 and from the dependency
tree we extract the heads which we call exact head
words. For example, the head word of the sentence:
“John eats rice” is “eat”.
We take the synonyms, hyponyms and hyper-
nyms3 of both the query-head words and the
sentence-head words and form a set of words which
we call head-related words. We measured the exact
head score and the head-related score as follows:
ExactHeadScore =
?
w1?HeadSet
Countmatch(w1)
?
w1?HeadSet
Count(w1)
HeadRelatedScore =
?
w1?HeadRelSet
Countmatch(w1)
?
w1?HeadRelSet
Count(w1)
Where HeadSet is the set of head words in the sen-
tence and Countmatch is the number of matches
between the HeadSet of the query and the sen-
tence. HeadRelSet is the set of synonyms, hy-
ponyms and hypernyms of head words in the sen-
tence and Countmatch is the number of matches
between the head-related words of the query and the
sentence.
3.2 Lexical Semantic Features
We form a set of words which we call QueryRe-
latedWords by taking the important words from the
query, their first-sense synonyms, the nouns’ hy-
pernyms/hyponyms and important words from the
nouns’ gloss definitions.
Synonym overlap measure is the overlap be-
tween the list of synonyms of the important words
extracted from the candidate sentence and the
QueryRelatedWords. Hypernym/hyponym overlap
measure is the overlap between the list of hypernyms
and hyponyms of the nouns extracted from the sen-
tence and the QueryRelatedWords, and gloss overlap
measure is the overlap between the list of important
words that are extracted from the gloss definitions
of the nouns of the sentence and the QueryRelated-
Words.
2http://www.cs.ualberta.ca/ lindek/minipar.htm
3hypernym and hyponym levels are restricted to 2 and 3 re-
spectively
3.3 Statistical Similarity Measures
Statistical similarity measures are based on the
co-occurance of similar words in a corpus. We
have used two statistical similarity measures:
1. Dependency-based similarity measure and 2.
Proximity-based similarity measure.
Dependency-based similarity measure uses the
dependency relations among words in order to mea-
sure the similarity. It extracts the dependency triples
then uses statistical approach to measure the similar-
ity. Proximity-based similarity measure is computed
based on the linear proximity relationship between
words only. It uses the information theoretic defini-
tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin4.
Using the data, one can retrieve most similar words
for a given word. The similar words are grouped into
clusters. Note that, for a word there can be more than
one cluster. Each cluster represents the sense of the
word and its similar words for that sense.
For each query word, we extract all of its clus-
ters from the data. Now, in order to determine the
right cluster for a query word, we measure the over-
lap score between the QueryRelatedWords and the
clusters of words. The hypothesis is that, the cluster
that has more words common with the QueryRelat-
edWords is the right cluster. We chose the cluster for
a word which has the highest overlap score.
Once we get the clusters for the query words, we
measured the overlap between the cluster words and
the sentence words as follows:
Measure =
?
w1?SenWords
Countmatch(w1)
?
w1?SenWords
Count(w1)
Where, SenWords is the set of important words ex-
tracted from the sentence and Countmatch is the number
of matches between the sentence words and the clusters
of similar words of the query words.
3.4 Graph-based Similarity Measure
In LexRank (Erkan and Radev, 2004), the concept of
graph-based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A sim-
ilarity graph is produced for the sentences in the docu-
ment collection. In the graph, each node represents a
sentence. The edges between the nodes measure the co-
sine similarity between the respective pair of sentences.
The degree of a given node is an indication of how much
important the sentence is. Once the similarity graph is
4http://www.cs.ualberta.ca/ lindek/downloads.htm
307
constructed, the sentences are then ranked according to
their eigenvector centrality. To apply LexRank to query-
focused context, a topic-sensitive version of LexRank is
proposed in (Otterbacher et al., 2005). We followed a
similar approach in order to calculate this feature. The
score of a sentence is determined by a mixture model of
the relevance of the sentence to the query and the similar-
ity of the sentence to other high-scoring sentences.
3.5 Syntactic and Semantic Features:
So far, we have included the features of type Bag of
Words (BOW). The task like query-based summarization
that requires the use of more complex syntactic and se-
mantics, the approaches with only BOW are often inade-
quate to perform fine-level textual analysis. We extracted
three features that incorporate syntactic/semantic infor-
mation.
3.5.1 Basic Element (BE) Overlap Measure
The “head-modifier-relation” triples, extracted from
the dependency trees are considered as BEs in our exper-
iment. The triples encode some syntactic/semantic infor-
mation and one can quite easily decide whether any two
units match or not- considerably more easily than with
longer units (Zhou et al., 2005). We used the BE package
distributed by ISI5 to extract the BEs for the sentences.
Once we get the BEs for a sentence, we computed the
Likelihood Ratio (LR) for each BE following (Zhou et
al., 2005). Sorting BEs according to their LR scores pro-
duced a BE-ranked list. Our goal is to generate a sum-
mary that will answer the user questions. The ranked
list of BEs in this way contains important BEs at the top
which may or may not be relevant to the user questions.
We filter those BEs by checking whether they contain any
word which is a query word or a QueryRelatedWords (de-
fined in Section 3.2). The score of a sentence is the sum
of its BE scores divided by the number of BEs in the sen-
tence.
3.5.2 Syntactic Feature
Encoding syntactic structure is easier and straight for-
ward. Given a sentence (or query), we first parse it into
a syntactic tree using a syntactic parser (i.e. Charniak
parser) and then we calculate the similarity between the
two trees using the tree kernel defined in (Collins and
Duffy, 2001).
3.5.3 Shallow-semantic Feature
Though introducing BE and syntactic information
gives an improvement on BOW by the use of depen-
dency/syntactic parses, but these, too are not adequate
when dealing with complex questions whose answers
are expressed by long and articulated sentences or even
5BE website:http://www.isi.edu/ cyl/BE
Figure 1: Example of semantic trees
paragraphs. Shallow semantic representations, bearing a
more compact information, could prevent the sparseness
of deep structural approaches and the weakness of BOW
models (Moschitti et al., 2007).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of accurate
automatic Semantic Role Labeling (SRL) systems like
ASSERT (Hacioglu et al., 2003). For example, consider
the PB annotation:
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow se-
mantic representation that can be matched against other
semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the Italian
lira][ARG2 as their currency]
In order to calculate the semantic similarity between
the sentences, we first represent the annotated sentence
using the tree structures like Figure 1 which we call Se-
mantic Tree (ST). In the semantic tree, arguments are re-
placed with the most important word-often referred to as
the semantic head.
The sentences may contain one or more subordinate
clauses. For example the sentence, “the Vatican, located
wholly within Italy uses the Italian lira as their currency.”
gives the STs as in Figure 2. As we can see in Fig-
ure 2(A), when an argument node corresponds to an en-
tire subordinate clause, we label its leaf with ST, e.g.
the leaf of ARG0. Such ST node is actually the root of
the subordinate clause in Figure 2(B). If taken separately,
such STs do not express the whole meaning of the sen-
tence, hence it is more accurate to define a single struc-
ture encoding the dependency between the two predicates
as in Figure 2(C). We refer to this kind of nested STs as
STNs.
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of com-
mon subtrees between two trees. Such subtrees are sub-
ject to the constraint that their nodes are taken with all
or none of the children they have in the original tree.
308
Figure 2: Two STs composing a STN
Though, this definition of subtrees makes the TK func-
tion appropriate for syntactic trees but at the same time
makes it not well suited for the semantic trees (ST) de-
fined above. For instance, although the two STs of Fig-
ure 1 share most of the subtrees rooted in the ST node,
the kernel defined above computes only one match (ST
ARG0 TARGET ARG1 ARG2) which is not useful.
The critical aspect of the TK function is that the pro-
ductions of two evaluated nodes have to be identical to
allow the match of further descendants. This means that
common substructures cannot be composed by a node
with only some of its children as an effective ST represen-
tation would require. (Moschitti et al., 2007) solve this
problem by designing the Shallow Semantic Tree Kernel
(SSTK) which allows to match portions of a ST. We fol-
lowed the similar approach to compute the SSTK.
4 Ranking Sentences
In this section, we describe the scoring techniques in de-
tail.
4.1 Learning Feature-weights: A Local Search
Strategy
In order to fine-tune the weights of the features, we used
a local search technique with simulated annealing to find
the global maximum. Initially, we set all the feature-
weights, w1, · · · , wn, as equal values (i.e. 0.5) (see Al-
gorithm 1). Based on the current weights we score the
sentences and generate summaries accordingly. We eval-
uate the summaries using the automatic evaluation tool
ROUGE (Lin, 2004) (described in Section 6) and the
ROUGE value works as the feedback to our learning
loop. Our learning system tries to maximize the ROUGE
score in every step by changing the weights individually
by a specific step size (i.e. 0.01). That means, to learn
weight wi, we change the value of wi keeping all other
weight values (wj?j 6=i) stagnant. For each weight wi,
the algorithm achieves the local maximum of ROUGE
value. In order to find the global maximum we ran this
algorithm multiple times with different random choices
of initial values (i.e. simulated annealing).
Input: Stepsize l, Weight Initial Value v
Output: A vector ~w of learned weights
Initialize the weight values wi to v.
for i? 1 to n do
rg1 = rg2 = prev = 0
while (true) do
scoreSentences(~w)
generateSummaries()
rg2 = evaluateROUGE()
if rg1 ? rg2 then
prev = wi
wi+ = l
rg1 = rg2
else
break
end
end
end
return ~w
Algorithm 1: Tuning weights using Local Search
technique
Once we have learned the feature-weights, our empir-
ical method computes the final scores for the sentences
using the formula:
scorei = ~xi. ~w (1)
Where, ~xi is the feature vector for i-th sentence, ~w is
the weight vector and scorei is the score of i-th sentence.
4.2 K-means Learning
We start with a set of initial cluster centers and go through
several iterations of assigning each object to the cluster
whose center is closest. After all objects have been as-
signed, we recompute the center of each cluster as the
centroid or mean (µ) of its members.
Once we have learned the means of the clusters using
the k-means algorithm, our next task is to rank the sen-
tences according to a probability model. We have used
Bayesian model in order to do so. Bayes’ law says:
P (qk|~x,?) =
p(~x|qk,?)P (qk|?)
?K
k=1 p(~x|qk,?)p(qk|?)
(2)
where qk is a class, ~x is a feature vector repre-
senting a sentence and ? is the parameter set of all
class models. We set the weights of the clusters as
equiprobable (i.e. P (qk|?) = 1/K). We calculated
309
p(x|qk,?) using the gaussian probability distribu-
tion. The gaussian probability density function (pdf)
for the d-dimensional random variable ~x is given by:
p(µ,?)(~x) =
e
?1
2 (~x?µ)
T??1(~x?µ)
?
2pi
d?
det(?)
(3)
where µ, the mean vector and ?, the covariance
matrix are the parameters of the gaussian distribu-
tion. We get the means (µ) from the k-means algo-
rithm and we calculate the covariance matrix using
the unbiased covariance estimation:
?ˆ =
1
N ? 1
N?
i=1
(xj ? µj)(xi ? µi)
T (4)
4.3 EM Learning
EM is an iterative two step procedure:
1. Expectation-step and 2. Maximization-step.
In the expectation step, we compute expected values
for the hidden variables hi,j which are cluster mem-
bership probabilities. Given the current parameters,
we compute how likely an object belongs to any
of the clusters. The maximization step computes
the most likely parameters of the model given the
cluster membership probabilities. The data-points
are considered to be generated by a mixture model
of k-gaussians of the form:
P (~x) =
k?
i=1
P (C = i)P (~x|µi,?i) (5)
Where the total likelihood of model ? with k
components given the observed data points, X =
x1, · · · , xn is:
L(?|X) =
n?
i=1
k?
j=1
P (C = j)P (xi|?j)
=
n?
i=1
k?
j=1
wjP (xi|µj ,?j)
?
n?
i=1
log
k?
j=1
wjP (xi|µj ,?j)
where P is the probability density function (i.e.
eq 3). µj and ?j are the mean and covariance ma-
trix of component j, respectively. Each component
contributes a proportion, wj , of the total population,
such that:
?K
j=1wj = 1.
However, a significant problem with the EM al-
gorithm is that it converges to a local maximum
of the likelihood function and hence the quality of
the result depends on the initialization. In order
to get good results from using random starting val-
ues, we can run the EM algorithm several times
and choose the initial configuration for which we
get the maximum log likelihood among all con-
figurations. Choosing the best one among several
runs is very computer intensive process. So, to im-
prove the outcome of the EM algorithm on gaus-
sian mixture models it is necessary to find a better
method of estimating initial means for the compo-
nents. To achieve this aim we explored the widely
used “k-means” algorithm as a cluster (means) find-
ing method. That means, the means found by k-
means clustering above will be utilized as the initial
means for EM and we calculate the initial covari-
ance matrices using the unbiased covariance estima-
tion procedure (eq:4).
Once the sentences are clustered by EM al-
gorithm, we filter out the sentences which are
not query-relevant by checking their probabilities,
P (qr|xi,?) where, qr denotes the cluster “query-
relevant”. If for a sentence xi, P (qr|xi,?) > 0.5
then xi is considered to be query-relevant.
Our next task is to rank the query-relevant sen-
tences in order to include them in the summary. This
can be done easily by multiplying the feature vector
~xi with the weight vector ~w that we learned by the
local search technique (eq:1).
5 Redundancy Checking
When many of the competing sentences are included
in the summary, the issue of information overlap be-
tween parts of the output comes up, and a mecha-
nism for addressing redundancy is needed. There-
fore, our summarization systems employ a final level
of analysis: before being added to the final output,
the sentences deemed to be important are compared
to each other and only those that are not too simi-
lar to other candidates are included in the final an-
swer or summary. Following (Zhou et al., 2005), we
modeled this by BE overlap between an intermedi-
ate summary and a to-be-added candidate summary
310
sentence. We call this overlap ratio R, where R is
between 0 and 1 inclusively. Setting R = 0.7 means
that a candidate summary sentence, s, can be added
to an intermediate summary, S, if the sentence has a
BE overlap ratio less than or equal to 0.7.
6 Experimental Evaluation
6.1 Evaluation Setup
We used the main task of Document Understanding
Conference (DUC) 2007 for evaluation. The task
was: “Given a complex question (topic description)
and a collection of relevant documents, the task is to
synthesize a fluent, well-organized 250-word sum-
mary of the documents that answers the question(s)
in the topic.”
NIST assessors developed topics of interest to
them and choose a set of 25 documents relevant
(document cluster) to each topic. Each topic and its
document cluster were given to 4 different NIST as-
sessors. The assessor created a 250-word summary
of the document cluster that satisfies the information
need expressed in the topic statement. These multi-
ple “reference summaries” are used in the evaluation
of summary content.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-grams (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup.
One purpose of our experiments is to study the
impact of different features for complex question
answering task. To accomplish this, we generated
summaries for the topics of DUC 2007 by each of
our seven systems defined as below:
The LEX system generates summaries based on
only lexical features: n-gram (n=1,2,3,4), LCS,
WLCS, skip bi-gram, head, head synonym. The
LSEM system considers only lexical semantic
features: synonym, hypernym/hyponym, gloss,
dependency-based and proximity-based similarity.
The COS system generates summary based on the
graph-based method. The SYS1 system considers
all the features except the BE, syntactic and seman-
tic features. The SYS2 system considers all the fea-
tures except the syntactic and semantic features. The
SYS3 considers all the features except the semantic
and the ALL6 system generates summaries taking
all the features into account.
6.2 Evaluation Results
Table 17 to Table 3, Table 4 to Table 6 and Table 7 to
Table 9 show the evaluation measures for k-means,
EM and empirical approaches respectively. As Ta-
ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3
gets 4-32% and ALL gets 3-36% improvement in
ROUGE-2 scores over the SYS1 system. We get best
ROUGE-W (Table 2) scores for SYS2 (i.e. includ-
ing BE) but SYS3 and ALL do not perform well in
this case. SYS2 improves the ROUGE-W F-score by
1% over SYS1. We do not get any improvement in
ROUGE-SU (Table 3) scores when we include any
kind of syntactic/semantic structures.
The case is different for EM and empirical ap-
proaches. Here, in every case we get a significant
amount of improvement when we include the syn-
tactic and/or semantic features. For EM (Table 4 to
Table 6), the ratio of improvement in F-scores over
SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-
24% for ALL. In our empirical approach (Table 7
to Table 9), SYS2, SYS3 and ALL improve the F-
scores by 3-11%, 7-15% and 8-19% over SYS1 re-
spectively. These results clearly indicate the positive
impact of the syntactic/semantic features for com-
plex question answering task.
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.074 0.077 0.086 0.075 0.075 0.078 0.077
P 0.081 0.084 0.093 0.081 0.098 0.107 0.110
F 0.078 0.080 0.089 0.078 0.085 0.090 0.090
Table 1: ROUGE-2 measures in k-means learning
Table 10 shows the F-scores of the ROUGE mea-
sures for one baseline system, the best system in
DUC 2007 and our three scoring techniques con-
sidering all features. The baseline system gener-
6SYS2, SYS3 and ALL systems show the impact of BE,
syntactic and semantic features respectively
7R stands for Recall, P stands for Precision and F stands for
F-score
311
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.098 0.097 0.101 0.099 0.101 0.097 0.097
P 0.195 0.194 0.200 0.237 0.233 0.241 0.237
F 0.130 0.129 0.134 0.140 0.141 0.139 0.138
Table 2: ROUGE-W measures in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.131 0.127 0.139 0.136 0.135 0.135 0.135
P 0.155 0.152 0.162 0.176 0.171 0.174 0.174
F 0.142 0.139 0.150 0.153 0.151 0.152 0.152
Table 3: ROUGE-SU in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.089 0.080 0.087 0.085 0.085 0.089 0.091
P 0.096 0.087 0.094 0.092 0.095 0.116 0.138
F 0.092 0.083 0.090 0.088 0.090 0.101 0.109
Table 4: ROUGE-2 measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.103 0.096 0.101 0.102 0.101 0.102 0.101
P 0.205 0.193 0.200 0.203 0.218 0.222 0.223
F 0.137 0.128 0.134 0.136 0.138 0.139 0.139
Table 5: ROUGE-W measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.146 0.128 0.138 0.143 0.144 0.145 0.144
P 0.171 0.153 0.162 0.168 0.177 0.186 0.185
F 0.157 0.140 0.149 0.154 0.159 0.163 0.162
Table 6: ROUGE-SU measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.086 0.080 0.087 0.087 0.090 0.095 0.099
P 0.093 0.087 0.094 0.094 0.112 0.115 0.116
F 0.089 0.083 0.090 0.090 0.100 0.104 0.107
Table 7: ROUGE-2 in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.102 0.096 0.101 0.102 0.102 0.104 0.105
P 0.203 0.193 0.200 0.204 0.239 0.246 0.247
F 0.135 0.128 0.134 0.137 0.143 0.147 0.148
Table 8: ROUGE-W in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.144 0.129 0.138 0.145 0.146 0.149 0.150
P 0.169 0.153 0.162 0.171 0.182 0.195 0.197
F 0.155 0.140 0.150 0.157 0.162 0.169 0.170
Table 9: ROUGE-SU in empirical approach
ates summaries by returning all the leading sen-
tences (up to 250 words) in the ?TEXT ? field of
the most recent document(s). It shows that the em-
pirical approach outperforms the other two learning
techniques and EM performs better than k-means al-
gorithm. EM improves the F-scores over k-means
by 0.7-22.5%. Empirical approach improves the F-
scores over k-means and EM by 5.9-20.2% and 3.5-
6.5% respectively. Comparing with the DUC 2007
participants our systems achieve top scores and for
some ROUGE measures there is no statistically sig-
nificant difference between our system and the best
DUC 2007 system.
System ROUGE-
1
ROUGE-
2
ROUGE-
W
ROUGE-
SU
Baseline 0.335 0.065 0.114 0.113
Best 0.438 0.122 0.153 0.174
k-means 0.390 0.090 0.138 0.152
EM 0.399 0.109 0.139 0.162
Empirical 0.413 0.107 0.148 0.170
Table 10: F-measures for different systems
7 Conclusion and Future Work
Our experiments show the following: (a) our ap-
proaches achieve promising results, (b) empirical
approach outperforms the other two learning and
EM performs better than the k-means algorithm for
this particular task, and (c) our systems achieve bet-
ter results when we include BE, syntactic and se-
mantic features.
In future, we have the plan to decompose the com-
plex questions into several simple questions before
measuring the similarity between the document sen-
tence and the query sentence. We expect that by de-
composing complex questions into the sets of sub-
questions that they entail, systems can improve the
average quality of answers returned and achieve bet-
ter coverage for the question as a whole.
312
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625–632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457–479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answer-
ing complex questions with random walk models. In
Proceedings of the 29th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 220 – 227. ACM.
T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004.
Dependency-based sentence alignment for multiple
document summarization. In Proceedings of Coling
2004, pages 446–452, Geneva, Switzerland. COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In Proceedings of the PASCAL Challenges Workshop:
Recognising Textual Entailment Challenge.
J. Li, L. Sun, C. Kit, and J. Webster. 2007. A Query-
Focused Multi-Document Summarizer Based on Lex-
ical Chains. In Proceedings of the Document Under-
standing Conference, Rochester. NIST.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74–81, Barcelona, Spain.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, page 4148, New
York, USA.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776–783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915–922,
Vancouver, Canada.
P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyder-
abad at DUC 2007. In Proceedings of the Document
Understanding Conference, Rochester. NIST.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping de-
pendencies trees: An application to question answer-
ing. In Proceedings of AI & Math, Florida, USA.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
Summarization System: Microsoft Research at DUC
2007 . In proceedings of the Document Understanding
Conference, Rochester. NIST.
D. Zhang and W. S. Lee. 2003. A Language Mod-
eling Approach to Passage Question Answering. In
Proceedings of the Twelfth Text REtreival Conference,
pages 489–495, Gaithersburg, Maryland.
L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-based
Multi-dccument Summarizer with Query Interpreta-
tion. In Proceedings of Document Understanding
Conference, Vancouver, B.C., Canada.
313
