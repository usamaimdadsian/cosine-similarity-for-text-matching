Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698–707,
Singapore, 6-7 August 2009. c©2009 ACL and AFNLP
Tree Kernel-based SVM with Structured Syntactic Know-
ledge for BTG-based Phrase Reordering 
 
 
Min Zhang             Haizhou Li  
Institute for Infocomm Research  
1 Fusionopolis Way,#21-01 Connexis (South Tower) 
Singapore 138632 
{mzhang,hli}@i2r.a-star.edu.sg 
 
 
  
 
Abstract 
Structured syntactic knowledge is important 
for phrase reordering. This paper proposes us-
ing convolution tree kernel over source parse 
tree to model structured syntactic knowledge 
for BTG-based phrase reordering in the con-
text of statistical machine translation. Our 
study reveals that the structured syntactic fea-
tures over the source phrases are very effective 
for BTG constraint-based phrase reordering 
and those features can be well captured by the 
tree kernel. We further combine the structured 
features and other commonly-used linear fea-
tures into a composite kernel. Experimental re-
sults on the NIST MT-2005 Chinese-English 
translation tasks show that our proposed 
phrase reordering model statistically signifi-
cantly outperforms the baseline methods. 
1 Introduction  
Phrase-based method (Koehn et al., 2003; Och 
and Ney, 2004; Koehn et al., 2007) and syntax-
based method (Wu, 1997; Yamada and Knight, 
2001; Eisner, 2003; Chiang, 2005; Cowan et al., 
2006; Marcu et al., 2006; Liu et al., 2007; Zhang 
et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi 
and Huang, 2008) represent the state-of-the-art 
technologies in statistical machine translation 
(SMT). As the two technologies are complemen-
tary in many ways, an interesting research topic 
is how to combine the strengths of the two me-
thods. Many research efforts have been made to 
address this issue, which can be summarized into 
two ideas. One is to add syntax into phrase-based 
model while another one is to enhance syntax-
based model to handle non-syntactic phrases. In 
this paper, we bring forward the first idea by 
studying the issue of how to utilize structured 
syntactic features for phrase reordering in a 
phrase-based SMT system with BTG (Bracketing 
Transduction Grammar) constraints (Wu, 1997). 
Word and phrase reordering is a crucial com-
ponent in a SMT system. In syntax-based method, 
word reordering is implicitly addressed by trans-
lation rules, thus the performance is subject to 
parsing errors to a large extent (zhang et al., 
2007a) and the impact of syntax on reordering is 
difficult to single out (Li et al., 2007). In phrase-
based method, local word reordering1 can be ef-
fectively captured by phrase pairs directly while 
local phrase reordering is explicitly modeled by 
phrase reordering model and distortion model. 
Recently, many phrase reordering methods have 
been proposed, ranging from simple distance-
based distortion model (Koehn  et al., 2003; Och 
and Ney, 2004), flat reordering model (Wu, 1997; 
Zens et al., 2004), lexicalized reordering model 
(Tillmann, 2004; Kumar and Byrne, 2005), to 
hierarchical phrase-based model (Chiang, 2005; 
Setiawan et al., 2007) and classifier-based reor-
dering model with linear features (Zens and Ney, 
2006; Xiong et al., 2006; Zhang et al., 2007a; 
Xiong et al., 2008). However, one of the major 
limitations of these advances is the structured 
syntactic knowledge, which is important to glob-
al reordering (Li et al., 2007; Elming, 2008), has 
not been well exploited. This makes the phrase-
based method particularly weak in handling 
global phrase reordering. From machine learning 
viewpoint (Vapnik, 1995), it is computationally 
infeasible to explicitly generate features involv-
ing structured information in many NLP applica-
                                                 
1 This paper follows the term convention of global reorder-
ing and local reordering of Li et al. (2007), between which 
the distinction is solely defined by reordering distance 
(whether beyond four source words) (Li et al., 2007). 
698
tions. For example, one cannot enumerate effi-
ciently all the sub-tree features for a full parse 
tree. This would be the reason why structured 
features are not fully utilized in previous statis-
tical feature-based phrase reordering model. 
Thanks to the nice property of kernel-based 
machine learning method that can implicitly ex-
plore (structured) features in a high dimensional 
feature space (Vapnik, 1995), in this paper we 
propose using convolution tree kernel (Haussler, 
1999; Collins and Duffy, 2001) to explore the 
structured syntactic knowledge for phrase reor-
dering and further combine the tree kernel with 
other diverse linear features into a composite 
kernel to strengthen the model’s predictive abili-
ty. Indeed, using tree kernel methods to mine 
structured knowledge has shown success in some 
NLP applications like parsing (Collins and Duffy, 
2001), semantic role labeling (Moschitti, 2004; 
Zhang et al., 2007b), relation extraction (Zhang 
et al., 2006), pronoun resolution (Yang et al., 
2006) and question classification (Zhang and 
Lee, 2003). However, to our knowledge, such 
technique still remains unexplored for phrase 
reordering. 
In this paper, we look into the phrase reorder-
ing problem in two aspects: 1) how to model and 
optimize structured features, and 2) how to com-
bine the structured features with other linear fea-
tures and further integrate them into the log-
linear model-based translation framework. Our 
study shows that: 1) the structured syntactic fea-
tures are very useful and 2) our kernel-based 
model can well explore diverse knowledge, in-
cluding previously-used linear features and the 
structured syntactic features, for phrase reorder-
ing. Our model displays one advantage over the 
previous work that it is able to utilize the struc-
tured syntactic features without the need for ex-
tensive feature engineering in decoding a parse 
tree into a set of linear syntactic features. 
To have a more insightful evaluation, we de-
sign three experiments with three different eval-
uation metrics. Experimental results on the NIST 
MT-2005 Chinese-English translation tasks show 
that our method statistically significantly outper-
forms the baseline methods in term of the three 
different evaluation metrics. 
The rest of the paper is organized as follows. 
Section 2 introduces the baseline method of 
BTG-based phrase translation method while sec-
tion 3 discusses the proposed method in detail. 
The experimental results are reported and dis-
cussed in section 4. Finally, we conclude the pa-
per in section 5. 
2 Baseline System and Method 
We use the MaxEnt-based BTG translation sys-
tem (Xiong et al., 2006) as our baseline. It is a 
phrase-based SMT system with BTG reordering 
constraint. The system uses the BTG lexical 
translation rules ( ? ? ?/? ) to translate the 
source phrase ?  into target phrase ? , and the 
BTG merging rules (? ? ??, ??| ? ?, ? ? ) to 
combine two neighboring phrases with a straight 
or inverted order. In the translation model, the 
BTG lexical rules are weighted with several fea-
tures, such as phrase translation, word penalty 
and language models, in a log-linear form. With 
the BTG constraint, the reordering model ? is 
defined on the two neighboring phrases ??  and 
?? and their order ? ? ?????????, ????????? as 
follows: 
? ? f(?, ??, ??)                                  (1) 
In the baseline system, a MaxEnt-based clas-
sifier with boundary words of the two neighbor-
ing phrases as features is used to model the 
merging/reordering order. The baseline MaxEnt-
based reordering model is formulized as follows: 
? ? ??(?|??, ??) ? ???(? ????(?,?
?,??))?
? ???(? ????(?,??,??))??     (2) 
where the functions  ??(?, ??, ??) ? ?0,1?  are 
model feature functions using the boundary 
words of the two neighboring phrases as features, 
and ?? are feature weights that are trained based 
on the MaxEnt-based criteria. 
3 Tree Kernel-based Phrase Reordering 
Model  
3.1 Kernel-based Classifier Solution to 
Phrase Reordering 
In this paper, phrase reordering is recast as a 
classification issue as done in previous work 
(Xiong et al., 2006 & 2008; Zhang et al., 2007a). 
In training, we use a machine learning algorithm 
training on the annotated phrase reordering in-
stances that are automatically extracted from 
word-aligned, source sentence parsed training 
corpus, to learn a classifier. In testing (decoding), 
the learned classifier is applied to two adjacent 
source phrases to decide whether they should be 
merged (straight) or reordered (inverted) and 
what their probabilities are, and then these prob-
abilities are used as one feature in the log-linear 
model in a phrase-based decoder. 
In addition to the previously-used linear fea-
tures, we are more interested in the value of 
structured syntax in phrase reordering and how 
to capture it using kernel methods. However, not 
699
all classifiers are able to work with kernel me-
thods. Only those dot-product-based classifiers 
can work with kernels by replacing the dot prod-
uct with a kernel function, where the kernel func-
tion is able to directly calculate the similarity 
between two (structured) objects without enume-
rating them into linear feature vectors. In this 
paper, we select SVM as our classifier. In this 
section, we first define the structured syntactic 
features and introduce the commonly used linear 
features, and then discuss how to utilize these 
features by kernel methods together SVM for 
phrase reordering 
3.2 Structured Syntactic Features 
A reordering instance ? ? ???, ??? (see Eq.1) in 
this paper refers to two adjacent source phrases 
??  and ?? to be translated. The structured syn-
tactic feature spaces of a reordering instance are 
defined as the portion of a parse tree of the 
source sentence that at least covers the span of 
the reordering instance (i.e. the two neighboring 
phrases). The syntactic features are defined as all 
 
T1) Minimum Sub-Tree (MST) 
                               
T2) Minimum Sub-Structure (MSS)                T4) Chunking Tree (CT) 
 
 
 
T3) Context-sensitive Minimum Sub-Structure (CMSS) 
 
Figure 1. Different representations of structured syntactic features of a reordering instance in the example 
sentence excerpted from our training corpus “…??/build  ??/scale??/mighty ?/of ??/various 
types ??/qualified personnel  ??/contingent ??/above all  ??/urgently  ??/necessary ??
/central authorities  ??/overall  ??/planning…(To build a mighty contingent of qualified personnel of 
various types, it is necessary, above all, for the central authorities to make overall planning.) ”, where “?
?/various types ??/qualified personnel  ??/contingent (contingent of qualified personnel of various 
types)” is the 1st/left phrase and “??/above all  ??/urgent  ??/necessary (it is necessary, above all, 
…)” is the 2nd/right phrase. Note that different function tags are attached to the grammar tag of each inter-
nal node. 
700
the possible subtrees in the structured feature 
spaces. We can see that the structured feature 
spaces and their features are encapsulated by a 
full parse tree of source sentences. Thus, it is 
critical to understand which portion of a parse 
tree (i.e. structured feature space) is the most ef-
fective to represent a reordering instance. Moti-
vated by the work of (Zhang et al., 2006), we 
here examine four cases that contain different 
sub-structures as shown in Fig. 1. 
 
(1) Minimum Sub-Tree (MST): the sub-tree 
rooted by the nearest common ancestor of the 
two phrases. This feature records the minimum 
sub-structure covering the two phrases and its 
left and right contexts as shown in Fig 1.T1. 
(2) Minimum Sub-Structure (MSS): the smal-
lest common sub-structure covering the two 
phrases. It is enclosed by the shortest path link-
ing the two phrases. Thus, its leaf nodes exactly 
consist of all the phrasal words. 
(3) Context-sensitive Minimum Sub-Structure 
(CMSS): the MSS extending with the 1st left 
sibling node of the left phrase and the 1st right 
sibling node of the right phrase and their descen-
dants. If sibling is unavailable, then we move to 
the parent of current node and repeat the same 
process until the sibling is available or the root of 
the MST is reached. 
(4) Chunking Tree (CT): the base phrase list 
extracted from the MSS. We prune out all the 
internal structures of the MSS and only keep the 
root node and the base phrase list for generating 
the chunking tree. 
Fig. 1 illustrates the different representations 
of an example reordering instance. T1 is the MST 
for the example instance, where the sub-structure 
circled by a dotted line is the MSS, which is also 
shown in T2 for clarity. We can see that the MSS 
is a subset of the MST. By T2 we would like to 
evaluate whether the structured information is 
effective for phrase reordering while by compar-
ing the system performance when using T1 and 
T2, we would like to evaluate whether the struc-
tured context information embedded in the MST 
is useful to phrase reordering. T3 is the CMSS, 
where the two sub-structures circled by dotted 
lines are included as the context to T2 and make 
T3 limited context-sensitive. This is to evaluate 
whether the limited context information in the 
CMSS is helpful. By comparing the performance 
of T1 and T3, we would like to see whether the 
larger context in T1 is a noisy feature. T4 is the 
CT, where only the basic structured information 
is kept. By comparing the performance of T2 and 
T4, we would like to study whether the high-level 
structured syntactic features in T2 are useful to 
phrase reordering. 
After defining the four structured feature 
spaces, we further partition each feature space 
into five parts according to their functionalities. 
Because it only makes sense to evaluate two par-
titions of the same functionality between two 
reordering instances, the feature space partition 
leads to a more precise similarity calculation. As 
shown in Fig 1, all the internal nodes in each par-
tition are labeled with a unique function tag in 
the following way: 
• Left Context (-lc): nodes in this partition 
do not cover any phrase word and they are 
all in the left of the left phrase. 
• Right Context (-rc): nodes in this partition 
do not cover any phrase word and they are 
all in the right of the right phrase. 
• Left Phrase (-lp): nodes in this partition 
only cover the first phrase and/or its left 
context. 
• Right Phrase (-rp): nodes in this partition 
only cover the second phrase and/or its right 
context. 
• Shared Part (-sp): nodes in this partition at 
least cover both of the two phrases partially. 
No lexical word is tagged since it is not a part 
of the structured features, and therefore not par-
ticipating in the tree kernel computing. 
3.3 Linear Features 
In our study, we define the following lexicalized 
linear features which are easily to be extracted 
and integrated to our composite kernel: 
• Leftmost and rightmost boundary words of 
the left and right source phrases 
• Leftmost and rightmost boundary words of 
the left and right target phrases 
• Internal words of the four phrases (exclud-
ing boundary words) 
• Target language model (LM) score differ-
ence  (monotone-inverted) 
In total, we arrive at 13 features, including 8 
boundary word features, 4 (kinds of) internal 
word features and 1 LM feature. The first 12 fea-
tures have been proven useful (Xiong et al., 
2006; Zhang et al., 2007a) to phrase reordering. 
LM score is certainly a strong evidence for mod-
eling word orders and lexical selection. Although 
it is already used as a standalone feature in the 
log-linear model, we also would like to explicitly 
re-optimize it together with other reordering fea-
tures in our reordering model. 
701
3.4 Tree Kernel, Composite Kernel and In-
tegrating into our Reordering Model 
As discussed before, we use convolution tree 
kernel to capture the structured syntactic feature 
implicitly by directly computing similarity be-
tween the parse-tree representations of two reor-
dering instances with explicitly enumerating all 
the features one by one. In convolution tree ker-
nel (Collins and Duffy, 2001), a parse tree T  is 
implicitly represented by a vector of integer 
counts of each sub-tree type (regardless of its 
ancestors): 
 
( )T? = (# subtree1(T), …, # subtreen(T))  
where # subtreei(T) is the occurrence number of 
the ith sub-tree type (subtreei) in T. Since the 
number of different sub-trees is exponential with 
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector ( )T? . To 
solve this computational issue, Collins and Duffy 
(2001) proposed the following parse tree kernel 
to calculate the dot product between the above 
high dimensional vectors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )
isubtree
I n  is a function 
that is 1 iff the subtreei occurs with root at node n 
and zero otherwise, and 1 2( , )n n?  is the number of 
the common subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by 
the following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = × ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes. In ad-
dition, the recursive Rule 3 holds because given 
two nodes with the same children, one can con-
struct common sub-trees using these children and 
common sub-trees of further offspring. The time 
complexity for computing this kernel is
1 2(| | | |)O N N? and in practice in near to linear 
computational time without the need of enume-
rating all subtree features.  
In our study, the linear feature-based similarity 
is simply calculated using dot-product. We then 
define the following composite kernel to com-
bine the structured features-based and the linear 
features-based similarities:  
 
??(??, ??) ? ? · ??(??, ??) ? (1 ? ?) · ??(??, ??) (3) 
 
where Kt is the tree kernel over the structured 
features and Kl is the linear kernel (dot-product) 
over the linear features. The composite kernel Kc 
is a linear combination of the two individual ker-
nels, where the coefficient ? is set to its default 
value 0.3 as that in Moschitti (2004)’s implemen-
tation. The kernels return the similarities be-
tween two reordering instances based on their 
features used. Our basic assumption is, the more 
similar the two reordering instances of x1 and x2 
are, the more chance they share the same order. 
Now let us see how to integrate the kernel 
functions into SVM. The linear classifier learned 
by SVM is formulized as: 
( ) sgn( )i i iif x y a x x b= • +?                    (4) 
where ia is the weight of a support vector ix (i.e., 
a support reordering instance ?? ? ???, ???in our 
study), iy  is its class label (1:  ????????  or -
1: ???????? in our study) and b is the intercept 
of the hyperplane. An input reordering instance x
is classified as positive (negative) if ( )f x >0 (
( )f x <0). 
Based on the linear classifier, a kernelized 
SVM can be easily implemented by simply re-
placing the dot product ix x? in Eq (4) with a 
kernel function ( , )iK x x . Thus, the kernelized 
SVM classifier is formulated as: 
( ) sgn( ( , ) )i i iif x y a K x x b= +?                 (5) 
where ( , )iK x x is either ( , )c iK x x , ( , )t iK x x or  
( , )l iK x x in our study. Following Eq (1), our 
reordering model (implemented by the kerne-
lized SVM) can be formulized as follows: 
 
? ? f(?, ??, ??) ? ????(?|? ? ???, ???) 
? ???(? (?????)?, ??) ? ?)? )                   (6)  
 
A reordering instance x is classified as straight 
(or inverted) if ????(?|?) ? 0 (or ????(?|?) ?
0). Eq (6) and Eq (2) show the difference be-
tween our kernalized SVM-based reordering 
702
model and the MaxEnt-based reordering model. 
The main difference between them lies in that 
our model is able to utilize structured syntactic 
features by kernalized SVM while the previous 
work can only use lexicalized word features by 
MaxEnt-based classifier.  
Finally, because the return value of  
????(?|?)  is a distance function rather than a 
probability, we use a sigmoid function to convert 
????(?|?) to a posterior probability as shown 
using the following to functions and apply it as 
one feature to the log-linear model in the decod-
ing.  
( | )
1
( | )
1 svmp o x
P straight x
e?
=
+
    and  
( | )
1
( | )
1 svmp o x
P inverted x
e
=
+
 
where straight represents a positive instance and 
inverted represents a negative instance. 
4 Experiments and Discussion 
4.1 Experimental Settings 
Basic Settings: we evaluate our method on Chi-
nese-English translation task. We use the FBIS 
corpus as training set, the NIST MT-2002 test set 
as development (dev) set and the NIST MT-2005 
test set as test set. The Stanford parser (Klein and 
Manning, 2003) is used to parse Chinese sen-
tences on the training, dev and test sets. GIZA++ 
(Och and Ney, 2004) and the heuristics “grow-
diag-final-and” are used to generate m-to-n word 
alignments. The translation model is trained on 
the FBIS corpus and a 4-gram language model is 
trained on the Xinhua portion of the English Gi-
gaword corpus using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Kenser and Ney, 1995). For the 
MER training (Och, 2003), we modify Koehn’s 
MER trainer (Koehn, 2004) to train our system. 
For significance test, we use Zhang et al’s im-
plementation (Zhang et al, 2004). 
Baseline Systems: we set three baseline sys-
tems: B1) Moses (Koehn et al., 2007) that uses 
lexicalized unigram reordering model to predict 
three orientations: monotone, swap and discon-
tinuous; B2) MaxEnt-based reordering model 
with lexical boundary word features only (Xiong 
et al., 2006); B3) Linguistically annotated reor-
dering model for BTG-based (LABTG) SMT 
(Xiong et al., 2008). For Moses, we used the de-
fault settings. We build a CKY-style decoder and 
integrate the corresponding reordering modelling 
methods into the decoder to implement the 2nd 
and the 3rd baseline systems and our system. Ex-
cept reordering models, all the four systems use 
the same features in translation model, language 
model and distortion model as Moses in the log-
linear framework. We tune the four systems us-
ing the strategies as discussed previously in this 
section. 
Reordering Model Training: we extract all 
reordering instances from the m-to-n word-
aligned training corpus. The reordering instances 
include the two source phrases, two target phras-
es, order label and its corresponding parse tree. 
We generate the boundary word features from 
the extracted reordering instances in the same 
way as discussed in Xiong et al. (2006) and use 
Zhang’s MaxEnt Tools 2  to train a reordering 
model for the 2nd baseline system. Similarly, we 
use the algorithm 1 in Xiong et al. (2008) to ex-
tract features and use the same MaxEnt Tools to 
train a reordering model for the 3rd baseline sys-
tem. Based on the extracted reordering instances, 
we generate the four structured features and the 
linear features, and then use the Tree Kernel 
Tools (Moschitti, 2004) to train our kernel-based 
reordering model (linear, tree and composite). 
Experimental Design and Evaluation Met-
rics: we design three experiments and evaluate 
them using three metrics.  
 Classification-based: in the first experiment, 
we extract all reordering instances and their fea-
tures from the dev and test sets, and then use the 
reordering models trained on the training set to 
classify (label) those instances extracted from the 
dev and test sets. In this way, we can isolate the 
reordering problem from the influence of others, 
such as translation model, pruning and decoding 
strategies, to better examine the reordering mod-
els’ ability and to give analytical insights into the 
features. Classification Accuracy (CAcc), the 
percentage of the correctly labeled instances over 
all trials, is used as the evaluation metric.  
Forced decoding3-based and normal decoding-
based: the two experiments evaluate the reorder-
ing models through a real SMT system. The 
reordering model and the language model are the 
same in the two experiments. However, in forced 
decoding, we train two translation models, one 
using training data only while another using both 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
3 A normal SMT decoder filters a translation model accord-
ing to the source sentences, whereas in forced decoding, a 
translation model is filtered based on both source sentence 
and target references. In other words, in forced decoding, 
the decoder is forced to use those phrases whose translations 
are already in the references. 
703
training, dev and test data. By forced decoding, 
we aim to isolate the reordering problem from 
those of OOV and lexical selections resulting 
from imperfect translation model in the context 
of a real SMT task. Besides the the case-sensitive 
BLEU-4 (Papineni et al., 2002) used in the two 
experiments, we design another evaluation me-
trics Reordering Accuracy (RAcc) for forced de-
coding evaluation. RAcc is the percentage of the 
adjacent word pairs with correct word order 4 
over all words in one-best translation results. 
Similar to BLEU score, we also use the similar 
Brevity Penalty BP (Papineni et al., 2002) to pe-
nalize the short translations in computing RAcc. 
Finally, please note for the three evaluation me-
trics, the higher values represent better perfor-
mance. 
 
Feature Spaces CAcc (%) 
Dev Test 
Minimum Sub-Tree (MST) 89.87 89.92
Minimum Sub-Structure (MSS) 87.95 87.88
Context-Sensitive MSS (CMSS) 89.11 89.01
Chunking Tree (CT) 86.17 86.21
Linear Features (Kl) 90.79 90.46
Kl w/o using LM feature (Kl-LM) 84.24 84.06
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MST w/o the 5 function tags 86.94 87.03
All are straight (monotonic) 78.92 78.67
 
Table 1: Performance of our methods on the 
dev and test sets with different feature combi-
nations 
4.2 Experimental Results 
Classification of Instances: Table 1 reports the 
performance of our defined four structured fea-
tures, linear feature and the composite kernel. 
The results are summarized as follows. 
The last row reports the performance without 
using any reordering features. We just suppose 
that all the translations are monotonic, no reor-
dering happens. The CAccs of 78.92% and 78.67% 
serve as the bottom line in our study. Compared 
with the bottom line, the tree kernels over the 4 
structured features are very effective for phrase 
                                                 
4 An adjacent word pair wiwi+1 in a translation have correct 
word order if and only if wi appears before wi+1 in transla-
tion references. Note than the two words may not be adja-
cent in the references even if they have correct word order. 
reordering since only structured information is 
used in the tree kernel5. 
The CTs performs the worst among the 4 
structured features. This suggests that the middle 
and high-level structures beyond base phrases are 
very useful for phrase reordering. The MSSs 
show lower performance than the CMSSs and 
the MSTs achieve the best performance. This 
clearly indicates that the structured context in-
formation is useful for phrase reordering. For this 
reason, the subsequent discussions are focused 
on the MSTs, unless otherwise specified. The 
MSSs without using the 5 function tags perform 
much worse than the original ones. This suggests 
that the partitions of the structured feature spaces 
are very helpful, which can effectively avoid the 
undesired matching between partitions of differ-
ent functionalities. Comparison of Kl and Kl-LM 
shows the LM plays an important role in phrase 
reordering. The composite kernel (Kc) performs 
much better than the two individual kernels. This 
suggests that the structured and linear features 
are complementary and the composite kernel can 
well integrate them for phrase reordering. 
 
Methods CAcc (%) 
Dev Test 
Minimum Sub-Tree (MST) 89.87 89.92
Linear Features (Kl) 90.79 90.46
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MaxEnt+boundary word (B2) 88.33 86.97
MaxEnt+linguistic features (B3_1) 84.83 83.92
MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18
 
Table 2: Performance comparison of different me-
thods 
 
Table 2 compares the performance of the base-
line methods with ours. Comparison between 
B3_1 and MST clearly demonstrates that the 
structured syntactic features are much more ef-
fective than the linear syntactic features that are 
manually extracted via heuristics. It also suggests 
that the tree kernel can well capture the struc-
tured features implicitly. Kl outperforms B2. This 
is mainly due to the contribution of LM features. 
B2 (MaxEnt-based) significantly outperforms Kl-
LM in Table 1 (SVM-based). This suggests that 
phrase reordering may not be a good linearly bi-
nary-separable task if only boundary word fea-
tures are used. Our composite kernel (Kc) signifi-
cantly outperforms LABTG (B3). This mainly 
                                                 
5 The tree kernel algorithm only compares internal struc-
tures. It does not concern any lexical leaf nodes.   
704
attributes to the contributions of structured syn-
tactic features, LM and the tree kernel. 
 
Forced Decoding: Table 3 compares the per-
formance of our composite kernel with that of 
the LABTG (Baseline 3) in forced decoding. As 
discussed before, here we try two translation 
models.  
The composite kernel outperforms the 
LABTG in all test cases. This further validates 
the effectiveness of the kernel methods in phrase 
reordering. There are still around 30% words 
reordered incorrectly even if we use the transla-
tion model trained on both training, dev and test 
sets. This reveals the limitations of current SMT 
modeling methods and suggests interesting fu-
ture work in this area. The source language 
OOV6 rate in forced decoding (13.6%) is much 
higher that in normal decoding (6.22%, see table 
4). This is mainly due to the fact that the phrase 
table in forced decoding is filtered out based on 
both source and target languages while in normal 
decoding it is based on source language only. As 
a result, more phrases are filtered out in the 
forced decoding. There is 1.4% OOV even if the 
translation model is trained on the test set. This is 
due to the incorrect word alignment, large-span 
word alignment and different English tokeniza-
tion strategies used in BLEU-scoring tool and 
ours. 
 
Methods Test Set (%) 
RAcc OOV BLEU
Composite Kernel (Kc) 
  +translation model on 
   Training, dev and test 
51.03 
72.67 
 
13.6 
1.41 
 
38.56 
62.87 
 
MaxEnt+LABTG (B3) 
  +translation model on  
    training, dev and test 
48.96 
71.45 
 
13.6 
1.41 
 
37.32 
62.14 
 
 
Table 3: Performance comparison of forced de-
coding 
 
Methods Test Set 
 BLEU(%) OOV(%)
Composite Kernel (Kc) 27.65 6.26 
Moses (B1) 25.71 6.17 
MaxEnt+boundary word(B2) 25.99 6.22 
MaxEnt+LABTG (B3) 26.63 6.22 
 
Table 4: Performance comparison 
 
                                                 
6 OOV means a source words has no any English translation 
according to the translation model. OOV rate is the percent-
age of the number of OOV words over all the source words.  
Normal Decoding/Translation: Table 4 reports 
the translation performance of our system and 
the three baseline systems. 
Moses (B1) and the MaxEnt-based boundary 
word model (B2) achieve comparable perfor-
mance. This means the lexicalized orientation-
based reordering model in Moses performs simi-
larly to the boundary word-based reordering 
model since the two models are both lexical 
word-based. However, theoretically, the Max-
Ent-based model may suffer less from data 
sparseness issue since it does not depends on 
internal phrasal words and uses MaxEnt to op-
timize feature weights while the orientation-
based model uses relative frequency of the entire 
phrases to compute the posterior probabilities. s. 
The MaxEnt-based LABTG model significantly 
outperforms (p<0.05) the MaxEnt-based boun-
dary word model and the lexicalized orientation-
based reordering model. This indicates that the 
linearly linguistically syntactic information is a 
useful feature to phrase reordering. 
Our composite kernel-based model signifi-
cantly outperforms (p<0.01) the three baseline 
methods. This again proves that the structured 
syntactic features are much more effective than 
the linear syntactic features for phrase reordering 
and the tree kernel method can well capture the 
informative structured features. The four me-
thods show very slight difference in OOV rates. 
This is mainly due to the difference in implemen-
tation detail, such as different OOV penalties and 
other pruning thresholds.  
5 Conclusion and Future Work 
Structured syntactic knowledge is very useful to 
phrase reordering. This paper provides insights 
into how the structured feature can be used for 
phrase reordering. In previous work, the struc-
tured features are selected manually by heuristics 
and represented by a linear feature vector. This 
may largely compromise the contribution of the 
structured features to phrase reordering. Thanks 
to the nice properties of kernel-based learning 
method and SVM classifier, we propose leverag-
ing on the kernelized SVM learning algorithm to 
address the problem. Specifically, we propose 
using convolution tree kernel to capture the 
structured features and design a composite kernel 
to combine the structured features and other li-
near features for phrase reordering. The tree ker-
nel is able to directly take the structured reorder-
ing instances as inputs and compute their similar-
ities without enumerating them into a set of liner 
705
features. In addition, we also study how to find 
the optimal structured feature space and how to 
partition the structured feature spaces according 
to their functionalities. Finally, we evaluate our 
method on the NIST MT-2005 Chinese-English 
translation tasks. To provide insights into the 
model, we design three kinds of experiments to-
gether with three different evaluation metrics. 
Experimental results show that the structured 
features are very effective and our composite 
kernel can well capture both the structured and 
the linear features without the need for extensive 
feature engineering. It also shows that our me-
thod significantly outperforms the baseline me-
thods. 
The tree kernel-based phrase reordering me-
thod is not only applicable to adjacent phrases. It 
is able to work with any long phrase pairs with 
gap of any length in-between. We will study this 
case in the near future. We would also like to use 
one individual tree kernel for one partition in a 
structured feature space. In doing so, we are able 
to give different weights to different partitions 
according to their functionalities and contribu-
tions. Note that these weights can be automati-
cally tuned and optimized easily against a dev 
set. 
References 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
Michael Collins and N. Duffy. 2001. Convolution 
Kernels for Natural Language. NIPS-2001. 
M. R. Costa-jussà and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. EMNLP-06. 70-76. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Jakob Elming. 2008. Syntactic Reordering Integrated 
with Phrase-Based SMT. COLING-08. 209-216. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reorder-
ing Model. EMNLP-08. 848-856. 
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. TR UCS-CRL-99-10. 
T. Joachims. 1998. Text Categorization with SVM: 
learning with many relevant features. ECML-98. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95, 181-184 
Philipp Koehn, F. Och and D. Marcu. 2003. Statistical 
phrase-based translation. HLT-NAACL-03.  
Philipp Koehn, H. Hoang, A. Birch, C. C.-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
SMT. ACL-07 (poster). 77-180. 
Shankar Kumar and William Byrne. 2005. Local 
Phrase Reordering Models for Statistical Machine 
Translation. HLT-EMNLP-2005. 161-168. 
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, 
Minghui Li and Yi Guan. 2007. A Probabilistic 
Approach to Syntax-based Reordering for Statis-
tical Machine Translation. ACL-07. 720-727. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: SMT with Syntactified Target Lan-
guage Phrases. EMNLP-06. 44-52. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-04. 
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto 
and Kazuteru Ohashi. 2006. A Clustered Global 
Phrase Reordering Model for Statistical Machine 
Translation. COLING-ACL-06. 713-720. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and H. Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Methods. 
Computational Linguistics, 29(1):20-51. 
Franz J. Och and H. Ney. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, S. Roukos, T. and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-02. 311-318. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 
2007. Ordering Phrases with Function Words. 
ACL-07. 712-719. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
New String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. ACL-HLT-08. 577-585. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. HLT-
NAACL-04 (short paper). 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
706
Chao Wang, M. Collins and P. Koehn. 2007. Chinese 
Syntactic Reordering for Statistical Machine 
Translation. EMNLP-CONLL-07. 734-745. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpo-
ra. Computational Linguistics, 23(3):377-403. 
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT System with Automatically Learned 
Rewrite Patterns. COLING-04. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521–528. 
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li. 
2008. A Linguistically Annotated Reordering Mod-
el for BTG-based Statistical Machine Translation. 
ACL-HLT-08 (short paper). 149-152. 
Kenji Yamada and K. Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. 
Kernel-Based Pronoun Resolution with Structured 
Syntactic Knowledge. COLING-ACL-06. 41-48. 
Richard Zens, H. Ney, T. Watanabe and E. Sumita. 
2004. Reordering Constraints for Phrase-Based 
Statistical Machine Translation. COLING-04. 
Richard Zens and Hermann Ney. 2006. Discrimina-
tive Reordering Models for Statistical Machine 
Translation. WSMT-2006. 
Dell Zhang and W. Lee. 2003. Question classification 
using support vector machines. SIGIR-03. 
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-06. 825-832. 
Dongdong Zhang, M. Li, C.H. Li and M. Zhou. 
2007a. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT. EMNLP-CONLL-07. 
533-540. 
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu 
and S. Li. 2007b. A Grammar-driven Convolution 
Tree Kernel for Semantic Role Classification. 
ACL-07. 200-207. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007c. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation.MT-Summit-07. 535-542 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li, 
Chew Lim Tan and Chew Lim Tan and Sheng Li. 
2008a. A Tree Sequence Alignment-based Tree-to-
Tree Translation Model. ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104 
Ying Zhang, Stephan Vogel and Alex Waibel. 2004. 
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
707
