Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 766–776, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Cross-Lingual Language Modeling with Syntactic Reordering for
Low-Resource Speech Recognition
Ping Xu and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong
xuping@ust.hk, pascale@ece.ust.hk
Abstract
This paper proposes cross-lingual language
modeling for transcribing source resource-
poor languages and translating them into tar-
get resource-rich languages if necessary. Our
focus is to improve the speech recognition
performance of low-resource languages by
leveraging the language model statistics from
resource-rich languages. The most challeng-
ing work of cross-lingual language modeling
is to solve the syntactic discrepancies between
the source and target languages. We therefore
propose syntactic reordering for cross-lingual
language modeling, and present a first result
that compares inversion transduction grammar
(ITG) reordering constraints to IBM and lo-
cal constraints in an integrated speech tran-
scription and translation system. Evaluations
on resource-poor Cantonese speech transcrip-
tion and Cantonese to resource-rich Mandarin
translation tasks show that our proposed ap-
proach improves the system performance sig-
nificantly, up to 3.4% relative WER reduction
in Cantonese transcription and 13.3% relative
bilingual evaluation understudy (BLEU) score
improvement in Mandarin transcription com-
pared with the system without reordering.
1 Introduction
Statistical language modeling techniques have
achieved remarkable success in speech and language
processing (Clarkson and Rosenfeld, 1997; Stolcke,
2002). However, this success largely depends on the
availability of a large amount of suitable text data in
a language. Without sufficient text data for training,
it is very difficult to build a practical and usable sta-
tistical language model. Therefore, most of the ad-
vances have been reported in so called resource-rich
language such as English, Mandarin and Japanese,
after creating linguistic resources of these languages
at considerable cost. Today there are more than
6000 living languages spoken in the world (Gordon
et al., 2005), and most of them have little transcribed
texts and are considered as resource-poor languages
(Nakov and Ng, 2009). Many of these languages are
actually spoken by a huge number of speakers (e.g.
some Chinese and Indian languages), and thus there
is still a great demand to build speech and language
processing systems for these languages.
Owing to data scarcity, most often an interpo-
lation (Bellegarda, 2004) of language models be-
tween a resource-poor language and a resource-rich
language is used in most low-resource ASR sys-
tems. Some researchers have proposed transform-
ing resource-rich language models to resource-poor
language models by word-level transduction, either
in a context-independent or context-dependent man-
ner (Hori et al., 2003; Akita and Kawahara, 2006;
Jensson et al., 2009; Neubig et al., 2010). In (Jens-
son et al., 2009), a simple dictionary based context-
independent transduction from a resource-rich lan-
guage to a resource-poor language is exploited to
improve speech recognition of the resource-poor
language. In (Hori et al., 2003; Akita and Kawahara,
2006; Neubig et al., 2010), context-dependent trans-
duction is exploited. In their case, the resource-poor
language is a spoken language, and the resource-rich
language is a written language. They carried out lan-
guage model transformation since the input speech
766
is in speaking-style and the output text is in written-
style.
Others have investigated cross-lingual informa-
tion between a resource-poor language and a
resource-rich language. In (Khudanpur and Kim,
2002), cross-language cues are used to improve a
language model of a resource-poor language. They
used cross-lingual unigram probabilities trained
from a story-specific parallel corpus of the resource-
poor and resource-rich languages. They interpo-
late the language model of the resource-poor lan-
guage with those unigram probabilities. In (Kim and
Khudanpur, 2003), an n-gram language model in a
resource-poor language is interpolated with cross-
lingual unigram trigger probabilities. These triggers
are word pairs of the resource-poor and resource-
rich languages with the highest mutual information
across these two languages. Another way of esti-
mating those unigram probabilities is using latent
semantic analysis by measuring cosine similarities
from a document-aligned corpus for any given word
pair (Kim and Khudanpur, 2004).
Both interpolation and word-level transduction
approaches fail to meet the challenge of syntac-
tic discrepancies between the resource-poor and
resource-rich languages. This syntactic discrepan-
cies exist, for example, even between the Sinitic lan-
guages and Indian languages1 of the same family.
Sinitic languages such as Cantonese/Yue, Shang-
hai/Wu, etc. are officially considered as ”dialects”
of the standard Chinese Mandarin (or Putonghua)2.
However, they differ greatly from Mandarin in all
aspects and are not mutually comprehensible. For
instance, in addition to lexical and pronunciation
differences, Cantonese Chinese (Lee, 2011) differs
syntactically from Mandarin as well - we found that
there are approximately 10% syntactic inversions
between sentences of the two forms of Chinese.
We suggest that a better approach than interpo-
lation and word-level transduction is to use cross-
lingual language modeling with syntactic reorder-
1For example, Hindi and Malayalam (Geethakumary, 2002).
2Since Cantonese does not have an official written form,
there are very few written texts available for training language
models. In this paper, we treat Cantonese as a typical resource-
poor language and Mandarin as a typical resource-rich lan-
guage. This language pair will be used for illustration purposes
throughout this paper.
ing. A reordering model with reordering constraints,
such as ITG constraints (Wu, 1997), IBM con-
straints (Berger et al., 1996), and local constraints
(Kumar and Byrne, 2005) can account for the syn-
tactic differences. It has been shown in (Zens and
Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007)
that ITG constraints perform better than other con-
straints when tackling the reordering between many
language pairs. Previous work on weighted finite-
state transducer (WFST) based speech translation
such as (Casacuberta et al., 2004; Zhou et al., 2005;
Zhou et al., 2006; Mathias and Byrne, 2006; Ma-
tusov et al., 2006; Saon and Picheny, 2007) only
train the reordering model using IBM constraints,
local constraints or ad hoc rules. We will use
ITG constraints, which have only been applied to
text translation tasks before, to model the syntactic
differences in cross-lingual language modeling for
speech recognition.
We will implement a cross-lingual language
model using WFSTs, and integrate it into a WFST-
based speech recognition search space to give both
resource-poor language and resource-rich language
transcriptions. This creates an integrated speech
transcription and translation framework.
This paper is organized as follows: Section 2
presents our proposed cross-lingual language mod-
eling with syntactic reordering. In Section 3, we dis-
cuss speech recognition with cross-lingual language
models. Section 4 and 5 give the experimental setup
and results. We conclude our work at the end of this
paper.
2 Cross-lingual Language Modeling with
Syntactic Reordering
In automatic speech recognition (ASR), given an ob-
served source speech vector X, the decoding pro-
cess searches the best word sequence vˆI1 (consists
of words v1, v2, ..., vI ) by maximizing the posterior
probability P (vI1 |X), where vI1 is the source tran-
script representing the transcription of the source
speech (see Eq. (1)). According to Bayes’ law,
we can decompose P (vI1 |X) into an acoustic model
P (X|vI1) and a language model P (vI1). If a source
language Lv is a resource-rich language, then the
language model P (vI1) can be well estimated from
sufficient training texts. However, if the source lan-
767
guage Lv is a resource-poor language, then the lan-
guage model P (vI1) cannot be reliably or robustly
estimated due to lack of training texts.
vˆI1 = argmax
vI1
P (vI1 |X) (1)
= argmax
vI1
P (X|vI1)P (vI1)
= argmax
vI1
P (X|vI1)
?
wJ1
P (vI1 |wJ1 )P (wJ1 )
? argmax
vI1
P (X|vI1)max
wJ1
P (vI1 |wJ1 )P (wJ1 )
Since this paper tackles the language modeling
challenge for low-resource speech recognition, here
we just assume that the source language Lv is a
resource-poor language. We further assume that
there is a target language Lw, which is a resource-
rich language closely related to the language Lv.
In order to improve the language model P (vI1)
of the resource-poor language Lv, we introduce
cross-lingual language modeling by decomposing
the language model P (vI1) into a translation model
P (vI1 |wJ1 ) and a language model P (wJ1 ) of the
resource-rich language Lw (see Eq. (1)). wJ1 is
the target resource-rich language transcript that con-
sists of words w1, w2, ..., wJ . P (vI1 |wJ1 )P (wJ1 ) is
defined as a cross-lingual language model. It lever-
ages the abundant statistics from the language model
P (wJ1 ) to improve the language model P (vI1) of the
resource-poor language.
The translation model P (vI1 |wJ1 ) can be esti-
mated by addressing the discrepancies between the
resource-poor language Lv and the resource-rich
language Lw, which can be modeled from a paral-
lel corpus of the Lv transcript vI1 and the Lw tran-
script wJ1 . For the syntactic inversions, we reorder
the word or phrase positions of the Lw language
model into those of the Lv language model. We
have observed that most of the words are aligned
monotonically between Lv and Lw within a phrase.
This paper, therefore only considers phrase-level re-
ordering, which effectively preserves the monotonic
word sequences within phrases, and significantly re-
duces the number of reordering paths compared with
word-level reordering.
2.1 Preprocessing: Phrase Extraction and
Segmentation
Our discussion starts with phrase extraction from the
parallel corpus. We define a phrase sequence v˜K1
(consists of phrases v˜1, v˜2, ..., v˜K ) segmented from
the word-level Lv transcript vI1 and w˜K1 (consists of
phrases w˜1, w˜2, ..., w˜K ) segmented from the word-
level Lw transcript wJ1 . Furthermore, we define a
reordering sequence rK1 , of which the detail can be
found in Section 2.2.
The phrase-level translation model P (vI1 |wJ1 ) is
decomposed into four components (see Eq. (2)):
segmentation model P (w˜K1 |wJ1 ), phrasal reorder-
ing model P (rK1 |w˜K1 , wJ1 ), phrase-to-phrase trans-
duction model P (v˜K1 |rK1 , w˜K1 , wJ1 ) and reconstruc-
tion model P (vI1 |v˜K1 , rK1 , w˜K1 , wJ1 ). Before present-
ing each component model, we need to extract two
phrase tables for the Lv transcript and the Lw tran-
script, respectively.
P (vI1 |wJ1 ) ? max
v˜K1 ,r
K
1 ,w˜
K
1
P (w˜K1 |wJ1 ) ·
P (rK1 |w˜K1 , wJ1 ) ·
P (v˜K1 |rK1 , w˜K1 , wJ1 ) ·
P (vI1 |v˜K1 , rK1 , w˜K1 , wJ1 ) (2)
The phrase extraction is based on word-to-word
alignments of the parallel corpus. We train word
alignments in both directions with GIZA++, and
then symmetrize the two alignments using the re-
fined method (Och and Ney, 2003). Figure 1 shows
an example of word-to-word alignment results be-
tween an Lv transcript (Cantonese) and an Lw
transcript (Mandarin), from which phrase-to-phrase
alignments are derived by identifying deletion, sub-
stitution, insertion and inversion.
Prior to phrasal reordering, the segmentation
model P (w˜K1 |wJ1 ) implemented by a segmentation
WFST Sw is applied to segment a word sequence
wJ1 in the Lw language model into a phrase sequence
{w˜1, w˜2, ..., w˜K}. The maximum number of words
that can be segmented into one phrase is controlled
by a segmentation order s. An example of Sw is
shown in Figure 3(a1). It segments a word sequence
{w1, w2, w3} into a phrase sequence {w1, w2 w3}
after performing composition (Mohri, 2009) with the
target Lw language model (see Figure 3(b1 & b2))3.
3The “ ” symbol is used to indicate the concatenation of con-
768
DeletionSubstitution Inversion Inversion & Insertion
i
iv
ji 
'
~
kv
kw
~
jw
j
4-6
4-7
1 2
1-1 2-2
??? ?
k’=1
3
3-4
?
k’=2
4
?
k’=3
5
5-8
??
k’=4
6
6-5
?
k’=5
7 8
7-3 8-3
? ?
k’=6
??
k=1
21
?
k=2
3
?
k=3
4
?
k=4
5
? ?
k=5
6 7
??
k=6
8
Substitution Substitution
Figure 1: An example (in English: Please give me an ad-
dress first) of phrase extraction from word-to-word align-
ments. i and j are word indexes. k? and k are phrase
indexes. i?j represents the word-to-word alignment.
k?k? represents the indentified phrase-to-phrase align-
ment.
2.2 Phrasal Reordering Model
Given a phrase sequence {w˜1, w˜2, ..., w˜K} of the
Lw transcript, the role of the reordering model
P (rK1 |w˜K1 , wJ1 ) is to reorder phrase positions of the
Lw transcript into those of the Lv transcript by per-
mutation of w˜K1 according to a reordering sequence
{rK1 : rk ? {1, 2, ...,K}, rk 6= rk? 6=k}. The
phrase sequence {w˜1, w˜2, ..., w˜K} is therefore re-
ordered into {w˜r1 , w˜r2 , ..., w˜rK } consequently (see
Figure 2 where K = 3). Since arbitrary permuta-
tions of K phrases are NP-hard (Knight, 1999), re-
ordering constraints have to be set over rK1 to reduce
the number of permutations.
There are three reordering constraints widely used
in statistical machine translation, namely local con-
straints, IBM constraints and ITG constraints. Here
we would like to point out that this is the first
time that reordering constraints have been incorpo-
rated into a cross-lingual language model for speech
recognition.
Reordering Constraints
Local constraints make the restriction that one
phrase can jump at most L?1 phrases either forward
or backward, where L is the reordering distance (or
window size of permutation)4 . The generation of rK1
under local constraints can be viewed as solving of
the following problem (Kløve, 2009):
secutive words forming a phrase.
4The concept of reordering distance also applies to other
constraints.
How many permutations of
{1, 2, . . . k . . . ,K} satisfy |rk ? k| < L
for all k?
IBM constraints, a superset of local constraints
(Dreyer et al., 2007), generate permutations rK1 de-
viate from the monotonic phrase order {rK1 : rk =
k}. More specifically, any phrase position rk can be
selected from the positions of the first m yet uncov-
ered phrases (see Eq. (3)). A typical value of m is 4
(Zens and Ney, 2003), and we write IBM constraints
with m = 4 as IBM(4).
rk ?
?
?
?
?
?
?
?
?
?
?
?
{1, 2, ..., k ? 1 +m; rk 6= rk? 6=k}
if k ? K + 1?m,
{1, 2, ...,K; rk 6= rk? 6=k}
if K + 1?m < k ? K.
(3)
ITG constraints provide a more faithful coverage
of syntactic reordering in the parallel data than lo-
cal constraints and IBM constraints. Our presenta-
tion of ITG constraints starts with defining of some
permutation sets. Let SK be the set of permuta-
tions on {1,2,. . . ,K}. A permutation rK1 ? SK ,
where rK1 = r1r2 . . . rK , contains a subsequence
of type ? ? SM if and only if a sequence of in-
dices 1 ? i1 < i2 < . . . < iM ? K exists such
that ri1ri2 . . . riM has all the same pairwise compar-
isons as ? . We denote the set of permutations of SK
not containing subsequences of type ? by SK(?). If
we have sets SK(?1), . . . , SK(?p), we denote the set
SK(?1)? . . .?SK(?p) by SK(?1, . . . , ?p) (Barcucci
et al., 2000). ITG constraints allow the permutation
set SK(3142, 2413), which forbids subsequence of
type (3, 1, 4, 2) and its dual (2, 4, 1, 3). Explicitly,
ITG constraints avoid any permutation rK1 satisfy-
ing either ri2 < ri4 < ri1 < ri3 or ri3 < ri1 <
ri4 < ri2 , where 1 ? i1 < i2 < i3 < i4 ? K . In
(Wu, 1997), these forbidden subsequences are called
“inside-out” transpositions. They are fairly distorted
matchings, and hardly observed in real parallel data.
In order to get an intuitive sense of the reordering
capability of those three constraints, we list the num-
ber of permutations under local constraints, IBM
constraints as well as ITG constraints5 in Table 1.
5Interestingly, when K = L, the number of permuta-
tions under ITG constraints NITG = |SK(3142, 2413)|, and
|SK(3142, 2413)| equals the K?1-th Schro¨der numbers sK?1
(Ehrenfeucht et al., 1998)
769
Table 1: Comparison of permutation number under local constraints (NLocal), IBM constraints (NIBM(4)) and ITG
constraints (NITG). The comparison is constrained by the phrase number K and the reordering distance L.
K=2 K=3 K=4 K=5 K=6 K=7 K=8 K=9 K=10
NLocal 2 3 5 8 13 21 34 55 89
L=2 NIBM(4) 2 3 5 8 13 21 34 55 89
NITG 2 3 5 8 13 21 34 55 89
NLocal 2 6 14 31 73 172 400 932 2177
L=3 NIBM(4) 2 6 14 31 73 172 400 932 2177
NITG 2 6 12 25 57 124 268 588 1285
NLocal 2 6 24 78 230 675 2069 6404 19708
L=4 NIBM(4) 2 6 24 78 230 675 2069 6404 19708
NITG 2 6 22 52 122 321 885 2304 5880
NLocal 2 6 24 120 504 1902 6902 25231 95401
L=5 NIBM(4) 2 6 24 96 330 1066 3451 11581 39264
NITG 2 6 22 90 236 602 1714 5269 16385
NLocal 2 6 24 120 720 3720 17304 76110 329462
L=6 NIBM(4) 2 6 24 96 384 1374 4718 16275 57749
NITG 2 6 22 90 394 1108 3014 9038 29618
We can see that given the same K (K ? 10) and
L (L ? 6), IBM constraints have less permutations
than local constraints, and ITG constraints have less
permutations than IBM constraints in general (only
one exception when K = L = 6). These obser-
vations indicate that ITG constraints can filter out
more unlikely permutations for a fixed reordering
distance, resulting in longer distance reordering ca-
pability.
Table 1 also tells us that the phrase number K
and the reordering distance L for any of the con-
straints cannot be too large for practical implemen-
tation. For instance, if L = 6 and K goes from 6 to
7, the order of magnitude of NLocal, NIBM(4) and
NITG increases from 2 to 3. Hence, phrases for per-
mutation should be selective to cover the most pos-
sible re-orderings. If long reordering distances are
allowed, unlikely permutations should be pruned so
that the memory consumption becomes manageable.
Reordering Sequence Distribution
So far we have discussed the issue that how to
generate permutations for the reordering model us-
ing reordering constraints. Another issue is how to
parameterize the reordering sequence distribution.
Both ITG constraints and other constraints assume
that all permutations are equally probable. However,
it makes sense to restrict those non-monotonic re-
orderings when performing the translation. This not
only helps the search of the most likely permutation,
but also guides the pruning of unlikely permutations.
P (rK1 |w˜K1 , wJ1 ) = P (r1)
K
?
k=2
P (rk|rk?1, w˜K1 )
= P (r1)
K
?
k=2
P (rk|rk?1) (4)
We make a first order Markov assumption over the
phrasal reordering model P (rK1 |w˜K1 , wJ1 ) (see Eq.
(4)). The reordering sequence distribution is param-
eterized to assign decreasing likelihood to phrase re-
orderings {w˜r1 , w˜r2 , . . . , w˜rK} that diverge from the
original word order (Och et al., 1999; Kumar et al.,
2005). Suppose w˜rk = wl
?
l and w˜rk?1 = w
q?
q , the
reordering sequence distribution is set as Eq. (5),
where p0 is a tuning factor. We normalize the proba-
bilities P (rk|rk?1) such that
?K
k?=1,k? 6=rk?1 P (rk =
k?|rk?1) = 1.
P (rk|rk?1) = p|l?q
??1|
0
P (r1 = k) =
1
K ; k ? {1, 2, ...,K}
(5)
770
Assume that we have a phrase sequence
{w˜1, w˜2, w˜3}, Figure 2 shows the phrasal reordering
model implemented by a reordering WFST ?r under
the first order Markov assumption for this phrase se-
quence.
Figure 3(a2) gives one more example of ?r,
which reorders the phrase sequence {w1, w2 w3}
into {w2 w3, w1}6. Within the WFST paradigm, re-
ordering models under any of those constraints can
be integrated into the cross-lingual language model.
)(/~:~ 111 rPwwr )|(/
~:~ 1222 rrPwwr )|(/
~:~ 2333 rrPwwr
Figure 2: An example of reordering WFST ?r imple-
menting the phrasal reordering model under the first or-
der Markov assumption.
2.3 Phrase-to-Phrase Transduction Model
Once the phrase sequence of the Lw transcript
is reordered into the Lv transcript order, we use
the phrase-to-phrase transduction model specified in
Eq. (6) to perform the cross-language transduction.
Given sufficient parallel training data, the context-
dependent phrase-to-phrase transduction model can
be estimated using the GIATI method (Casacu-
berta and Vidal, 2004). However, for the trans-
lation task with scarce training data, the context-
dependent transduction probabilities may not be re-
liably estimated. Therefore, we assume that a phrase
v˜k is generated independently by each phrase w˜rk .
C(v˜k, w˜rk) is the number of times that phrase v˜k is
aligned to w˜rk in the parallel corpus. This model can
be implemented by a WFST Tvw which transduces
v˜k to w˜rk . Figure 3(a3) shows an example of Tvw
transducing v2 v3 to w2 w3.
P (v˜K1 |rK1 , w˜K1 , wJ1 ) = P (v˜K1 |rK1 , w˜K1 )
=
K
?
k=1
Pk(v˜k|w˜rk)
=
K
?
k=1
C(v˜k, w˜rk)
?
v˜k
C(v˜k, w˜rk)
(6)
2.4 Reconstruction Model
Reconstruction model P (vI1 |v˜K1 , rK1 , w˜K1 , wJ1 ) oper-
ates in the opposite direction as the segmentation
6For simplicity, reordering sequence distributions are not
shown there.
model. It generates a word sequence vI1 from a
phrase sequence v˜K1 . The reconstruction model can
be implemented by a WFST Rv. An example of
Rv is shown in Figure 3(a4), which reconstructs a
phrase v2 v3 into a word sequence {v2, v3}.
3 Speech Recognition with Cross-Lingual
Language Models
The translation model P (vI1 |wJ1 ) can be constructed
via WFST composition (denoted by ?) (Mohri,
2009) of all the component models as shown in Eq.
(7) and Figure 3, where T is the final composed
WFST that transduces vI1 to wJ1 .
T = Rv ? Tvw ? ?r ? Sw (7)
The cross-lingual language model Gcl is con-
structed through composition (see Eq. (8)) of
the translation model and a resource-rich language
model G.
Gcl = T ?G = Rv ? Tvw ? ?r ? Sw ?G (8)
As the way of integrating a resource-rich lan-
guage model G into ASR search space (Mohri et al.,
2008), we can integrate the cross-lingual language
model Gcl into ASR search space in a globally op-
timized way as well. The search space can be im-
plemented using a transducer ASR, which is for-
mulated with a unified WFST approach as shown
in Eq. (9). Here H transduces HMM states to
context-dependent phones. C represents a trans-
duction from context-dependent phones to context-
independent phones. L is a lexicon transducer which
maps context-independent phone sequences to word
strings restricted to the input symbols of the cross-
lingual language model transducer Gcl.
ASR = H ? C ? L ?Gcl (9)
Eq. (9) outputs the recognition result in a resource-
rich language. If recognition system requires recog-
nition outputs in a resource-poor language, then the
search space should be constructed as Eq. (10),
where ? is a projection (Mohri, 2009) operator
which projects the input label to the output label.
Before decoding, the recognition transducer ASR
can be optimized by a determinization operation
right after each composition.
ASR = H ? C ? L ? ?(Gcl) (10)
771




  
  
(a1) Segmentation WFST Sw (b1) Written-style language model G











0 1w 1 : w 1
2w 2 : w 2
3
w 2 _ w 3 : w 2 4
w 3 : w 3
- :w3
(a2) Reordering WFST ?r (b2) Sw ?G





		
		




 	








(a3) Phrase-to-phrase transduction WFST Tvw (b3) ?r ? Sw ?G
 






		
	




		
	





	



	





0
1w 1 : w 1
2v2_v3:w1
3
w 1 _ # 1 : w 1
4
v2_v3:w2
w 1 : w 2
v 2 _ v 3 _ # 1 : w 2
5w 2 : w 2
6
- :w3
w 3 : w 3
(a4) Reconstruction WFST Rv (b4) Tvw ? ?r ? Sw ?G
 



	













	







(b5) Rv ? Tvw ? ?r ? Sw ?G
Figure 3: Illustration of constructing a cross-lingual language model via WFSTs: a word sequence {w1, w2, w3}
represented by the Lw language model G (b1) is segmented into a phrase sequence {w1, w2 w3} (b2); {w1, w2 w3} is
reordered into {w2 w3, w1} (b3); phrase w2 w3 is transduced to v2 v3 (b4); phrase v2 v3 is reconstructed into a word
sequence {v2, v3} (b5). wk and vk represent wk and vk, respectively. ”-” refers to ? or null symbol. Auxiliary symbols
#1,#2, · · · are used to make the WFST determinizable (Mohri, 2009) such that the transducer can be optimized by a
determinization (Mohri, 2009) operation which significantly reduces the search network size.
772
4 Experimental Setup
4.1 Corpus and Model Training
To investigate the performance of our proposed
cross-lingual language models, we have chosen
Cantonese as a resource-poor language and Man-
darin as a resource-rich language. We have col-
lected Cantonese parliamentary speech from the
Hong Kong Legislative Council. Currently we only
have 4152 parallel transcribed sentences containing
19.4 hours of speech. It is separated into three sets,
a training set (11.9 hours, 2700 sentences), a de-
velopment set (3.7 hours, 788 sentences), and an
evaluation set (3.8 hours, 664 sentences). The sen-
tences in the evaluation set are a bit longer than
those in the development set. The parallel transcrip-
tions of the training set constitute a parallel cor-
pus, which includes Cantonese transcription (man-
ual transcription) of 106k words and Mandarin tran-
scription (Hansard7 transcription) of 80k words. The
statistics of substitutions, insertions, deletions and
inversions identified in the parallel corpus are shown
in Table 2. Besides the parallel corpus, we have a
set of additional Mandarin transcriptions, which has
31M words.
Table 2: No. of substitutions, insertions, deletions and
inversions identified in the parallel corpus with different
segmentation order s.
Segmentation Order s = 2 s = 3 s = 4 s = 5
Substitutions 30921 22723 19011 17106
Insertions 4657 3820 3641 3295
Deletions 1365 1158 1066 1030
Inversions 3000 2876 2814 2779
Total 39943 30577 26532 24210
The training set is used for training an acous-
tic model (including H and C) using a Maximum
Likelihood criterion. It adopts 13 MFCC coeffi-
cients, together with 13 delta coefficients and 13 ac-
celeration coefficients as the acoustic features. The
acoustic model comprises 73 Hidden Markov Mod-
els (HMMs) to represent 70 Cantonese phonemes as
well as silence, short pause, and noise. During the
acoustic model training, tied-state cross-word tri-
phones are constructed by decision tree clustering.
7Hansard is a name of the printed transcripts of parliamen-
tary debates.
The parallel corpus is used for training the trans-
lation model T . Together with the parallel corpus,
the additional Mandarin transcriptions are used for
training an interpolated word-level trigram language
model G, where the lexicon size is about 28K. A
modified scheme of Kneser-Ney discounting is ap-
plied for the language model G with a back-off
threshold of 1 for unigram and 2 for bigram. The
cross-lingual language model Gcl can be obtained
by composition of T and G.
4.2 Decoding and Evaluation Method
Decoding of the speech recognition search space
ASR is performed by T 3 Decoder (Dixon et
al., 2009), which is a state-of-the-art WFST-based
LVCSR speech decoder. Decoding of ASR in Eq.
(9) gives Mandarin outputs. Decoding of ASR in
Eq. (10) gives Cantonese outputs.
In our experiments, we use the following evalua-
tion criteria:
WER (word error rate). The WER is computed
as the minimum number of substitution, insertion
and deletion operations that have to be performed
to convert the generated sentence into the reference
sentence (Zens et al., 2004). The WER relates the
speech recognition accuracy. The lower WER, the
better.
BLEU (bilingual evaluation understudy) score.
The BLEU score measures the precision of n-grams
(unigrams, bigrams, trigrams and fourgrams) with
respect to a reference translation with a penalty for
too short sentences (Papineni et al., 2002). The
BLEU score reflects the translation accuracy. The
larger BLEU score, the better.
We perform WER evaluation of decoding out-
puts of Eq. (10) and BLEU score evaluation of
decoding outputs of Eq. (9) using the evaluation
set. The WER evaluation is on the Cantonese output
against the Cantonese reference transcription (man-
ual transcription). The BLEU score evaluation is on
the Mandarin output against the Mandarin reference
transcription (Hansard transcription).
4.3 Parameter Settings
The performance of our proposed cross-lingual lan-
guage models is sensitive to many parameters.
Firstly, segmentation order s affects phrase extrac-
tion. The optimal value depends on the language
773
Table 3: WER and BLEU score for decoding results of H ? C ? L ?G, H ? C ? L ? ?(Gcl) without reordering, and
H ? C ? L ? ?(Gcl) with reordering under various constraints.
Models H ? C ? L ?G
H ? C ? L ? ?(Gcl) H ? C ? L ? ?(Gcl)
Gcl = T3 ?G Gcl = T3 ?G,T3 = Rv ? Tvw ??r ? Sw
T3 = Rv ? Tvw ? Sw Local Constraints IBM Constraints ITG Constraints
WER(%) 29.85 27.05 26.35 26.20 26.13
BLEU N/A 29.23 32.29 32.81 33.12
pair and the size of corpus. Secondly, p0 in the
first order Markov assumption affects the decoding
results. Thirdly, the number of reordering permu-
tations or paths are formidable when the reorder-
ing distance L is long as suggested by Table 1.
Therefore, we apply histogram pruning to reorder-
ing paths, which only maintains top N most likely
ones. The development set is used for tuning param-
eters p0 and N .
5 Experimental Results
The evaluation results of the proposed cross-lingual
language models Gcl with reordering under various
constraints are presented in Table 3, where Gcl =
Ts?G = T3?G.8 In general, reordering has a signif-
icant effect on enhancing the performance of recog-
nition and translation in the sense of WER reduc-
tion and BLEU improvement. Compared with the
cross-lingual language model without reordering,
the cross-lingual language model with reordering
under local constraints gives 0.70% absolute WER
reduction and 3.06 absolute BLEU improvement.
The cross-lingual language model with reordering
under IBM constraints gives 0.85% absolute WER
reduction and 3.58 absolute BLEU improvement.
The cross-lingual language model with reordering
under ITG constraints yields the best performance,
with 0.92% absolute WER reduction and 3.89 abso-
lute BLEU improvement. All WER improvements
pointed out here are statistically significant at 99%
confidence according to a two-proportional z-test,
and all BLEU improvements are statistically signifi-
cant at 95% confidence according to a paired student
t-test using bootstrap resampling.
8We have chosen segmentation order s = 3 because it works
the best in our system.
6 Conclusions
We have proposed cross-lingual language model-
ing with phrase-level syntactic reordering for low-
resource speech recognition. The cross-lingual lan-
guage modeling enriches a resource-poor language
model by leveraging the language model from a
closely related resource-rich language. It provides
an effective method to solve the low-resource lan-
guage modeling challenge by using a large amount
of resource-rich language (e.g. Mandarin) data
and a small amount of resource-poor language (e.g.
Cantonese) data, as well as some parallel data of
resource-poor and resource-rich languages. With
a cross-lingual language model, our ASR system
can decode speech into transcriptions, either in a
resource-poor language or a resource-rich language,
using a single WFST-based speech decoder.
We have presented a first end-to-end WFST
source to target language transcription and transla-
tion system with syntactic reordering and global op-
timization. Our work is the first to use ITG con-
straints for the syntactic reordering in such an in-
tegrated system. We also did comparative study
of ITG constraints, IBM constraints and local con-
straints in the reordering model, for completeness.
We have also presented the determinizable design of
each transducer for composing a cross-lingual lan-
guage model such that we can optimize the search
network by determinization. This is crucially im-
portant to successfully build a practical integrated
system, and, of course, the work is extremely chal-
lenging.
Experiments on Cantonese recognition and Can-
tonese to Mandarin translation tasks have shown that
our proposed cross-lingual language model substan-
tially improves the performance of the recognition
and translation. The best system gives 12.5% rel-
ative WER reduction in Cantonese (resource-poor
774
language) transcriptions over the system using inter-
polation. The best reordering model gives 3.4% rela-
tive WER reduction and 13.3% relative BLEU score
improvement in Mandarin (resource-rich language)
transcriptions over the system without reordering.
The improvements have been found to be statisti-
cally significant.
Even though the objective of our work is for
speech recognition, our proposed cross-lingual lan-
guage modeling can be easily applied to speech
translation of other language pairs for efficient di-
rect decoding from source speech to target text.
7 Acknowledgments
This work is partially supported by ITS/189/09 and
CERG#612211. The authors would like to thank Dr.
Tasuku Oonishi for providing access to the T 3 de-
coder, and thank Prof. Sadaoki Furui and his team
for useful discussions. Thanks should go to Yue Yu
and Percy Cheung for collecting the Cantonese and
Mandarin parallel data. Thanks also go to Ricky
Chan for training the Cantonese acoustic model and
Dr. Markus Saers for helping on training the GIZA
word-to-word alignment models.
References
Y. Akita and T. Kawahara. 2006. Efficient estimation
of language model statistics of spontaneous speech via
statistical transformation model. In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, volume 1, pages 1049–1052.
E. Barcucci, A. Del Lungo, E. Pergola, and R. Pinzani.
2000. Permutations avoiding an increasing number
of length-increasing forbidden subsequences. Dis-
crete Mathematics and Theoretical Computer Science,
4(1):31–44.
J.R. Bellegarda. 2004. Statistical language model adap-
tation: review and perspectives. Speech communica-
tion, 42(1):93–108.
A.L. Berger, P.F. Brown, S.A. Della Pietra, V.J.
Della Pietra, A.S. Kehler, and R.L. Mercer. 1996.
Language translation apparatus and method us-
ing context-based translation models. US Patent
5,510,981.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(2):205–225.
F. Casacuberta, H. Ney, F.J. Och, et al. 2004. Some ap-
proaches to statistical and finite-state speech-to-speech
translation. Computer Speech & Language, 18(1):25–
47.
P. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit. In
5th European Conference on Speech Communication
and Technology.
P.R. Dixon, T. Oonishi, K. Iwano, and S. Furui. 2009.
Recent development of wfst-based speech recognition
decoder. In Proceedings of 2009 APSIPA Annual Sum-
mit and Conference, pages 138–147, Sapporo, Japan.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103–110, Rochester,
New York.
A. Ehrenfeucht, T. Harju, P. Ten Pas, and G. Rozenberg.
1998. Permutations, parenthesis words, and schro¨der
numbers. Discrete mathematics, 190(1):259–264.
V. Geethakumary. 2002. A contrastive analysis of hindi
and malayalam. Language in India.
R.G. Gordon, B.F. Grimes, and Summer Institute of Lin-
guistics. 2005. Ethnologue: Languages of the world,
volume 15. SIL International, Dallas TX, USA.
T. Hori, D. Willett, and Y. Minami. 2003. Lan-
guage model adaptation using wfst-based speaking-
style translation. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume 1, pages 228–231.
A.T. Jensson, T. Oonishi, K. Iwano, and S. Furui. 2009.
Development of a wfst based speech recognition sys-
tem for a resource deficient language using machine
translation. In Proceedings of APSIPA ASC 2009:
Asia-Pacific Signal and Information Processing Asso-
ciation, 2009 Annual Summit and Conference, pages
50–56.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts,
pages 167–174. Association for Computational Lin-
guistics.
S. Khudanpur and W. Kim. 2002. Using cross-language
cues for story-specific language modeling. In 7th In-
ternational Conference on Spoken Language Process-
ing.
W. Kim and S. Khudanpur. 2003. Cross-lingual lexical
triggers in statistical language modeling. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 17–24. Associa-
tion for Computational Linguistics.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for language modeling. In Proceed-
775
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing, volume 1, pages
I257–I260. IEEE.
T. Kløve. 2009. Generating functions for the number
of permutations with limited displacement. The Elec-
tronic Journal of Combinatorics, 16(R104).
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proceedings of Human Language Technology Confer-
ence / Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 161–168,
Vancouver, Canada.
S. Kumar, Y. Deng, and W. Byrne. 2005. A weighted
finite state transducer translation template model for
statistical machine translation. Natural Language En-
gineering, 12(1):35–75.
J. Lee. 2011. Toward a parallel corpus of spoken can-
tonese and written chinese. In Proceedings of the 5th
International Joint Conference on Natural Language
Processing, pages 1462–1466, Chiang Mai, Thailand.
L. Mathias and W. Byrne. 2006. Statistical phrase-based
speech translation. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume 1, pages 561–564.
E. Matusov, S. Kanthak, and H. Ney. 2006. Integrating
speech recognition and machine translation: Where do
we stand? In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, volume 5, pages V1217–V1220. IEEE.
M. Mohri, F. C. N. Pereira, and M. Riley. 2008.
Speech recognition with weighted finite-state trans-
ducers. Handbook on Speech Processing and Speech
Communication, Part E: Speech Recognition.
M. Mohri. 2009. Weighted automata algorithms. Hand-
book of Weighted Automata, pages 213–254.
P. Nakov and H.T. Ng. 2009. Improved statistical ma-
chine translation for resource-poor languages using re-
lated resource-rich languages. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, volume 3, pages 1358–1367.
Association for Computational Linguistics.
G. Neubig, Y. Akita, S. Mori, and T. Kawahara. 2010.
Improved statistical models for smt-based speaking
style transformation. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing, pages 5206–5209.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the Joint SIGDAT Conf. on EMNLP
and VLC, pages 20–28, College Park, MD, USA.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311–318. Association for Computational Linguistics.
G. Saon and M. Picheny. 2007. Lattice-based viterbi
decoding techniques for speech translation. In Au-
tomatic Speech Recognition & Understanding, 2007.
ASRU. IEEE Workshop on, pages 386–389. IEEE.
A. Stolcke. 2002. Srilm-an extensible language model-
ing toolkit. In 7th International Conference on Spoken
Language Processing.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In Proceedings of the 41st Annual Meeting on Associ-
ation for Computational Linguistics, pages 144–151,
Sapporo, Japan. Association for Computational Lin-
guistics.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
205–211, Geneva, Switzerland. Association for Com-
putational Linguistics.
B. Zhou, S.F. Chen, and Y. Gao. 2005. Constrained
phrase-based translation using weighted finite-state
transducers. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 1017–1020.
B. Zhou, S. F. Chen, and Y. Gao. 2006. Folsom: A fast
and memory-efficient phrase-based approach to statis-
tical machine translation. In Spoken Language Tech-
nology Workshop, pages 226–229. IEEE.
776
