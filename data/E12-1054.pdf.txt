Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics
Measuring Contextual Fitness Using Error Contexts Extracted from the
Wikipedia Revision History
Torsten Zesch
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information, Frankfurt
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita¨t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
We evaluate measures of contextual fitness
on the task of detecting real-word spelling
errors. For that purpose, we extract nat-
urally occurring errors and their contexts
from the Wikipedia revision history. We
show that such natural errors are better
suited for evaluation than the previously
used artificially created errors. In partic-
ular, the precision of statistical methods
has been largely over-estimated, while the
precision of knowledge-based approaches
has been under-estimated. Additionally, we
show that knowledge-based approaches can
be improved by using semantic relatedness
measures that make use of knowledge be-
yond classical taxonomic relations. Finally,
we show that statistical and knowledge-
based methods can be combined for in-
creased performance.
1 Introduction
Measuring the contextual fitness of a term in its
context is a key component in different NLP ap-
plications like speech recognition (Inkpen and
De´silets, 2005), optical character recognition
(Wick et al., 2007), co-reference resolution (Bean
and Riloff, 2004), or malapropism detection (Bol-
shakov and Gelbukh, 2003). The main idea is al-
ways to test what fits better into the current con-
text: the actual term or a possible replacement that
is phonetically, structurally, or semantically simi-
lar. We are going to focus on malapropism detec-
tion as it allows evaluating measures of contex-
tual fitness in a more direct way than evaluating
in a complex application which always entails in-
fluence from other components, e.g. the quality of
the optical character recognition module (Walker
et al., 2010).
A malapropism or real-word spelling error oc-
curs when a word is replaced with another cor-
rectly spelled word which does not suit the con-
text, e.g. “People with lots of honey usually
live in big houses.”, where ‘money’ was replaced
with ‘honey’. Besides typing mistakes, a major
source of such errors is the failed attempt of au-
tomatic spelling correctors to correct a misspelled
word (Hirst and Budanitsky, 2005). A real-word
spelling error is hard to detect, as the erroneous
word is not misspelled and fits syntactically into
the sentence. Thus, measures of contextual fitness
are required to detect words that do not fit their
contexts.
Existing measures of contextual fitness can be
categorized into knowledge-based (Hirst and Bu-
danitsky, 2005) and statistical methods (Mays et
al., 1991; Wilcox-OHearn et al., 2008). Both
test the lexical cohesion of a word with its con-
text. For that purpose, knowledge-based ap-
proaches employ the structural knowledge en-
coded in lexical-semantic networks like WordNet
(Fellbaum, 1998), while statistical approaches
rely on co-occurrence counts collected from large
corpora, e.g. the Google Web1T corpus (Brants
and Franz, 2006).
So far, evaluation of contextual fitness mea-
sures relied on artificial datasets (Mays et al.,
1991; Hirst and Budanitsky, 2005) which are cre-
ated by taking a sentence that is known to be cor-
rect, and replacing a word with a similar word
from the vocabulary. This has a couple of dis-
advantages: (i) the replacement might be a syn-
onym of the original word and perfectly valid in
the given context, (ii) the generated error might
529
be very unlikely to be made by a human, and
(iii) inserting artificial errors often leads to un-
natural sentences that are quite easy to correct,
e.g. if the word class has changed. However,
even if the word class is unchanged, the origi-
nal word and its replacement might still be vari-
ants of the same lemma, e.g. a noun in singu-
lar and plural, or a verb in present and past form.
This usually leads to a sentence where the error
can be easily detected using syntactical or statis-
tical methods, but is almost impossible to detect
for knowledge-based measures of contextual fit-
ness, as the meaning of the word stays more or
less unchanged. To estimate the impact of this is-
sue, we randomly sampled 1,000 artificially cre-
ated real-word spelling errors1 and found 387 sin-
gular/plural pairs and 57 pairs which were in an-
other direct relation (e.g. adjective/adverb). This
means that almost half of the artificially created
errors are not suited for an evaluation targeted at
finding optimal measures of contextual fitness, as
they over-estimate the performance of statistical
measures while underestimating the potential of
semantic measures. In order to investigate this
issue, we present a framework for mining natu-
rally occurring errors and their contexts from the
Wikipedia revision history. We use the resulting
English and German datasets to evaluate statisti-
cal and knowledge-based measures.
We make the full experimental framework pub-
licly available2 which will allow reproducing our
experiments as well as conducting follow-up ex-
periments. The framework contains (i) methods
to extract natural errors from Wikipedia, (ii) ref-
erence implementations of the knowledge-based
and the statistical methods, and (iii) the evalua-
tion datasets described in this paper.
2 Mining Errors from Wikipedia
Measures of contextual fitness have previously
been evaluated using artificially created datasets,
as there are very few sources of sentences with
naturally occurring errors and their corrections.
Recently, the revision history of Wikipedia has
been introduced as a valuable knowledge source
for NLP (Nelken and Yamangil, 2008; Yatskar et
al., 2010). It is also a possible source of natural
errors, as it is likely that Wikipedia editors make
1The same artificial data as described in Section 3.2.
2http://code.google.com/p/dkpro-spelling-asl/
real-word spelling errors at some point, which
are then corrected in subsequent revisions of the
same article. The challenge lies in discriminating
real-word spelling errors from all sorts of other
changes, including non-word spelling errors, re-
formulations, or the correction of wrong facts.
For that purpose, we apply a set of precision-
oriented heuristics narrowing down the number
of possible error candidates. Such an approach
is feasible, as the high number of revisions in
Wikipedia allows to be extremely selective.
2.1 Accessing the Revision Data
We access the Wikipedia revision data using
the freely available Wikipedia Revision Toolkit
(Ferschke et al., 2011) together with the JWPL
Wikipedia API (Zesch et al., 2008a).3 The API
outputs plain text converted from Wiki-Markup,
but the text still contains a small portion of left-
over markup and other artifacts. Thus, we per-
form additional cleaning steps removing (i) to-
kens with more than 30 characters (often URLs),
(ii) sentences with less than 5 or more than 200
tokens, and (iii) sentences containing a high frac-
tion of special characters like ‘:’ usually indicat-
ing Wikipedia-specific artifacts like lists of lan-
guage links. The remaining sentences are part-of-
speech tagged and lemmatized using TreeTagger
(Schmid, 2004). Using these cleaned and anno-
tated articles, we form pairs of adjacent article re-
visions (ri and ri+1).
2.2 Sentence Alignment
Fully aligning all sentences of the adjacent revi-
sions is a quite costly operation, as sentences can
be split, joined, replaced, or moved in the arti-
cle. However, we are only looking for sentence
pairs which are almost identical except for the
real-word spelling error and its correction. Thus,
we form all sentence pairs and then apply an ag-
gressive but cheap filter that rules out all sentences
which (i) are equal, or (ii) whose lengths differ
more than a small number of characters. For the
resulting much smaller subset of sentence pairs,
we compute the Jaro distance (Jaro, 1995) be-
tween each pair. If the distance exceeds a cer-
tain threshold tsim (0.05 in this case), we do not
further consider the pair. The small amount of re-
maining sentence pairs is passed to the sentence
pair filter for in-depth inspection.
3http://code.google.com/p/jwpl/
530
2.3 Sentence Pair Filtering
The sentence pair filter further reduces the num-
ber of remaining sentence pairs by applying a set
of heuristics including surface level and semantic
level filters. Surface level filters include:
Replaced Token Sentences need to consist of
identical tokens, except for one replaced token.
No Numbers The replaced token may not be a
number.
UPPER CASE The replaced token may not be
in upper case.
Case Change The change should not only in-
volve case changes, e.g. changing ‘english’ into
‘English’.
Edit Distance The edit distance between the
replaced token and its correction need to be be-
low a certain threshold.
After applying the surface level filters, the re-
maining sentence pairs are well-formed and con-
tain exactly one changed token at the same posi-
tion in the sentence. However, the change does
not need to characterize a real-word spelling er-
ror, but could also be a normal spelling error or a
semantically motivated change. Thus, we apply a
set of semantic filters:
Vocabulary The replaced token needs to occur
in the vocabulary. We found that even quite com-
prehensive word lists discarded too many valid
errors as Wikipedia contains articles from a very
wide range of domains. Thus, we use a frequency
filter based on the Google Web1T n-gram counts
(Brants and Franz, 2006). We filter all sentences
where the replaced token has a very low unigram
count. We experimented with different values and
found 25,000 for English and 10,000 for German
to yield good results.
Same Lemma The original token and the re-
placed token may not have the same lemma, e.g.
‘car’ and ‘cars’ would not pass this filter.
Stopwords The replaced token should not be in
a short list of stopwords (mostly function words).
Named Entity The replaced token should not
be part of a named entity. For this purpose, we
applied the Stanford NER (Finkel et al., 2005).
Normal Spelling Error We apply the Jazzy
spelling detector4 and rule out all cases in which
it is able to detect the error.
Semantic Relation If the original token and the
replaced token are in a close lexical-semantic rela-
4http://jazzy.sourceforge.net/
tions, the change is likely to be semantically mo-
tivated, e.g. if “house” was replaced with “hut”.
Thus, we do not consider cases, where we detect
a direct semantic relation between the original and
the replaced term. For this purpose, we use Word-
Net (Fellbaum, 1998) for English and GermaNet
(Lemnitzer and Kunze, 2002) for German.
3 Resulting Datasets
3.1 Natural Error Datasets
Using our framework for mining real-word
spelling errors in context, we extracted an En-
glish dataset5, and a German dataset6. Although
the output generally was of high quality, man-
ual post-processing was necessary7, as (i) for
some pairs the available context did not provide
enough information to decide which form was
correct, and (ii) a problem that might be spe-
cific to Wikipedia – vandalism. The revisions are
full of cases where words are replaced with simi-
lar sounding but greasy alternatives. A relatively
mild example is “In romantic comedies, there is
a love story about a man and a woman who fall
in love, along with silly or funny comedy farts.”,
where ‘parts’ was replaced with ‘farts’ only to be
changed back shortly afterwards by a Wikipedia
vandalism hunter. We removed all cases that re-
sulted from obvious vandalism. For further ex-
periments, a small list of offensive terms could be
added to the stopword list to facilitate this pro-
cess.
A connected problem is correct words that get
falsely corrected by Wikipedia editors (without
the malicious intend from the previous examples,
but with similar consequences). For example, the
initially correct sentence “Dung beetles roll it into
a ball, sometimes being up to 50 times their own
weight.” was ‘corrected’ by exchanging weight
with wait. We manually removed such obvious
mistakes, but are still left with some borderline
cases. In the sentence “By the 1780s the goals
of England were so full that convicts were often
chained up in rotting old ships.” the obvious error
5Using a revision dump from April 5, 2011.
6Using a revision dump from August 13, 2010.
7The most efficient and precise way of finding real-word
spelling errors would of course be to apply measures of con-
textual fitness. However, the resulting dataset would then
only contain errors that are detectable by the measures we
want to evaluate – a clearly unacceptable bias. Thus, a cer-
tain amount of manual validation is inevitable.
531
‘goal’ was changed by some Wikipedia editor to
‘jail’. However, actually it should have been the
old English form for jail ‘gaol’ which can be de-
duced when looking at the full context and later
versions of the article. We decided to not remove
these rare cases, because ‘jail’ is a valid correction
in this context.
After manual inspection, we are left with 466
English and 200 German errors. Given that we
restricted our experiment to 5 million English and
German revisions, much larger datasets can be ex-
tracted if the whole revision history is taken into
account. Our snapshot of the English Wikipedia
contains 305·106 revisions. Even if not all of them
correspond to article revisions, it is safe to assume
that more than 10,000 real-word spelling errors
can be extracted from this version of Wikipedia.
Using the same amount of source revisions, we
found significantly more English than German er-
rors. This might be due to (i) English having more
short nouns or verbs than German that are more
likely to be confused with each other, and (ii) the
English Wikipedia being known to attract a larger
amount of non-native editors which might lead to
higher rates of real-word spelling errors. How-
ever, this issue needs to be further investigated
e.g. based on comparable corpora build on the ba-
sis of different language editions of Wikipedia.
Further refining the identification of real-word er-
rors in Wikipedia would allow evaluating how fre-
quent such errors actually occur, and how long
it takes the Wikipedia editors to detect them. If
errors persist over a long time, using measures
of contextual fitness for detection would be even
more important.
Another interesting observation is that the av-
erage edit distance is around 1.4 for both datasets.
This means that a substantial proportion of errors
involve more than one edit operation. Given that
many measures of contextual fitness allow at most
one edit, many naturally occurring errors will not
be detected. However, allowing a larger edit dis-
tance enormously increases the search space re-
sulting in increased run-time and possibly de-
creased detection precision due to more false pos-
itives.
3.2 Artificial Error Datasets
In contrast to the quite challenging process of
mining naturally occurring errors, creating artifi-
cial errors is relatively straightforward. From a
corpus that is known to be free of spelling errors,
sentences are randomly sampled. For each sen-
tence, a random word is selected and all strings
with edit distance smaller than a given threshold
(2 in our case) are generated. If one of those gen-
erated strings is a known word from the vocabu-
lary, it is picked as the artificial error.
Previous work on evaluating real-word spelling
correction (Hirst and Budanitsky, 2005; Wilcox-
OHearn et al., 2008; Islam and Inkpen, 2009)
used a dataset sampled from the Wall Street Jour-
nal corpus which is not freely available. Thus, we
created a comparable English dataset of 1,000 ar-
tificial errors based on the easily available Brown
corpus (Francis W. Nelson and Kuc¸era, 1964).8
Additionally, we created a German dataset with
1,000 artificial errors based on the TIGER cor-
pus.9
4 Measuring Contextual Fitness
There are two main approaches for measuring the
contextual fitness of a word in its context: the
statistical (Mays et al., 1991) and the knowledge-
based approach (Hirst and Budanitsky, 2005).
4.1 Statistical Approach
Mays et al. (1991) introduced an approach based
on the noisy-channel model. The model assumes
that the correct sentence s is transmitted through
a noisy channel adding ‘noise’ which results in a
word w being replaced by an error e leading the
wrong sentence s? which we observe. The prob-
ability of the correct word w given that we ob-
serve the error e can be computed as P (w|e) =
P (w) · P (e|w). The channel model P (e|w) de-
scribes how likely the typist is to make an error.
This is modeled by the parameter ?.10 The re-
maining probability mass (1 ? ?) is distributed
equally among all words in the vocabulary within
an edit distance of 1 (edits(w)):
P (e|w) =
{
? if e = w
(1? ?)/|edits(w)| if e 6= w
The source model P (w) is estimated using a
trigram language model, i.e. the probability of the
8http://www.archive.org/details/BrownCorpus (CC-by-na).
9http://www.ims.uni-stuttgart.de/projekte/TIGER/
The corpus contains 50,000 sentences of German newspaper
text, and is freely available under a non-commercial license.
10We optimize ? on a held-out development set of errors.
532
intended word wi is computed as the conditional
probability P (wi|wi?1wi?2). Hence, the proba-
bility of the correct sentence s = w1 . . . wn can
be estimated as
P (s) =
n+2?
i=1
P (wi|wi?1wi?2)
The set of candidate sentences Sc contains all ver-
sions of the observed sentence s? derived by re-
placing one word with a word from edits(w),
while all other words in the sentence remain
unchanged. The correct sentence s is those
sentence from Sc that maximizes P (s|s?) =
argmaxs?Sc P (s) · P (s
?|s).
4.2 Knowledge Based Approach
Hirst and Budanitsky (2005) introduced a
knowledge-based approach that detects real-word
spelling errors by checking the semantic relations
of a target word with its context. For this pur-
pose, they apply WordNet as the source of lexical-
semantic knowledge.
The algorithm flags all words as error can-
didates and then applies filters to remove those
words from further consideration that are unlikely
to be errors. First, the algorithm removes all
closed-class word candidates as well as candi-
dates which cannot be found in the vocabulary.
Candidates are then tested for having lexical co-
hesion with their context, by (i) checking whether
the same surface form or lemma appears again in
the context, or (ii) a semantically related concept
is found in the context. In both cases, the candi-
date is removed from the list of candidates. For
each remaining possible real-word spelling error,
edits are generated by inserting, deleting, or re-
placing characters up to a certain edit distance
(usually 1). Each edit is then tested for lexical
cohesion with the context. If at least one of it fits
into the context, the candidate is selected as a real-
word error.
Hirst and Budanitsky (2005) use two additional
filters: First, they remove candidates that are
“common non-topical words”. It is unclear how
the list of such words was compiled. Their list
of examples contains words like ‘find’ or ‘world’
which we consider to be perfectly valid candi-
dates. Second, they also applied a filter using a
list of known multi-words, as the probability for
words to accidentally form multi-words is low.
Dataset P R F
Artificial-English .77 .50 .60
Natural-English .54 .26 .35
Artificial-German .90 .49 .63
Natural-German .77 .20 .32
Table 1: Performance of the statistical approach using
a trigram model based on Google Web1T.
It is unclear which list was used. We could use
multi-words from WordNet, but coverage would
be rather limited. We decided not to use both fil-
ters in order to better assess the influence of the
underlying semantic relatedness measure on the
overall performance.
The knowledge based approach uses semantic
relatedness measures to determine the cohesion
between a candidate and its context. In the exper-
iments by Budanitsky and Hirst (2006), the mea-
sure by (Jiang and Conrath, 1997) yields the best
results. However, a wide range of other measures
have been proposed, cf. (Zesch and Gurevych,
2010). Some measures using a wider defini-
tion of semantic relatedness (Gabrilovich and
Markovitch, 2007; Zesch et al., 2008b) instead
of only using taxonomic relations in a knowledge
source.
As semantic relatedness measures usually re-
turn a numeric value, we need to determine a
threshold ? in order to come up with a binary
related/unrelated decision. Budanitsky and Hirst
(2006) used a characteristic gap in the stan-
dard evaluation dataset by Rubenstein and Good-
enough (1965) that separates unrelated from re-
lated word pairs. We do not follow this approach,
but optimize the threshold on a held-out develop-
ment set of real-word spelling errors.
5 Results & Discussion
In this section, we report on the results obtained
in our evaluation of contextual fitness measures
using artificial and natural errors in English and
German.
5.1 Statistical Approach
Table 1 summarizes the results obtained by the
statistical approach using a trigram model based
on the Google Web1T data (Brants and Franz,
2006). On the English artificial errors, we ob-
serve a quite high F-measure of .60 that drops to
533
Dataset N-gram model Size P R F
Art-En
Google Web
7 · 1011 .77 .50 .60
7 · 1010 .78 .48 .59
7 · 109 .76 .42 .54
Wikipedia 2 · 109 .72 .37 .49
Nat-En
Google Web
7 · 1011 .54 .26 .35
7 · 1010 .51 .23 .31
7 · 109 .46 .19 .27
Wikipedia 2 · 109 .49 .19 .27
Art-De
Google Web
8 · 1010 .90 .49 .63
8 · 109 .90 .47 .61
8 · 108 .88 .36 .51
Wikipedia 7 · 108 .90 .37 .52
Nat-De
Google Web
8 · 1010 .77 .20 .32
8 · 109 .68 .14 .23
8 · 108 .65 .10 .17
Wikipedia 7 · 108 .70 .13 .22
Table 2: Influence of the n-gram model on the perfor-
mance of the statistical approach.
.35 when switching to the naturally occurring er-
rors which we extracted from Wikipedia. On the
German dataset, we observe almost the same per-
formance drop (from .63 to .32).
These observations correspond to our earlier
analysis where we showed that the artificial data
contains many cases that are quite easy to correct
using a statistical model, e.g. where a plural form
of a noun is replaced with its singular form (or
vice versa) as in “I bought a car.” vs. “I bought
a cars.”. The naturally occurring errors often con-
tain much harder contexts, as shown in the fol-
lowing example: “Through the open window they
heard sounds below in the street: cartwheels, a
tired horse’s plodding step, vices.” where ‘vices’
should be corrected to ‘voices’. While the lemma
‘voice’ is clearly semantically related to other
words in the context like ‘hear’ or ‘sound’, the
position at the end of the sentence is especially
difficult for the trigram-based statistical approach.
The only trigram that connects the error to the
context is (‘step’, ‘,’, vices/voices) which will
probably yield a low frequency count even for
very large trigram models. Higher order n-gram
models would help, but suffer from the usual data-
sparseness problems.
Influence of the N-gram Model For building
the trigram model, we used the Google Web1T
data, which has some known quality issues and is
Dataset P R F
Artificial-English .26 .15 .19
Natural-English .29 .18 .23
Artificial-German .47 .16 .24
Natural-German .40 .13 .19
Table 3: Performance of the knowledge-based ap-
proach using the JiangConrath semantic relatedness
measure.
not targeted towards the Wikipedia articles from
which we sampled the natural errors. Thus, we
also tested a trigram model based on Wikipedia.
However, it is much smaller than the Web model,
which leads us to additionally testing smaller Web
models. Table 2 summarizes the results.
We observe that “more data is better data” still
holds, as the largest Web model always outper-
forms the Wikipedia model in terms of recall. If
we reduce the size of the Web model to the same
order of magnitude as the Wikipedia model, the
performance of the two models is comparable.
We would have expected to see better results for
the Wikipedia model in this setting, but its higher
quality does not lead to a significant difference.
Even if statistical approaches quite reliably de-
tect real-word spelling errors, the size of the re-
quired n-gram models remains a serious obstacle
for use in real-world applications. The English
Web1T trigram model is about 25GB, which cur-
rently is not suited for being applied in settings
with limited storage capacities e.g. for intelligent
input assistance in mobile devices. As we have
seen above, using smaller models will decrease
recall to a point where hardly any error will be de-
tected anymore. Thus, we will now have a look on
knowledge-based approaches which are less de-
manding in terms of the required resources.
5.2 Knowledge-based Approach
Table 3 shows the results for the knowledge-based
measure. In contrast to the statistical approach,
the results on the artificial errors are not higher
than on the natural errors, but almost equal for
German and even lower for English; another piece
of evidence supporting our view that the proper-
ties of artificial datasets over-estimate the perfor-
mance of statistical measures.
Influence of the Relatedness Measure As was
pointed out before, Budanitsky and Hirst (2006)
534
Dataset Measure ? P R F
Art-En
JiangConrath 0.5 .26 .15 .19
Lin 0.5 .22 .17 .19
Lesk 0.5 .19 .16 .17
ESA-Wikipedia 0.05 .43 .13 .20
ESA-Wiktionary 0.05 .35 .20 .25
ESA-Wordnet 0.05 .33 .15 .21
Nat-En
JiangConrath 0.5 .29 .18 .23
Lin 0.5 .26 .21 .23
Lesk 0.5 .19 .19 .19
ESA-Wikipedia 0.05 .48 .14 .22
ESA-Wiktionary 0.05 .39 .21 .27
ESA-Wordnet 0.05 .36 .15 .21
Table 4: Performance of knowledge-based approach
using different relatedness measures.
show that the measure by Jiang and Conrath
(1997) yields the best results in their experi-
ments on malapropism detection. In addition, we
test another path-based measure by Lin (1998),
the gloss-based measure by Lesk (1986), and
the ESA measure (Gabrilovich and Markovitch,
2007) based on concept vectors from Wikipedia,
Wiktionary, and WordNet. Table 4 summarizes
the results. In contrast to the findings of Budanit-
sky and Hirst (2006), JiangConrath is not the best
path-based measure, as Lin provides equal or bet-
ter performance. Even more importantly, other
(non path-based) measures yield better perfor-
mance than both path-based measures. Especially
ESA based on Wiktionary provides a good over-
all performance, while ESA based on Wikipedia
provides excellent precision. The advantage of
ESA over the other measure types can be ex-
plained with its ability to incorporate semantic re-
lationships beyond classical taxonomic relations
(as used by path-based measures).
5.3 Combining the Approaches
The statistical and the knowledge-based approach
use quite different methods to assess the con-
textual fitness of a word in its context. This
makes it worthwhile trying to combine both ap-
proaches. We ran the statistical method (using the
full Wikipedia trigram model) and the knowledge-
based method (using the ESA-Wiktionary related-
ness measure) in parallel and then combined the
resulting detections using two strategies: (i) we
merge the detections of both approaches in order
to obtain higher recall (‘Union’), and (ii) we only
Dataset Comb.-Strategy P R F
Artificial-English
Best-Single .77 .50 .60
Union .52 .55 .54
Intersection .91 .15 .25
Natural-English
Best-Single .54 .26 .35
Union .40 .36 .38
Intersection .82 .11 .19
Table 5: Results obtained by a combination of the best
statistical and knowledge-based configuration. ‘Best-
Single’ is the best precision or recall obtained by a sin-
gle measure. ‘Union’ merges the detections of both
approaches. ‘Intersection’ only detects an error if both
methods agree on a detection.
count an error as detected if both methods agree
on a detection (‘Intersection’). When compar-
ing the combined results in Table 5 with the best
precision or recall obtained by a single measure
(‘Best-Single’), we observe that precision can be
significantly improved using the ‘Union’ strategy,
while recall is only moderately improved using
the ‘Intersect’ strategy. This means that (i) a large
subset of errors is detected by both approaches
that due to their different sources of knowledge
mutually reinforce the detection leading to in-
creased precision, and (ii) a small but otherwise
undetectable subset of errors requires considering
detections made by one approach only.
6 Related Work
To our knowledge, we are the first to create a
dataset of naturally occurring errors based on the
revision history of Wikipedia. Max and Wis-
niewski (2010) used similar techniques to create
a dataset of errors from the French Wikipedia.
However, they target a wider class of errors in-
cluding non-word spelling errors, and their class
of real-word errors conflates malapropisms as
well as other types of changes like reformulations.
Thus, their dataset cannot be easily used for our
purposes and is only available in French, while
our framework allows creating datasets for all ma-
jor languages with minimal manual effort.
Another possible source of real-word spelling
errors are learner corpora (Granger, 2002), e.g.
the Cambridge Learner Corpus (Nicholls, 1999).
However, annotation of errors is difficult and
costly (Rozovskaya and Roth, 2010), only a small
fraction of observed errors will be real-word
spelling errors, and learners are likely to make dif-
535
ferent mistakes than proficient language users.
Islam and Inkpen (2009) presented another sta-
tistical approach using the Google Web1T data
(Brants and Franz, 2006) to create the n-gram
model. It slightly outperformed the approach by
Mays et al. (1991) when evaluated on a corpus of
artificial errors based on the WSJ corpus. How-
ever, the results are not directly comparable, as
Mays et al. (1991) used a much smaller n-gram
model and our results in Section 5.1 show that
the size of the n-gram model has a large influence
on the results. Eventually, we decided to use the
Mays et al. (1991) approach in our study, as it is
easier to adapt and augment.
In a re-evaluation of the statistical model by
Mays et al. (1991), Wilcox-OHearn et al. (2008)
found that it outperformed the knowledge-based
method by Hirst and Budanitsky (2005) when
evaluated on a corpus of artificial errors based on
the WSJ corpus. This is consistent with our find-
ings on the artificial errors based on the Brown
corpus, but - as we have seen in the previous sec-
tion - evaluation on the naturally occurring errors
shows a different picture. They also tried to im-
prove the model by permitting multiple correc-
tions and using fixed-length context windows in-
stead of sentences, but obtained discouraging re-
sults.
All previously discussed methods are unsuper-
vised in a way that they do not rely on any training
data with annotated errors. However, real-word
spelling correction has also been tackled by su-
pervised approaches (Golding and Schabes, 1996;
Jones and Martin, 1997; Carlson et al., 2001).
Those methods rely on predefined confusion-sets,
i.e. sets of words that are often confounded e.g.
{peace, piece} or {weather, whether}. For each
set, the methods learn a model of the context in
which one or the other alternative is more proba-
ble. This yields very high precision, but only for
the limited number of previously defined confu-
sion sets. Our framework for extracting natural
errors could be used to increase the number of
known confusion sets.
7 Conclusions and Future Work
In this paper, we evaluated two main approaches
for measuring the contextual fitness of terms: the
statistical approach by Mays et al. (1991) and
the knowledge-based approach by Hirst and Bu-
danitsky (2005) on the task of detecting real-
word spelling errors. For that purpose, we ex-
tracted a dataset with naturally occurring errors
and their contexts from the Wikipedia revision
history. We show that evaluating measures of con-
textual fitness on this dataset provides a more re-
alistic picture of task performance. In particular,
using artificial datasets over-estimates the perfor-
mance of the statistical approach, while it under-
estimates the performance of the knowledge-
based approach.
We show that n-gram models targeted towards
the domain from which the errors are sampled
do not improve the performance of the statisti-
cal approach if larger n-gram models are avail-
able. We further show that the performance of
the knowledge-based approach can be improved
by using semantic relatedness measures that in-
corporate knowledge beyond the taxonomic rela-
tions in a classical lexical-semantic resource like
WordNet. Finally, by combining both approaches,
significant increases in precision or recall can be
achieved.
In future work, we want to evaluate a wider
range of contextual fitness measures, and learn
how to combine them using more sophisticated
combination strategies. Both - the statistical as
well as the knowledge-based approach - will ben-
efit from a better model of the typist, as not all
edit operations are equally likely (Kernighan et
al., 1990). On the side of the error extraction, we
are going to further improve the extraction pro-
cess by incorporating more knowledge about the
revisions. For example, vandalism is often re-
verted very quickly, which can be detected when
looking at the full set of revisions of an article.
We hope that making the experimental frame-
work publicly available will foster future research
in this field, as our results on the natural errors
show that the problem is still quite challenging.
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We Andreas Kellner and Tristan Miller for check-
ing the datasets, and the anonymous reviewers for
their helpful feedback.
536
References
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proc. of HLT/NAACL, pages
297–304.
Igor A. Bolshakov and Alexander Gelbukh. 2003. On
Detection of Malapropisms by Multistage Colloca-
tion Testing. In Proceedings of NLDB-2003, 8th
International Workshop on Applications of Natural
Language to Information Systems, number Cic.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
Andrew J Carlson, Jeffrey Rosen, and Dan Roth.
2001. Scaling Up Context-Sensitive Text Correc-
tion. In Proceedings of IAAI.
C Fellbaum. 1998. WordNet An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently
Accessing Wikipedia’s Edit History. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. System Demonstrations, pages
97–102, Portland, OR, USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics -
ACL ’05, pages 363–370, Morristown, NJ, USA.
Association for Computational Linguistics.
Francis W. Nelson and Henry Kuc¸era. 1964. Manual
of information to accompany a standard corpus of
present-day edited American English, for use with
digital computers.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Arti-
ficial Intelligence, pages 1606–1611.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining Trigram-based and feature-based methods
for context-sensitive spelling correction. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics -, pages 71–78, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sylviane Granger, 2002. A birds-eye view of learner
corpus research, pages 3–33. John Benjamins Pub-
lishing Company.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87–111, March.
Diana Inkpen and Alain De´silets. 2005. Semantic
similarity for detecting recognition errors in auto-
matic speech transcripts. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing -
HLT ’05, number October, pages 49–56, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using Google Web IT 3-grams.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing Vol-
ume 3 - EMNLP ’09, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
M A Jaro. 1995. Probabilistic linkage of large public
health data file. Statistics in Medicine, 14:491–498.
Jay J Jiang and David W Conrath. 1997. Seman-
tic Similarity Based on Corpus Statistics and Lex-
ical Taxonomy. In Proceedings of the 10th Inter-
national Conference on Research in Computational
Linguistics, Taipei, Taiwan.
Michael P Jones and James H Martin. 1997. Contex-
tual spelling correction using latent semantic analy-
sis. In Proceedings of the fifth conference on Ap-
plied natural language processing -, pages 166–
173, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Mark D Kernighan, Kenneth W Church, and
William A Gale. 1990. A Spelling Correc-
tion Program Based on a Noisy Channel Model.
In Proceedings of the 13th International Confer-
ence on Computational Linguistics, pages 205–210,
Helsinki, Finland.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet - Representation, Visualization, Application.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
pages 1485–1491.
M Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine
cone from an ice cream cone. Proceedings of the
5th annual international conference, pages 24–26.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In Proceedings of International
Conference on Machine Learning, pages 296–304,
Madison, Wisconsin.
Aurelien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedias Revision History. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC’10),
pages 3143–3148.
Eric Mays, Fred. J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517–522.
537
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia’s Article Revision History for Train-
ing Computational Linguistics Algorithms. In
Proceedings of the AAAI Workshop on Wikipedia
and Artificial Intelligence: An Evolving Synergy
(WikiAI), WikiAI08.
Diane Nicholls. 1999. The Cambridge Learner Cor-
pus - Error Coding and Analysis for Lexicography
and ELT. In Summer Workshop on Learner Cor-
pora, Tokyo, Japan.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL Errors: Challenges and Rewards. In The 5th
Workshop on Innovative Use of NLP for Building
Educational Applications (NAACL-HLT).
H Rubenstein and J B Goodenough. 1965. Contextual
Correlates of Synonymy. Communications of the
ACM, 8(10):627–633.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland.
Daniel D. Walker, William B. Lund, and Eric K. Ring-
ger. 2010. Evaluating Models of Latent Document
Semantics in the Presence of OCR Errors. Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, (October):240–
250.
M. Wick, M. Ross, and E. Learned-Miller. 2007.
Context-sensitive error correction: Using topic
models to improve OCR. In Ninth International
Conference on Document Analysis and Recogni-
tion (ICDAR 2007) Vol 2, pages 1168–1172. Ieee,
September.
Amber Wilcox-OHearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In Proceedings of the 9th
international conference on Computational linguis-
tics and intelligent text processing (CICLing).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ’10, pages 365–368.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom
of Crowds versus Wisdom of Linguists - Measur-
ing the Semantic Relatedness of Words. Journal of
Natural Language Engineering, 16(1):25–59.
Torsten Zesch, Christof Mu¨ller, and Iryna Gurevych.
2008a. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC).
Torsten Zesch, Christof Mu¨ller, and Iryna Gurevych.
2008b. Using wiktionary for computing semantic
relatedness. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 861–867,
Chicago, IL, USA, Jul.
538
