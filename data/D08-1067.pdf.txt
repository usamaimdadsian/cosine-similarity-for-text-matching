Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 640–649,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Unsupervised Models for Coreference Resolution
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
We present a generative model for unsuper-
vised coreference resolution that views coref-
erence as an EM clustering process. For
comparison purposes, we revisit Haghighi
and Klein’s (2007) fully-generative Bayesian
model for unsupervised coreference resolu-
tion, discuss its potential weaknesses and con-
sequently propose three modifications to their
model. Experimental results on the ACE data
sets show that our model outperforms their
original model by a large margin and com-
pares favorably to the modified model.
1 Introduction
Coreference resolution is the problem of identifying
which mentions (i.e., noun phrases) refer to which
real-world entities. The availability of annotated
coreference corpora produced as a result of the MUC
conferences and the ACE evaluations has prompted
the development of a variety of supervised machine
learning approaches to coreference resolution in re-
cent years. The focus of learning-based coreference
research has also shifted from the acquisition of a
pairwise model that determines whether two men-
tions are co-referring (e.g., Soon et al. (2001), Ng
and Cardie (2002), Yang et al. (2003)) to the de-
velopment of rich linguistic features (e.g., Ji et al.
(2005), Ponzetto and Strube (2006)) and the ex-
ploitation of advanced techniques that involve joint
learning (e.g., Daume´ III and Marcu (2005)) and
joint inference (e.g., Denis and Baldridge (2007))
for coreference resolution and a related extraction
task. The rich features, coupled with the increased
complexity of coreference models, have made these
supervised approaches more dependent on labeled
data and less applicable to languages for which lit-
tle or no annotated data exists. Given the growing
importance of multi-lingual processing in the NLP
community, however, the development of unsuper-
vised and weakly supervised approaches for the au-
tomatic processing of resource-scarce languages has
become more important than ever.
In fact, several popular weakly supervised learn-
ing algorithms such as self-training, co-training
(Blum and Mitchell, 1998), and EM (Dempster et
al., 1977) have been applied to coreference resolu-
tion (Ng and Cardie, 2003) and the related task of
pronoun resolution (Mu¨ller et al., 2002; Kehler et
al., 2004; Cherry and Bergsma, 2005). Given a small
number of coreference-annotated documents and a
large number of unlabeled documents, these weakly
supervised learners aim to incrementally augment
the labeled data by iteratively training a classifier1
on the labeled data and using it to label mention
pairs randomly drawn from the unlabeled documents
as COREFERENT or NOT COREFERENT. However,
classifying mention pairs using such iterative ap-
proaches is undesirable for coreference resolution:
since the non-coreferent mention pairs significantly
outnumber their coreferent counterparts, the result-
ing classifiers generally have an increasing tendency
to (mis)label a pair as non-coreferent as bootstrap-
ping progresses (see Ng and Cardie (2003)).
Motivated in part by these results, we present a
generative, unsupervised model for probabilistically
1For co-training, a pair of view classifiers are trained; and
for EM, a generative model is trained instead.
640
inducing coreference partitions on unlabeled doc-
uments, rather than classifying mention pairs, via
EM clustering (Section 2). In fact, our model com-
bines the best of two worlds: it operates at the
document level, while exploiting essential linguistic
constraints on coreferent mentions (e.g., gender and
number agreement) provided by traditional pairwise
classification models.
For comparison purposes, we revisit a fully-
generative Bayesian model for unsupervised coref-
erence resolution recently introduced by Haghighi
and Klein (2007), discuss its potential weaknesses
and consequently propose three modifications to
their model (Section 3). Experimental results on the
ACE data sets show that our model outperforms their
original model by a large margin and compares fa-
vorably to the modified model (Section 4).
2 Coreference as EM Clustering
In this section, we will explain how we recast un-
supervised coreference resolution as EM clustering.
We begin by introducing some of the definitions and
notations that we will use in this paper.
2.1 Definitions and Notations
A mention can be a pronoun, a name (i.e., a proper
noun), or a nominal (i.e., a common noun). An en-
tity is a set of coreferent mentions. Given a docu-
ment D consisting of n mentions, m1, . . . ,mn, we
use Pairs(D) to denote the set of
(n
2
)
mention pairs,
{mij | 1 ? i < j ? n}, where mij is formed
from mentions mi and mj . The pairwise probabil-
ity formed from mi and mj refers to the probabil-
ity that the pair mij is coreferent and is denoted as
Pcoref (mij). A clustering of n mentions is an n x
n Boolean matrix C , where Cij (the (i,j)-th entry of
C) is 1 if and only if mentions mi and mj are coref-
erent. An entry in C is relevant if it corresponds
to a mention pair in Pairs(D). A valid clustering
is a clustering in which the relevant entries satisfy
the transitivity constraint. In other words, C is valid
if and only if (Cij = 1 ? Cjk = 1) =? Cik = 1
? 1 ? i < j < k ? n. Hence, a valid clustering
corresponds to a partition of a given set of mentions,
and the goal of coreference resolution is to produce
a valid clustering in which each cluster corresponds
to a distinct entity.
2.2 The Model
As mentioned previously, our generative model op-
erates at the document level, inducing a valid clus-
tering on a given document D. More specifically,
our model consists of two steps. It first chooses a
clustering C based on some clustering distribution
P (C), and then generates D given C:
P (D,C) = P (C)P (D | C).
To facilitate the incorporation of linguistic con-
straints defined on a pair of mentions, we represent
D by its mention pairs, Pairs(D). Now, assuming
that these mention pairs are generated conditionally
independently of each other given Cij ,
P (D | C) =
?
mij?Pairs(D)
P (mij | Cij).
Next, we represent mij as a set of seven features
that is potentially useful for determining whether mi
and mj are coreferent (see Table 1).2 Hence, we can
rewrite P (D | C) as
?
mij?Pairs(D)
P (m1ij , . . . ,m7ij | Cij),
where mkij is the value of the kth feature of mij .
To reduce data sparseness and improve the es-
timation of the above probabilities, we make con-
ditional independence assumptions about the gen-
eration of these feature values. Specifically, as
shown in the first column of Table 1, we di-
vide the seven features into three groups (namely,
strong coreference indicators, linguistic constraints,
and mention types), assuming that two feature
values are conditionally independent if and only
if the corresponding features belong to differ-
ent groups. With this assumption, we can de-
compose P (m1ij , . . .m7ij | Cij) into a product
of three probabilities: P (m1ij,m2ij ,m3ij | Cij),
P (m4ij ,m5ij,m6ij | Cij), and P (m7ij | Cij). Each of
these distributions represents a pair of multinomial
distributions, one for the coreferent mention pairs
(Cij = 1) and the other for the non-coreferent men-
tion pairs (Cij = 0). Hence, the set of parameters
of our model, ?, consists of P (m1,m2,m3 | c),
P (m4,m5,m6 | c), and P (m7 | c).
2See Soon et al. (2001) for details on feature value compu-
tations. Note that all feature values are computed automatically.
641
Feature Type Feature ID Feature Description
Strong 1 STR MATCH T if neither of the two mentions is a pronoun and after discarding determiners,
Coreference the string denoting mention mi is identical to that of mention mj ; else F.
Indicators 2 ALIAS T if one mention is an acronym, an abbreviation, or a name variant of the
other; else F. For instance, Bill Clinton and President Clinton are aliases, so
are MIT and Massachusetts Institute of Technology.
3 APPOSITIVE T if the mentions are in an appositive relationship; else F.
Linguistic 4 GENDER T if the mentions agree in gender; F if they disagree; NA if gender information
Constraints for one or both mentions cannot be determined.
5 NUMBER T if the mentions agree in number; F if they disagree; NA if number informa-
tion for one or both mentions cannot be determined.
6 SEM CLASS T if the mentions have the same semantic class; F if they don’t; NA if the
semantic class information for one or both mentions cannot be determined.
Mention Types 7 NPTYPE the feature value is the concatenation of the mention type of the two mentions,
titj , where ti, tj ? { PRONOUN, NAME, NOMINAL }.
Table 1: Feature set for representing a mention pair. The first six features are relational features that test whether some
property P holds for the mention pair under consideration and indicate whether the mention pair is TRUE or FALSE
w.r.t. P; a value of NOT APPLICABLE is used when property P does not apply.
2.3 The Induction Algorithm
To induce a clustering C on a document D, we run
EM on our model, treating D as observed data and
C as hidden data. Specifically, we use EM to itera-
tively estimate the model parameters, ?, from doc-
uments that are probabilistically labeled (with clus-
terings) and apply the resulting model to probabilis-
tically re-label a document (with clusterings). More
formally, we employ the following EM algorithm:
E-step: Compute the posterior probabilities of the
clusterings, P (C|D,?), based on the current ?.
M-step: Using P (C|D,?) computed in the E-step,
find the ?? that maximizes the expected complete
log likelihood,
?
C P (C|D,?) log P (D,C|??).
We begin the induction process at the M-step.3 To
find the ? that maximizes the expected complete log
likelihood, we use maximum likelihood estimation
with add-one smoothing. Since P (C|D,?) is not
available in the first EM iteration, we instead use
an initial distribution over clusterings, P (C). The
question, then, is: which P (C) should we use? One
possibility is the uniform distribution over all (possi-
bly invalid) clusterings. Another, presumably better,
choice is a distribution that assigns non-zero prob-
ability mass to only the valid clusterings. Yet an-
other possibility is to set P (C) based on a docu-
ment labeled with coreference information. In our
experiments, we employ this last method, assigning
3Another possibility, of course, is to begin at the E-step by
making an initial guess at ?.
a probability of one to the correct clustering of the
labeled document (see Section 4.1 for details).
After (re-)estimating ? in the M-step, we proceed
to the E-step, where the goal is to find the condi-
tional clustering probabilities. Given a document
D, the number of coreference clusterings is expo-
nential in the number of mentions in D, even if
we limit our attention to those that are valid. To
cope with this computational complexity, we ap-
proximate the E-step by computing only the condi-
tional probabilities that correspond to the N most
probable coreference clusterings given the current
?. We identify the N most probable clusterings and
compute their probabilities as follows. First, using
the current ?, we reverse the generative model and
compute Pcoref (mij) for each mention pair mij in
Pairs(D). Next, using these pairwise probabilities,
we apply Luo et al.’s (2004) Bell tree approach to
coreference resolution to compute the N -best clus-
terings and their probabilities (see Section 2.4 for
details). Finally, to obtain the required conditional
clustering probabilities for the E-step, we normalize
the probabilities assigned to the N -best clusterings
so that they sum to one.
2.4 Computing the N-Best Partitions
As described above, given the pairwise probabilities,
we use Luo et al.’s (2004) algorithm to heuristically
compute the N -best clusterings (or, more precisely,
N -best partitions4) and their probabilities based on
4Note that Luo et al.’s search algorithm only produces valid
clusterings, implying that the resulting N -best clusterings are
642
Input: M = {m1, ..., mn}: mentions, N : no. of best partitions
Output: N -best partitions
1: // initialize the data structures that store partial partitions
2: H1 := {PP := {[m1]}}, S(PP ) = 1
3: H2, ..., Hn = ?
4: for i = 2 to n
5: // process each partial partition
6: foreach PP ? Hi?1
7: // process each cluster in PP
8: foreach C ? PP
9: Extend PP to PP ? by linking mi to C
10: Compute S(PP ?)
11: Hi := Hi? {PP ?}
12: Extend PP to PP ? by putting mi into a new cluster
13: Compute S(PP ?)
14: Hi := Hi? {PP ?}
15: return N most probable partitions in Hn
Figure 1: Our implementation of Luo et al.’s algorithm
the Bell tree. Informally, each node in a Bell tree
corresponds to an ith-order partial partition (i.e., a
partition of the first i mentions of the given docu-
ment), and the ith level of the tree contains all possi-
ble ith-order partial partitions. Hence, the set of leaf
nodes constitutes all possible partitions of all of the
mentions. The search for the N most probable parti-
tions starts at the root, and a partitioning of the men-
tions is incrementally constructed as we move down
the tree. Since an exhaustive search is computation-
ally infeasible, Luo et al. employ a beam search pro-
cedure to explore only the most probable paths at
each step of the search process. Figure 1 shows our
implementation of this heuristic search algorithm.
The algorithm takes as input a set of n mentions
(and their pairwise probabilities), and returns the N
most probable partitionings of the mentions. It uses
data structures S and the Hi’s to store intermediate
results. Specifically, S(PP ) stores the score of the
partial partition PP . Hi is associated with the ith
level of the Bell tree, and is used to store the most
probable ith-order partial partitions. Each Hi has a
maximum size of 2N : if more than 2N partitions
are inserted into a given Hi, then only the 2N most
probable ones will be stored. This amounts to prun-
ing the search space by employing a beam size of
2N (i.e., expanding only the 2N most probable par-
tial partitions) at each step of the search.
The algorithm begins by initializing H1 with the
only partial partition of order one, {[m1]}, which
indeed partitions. This is desirable, as there is no reason for us
to put non-zero probability mass on invalid clusterings.
has a score of one (line 2). Then it processes the
mentions sequentially, starting with m2 (line 4).
When processing mi, it takes each partial partition
PP in Hi?1 and creates a set of ith-order parti-
tions by extending PP with mi in all possible ways.
Specifically, for each cluster C (formed by a subset
of the first i–1 mentions) in PP , the algorithm gen-
erates a new ith-order partition, PP ?, by linking mi
to C (line 9), and stores PP ? in Hi (line 11). The
score of PP ?, S(PP ?), is computed by using the
pairwise coreference probabilities as follows:
S(PP ?) = S(PP ) · max
mk?C
Pcoref (mki).
Of course, PP can also be extended by putting mi
into a new cluster (line 12). This yields PP ?, an-
other partition to be inserted into Hi (line 14), and
S(PP ?) = ?·S(PP )·(1? max
k?{1,...,i?1}
Pcoref (mki)),
where ? (the start penalty) is a positive constant (<
1) used to penalize partitions that start a new clus-
ter. After processing each of the n mentions using
the above steps, the algorithm returns the N most
probable partitions in Hn (line 15).
Our implementation of Luo et al.’s search algo-
rithm differs from their original algorithm only in
terms of the number of pruning strategies adopted.
Specifically, Luo et al. introduce a number of heuris-
tics to prune the search space in order to speed up the
search. We employ only the beam search heuristic,
with a beam size that is five times larger than theirs.
Our larger beam size, together with the fact that we
do not use other pruning strategies, implies that we
are searching through a larger part of the space than
them, thus potentially yielding better partitions.
3 Haghighi and Klein’s Coreference Model
To gauge the performance of our model, we com-
pare it with a Bayesian model for unsupervised
coreference resolution that was recently proposed by
Haghighi and Klein (2007). In this section, we will
give an overview of their model, discuss its weak-
nesses and propose three modifications to the model.
3.1 Notations
For consistency, we follow Haghighi and Klein’s
(H&K) notations. Z is the set of random variables
643
that refer to (indices of) entities. ?z is the set of
parameters associated with entity z. ? is the entire
set of model parameters, which includes all the ?z’s.
Finally, X is the set of observed variables (e.g., the
head of a mention). Given a document, the goal is
to find the most probable assignment of entity in-
dices to its mentions given the observed values. In
other words, we want to maximize P (Z|X). In a
Bayesian approach, we compute this probability by
integrating out all the parameters. Specifically,
P (Z|X) =
?
P (Z|X, ?)P (?|X)d?.
3.2 The Original H&K Model
The original H&K model is composed of a set of
models: the basic model and two other models
(namely, the pronoun head model and the salience
model) that aim to improve the basic model.5
3.2.1 Basic Model
The basic model generates a mention in a two-step
process. First, an entity index is chosen according to
an entity distribution, and then the head of the men-
tion is generated given the entity index based on an
entity-specific head distribution. Here, we assume
that (1) all heads H are observed and (2) a mention
is represented solely by its head noun, so nothing
other than the head is generated. Furthermore, we
assume that the head distribution is drawn from a
symmetric Dirichlet with concentration ?H . Hence,
P (Hi,j = h|Z,H?i,j) ? nh,z + ?H
where Hi,j is the head of mention j in document
i, and nh,z is the number of times head h is emit-
ted by entity index z in (Z,H?i,j).6 On the other
hand, since the number of entities in a document is
not known a priori, we draw the entity distribution
from a Dirichlet process with concentration ?, ef-
fectively yielding a model with an infinite number
of mixture components. Using the Chinese restau-
rant process representation (see Teh et al. (2006)),
P (Zij = z|Z?i,j) ?
{
? , if z = znew
nz , otherwise
5H&K also present a cross-document coreference model,
but since it focuses primarily on cross-document coreference
and improves within-document coreference performance by
only 1.5% in F-score, we will not consider this model here.
6H?i,j is used as a shorthand for H – {Hi,j}.
where nz is the number of mentions in Z?i,j labeled
with entity index z, and znew is a new entity index
not already in Z?i,j. To perform inference, we use
Gibbs sampling (Geman and Geman, 1984) to gen-
erate samples from this conditional distribution:
P (Zi,j |Z?i,j,H) ? P (Zi,j|Z?i,j)P (Hi,j |Z,H?i,j)
where the two distributions on the right are defined
as above. Starting with a random assignment of en-
tity indices to mentions, the Gibbs sampler itera-
tively re-samples an entity index according to this
posterior distribution given the current assignment.
3.2.2 Pronoun Head Model
Head generation in the basic model is too simplis-
tic: it has a strong tendency to assign the same en-
tity index to mentions having the same head. This is
particularly inappropriate for pronouns. Hence, we
need a different model for generating pronouns.
Before introducing this pronoun head model, we
need to augment the set of entity-specific param-
eters, which currently contains only a distribution
over heads (?hZ ). Specifically, we add distributions
?tZ , ?
g
Z , and ?nZ over entity properties: ?tZ is a
distribution over semantic types (PER, ORG, LOC,
MISC), ?gZ over gender (MALE, FEMALE, EITHER,
NEUTER), and ?nZ over number (SG, PL). We assume
that each of these distributions is drawn from a sym-
metric Dirichlet. A small concentration parameter
is used, since each entity should have a dominating
value for each of these properties.
Now, to estimate ?tZ , ?
g
Z , and ?nZ , we need to
know the gender, number, and semantic type of each
mention. For some mentions (e.g., “he”), these
properties are easy to compute; for others (e.g., “it”),
they are not. Whenever a mention has unobserved
properties, we need to fill in the missing values. We
could resort to sampling, but sampling these prop-
erties is fairly inefficient. So, following H&K, we
keep soft counts for each of these properties and use
them rather than perform hard sampling.
When an entity z generates a pronoun h using the
pronoun head model,7 it first generates a gender g, a
number n, and a semantic type t independently from
the distributions ?gz , ?nz , and ?tz; and then generates
h using the distribution P (H = h|G = g,N =
7While pronouns are generated by this pronoun head model,
names and nominals continue to be handled by the basic model.
644
n, T = t, ?). Note that this last distribution is a
global distribution that is independent of the chosen
entity index. ? is a parameter drawn from a symmet-
ric Dirichlet (with concentration ?P ) that encodes
our prior knowledge of the relationship between a
semantic type and a pronoun. For instance, given the
type PERSON, there is a higher probability of gener-
ating “he” than “it”. As a result, we maintain a list
of compatible semantic types for each pronoun, and
give a pronoun a count of (1 + ?P ) if it is compatible
with the drawn semantic type; otherwise, we give it
a count of ?P . In essence, we use this prior to prefer
the generation of pronouns that are compatible with
the chosen semantic type.
3.2.3 Salience Model
Pronouns typically refer to salient entities, so the
basic model could be improved by incorporating
salience. We start by assuming that each entity has
an activity score that is initially set to zero. Given
a set of mentions and an assignment of entity in-
dices to mentions, Z, we process the mentions in a
left-to-right manner. When a mention, m, is encoun-
tered, we multiply the activity score of each entity by
0.5 and add one to the activity score of the entity to
which m belongs. This captures the intuitive notion
that frequency and recency both play a role in deter-
mining salience. Next, we rank the entities based on
their activity scores and discretize the ranks into five
“salience” buckets S: TOP (1), HIGH (2–3), MID (4–
6), LOW (7+), and NONE. Finally, this salience in-
formation is used to modify the entity distribution:8
P (Zij = z|Z?i,j) ? nz · P (Mi,j |Si,j,Z)
where Si,j is the salience value of the jth mention
in document i, and Mi,j is its mention type, which
can take on one of three values: pronoun, name, and
nominal. P (Mi,j |Si,j,Z), the distribution of men-
tion type given salience, was computed from H&K’s
development corpus (see Table 2). According to
the table, pronouns are preferred for salient entities,
whereas names and nominals are preferred for enti-
ties that are less active.
8Rather than having just one probability term on the right
hand side of the sampling equation, H&K actually have a prod-
uct of probability terms, one for each mention that appears later
than mention j in the given document. However, they acknowl-
edge that having the product makes sampling inefficient, and
decided to simplify the equation to this form in their evaluation.
Salience Feature Pronoun Name Nominal
TOP 0.75 0.17 0.08
HIGH 0.55 0.28 0.17
MID 0.39 0.40 0.21
LOW 0.20 0.45 0.35
NONE 0.00 0.88 0.12
Table 2: Posterior distribution of mention type given
salience (taken from Haghighi and Klein (2007))
3.3 Modifications to the H&K Model
Next, we discuss the potential weaknesses of H&K’s
model and propose three modifications to it.
Relaxed head generation. The basic model fo-
cuses on head matching, and is therefore likely to
(incorrectly) posit the large airport and the small
airport as coreferent, for instance. In fact, head
matching is a relatively inaccurate indicator of coref-
erence, in comparison to the “strong coreference in-
dicators” shown in the first three rows of Table 1. To
improve H&K’s model, we replace head matching
with these three strong indicators as follows. Given
a document, we assign each of its mentions a head
index, such that two mentions have the same head
index if and only if at least one of the three strong
indicators returns a value of True. Now, instead of
generating a head, the head model generates a head
index, thus increasing the likelihood that aliases are
assigned the same entity index, for instance. Note
that this modification is applied only to the basic
model. In particular, pronoun generation continues
to be handled by the pronoun head model and will
not be affected. We hypothesize that this modifica-
tion would improve precision, as the strong indica-
tors are presumably more precise than head match.
Agreement constraints. While the pronoun head
model naturally prefers that a pronoun be generated
by an entity whose gender and number are compati-
ble with those of the pronoun, the entity (index) that
is re-sampled for a pronoun according to the sam-
pling equation for P (Zi,j |Z?i,j,H) may still not be
compatible with the pronoun with respect to gen-
der and number. The reason is that an entity in-
dex is assigned based not only on the head distri-
bution but also on the entity distribution. Since enti-
ties with many mentions are preferable to those with
few mentions, it is possible for the model to favor
the assignment of a grammatically incompatible en-
tity (index) to a pronoun if the entity is sufficiently
645
large. To eliminate this possibility, we enforce the
agreement constraints at the global level. Specifi-
cally, we sample an entity index for a given mention
with a non-zero probability if and only if the corre-
sponding entity and the head of the mention agree in
gender and number. We hypothesize that this modi-
fication would improve precision.
Pronoun-only salience. In Section 3.2.3, we mo-
tivate the need for salience using pronouns only,
since proper names can to a large extent be resolved
using string-matching facilities and are not particu-
larly sensitive to salience. Nominals (especially def-
inite descriptions), though more sensitive to salience
than names, can also be resolved by simple string-
matching heuristics in many cases (Vieira and Poe-
sio, 2000; Strube et al., 2002). Hence, we hypothe-
size that the use of salience for names and nominals
would adversely affect their resolution performance,
as incorporating salience could diminish the role of
string match in the resolution process, according to
the sampling equations. Consequently, we modify
H&K’s model by limiting the application of salience
to the resolution of pronouns only. We hypothesize
that this change would improve precision.
4 Evaluation
4.1 Experimental Setup
To evaluate our EM-based model and H&K’s model,
we use the ACE 2003 coreference corpus, which
is composed of three sections: Broadcast News
(BNEWS), Newswire (NWIRE), and Newspaper
(NPAPER). Each section is in turn composed of a
training set and a test set. Due to space limitations,
we will present evaluation results only for the test
sets of BNEWS and NWIRE, but verified that the
same performance trends can be observed on NPA-
PER as well. Unlike H&K, who report results us-
ing only true mentions (extracted from the answer
keys), we show results for true mentions as well as
system mentions that were extracted by an in-house
noun phrase chunker. The relevant statistics of the
BNEWS and NWIRE test sets are shown in Table 3.
Scoring programs. To score the output of the
coreference models, we employ the commonly-used
MUC scoring program (Vilain et al., 1995) and the
recently-developed CEAF scoring program (Luo,
2005). In the MUC scorer, recall is computed as
BNEWS NWIRE
Number of documents 51 29
Number of true mentions 2608 2630
Number of system mentions 5424 5197
Table 3: Statistics of the BNEWS and NWIRE test sets
the percentage of coreference links in the reference
partition that appear in the system partition; preci-
sion is computed in the same fashion as recall, ex-
cept that the roles of the reference partition and the
system partition are reversed. As a link-based scor-
ing program, the MUC scorer (1) does not reward
successful identification of singleton entities and (2)
tends to under-penalize partitions that have too few
entities. The entity-based CEAF scorer was pro-
posed in response to these two weaknesses. Specif-
ically, it operates by computing the optimal align-
ment between the set of reference entities and the
set of system entities. CEAF precision and recall
are both positively correlated with the score of this
optimal alignment, which is computed by summing
over each aligned entity pair the number of mentions
that appear in both entities of that pair. As a conse-
quence, a system that proposes too many entities or
too few entities will have low precision and recall.
Parameter initialization. We use a small amount
of labeled data for parameter initialization for the
two models. Specifically, for evaluations on the
BNEWS test data, we use as labeled data one
randomly-chosen document from the BNEWS train-
ing set, which has 58 true mentions and 102 system
mentions. Similarly for NWIRE, where the chosen
document has 42 true mentions and 72 system men-
tions. For our model, we use the labeled document
to initialize the parameters. Also, we set N (the
number of most probable partitions) to 50 and ? (the
start penalty used in the Bell tree) to 0.8, the latter
being recommended by Luo et al. (2004).
For H&K’s model, we use the labeled data to tune
the concentration parameter ?. While H&K set ? to
0.4 without much explanation, a moment’s thought
reveals that the choice of ? should reflect the frac-
tion of mentions that appear in a singleton cluster.
We therefore estimate this value from the labeled
document, yielding 0.4 for true mentions (which is
consistent with H&K’s choice) and 0.7 for system
mentions. The remaining parameters, the ?’s, are all
646
set to e?4, following H&K. In addition, as is com-
monly done in Bayesian approaches, we do not sam-
ple entities directly from the conditional distribution
P (Z|X); rather, we sample from this distribution
raised to the power exp cik?1 , where c=1.5, i is the
current iteration number that starts at 0, and k (the
number of sampling iterations) is set to 20. Finally,
due to sampling and the fact that the initial assign-
ment of entity indices to mentions is random, all the
reported results for H&K’s model are averaged over
five runs.
4.2 Results and Discussions
The Heuristic baseline. As our first baseline, we
employ a simple rule-based system that posits two
mentions as coreferent if and only if at least one of
the three strong coreference indicators listed in Ta-
ble 1 returns True. Results of this baseline, reported
in terms of recall (R), precision (P), and F-score (F)
using the MUC scorer and the CEAF scorer, are
shown in row 1 of Tables 4 and 5, respectively. Each
row in these tables shows performance using true
mentions and system mentions for the BNEWS and
NWIRE data sets. As we can see, (1) recall is gen-
erally low, since this simple heuristic can only iden-
tify a small fraction of the coreference relations; (2)
CEAF recall is consistently higher than MUC recall,
since CEAF also rewards successful identification of
non-coreference relations; and (3) precision for true
mentions is higher than that for system mentions,
since the number of non-coreferent pairs that satisfy
the heuristic is larger for system mentions.
The Degenerate EM baseline. Our second base-
line is obtained by running only one iteration of our
EM-based coreference model. Specifically, it starts
with the M-step by initializing the model parame-
ters using the labeled document, and ends with the
E-step by applying the resulting model (in combi-
nation with the Bell tree search algorithm) to ob-
tain the most probable coreference partition for each
test document. Since there is no parameter re-
estimation, this baseline is effectively a purely su-
pervised system trained on one (labeled) document.
Results are shown in row 2 of Tables 4 and 5.
As we can see, recall is consistently much higher
than precision, suggesting that the model has pro-
duced fewer entities than it should. Perhaps more
interestingly, in comparison to the Heuristic base-
line, Degenerate EM performs consistently worse
according to CEAF but generally better according to
MUC. This discrepancy stems from the aforemen-
tioned properties that MUC under-penalizes parti-
tions with too few entities, whereas CEAF lowers
both recall and precision when given such partitions.
Our EM-based coreference model. Our model
operates in the same way as the Degenerate EM
baseline, except that EM is run until convergence,
with the test set being used as unlabeled data for pa-
rameter re-estimation. Any performance difference
between our model and Degenerate EM can thus be
attributed to EM’s exploitation of the unlabeled data.
Results of our model are shown in row 3 of Tables
4 and 5. In comparison to Degenerate EM, MUC
F-score increases by 4-5% for BNEWS and 4-21%
for NWIRE; CEAF F-score increases even more dra-
matically, by 10-17% for BNEWS and 16-27% for
NWIRE. Improvements stem primarily from large
gains in precision and comparatively smaller loss in
recall. Such improvements suggest that our model
has effectively exploited the unlabeled data.
In comparison to the Heuristic baseline, we gener-
ally see increases in both recall and precision when
system mentions are used, and as a result, F-score
improves substantially by 7-15%. When true men-
tions are used, we still see gains in recall, but these
gains are accompanied by loss in precision. F-score
generally increases (by 2-22%), except for the case
with NWIRE where we see a 0.5% drop in CEAF
F-score as a result of a larger decrease in precision.
The Original H&K model. We use as our third
baseline the Original H&K model (see Section 3.2).
Results of this model are shown in row 4 of Tables
4 and 5.9 Overall, it underperforms our model by 6-
16% in MUC F-score and 6-14% in CEAF F-score,
due primarily to considerable drop in both recall and
precision in almost all cases.
The Modified H&K model. Next, we incorporate
our three modifications into the Original H&K base-
line one after the other. Results are shown in rows
5-7 of Tables 4 and 5. Several points deserve men-
tioning. First, the addition of each modification im-
proves the F-score for both true and system mentions
9The H&K results shown here are not directly comparable
with those reported in Haghighi and Klein (2007), since H&K
evaluated their system on the ACE 2004 coreference corpus.
647
Broadcast News (BNEWS) Newswire (NWIRE)
True Mentions System Mentions True Mentions System Mentions
Experiments R P F R P F R P F R P F
1 Heuristic Baseline 27.8 72.0 40.1 30.9 44.3 36.4 31.2 70.3 43.3 36.3 53.4 43.2
2 Degenerate EM Baseline 63.6 53.1 57.9 70.8 36.3 48.0 64.5 42.6 51.3 69.0 25.1 36.8
3 Our EM-based Model 56.1 71.4 62.8 42.4 66.0 51.6 47.0 68.3 55.7 55.2 60.6 57.8
4 Haghighi and Klein Baseline 49.4 60.2 54.3 50.8 40.7 45.2 44.7 55.5 49.5 43.0 40.9 41.9
5 + Relaxed Head Generation 53.0 65.4 58.6 48.3 45.7 47.0 45.1 62.5 52.4 40.9 50.0 45.0
6 + Agreement Constraints 53.6 68.7 60.2 50.4 47.5 48.9 44.6 63.7 52.5 41.7 51.2 46.0
7 + Pronoun-only Salience 56.8 68.3 62.0 52.2 53.0 52.6 46.8 66.2 54.8 44.3 57.3 50.0
8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6
Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets
Broadcast News (BNEWS) Newswire (NWIRE)
True Mentions System Mentions True Mentions System Mentions
Experiments R P F R P F R P F R P F
1 Heuristic Baseline 42.1 75.8 54.1 44.2 48.7 46.3 43.9 73.4 54.9 47.5 53.4 50.3
2 Degenerate EM Baseline 51.2 43.1 46.8 53.7 26.8 35.8 51.0 30.5 38.2 45.1 18.6 26.3
3 Our EM-based Model 53.3 60.5 56.7 47.5 59.6 52.9 49.2 60.7 54.4 53.5 52.1 52.8
4 Haghighi and Klein Baseline 43.7 48.8 46.1 46.0 33.9 39.0 45.5 51.7 48.4 44.6 39.2 41.7
5 + Relaxed Head Generation 45.8 52.4 48.9 45.4 39.6 42.3 46.0 57.0 50.9 44.5 48.3 46.3
6 + Agreement Constraints 51.8 60.5 55.8 50.6 43.8 47.0 47.8 60.1 53.2 46.5 50.4 48.4
7 + Pronoun-only Salience 53.9 59.9 56.7 52.3 49.9 51.1 49.6 62.8 55.4 47.4 55.7 51.2
8 Fully Supervised Model 55.0 63.3 58.8 56.2 64.2 59.9 54.7 64.7 59.3 56.5 65.4 60.6
Table 5: Results obtained using the CEAF scoring program for the Broadcast News and Newswire data sets
in both data sets using both scorers. These results
provide suggestive evidence that our modifications
are highly beneficial. The three modifications, when
applied in combination, improve Original H&K sub-
stantially by 5-8% in MUC F-score and 7-12% in
CEAF F-score, yielding results that compare favor-
ably to those of our model in almost all cases.
Second, the use of agreement constraints yields
larger improvements with CEAF than with MUC.
This discrepancy can be attributed to the fact that
CEAF rewards the correct identification of non-
coreference relations, whereas MUC does not. Since
agreement constraints are intended primarily for dis-
allowing coreference, they contribute to the success-
ful identification of non-coreference relations and as
a result yield gains in CEAF recall and precision.
Third, the results are largely consistent with our
hypothesis that these modifications enhance preci-
sion. Together, they improve the precision of the
Original H&K baseline by 8-16% (MUC) and 11-
16% (CEAF), yielding a coreference model that
compares favorably with our EM-based approach.
Comparison with a supervised model. Finally,
we compare our EM-based model with a fully super-
vised coreference resolver. Inspired by state-of-the-
art resolvers, we create our supervised classification
model by training a discriminative learner (the C4.5
decision tree induction system (Quinlan, 1993)) with
a diverse set of features (the 34 features described in
Ng (2007)) on a large training set (the entire ACE
2003 coreference training corpus), and cluster using
the Bell tree search algorithm. The fully supervised
results shown in row 8 of Tables 4 and 5 suggest that
our EM-based model has room for improvements,
especially when system mentions are used.
5 Conclusions
We have presented a generative model for unsuper-
vised coreference resolution that views coreference
as an EM clustering process. Experimental results
indicate that our model outperforms Haghighi and
Klein’s (2007) coreference model by a large margin
on the ACE data sets and compares favorably to a
modified version of their model. Despite these im-
provements, its performance is still not comparable
to that of a fully supervised coreference resolver.
A natural way to extend these unsupervised coref-
erence models is to incorporate additional linguis-
tic knowledge sources, such as those employed by
our fully supervised resolver. However, feature en-
gineering is in general more difficult for generative
models than for discriminative models, as the former
typically require non-overlapping features. We plan
to explore this possibility in future work.
648
Acknowledgments
We thank the three anonymous reviewers for their
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant 0812261.
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92–100.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
Proceedings of CoNLL, pages 88–95.
Hal Daume´ III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT/EMNLP, pages 97–104.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1–38.
Pascal Denis and Jason Baldridge. 2007. Global, joint
determination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
NAACL/HLT, pages 236–243.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721–741.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the ACL, pages 848–855.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of HLT/EMNLP, pages 17–24.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. Competitive self-trained pronoun
interpretation In Proceedings of HLT-NAACL 2004:
Short Papers, pages 33–36.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL, pages
135–142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT/EMNLP, pages
25–32.
Christoph Mu¨ller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
Proceedings of the ACL, pages 352–359.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689–
1694.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104–111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
HLT-NAACL: Main Proceedings, pages 173–180.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT/NAACL, pages 192–199.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
Michael Strube, Stefan Rapp, and Christoph Mu¨ller.
2002. The influence of minimum edit distance on ref-
erence resolution. In Proceedings of EMNLP, pages
312–319.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1527–1554.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539–
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45–52.
Xiaofeng Yang, GuoDong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the ACL, pages
176–183.
649
