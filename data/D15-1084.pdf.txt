Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 726–736,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Knowledge Base Unification via Sense Embeddings and Disambiguation
Claudio Delli Bovi
Department of
Computer Science
Sapienza University of Rome
dellibovi@di.uniroma1.it
Luis Espinosa-Anke
Department of Information and
Communication Technologies
Universitat Pompeu Fabra
luis.espinosa@upf.edu
Roberto Navigli
Department of
Computer Science
Sapienza University of Rome
navigli@di.uniroma1.it
Abstract
We present KB-UNIFY, a novel approach
for integrating the output of different
Open Information Extraction systems into
a single unified and fully disambiguated
knowledge repository. KB-UNIFY con-
sists of three main steps: (1) disambigua-
tion of relation argument pairs via a sense-
based vector representation and a large
unified sense inventory; (2) ranking of se-
mantic relations according to their degree
of specificity; (3) cross-resource relation
alignment and merging based on the se-
mantic similarity of domains and ranges.
We tested KB-UNIFY on a set of four
heterogeneous knowledge bases, obtain-
ing high-quality results. We discuss and
provide evaluations at each stage, and re-
lease output and evaluation data for the use
and scrutiny of the community
1
.
1 Introduction
The breakthrough of the Open Information Ex-
traction (OIE) paradigm opened up a research
area where Web-scale unconstrained Information
Extraction systems are developed to acquire and
formalize large quantities of knowledge. How-
ever, while successful, to date most state-of-the-
art OIE systems have been developed with their
own type inventories, and no portable ontologi-
cal structure. In fact, OIE systems can be very
different in nature. Early approaches (Etzioni et
al., 2008; Wu and Weld, 2010; Fader et al., 2011)
focused on extracting a large number of relations
from massive unstructured corpora, mostly rely-
ing on dependencies at the level of surface text.
Systems like NELL (Carlson et al., 2010) com-
bine a hand-crafted taxonomy of entities and re-
lations with self-supervised large-scale extraction
1
http://lcl.uniroma1.it/kb-unify
from the Web, but they require additional process-
ing for linking and integration (Dutta et al., 2014).
More recent work has focused, instead, on
deeper language understanding, especially at the
level of syntax and semantics (Nakashole et al.,
2012; Moro and Navigli, 2013). By leveraging
semantic analysis, knowledge gathered from un-
structured text can be adequately integrated and
used to enrich existing knowledge bases, such
as YAGO (Mahdisoltani et al., 2015), FREEBASE
(Bollacker et al., 2008) and DBPEDIA (Lehmann
et al., 2014). A large amount of reliable struc-
tured knowledge is crucial for OIE approaches
based on distant supervision (Mintz et al., 2009;
Riedel et al., 2010), even when multi-instance
multi-learning algorithms (Surdeanu et al., 2012)
or matrix factorization techniques (Riedel et al.,
2013; Fan et al., 2014) come into play to deal
with noisy extractions. For this reason a recent
trend of research has focused on Knowledge Base
(KB) completion (Nickel et al., 2012; Bordes et
al., 2013), exploiting the fact that distantly super-
vised OIE and structured knowledge can comple-
ment each other. However, the majority of integra-
tion approaches nowadays are not designed to deal
with many different resources at the same time.
We propose an approach where the key idea is to
bring together knowledge drawn from an arbitrary
number of OIE systems, regardless of whether
these systems provide links to some general-
purpose inventory, come with their own ad-hoc
structure, or have no structure at all. Knowledge
from each source, in the form of ?subject, predi-
cate, object? triples, is disambiguated and linked
to a single large sense inventory. This enables us
to discover alignments at a semantic level between
relations from different KBs, and to generate a
unified, fully disambiguated KB of entities and
semantic relations. KB-UNIFY achieves state-
of-the-art disambiguation and provides a general,
resource-independent representation of semantic
relations, suitable for any kind of KB.
726
The remainder of this paper is structured as fol-
lows: Section 2 reviews relevant related work;
Sections 3, 4, 5 and 6 describe in detail each stage
of the approach; Sections 7 and 8 describe the ex-
periments carried out and the results obtained; and
finally Section 9 summarizes our findings and dis-
cusses potential directions for future work.
2 Related Work
The integration of knowledge drawn from dif-
ferent sources has received much attention over
the last decade. Among the most notable examples
are resources like BabelNet (Navigli and Ponzetto,
2012), UBY (Gurevych et al., 2012) and YAGO
(Mahdisoltani et al., 2015). While great effort
has been put into aligning knowledge at the con-
cept level, most approaches do not tackle the prob-
lem of integrating heterogeneous knowledge at the
relation level, nor do they exploit effectively the
huge amount of information harvested with OIE
systems, even when this information is unambigu-
ously linked to a structured resource, as in (Nakas-
hole et al., 2012), or (Moro and Navigli, 2013).
In fact, as the number of resources increases, KB
alignment is already becoming an emergent re-
search field: Dutta et al. (2014) describe a method
for linking arguments in NELL triples to DBPE-
DIA by combining First Order Logic and Markov
Networks; Grycner and Weikum (2014) seman-
tify PATTY’s pattern synsets and connect them
to WordNet verbs; Lin et al. (2012) propose a
method to propagate FREEBASE types across RE-
VERB and deal with the problem of unlinkable
entities. All these approaches achieve very com-
petitive results in their respective settings, but un-
like the approach being proposed here, they limit
the task to 1-to-1 alignments. A few contributions
have tried to broaden the scope and include dif-
ferent resources at the same time, but with rather
different goals from ours. For example, Riedel et
al. (2013) propose a universal schema that inte-
grates structured data with OIE data by learning
latent feature vectors for entities and relations; the
KNOWLEDGE VAULT (Dong et al., 2014) uses a
graph-based probabilistic framework where prior
knowledge from existing resources (e.g. FREE-
BASE) improves Web extractions by predicting
their reliability. However, in both cases the main
objective is distantly supervised extraction from
unstructured text, rather than KB unification. A re-
cent trend of research focuses on learning embed-
ding models for structured knowledge and their
application to tasks like relation extraction and KB
completion (Socher et al., 2013; Weston et al.,
2013; Bordes et al., 2013). These approaches,
however, leverage embeddings at surface level,
which are suboptimal for our task, as will be dis-
cussed in Section 3. Since we require a com-
mon semantic framework for KB unification, we
use vector representations based on word senses,
which are mapped to a very large sense inventory.
This shared sense inventory, then, constitutes the
common ground in which disambiguation, align-
ment and final unification occurs.
3 Knowledge Base Unification: Overview
KB-UNIFY takes as input a set of KBs K =
{KB
1
, ...,KB
n
} and outputs a single, unified and
fully disambiguated KB, denoted as KB
?
. For
our purposes we can define a KB KB
i
as a triple
?E
i
, R
i
, T
i
?, where E
i
is a set of entities, R
i
is
a set of semantic relations, and T
i
is a set of
triples (facts) ?e
d
, r, e
g
? with subject and object
e
d
, e
g
? E
i
and predicate r ? R
i
. Depending
on the nature of each KB
i
, entities in E
i
might
be disambiguated and linked to an external in-
ventory (e.g. the entity Washington linked to the
Wikipedia page GEORGE WASHINGTON), or un-
linked and only available as ambiguous mentions
(e.g. the bare word washington might refer to the
president, the city or the state). We can thus par-
tition K into a subset of linked resources K
D
, and
one of unlinked resources K
U
. In order to align
very different and heterogeneous KBs at the se-
mantic level, KB-UNIFY exploits:
• A unified sense inventory S, which acts as
a superset for the inventories of individual
KBs. We choose BabelNet (Navigli and
Ponzetto, 2012) for this purpose: by merg-
ing complementary knowledge from different
resources (e.g. Wikipedia, WordNet, Wiki-
data and Wiktionary, among others), Babel-
Net provides a wide coverage of entities and
concepts whilst at the same time enabling
convenient inter-resource mappings for KB
i
in K
D
. For instance, each Wikipedia page (or
Wikidata item) has a corresponding synset in
BabelNet, which enables a one-to-one map-
ping between BabelNet’s synsets and entries
in, e.g., DBPEDIA or FREEBASE;
• A vector space model V
S
that enables a se-
mantic representation for every item in S.
Current distributional models, like word em-
727
Figure 1: Unification algorithm workflow
beddings (Mikolov et al., 2013), are not suit-
able to our setting: they are constrained to
surface word forms, and hence they inher-
ently retain ambiguity of polysemous words
and entity mentions. We thus leverage
SENSEMBED (Iacobacci et al., 2015), a novel
semantically-enhanced approach to embed-
dings. SENSEMBED is trained on a large
annotated corpus and produces continuous
representations for individual word senses
(sense embeddings), according to an under-
lying sense inventory.
Figure 1 illustrates the workflow of our KB uni-
fication approach. Entities coming from any
KB
i
? K
D
can be directly (and unambiguously)
mapped to the corresponding entries in S via Ba-
belNet inter-resource linking (Figure 1(a)): in the
above example, the entity Washington linked to
the Wikipedia page GEORGE WASHINGTON is in-
cluded in the BabelNet synset Washington
4
bn
.
In contrast, unlinked (and potentially ambiguous)
entities need an explicit disambiguation step (Fig-
ure 1(b)) connecting them to appropriate entries,
i.e. synsets, in S: this is the case, in the above
example, for the ambiguous mention washington
that has to be linked to either the president, the
city or the state. Therefore, our approach com-
prises two successive stages:
• A disambiguation stage (Section 5) where
all KB
i
? K are linked to S, either by
inter-resource mapping (Figure 1(a)) or dis-
ambiguation (Figure 1(b)), and all E
i
are
merged into a unified set of entities E
?
. As
a result of this process we obtain a set K
S
comprising all the KBs in K redefined using
the common sense inventory S;
• An alignment stage (Section 6, Figure 1(c))
where, for each pair of KBs KB
S
i
,KB
S
j
?
K
S
, we compare any relation pair ?r
i
, r
j
?,
r
i
? R
S
i
and r
j
? R
S
j
, in order to identify
cross-resource alignments and merge rela-
tions sharing equivalent semantics into rela-
tion clusters (relation synsets). This process
yields a unified set of relation synsets R
?
.
The overall result is KB
?
= ?E
?
, R
?
, T
?
?,
where T
?
is the set of all disambiguated
triples redefined over E
?
and R
?
.
4 Background
The disambiguation stage of our approach is
based on the interplay between two core compo-
nents: a vector space model V
S
, as introduced
in Section 3, which provides an unambiguous se-
mantic representation for each item in S; and a
Word Sense Disambiguation/Entity Linking sys-
tem, working on the same sense inventory S,
which discovers and disambiguates concepts and
entity mentions within a given input text. In this
section we briefly describe our choice for these
two components: SENSEMBED (Iacobacci et al.,
2015) and BABELFY (Moro et al., 2014).
SENSEMBED is a knowledge-based approach
for obtaining latent continuous representations of
individual word senses. Unlike other sense-based
embeddings approaches, like (Huang et al., 2012),
which address the inherent polysemy of word-
level representations relying solely on text cor-
pora, SENSEMBED exploits the structured knowl-
edge of a large sense inventory along with the dis-
tributional information gathered from text corpora.
In order to do this, SENSEMBED requires a sense-
annotated corpus; for each target word sense, then,
a representation is computed by maximizing the
log likelihood of the word sense with respect to
its context within the annotated text, similarly to
the word-based embeddings model. Following Ia-
cobacci et al. (2015), we trained SENSEMBED us-
ing the English Wikipedia and, as sense inventory,
BabelNet.
BABELFY
2
is a joint state-of-the-art approach
to multilingual Entity Linking and Word Sense
Disambiguation. Given the BabelNet lexical-
ized semantic network as underlying structure,
BABELFY first models each concept in the net-
work through its corresponding semantic signa-
ture by leveraging a graph random walk algorithm.
Then, given an input text, the generated seman-
tic signatures are used to construct a subgraph
2
http://babelfy.org
728
of the semantic network representing the mean-
ing of the content words in that text. BABELFY
then searches this subgraph for the intended sense
of each content word, by means of a densest-
subgraph heuristic that identifies high-coherence
interpretations. Given its unified approach that
covers concepts and named entities alike, and its
flexibility in disambiguating both bag-of-words
and proper text, BABELFY constitutes the most
convenient choice for linking relation triples to a
high-coverage sense inventory like BabelNet.
5 Disambiguation
In the disambiguation phase (Figure 1(b)), all
KB
i
? K
U
are linked to the unified sense in-
ventory S and added to the set of redefined KBs
K
S
. As explained in Section 3, while each KB
in K
D
can be unambiguously redefined via Babel-
Net inter-resource links and added to K
S
, KBs in
K
U
require an explicit disambiguation step. Given
KB
i
? K
U
, our disambiguation module (Figure
2) takes as input its set of unlinked triples T
i
and
outputs a set T
S
i
? T
i
of disambiguated triples
with subject-object pairs linked to S. The triples in
T
S
i
, together with their corresponding entity sets
and relation sets, constitute the redefined KB
S
i
which is then added to K
S
. However, applying
a straightforward approach that disambiguates all
triples in isolation might lead to very imprecise re-
sults, due to the lack of available context for each
individual triple. We thus devised a disambigua-
tion strategy that comprises three successive steps:
1. We identify a set of high-confidence seeds
from T
i
(Section 5.1), i.e. triples ?e
d
, r, e
g
?
where subject e
d
and object e
g
are highly
semantically related, and disambiguate them
using the senses that maximize their similar-
ity in our vector space V
S
;
2. We use the seeds to generate a ranking of
Figure 2: Disambiguation algorithm workflow
the relations in R
i
according to their degree
of specificity (Section 5.2). We represent
each r ? R
i
in our vector space V
S
and as-
sign higher specificity to relations whose ar-
guments are closer in V
S
;
3. We finally disambiguate the remaining non-
seed triples in T
i
(Section 5.3) starting from
the most specific relations, and jointly using
all participating argument pairs as context.
5.1 Identifying Seed Argument Pairs
The first stage of our disambiguation approach
aims at extracting reliable seeds from T
i
, i.e.
triples ?e
d
, r, e
g
? where subject e
d
and object e
g
can be confidently disambiguated without addi-
tional context. In order to do this we leverage
the sense embeddings associated with each can-
didate disambiguation for e
d
and e
g
. We consider
all the available senses for both e
d
and e
g
in S,
namely s
d
= {s
1
d
, ..., s
m
d
} and s
g
= {s
1
g
, ..., s
m
?
g
},
and the corresponding sets of sense embeddings
v
d
= {v
1
d
, ..., v
m
d
} and v
g
= {v
1
g
, ..., v
m
?
g
}. We
then select, among all possible pairs of senses, the
pair ?s
?
d
, s
?
g
? that maximizes the cosine similarity
between the corresponding embeddings ?v
?
d
, v
?
g
?:
?v
?
d
, v
?
g
? = argmax
v
d
?v
d
, v
g
?v
g
v
d
· v
g
?v
d
? ?v
g
?
(1)
For each disambiguated triple ?s
?
d
, r, s
?
g
?, the co-
sine similarity value associated with ?v
?
d
, v
?
g
? rep-
resents the disambiguation confidence ?
dis
. We
rank all such triples according to their confidence,
and select those above a given threshold ?
dis
. The
underlying assumption is that, for high-confidence
subject-object pairs, the embeddings associated
with the correct senses s
?
d
and s
?
g
will be closest
in V
S
compared to any other candidate pair. In-
tuitively, the more the relation r between e
d
and
e
g
is semantically well defined, the more this as-
sumption is justified. As an example, consider the
triple ?Armstrong, worked for, NASA?: among all
the possible senses for Armstrong (the astronaut,
the jazz musician the cyclist, etc.) and NASA (the
space agency, the racing organization, a Swedish
band, etc.) we expect the vectors corresponding to
the astronaut and the space agency to be closest in
the vector space model V
S
.
5.2 Relation Specificity Ranking
The assumption that, given an ambiguous
subject-object pair, correct argument senses are
729
the closest pair in the vector space (Section 5.1) is
easily verifiable for general relations (e.g. is a, is
part of ). However, as a semantic relation becomes
specific, its arguments are less guaranteed to be
semantically related (e.g. is a professor in the uni-
versity of ) and a disambiguation approach based
exclusively on similarity is prone to errors. On
the other hand, specific relations tend to narrow
down the scope of possible entity types occurring
as subject and object. In the above example, is a
professor in the university of requires entity pairs
with professors as subjects and cities as objects.
Our disambiguation strategy should thus vary ac-
cording to the specificity of the relations taken into
account. In order to consider this observation in
our disambiguation pipeline, we first need to es-
timate the degree of specificity for each relation
in the relation set R
i
of the target KB to be dis-
ambiguated. Given R
i
and a set of seeds from the
previous stage (Section 5.1), we apply a specificity
ranking policy and sort relations in R
i
from the
most general to the most specific. We compute the
generality Gen(r) of a given relation r by looking
at the spatial dispersion of the sense embeddings
associated with its seed subjects and objects. Let
v
D
(v
G
) be the set of sense embeddings associated
with the domain (range) seed arguments of r. For
both v
D
and v
G
, we compute the corresponding
centroid vectors µ
D
and µ
G
as:
µ
k
=
1
|v
k
|
?
v?v
k
v
?v?
, k ? {D,G} (2)
Then, the variances ?
2
D
and ?
2
G
are given by:
?
2
k
=
1
|v
k
|
?
v?v
k
(1? cos (v, µ
k
))
2
(3)
with k ? {D,G} as before. We finally compute
Gen(r) as the average of ?
2
D
and ?
2
G
. The result
of this procedure is a relation specificity ranking
that associates each relation r with its general-
ity Gen(r). Intuitively, we expect more general
relations to show higher variance (hence higher
Gen(r)), as their subjects and objects are likely
to be rather disperse throughout the vector space;
instead, arguments of very specific relations are
more likely to be clustered together in compact re-
gions, yielding lower values of Gen(r).
5.3 Disambiguation with Relation Context
In the third step, both the specificity ranking
and the seeds are exploited to disambiguate the
remaining triples in T
i
. To do this we leverage
BABELFY (Moro et al., 2014) (introduced in Sec-
tion 4). As we observed in Section 5.2, spe-
cific relations impose constraints on their subject-
object types and tend to show compact domains
and ranges in the vector space. Therefore, given
a triple ?e
d
, r, e
g
?, knowing that r is specific en-
ables us to put together all the triples in T
i
where
r occurs, and use them to provide meaningful con-
text for disambiguation. If r is general, instead, its
subject-object types are less constrained and addi-
tional triples do not guarantee to provide semanti-
cally related context.
At this stage, our algorithm takes as input the
set of triples T
i
, along with the associated disam-
biguation seeds (Section 5.1), the specificity rank-
ing (Section 5.2) and a specificity threshold ?
spec
.
T
i
is first partitioned into two subsets: T
spec
i
, com-
prising all the triples for which Gen(r) < ?
spec
,
and T
gen
i
= T
i
\ T
spec
i
. We then employ two dif-
ferent disambiguation strategies:
• For each distinct relation r occurring in
T
spec
i
, we first retrieve the subset T
spec
i,r
?
T
spec
i
of triples where r occurs, and then dis-
ambiguate T
spec
i,r
as a whole with BABELFY.
For each triple in T
spec
i,r
, context is provided
by all the remaining triples along with the
disambiguated seeds extracted for r.
• We disambiguate the remaining triples in
T
gen
i
one by one in isolation with BABELFY,
providing for each triple only the predicate
string r as additional context.
6 Cross-Resource Relation Alignment
After disambiguation (Section 5) each KB in
K is linked to the unified sense inventory S and
added to K
S
. However, eachKB
S
i
? K
S
still pro-
vides its own relation set R
S
i
? R
i
. Instead, in the
unified KB
?
, relations with equivalent semantics
should be considered as part of a single relation
synset even when they come from different KBs.
Therefore, at this stage, we apply an alignment al-
gorithm to identify pairs of relations from different
KBs having equivalent semantics. We exploit the
fact that each relation r is now defined over entity
pairs linked to S, and we generate a semantic rep-
resentation of r in the vector space V
S
based on
the centroid vectors of its domain and range. Due
to representing the semantics of relations on this
common ground, we can compare them by com-
puting their domain and range similarity in V
S
. We
730
first consider each KB
S
i
? K
S
and, for each rela-
tion r
i
in R
S
i
, we compute the corresponding cen-
troid vectors µ
r
i
d
and µ
r
i
g
using formula (2). Then,
for each pair of KBs ?KB
S
i
,KB
S
j
? ? K
S
× K
S
,
we compare all relation pairs ?r
i
, r
j
? ? R
S
i
× R
S
j
by computing the cosine similarity between do-
main centroids s
D
and between range centroids
s
G
:
s
k
=
µ
r
i
k
· µ
r
j
k
?µ
r
i
k
? ?µ
r
j
k
?
(4)
where µ
r
k
denotes the centroid associated with re-
lation r and k ? {D,G}. The average of s
D
and
s
G
gives us an alignment confidence ?
align
for the
pair ?r
i
, r
j
?. If confidence is above a given thresh-
old ?
align
then r
i
and r
j
are merged into the same
relation synset. Relations for which no alignment
is found are turned into singleton relation synsets.
As a result of this alignment procedure we obtain
the unified set of relations R
?
.
7 Experimental Setup
The setting for our experimental evaluation was
the following:
• We used BabelNet 3.0
3
as our unified sense
inventory for the unification procedure as
well as the underlying inventory for both BA-
BELFY and SENSEMBED. Currently, Babel-
Net contains around 14M synsets and repre-
sents the largest single multilingual reposi-
tory of entities and concepts;
• We selected PATTY (Nakashole et al., 2012)
and WISENET (Moro and Navigli, 2013)
as linked resources. We used PATTY with
FREEBASE types and pattern synsets derived
from Wikipedia, and WISENET 2.0 with
Wikipedia relational phrases;
• We selected NELL (Carlson et al., 2010) and
REVERB (Fader et al., 2011) as unlinked
resources. We used KB beliefs updated to
November 2014 for the former, and the set
of relation instances from ClueWeb09 for the
latter.
Comparative statistics in Table 1 show that the
input KBs are rather different in nature: NELL
is based on 298 predefined relations and contains
beliefs for about 2 million entities. The distri-
bution of entities over relations is however very
3
http://babelnet.org
K
U
K
D
NELL REVERB PATTY WISENET
# relations 298 1 299 844 1 631 531 245 935
# triples 2 245 050 14 728 268 15 802 946 2 271 807
# entities 1 996 021 3 327 425 1 087 907 1 636 307
Table 1: Statistics on the input KBs
(a)
(b)
Figure 3: Precision (left) and coverage (right) of disam-
biguated seeds at different values of ?
dis
for (a) the whole set
of triples in PATTY and (b) the subset of ambiguous triples
skewed, with 80.33% of the triples being instances
of the generalizations relationship. In con-
trast, REVERB contains a highly sparse relation
set (1,299,844 distinct relations) and more than
3 million distinct entities. PATTY features the
largest (and, together with WISENET, sparsest)
set of triples, with 1,631,531 distinct relations and
less than 10 triples per relation on average.
8 Experiments
8.1 Disambiguation
We tested our disambiguation approach exper-
imentally in terms of both disambiguated seed
quality (Section 8.1.1) and overall disambiguation
performance (Section 8.1.2). We created a de-
velopment set by extracting a subset of 6 million
triples from the largest linked KB in our experi-
mental setup, i.e. PATTY. Triples in PATTY are
automatically linked to YAGO, which is in turn
linked to WordNet and DBPEDIA. Since both re-
sources are also linked by BabelNet, we mapped
the original triples to the BabelNet sense inventory
and used them to tune our disambiguation module.
We also provide two baseline approaches: (1) di-
731
SENSEMBED Baseline
?
dis
0.5-0.7 0.7-0.9 0.9-1.0 0.5-0.7 0.7-0.9 0.9-1.0
PATTY .980 .980 1.000 .793 .780 1.000
WISENET .958 .960 .973 .726 .786 .791
NELL .955 .995 1.000 .800 .770 .885
REVERB .930 .940 .950 .775 .725 .920
Table 2: Disambiguation precision for all KBs
?
spec
= 0.8 ?
spec
= 0.5 ?
spec
= 0.3
all only seeds all only seeds all only seeds
PATTY 62.15 26.60 52.49 24.06 40.75 21.41
WISENET 60.00 37.46 54.44 22.26 53.58 16.62
NELL 76.97 62.98 50.95 20.71 44.70 4.36
REVERB 41.20 38.57 25.14 23.70 13.37 12.75
Table 3: Coverage results (%) for all KBs
rect disambiguation on individual triples with BA-
BELFY alone (without the seeds) and (2) direct
disambiguation of the seeds only (without BA-
BELFY).
8.1.1 Results: Disambiguated Seeds
We tuned our disambiguation algorithm by
studying the quality of the disambiguated seeds
(Section 5.1) extracted from the surface text triples
of PATTY. Figure 3 shows precision and cover-
age for increasing values of the confidence thresh-
old ?
dis
. We computed precision by checking
each disambiguated seed against the correspond-
ing linked triple in the development set, and cov-
erage as the ratio of covered triples. We analyzed
results for both the whole set of triples in PATTY
(Fig. 3a) and the subset of ambiguous triples (Fig.
3b), i.e. those triples whose subjects and objects
have at least two candidate senses each in the Ba-
belNet inventory. In both cases, precision of dis-
ambiguated seeds increases rapidly with ?
dis
, sta-
bilizing above 90% with ?
dis
> 0.25. Coverage
displays the opposite behavior, decreasing expo-
nentially with more confident outcomes, from 6
million triples to less than a thousand (for seeds
with confidence ?
dis
> 0.95). As a result, we
chose ?
dis
= 0.25 as optimal threshold value
throughout the rest of the evaluations.
In addition, we manually evaluated the disam-
biguated seeds extracted from both linked KBs
(PATTY and WISENET) and unlinked KBs (NELL
and REVERB). For each KB, we extracted up to
three random samples of 150 triples according to
different levels of confidence ?
dis
: the first sam-
ple included extraction with 0.5 ? ?
dis
< 0.7,
the second with 0.7 ? ?
dis
< 0.9, and the third
with ?
dis
? 0.9. Each sample was evaluated by
two human judges: for each disambiguated triple
KB-UNIFY Dutta et al. Baseline
all only seeds (? = 0.5)
Precision .852 .957 .931 .749
Recall .875 .117 .799 .608
F-score .864 .197 .857 .671
Table 4: Disambiguation results over NELL gold standard
?e
d
, r, e
g
?, we presented our judges with the sur-
face text arguments e
d
, e
g
and the relation string r,
along with the two BabelNet synsets correspond-
ing to the disambiguated arguments s
?
d
, s
?
g
, and we
asked whether the association of each subject and
object with the proposed BabelNet synset was cor-
rect. We then estimated precision as the average
proportion of correctly disambiguated triples. For
each sample we compared disambiguation preci-
sion using SENSEMBED, as in Section 5.1, against
the first baseline with BABELFY alone. Results,
reported in Table 2, show that our approach consis-
tently outperforms the baseline and provides high
precision over all samples and KBs.
8.1.2 Results: Disambiguation with Relation
Context
We then evaluated the overall disambiguation
output after specificity ranking (Section 5.2) and
disambiguation with relation context using BA-
BELFY (Section 5.3). We analyzed three config-
urations of the disambiguation pipeline, namely
?
spec
? {0.8, 0.5, 0.3}. We ran the algorithm over
both linked and unlinked KBs of our experimen-
tal setup, and computed the coverage for each KB
as the overall ratio of disambiguated triples. Re-
sults are reported in Table 3 and compared to the
coverage obtained from the disambiguated seeds
only: context-aware disambiguation substantially
increases coverage over all KBs. Table 3 also
shows that a restrictive ?
spec
results in lower cover-
age values, due to the increased number of triples
disambiguated without context.
Finally, we evaluated the quality of disam-
biguation on a publicly available dataset (Dutta
et al., 2014) comprising manual annotations for
NELL. This dataset provides a gold standard
of 1200 triples whose subjects and objects are
manually assigned a proper DBpedia URI. We
again used BabelNet’s inter-resource links to ex-
press DBpedia annotations with our sense inven-
tory and then sought, for each annotated triple in
the dataset, the corresponding triple in our disam-
biguated version of NELL with ?
dis
= 0.25 and
?
spec
= 0.8. We then repeated this process con-
732
Figure 4: Average argument similarity against Gen(r)
NELL REVERB PATTY WISENET
Precision .660 .715 .625 .750
Cohen’s kappa - .430 .620 .600
Table 5: Specificity ranking evaluation
sidering only the disambiguated seeds instead of
the whole disambiguation pipeline. In line with
(Dutta et al., 2014), we computed precision, recall
and F-score for each setting. Results are reported
in Table 4 and compared against those of Dutta et
al. (2014) and against our first baseline with BA-
BELFY alone. KB-UNIFY achieves the best result,
showing that a baseline based on state-of-the-art
disambiguation is negatively affected by the lack
of context for each individual triple. In contrast,
an approach that relies only on the disambiguated
seeds affords very high precision, but suffers from
dramatically lower coverage.
8.2 Specificity Ranking
We evaluated the specificity ranking (Section
5.2) generated by KB-UNIFY for all KBs of our
experimental setup. First of all, we empirically
validated our scoring function Gen(r) over each
resource: for each relation we computed the aver-
age cosine similarity among all its domain argu-
ments s¯
D
and among all its range arguments s¯
G
.
We then plotted the average s¯ of s¯
D
and s¯
G
against
Gen(r) for each relation r (Figure 4). As observed
in Section 5.2, the average similarity among do-
main and range arguments decreases for increas-
ing values of Gen(r), indicating that more gen-
eral relations allow less semantically constrained
subject-object types. We then used human judge-
ment to assess the quality of our specificity rank-
ings. First, each ranking was split into four quar-
NELL
High Gen(r) agent created
at location
Low Gen(r) person in economic sector
restaurant in city
REVERB
High Gen(r) is for
is in
Low Gen(r) enter Taurus in
carry oxygen to
PATTY
High Gen(r) located in
later served to
Low Gen(r) starting pitcher who played
league coach for
WISENET
High Gen(r) include
is a type of
Low Gen(r) lobe-finned fish lived during
took part in the Eurovision contest
Table 6: Examples of general and specific relations for all
KBs
tiles, and two human evaluators were presented
with a sample from the top quartile (i.e. a relation
falling into the most general category) and a sam-
ple from the bottom quartile (i.e. a relation falling
into the most specific category). We shuffled each
relation pair, showed it to our human judges, and
then asked which of the two relations they consid-
ered to be the more specific. Ranking precision
was computed by considering those pairs where
human choice agreed with the ranking. Finally,
we computed inter-annotator agreement on each
specificity ranking (except for NELL, due to the
small sample size) with Cohen’s kappa coefficient
(Cohen, 1968). Results for each ranking are re-
ported in Table 5, while some examples of general
and specific relations for each KB are shown in
Table 6. Disagreement between human choice and
ranking is higher in NELL (where the set of rela-
tions is quite small compared to other KBs) and in
PATTY (due to a sparser set of relations, biased
towards very specific patterns). Inter-annotator
agreement is instead lower for REVERB, where
unconstrained Web harvesting often results in am-
biguous relation strings.
8.3 Alignment
Due to the novelty of our approach, and hence
the lack of widely accepted gold standards and
testbeds, we evaluated our cross-resource relation
alignment algorithm (Section 6) by exploiting hu-
man judgement once again. Given the results of
733
PATTY-WISENET PATTY-REVERB NELL-REVERB
?
align
0.7 0.9 0.7 0.9 0.7 0.9
Prec. .68 .80 .58 .74 .61 .75
# Align. 128k 1.2k 47k 643 2.6k 88
PATTY-NELL WISENET-NELL WISENET-REVERB
?
align
0.7 0.9 0.7 0.9 0.7 0.9
Prec. .66 1.00 .70 .84 .59 .87
# Align. 2.6k 57 381 34 9.9k 169
Table 7: Cross-resource alignment evaluation
PATTY-WISENET ?
align
portrayed ’s character 0.84
debuted in first appeared in 0.86
PATTY-REVERB ?
align
language in is spoken in 0.81
mostly known for plays the role of 0.70
NELL-REVERB ?
align
bookwriter is a novel by 0.88
personleadscity is the mayor of 0.60
NELL-PATTY ?
align
worksfor was hired by 0.72
riveremptiesintoriver tributary of 0.89
NELL-WISENET ?
align
animaleatfood feeds on 0.72
teamhomestadium play their home games at 0.88
REVERB-WISENET ?
align
has a selection of offers 0.82
had grown up in was born and raised in 0.85
Table 8: Examples of cross resource relation alignments
Section 8.1, we considered the top 10k frequent
relations for each KB and ran the algorithm over
each possible pair of KBs with two different con-
figurations: ?
align
= 0.7 and ?
align
= 0.9. From
each pair of KBs ?KB
i
,KB
j
? we obtained a list
of candidate alignments, i.e. pairs of relations
?r
i
, r
j
? where r
i
? KB
i
and r
j
? KB
j
. From
each list we then extracted a random sample of
150 candidate alignments. We showed each align-
ment
4
?r
i
, r
j
? to two human judges, and asked
whether r
i
and r
j
represented the same relation.
The problem was presented in terms of paraphras-
ing: for each pair, we asked whether exchanging
r
i
and r
j
within a sentence would have changed
that sentence’s meaning. In line with Section 8.2
we computed precision based on the agreement
between human choice and automatic alignments.
Results are reported in Table 7. Our alignment al-
gorithm shows high precision in all pairings where
?
align
= 0.9. Alignment reliability decreases for
lower ?
align
, as relation pairs where r
i
is a gener-
alization of r
j
(or vice versa) tend to have similar
centroids in V
S
. The same holds for pairs where r
i
is the negation of r
j
(or vice versa). Even though
we could have utilized measures based on rela-
4
In the case of relation synsets, such as PATTY and
WISENET, we selected up to three random relation strings
from each synset.
tion string similarity (Dutta et al., 2015) to reduce
wrong alignments in these cases, by relying on a
purely semantic criterion we removed any prior as-
sumption on the format of input KBs. Some exam-
ples of alignments are shown in Table 8.
To conclude, we report statistics regarding the
unified KB
?
produced from the initial set of re-
sources in our experimental setup (cf. Section
7). We validated our thresholds for high-precision,
and selected ?
dis
= 0.25, ?
spec
= 0.8 and ?
align
= 0.8. Our alignment algorithm produced 56,673
confident alignments, out of which 2,207 relation
synsets were derived, with an average size of 16.82
individual relations per synset. As a result, we ob-
tained a unified KB
?
comprising 24,221,856 dis-
ambiguated triples defined over 1,952,716 distinct
entities and 2,675,296 distinct relations.
9 Conclusion and Future Work
We have presented KB-UNIFY, a novel, gen-
eral approach for disambiguating and seamlessly
unifying KBs produced by different OIE sys-
tems. KB-UNIFY represents entities and relations
using a shared semantic representation, leverag-
ing a unified sense inventory together with a
semantically-enhanced vector space model and a
disambiguation algorithm. This enables us to dis-
ambiguate unlinked resources (like NELL and RE-
VERB) with high precision and coverage, and to
discover relation-level cross-resource alignments
effectively. One of the key features of our strat-
egy is its generality: by representing each KB on
a common ground, we need no prior assumption
on the nature and format of the knowledge it en-
codes. We tested our approach experimentally on
a set of four very different KBs, both linked and
unlinked, and we evaluated disambiguation and
alignment results extensively at every stage, ex-
ploiting both human evaluations and public gold
standard datasets (when available). This work
opens compelling avenues for future work. We
plan to further exploit sense-enhanced unified rep-
resentations of relations in various ways: provid-
ing an ontological structure for the unified KB,
exploring complementary approaches for captur-
ing semantic relation alignments, and incorporat-
ing multilinguality.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
734
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database For Structur-
ing Human Knowledge. In Proceedings of SIG-
MOD, pages 1247–1250.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating Embeddings for Modeling Multi-
relational Data. In Advances in NIPS, volume 26,
pages 2787–2795.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. In Proceedings of
AAAI, pages 1306–1313.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or
Partial Credit. Psychological Bulletin, 70(4):213–
220.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge Vault: A Web-scale Approach to Probabilistic
Knowledge Fusion. In Proceedings of the SIGKDD,
pages 601–610.
Arnab Dutta, Christian Meilicke, and Simone Paolo
Ponzetto. 2014. A Probabilistic Approach for Inte-
grating Heterogeneous Knowledge Sources. In Pro-
ceedings of ESWC, pages 286–301.
Arnab Dutta, Christian Meilicke, and Heiner Stucken-
schmidt. 2015. Enriching Structured Knowledge
with Open Information. In Proceedings of WWW,
pages 267–277.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open Information Extraction
from the Web. Commun. ACM, 51(12):68–74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of EMNLP, pages 1535–
1545.
Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,
Thomas Fang Zheng, and Edward Y. Chang. 2014.
Distant Supervision for Relation Extraction with
Matrix Completion. In Proceedings of ACL, pages
839–849.
Adam Grycner and Gerhard Weikum. 2014. HARPY:
Hypernyms and Alignment of Relational Para-
phrases. In Proceedings of ACL, pages 2195–2204.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby: A large-scale unified
lexical-semantic resource based on LMF. In Pro-
ceedings of ACL, pages 580–590.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873–882.
Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Simi-
larity. In Proceedings of ACL, pages 95–105.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, and Christian Bizer. 2014. DB-
pedia - A Large-scale, Multilingual Knowledge Base
Extracted from Wikipedia. Semantic Web Journal,
pages 1–29.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No
Noun Phrase Left Behind: Detecting and Typing
Unlinkable Entities. In Proceedings of EMNLP-
CoNLL, pages 893–903.
Farzaneh Mahdisoltani, Joanna Biega, and Fabian M.
Suchanek. 2015. YAGO3: A Knowledge Base from
Multilingual Wikipedias. In CIDR.
Tomas Mikolov, Kal Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant Supervision for Relation Extrac-
tion Without Labeled Data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.
Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proceedings of
IJCAI, pages 2148–2154.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. TACL, 2:231–
244.
Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A Taxonomy
of Relational Patterns with Semantic Types. In
Proceedings of EMNLP-CoNLL, pages 1135–1145.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The Automatic Construction, Evaluation
and Application of a Wide-Coverage Multilingual
Semantic Network. Artificial Intelligence, 193:217–
250.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: Scalable Ma-
chine Learning for Linked Data. In Proceedings of
WWW, pages 271–280.
735
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling Relations and Their Mentions
Without Labeled Text. In Proceedings of ECML-
PKDD, pages 148–163.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In Proceedings of NAACL, pages 74–84.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with Neural
Tensor Networks for Knowledge Base Completion.
In Advances in NIPS, pages 926–934.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
Multi-label Learning for Relation Extraction. In
Proceedings of EMNLP-CoNLL, pages 455–465.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting Language
and Knowledge Bases with Embedding Models for
Relation Extraction. In Proceedings of EMNLP,
pages 1366–1371.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction using Wikipedia. In Proceedings of ACL,
pages 118–127.
736
