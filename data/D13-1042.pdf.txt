Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 436–446,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Joint Bootstrapping of Corpus Annotations and Entity Types
Hrushikesh Mohapatra Siddhanth Jain
IIT Bombay
Soumen Chakrabarti?
Abstract
Web search can be enhanced in powerful ways if to-
ken spans in Web text are annotated with disambiguated
entities from large catalogs like Freebase. Entity anno-
tators need to be trained on sample mention snippets.
Wikipedia entities and annotated pages offer high-quality
labeled data for training and evaluation. Unfortunately,
Wikipedia features only one-ninth the number of enti-
ties as Freebase, and these are a highly biased sample
of well-connected, frequently mentioned “head” entities.
To bring hope to “tail” entities, we broaden our goal to a
second task: assigning types to entities in Freebase but
not Wikipedia. The two tasks are synergistic: know-
ing the types of unfamiliar entities helps disambiguate
mentions, and words in mention contexts help assign
types to entities. We present TMI, a bipartite graphical
model for joint type-mention inference. TMI attempts
no schema integration or entity resolution, but exploits
the above-mentioned synergy. In experiments involving
780,000 people in Wikipedia, 2.3 million people in Free-
base, 700 million Web pages, and over 20 professional
editors, TMI shows considerable annotation accuracy im-
provement (e.g., 70%) compared to baselines (e.g., 46%),
especially for “tail” and emerging entities. We also com-
pare with Google’s recent annotations of the same corpus
with Freebase entities, and report considerable improve-
ments within the people domain.
1 Introduction
Thanks to automatic information extraction and se-
mantic Web efforts, keyword search over unstruc-
tured Web text is rapidly evolving toward entity-
and type-oriented queries (Guo et al., 2009; Pan-
tel et al., 2012) over semi-structured databases such
as Wikipedia, Freebase, and other forms of Linked
Data.
A key enabling component for such enhanced
search capability is a type and entity catalog. This
includes a directed acyclic graph of types under the
subTypeOf relation between types, and entities at-
tached to one or more types via instanceOf edges.
?soumen@cse.iitb.ac.in
YAGO (Suchanek et al., 2007) provides such a cat-
alog by unifying Wikipedia and WordNet, followed
by some cleanup.
Another enabling component is an annotated cor-
pus in which token spans (e.g., the word “Albert”)
are identified as a mention of an entity (e.g., the
Physicist Einstein). Equipped with suitable indices,
a catalog and an annotated corpus let us find “sci-
entists who played some musical instrument”, and
answer many other powerful classes of queries (Li
et al., 2010; Sawant and Chakrabarti, 2013).
Consequently, accurate corpus annotation has
been intensely investigated (Mihalcea and Csomai,
2007; Cucerzan, 2007; Milne and Witten, 2008;
Kulkarni et al., 2009; Han et al., 2011; Ratinov et
al., 2011; Hoffart et al., 2011). With two exceptions
(Zheng et al., 2012; Gabrilovich et al., 2013) that we
discuss later, public-domain corpus annotation work
has almost exclusively used Wikipedia and deriva-
tives, partly because Wikipedia provides not only a
standardized space of entities, but also reliably la-
beled mention text within its own documents, which
can be used to train machine learning algorithms for
entity disambiguation.
However, the high quality of Wikipedia comes
at the cost of low entity coverage (4.2 million)
and bias toward often-mentioned, richly-connected
“head” entities. Hereafter, Wikipedia entities are
called W . Freebase has fewer editorial controls, but
has at least nine times as many entities. This is par-
ticularly perceptible for people entities: one needs
to be relatively famous to be featured on Wikipedia,
but Freebase is less selective. Hereafter, Freebase
entities are called F .
As in any heavy-tailed distribution, even rela-
tively obsecure entities from F \W are collectively
mentioned a great many times on the Web, and in-
cluding them in Web annotation is critical, if entity-
oriented search is to impact the vast number of tail
436
queries submitted to Web search engines.
Primary goal — corpus annotation: We have
thus established a pressing need to bootstrap from a
small entity catalog W (such as Wikipedia entities),
and a small reference corpus CW (e.g., Wikipedia
text) reliably annotated with entities from W , to a
much larger catalog F (e.g., Freebase), and an open-
domain large payload corpus C (e.g., the Web).
We can and will use entities in F ? W 1 in the
bootstrapping process, but the real challenge is to
annotate C with mentions m of entities in F \W .
Unlike for F ?W , we have no training mentions for
F \W . Therefore, the main disambiguation signal
is from the immediate entity neighborhood N(e) of
the candidate entity e in the Freebase graph. I.e., if
m also reliably mentions some entity in N(e), then
e becomes a stronger candidate. Unfortunately, for
many “tail” entities e ? F \W , N(e) is sparse. Is
there hope for annotating the Web with tail entities?
Here, we achieve enhanced accuracy for the primary
annotation goal by extending it with a related sec-
ondary goal.
Secondary goal — entity typing: If we had avail-
able a suitable type catalog T with associated enti-
ties in W , which in turn have known textual men-
tions, we can build models of contexts referring to
types like chemists, sports people and politicans.
When faced with people called John Williams in
F \W , we may first try to associate them with types.
This can then help disambiguate mentions to specific
instances of John Williams in F \W . In principle,
useful information may also flow in the reverse di-
rection: words in mention contexts may help assign
types to entities in F \W . For reasons to be made
clear, we choose YAGO (Suchanek et al., 2007) as
the type catalog T accompanying entities in W .
Our contributions: We present TMI, a bootstrap-
ping system for solving the two tasks jointly. Apart
from matches between the context of m and entity
names in N(e), TMI combines and balances evi-
dence from two other sources to decide if e is men-
tioned at token span m, and has type t:
• a language model for the context in which enti-
ties of type t are usually mentioned
1With F=Freebase and W=Wikipedia, F ?W ?W but not
quite; W \ F is small but non-empty.
• correlations between t and certain path features
generated from N(e).
TMI uses a novel probabilistic graphical model for-
mulation to integrate these signals. We give a de-
tailed account of our design of node and edge po-
tentials, and a natural reject option (recall/precision
tradeoff).
We report on extensive experiments using YAGO
types, Wikipedia entities and text, Freebase en-
tities, and text from ClueWeb122, a 700-million-
page Web corpus. We focus on all people enti-
ties in Wikipedia and Freebase, and provide three
kinds of evaluation. First, we evaluate TMI on over
1100 entities in F ? W and 5500 snippets from
Wikipedia text, where it visibly improves upon base-
lines and a recently proposed alternative method
(Zheng et al., 2012). Second, we resort to exten-
sive manual evaluation of annotation on ClueWeb12
Web text with Freebase entities, by professional ed-
itors at a commercial search company. TMI again
clearly outperforms strong baselines, doing partic-
ularly well for nascent or tail entities. TMI im-
proves per-snippet accuracy, for some classes of
entities, from 46% to 70%, and pooled F1 score
from 66% to 73%. Third, we compare TMI an-
notations with Google’s FACC1 (Gabrilovich et al.,
2013) annotations restricted to people; TMI is sig-
nificantly better. Our annotations and related data
can be downloaded from http://www.cse.iit
b.ac.in/˜soumen/doc/CSAW/. To our knowl-
edge, this is among the first reports on extensive hu-
man evaluation of machine annotation for F \W on
a large Web corpus.
2 Related work
The vast majority of entity annotation work (Mi-
halcea and Csomai, 2007; Cucerzan, 2007; Milne
and Witten, 2008; Kulkarni et al., 2009; Han et al.,
2011; Ratinov et al., 2011; Hoffart et al., 2011) use
Wikipedia or derivative knowledge bases. (Ritter et
al., 2011) and (Zheng et al., 2012) are notable ex-
ceptions. (Ritter et al., 2011) use entity names for
distant supervision in POS tagging, chunking and
broad named entity typing in short tweets, which are
different from our goals.
Recently, others have investigated inferring types
2http://lemurproject.org/clueweb12/
437
Antony John
Williams
0g
gb
n
2k John
Williams
John 
Williams03
n
m
yf
z
0b
hb
qm
m
UK
St Aspah
chemist
academic
ChemSpider
type
type
founder
nationality
place of birth
Muskegon
Wisconsin
Madison
Athlete
American
football
player
place of birth
type
type
education education
??? type
Eunice 
Kanenstenhawi
Williams
children
… Antony John Williams, VP for 
Strategic Development and Head of 
the Cheminformatics group for the 
Royal Society of Chemistry has 
been honoured by Microsoft 
Research for his …
… to a 20-0 lead by the second 
quarter with running back John 
Williams’s 1 yard touchdown 
quarterback Neil O Donnell’s …
… Massachusetts, on 17 September 
1696, the daughter of Puritan 
minister Rev. John Williams and his 
wife Eunice Mather Williams …
ChemSpider, Chemistry
Steelers, Packers, touchdown, 
quarterback, Colts, quarter, yard, 
goal, Indianapolis, field, receiver
Massachusetts, Eunice Williams, 
minister, Puritan, kanenstenhawi, 
Rev.
Mention contexts:
Salient words from page containing contexts:
(a) (b) (c)
Harvard University
Fig. 1: Signal synergies. Three of the many people mentioned as “John Williams” on the Web are shown, with
Freebase MIDs. (a) Easy case where Freebase neighbors of 0ggbn2k match snippet and salient text, and type links are
also available. (b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippet
to 03nmvfz. (c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text,
which also match neighbors of 0bhbqmm.
of emerging entities (related to our secondary goal).
In concurrent work, (Nakashole et al., 2013) pro-
pose integer linear program formulations for infer-
ring types of emerging entities from the way their
mentions are embedded in curated relation-revealing
phrases. (Lin et al., 2012) earlier approached the
problem using weight propagation in a bipartite
graph connecting unknown to known entities via
textual relation patterns. Both note that this can
boost mention disambiguation accuracy.
The closest work to ours is by (Zheng et al.,
2012): they use semisupervised learning to anno-
tate a corpus with Freebase entities. Like (Milne
and Witten, 2008), they depend on unambiguous
entity-mention pairs to bootstrap a classifier, then
apply it to unlabeled, ambiguous mentions, creating
more training data. They use a per-type language
model like us (§3.3), but this is used as a secondary
cause for (word) feature generation, supplementing
and smoothing entity-specific language models. In
contrast, we use a rigorous graphical model to com-
bine new signals, not depending on naturally unam-
biguous mentions. Finally, in the interest of fully
automated evaluation, they limit their experiments
to F ?W and Wikipedia corpus, thus differing crit-
ically from our human evaluation on F and a Web
corpus.
(Gabrilovich et al., 2013) have recently released
FACC1: annotations of ClueWeb09 and ClueWeb12
with Freebase entities. Their algorithm is not yet
public. They report: “Due to the sheer size of the
data, it was not possible to verify all the automatic
annotations manually. Based on a small-scale hu-
man evaluation, the precision . . . is believed to be
around 80–85%. Estimating the recall is of course
difficult; however, it is believed to be around 70-
85%.” In §5, we will see that, for people entities,
TMI greatly increases recall beyond FACC1, keep-
ing precision unimpaired.
3 The three signals
Fig. 1 shows three Freebase entities mentioned as
“John Williams” in Web text, represented as nodes
with Freebase “MID”s e = 0ggbn2k, 03nmvfz,
and 0bhbqmm, embedded in their Freebase graph
neighborhoods. Owing to larger size and higher
flux, Freebase shows less editorial uniformity than
Wikipedia. This shows up in missing or non-
standard relation edges. Unlike YAGO, where
each entity is attached to one or more types, e =
0bhbqmm does not have a type link. Many people
have a link labeled profession, which is a second
438
kind of type link. Entities like e = 0bhbqmm also
have small, uninformative graph neighborhoods.
Also shown are three mention contexts, each rep-
resented by the snippet immediately surrounding the
mention, and salient words from the documents con-
taining each snippet. (Salient words may be ex-
tracted as words that contribute the largest compo-
nents to the document represented as a TFIDF vec-
tor.) (a) shows a favorable but relatively rare case
where e = 0ggbn2k has reliable type links. We can-
not assume there will be a 1-to-1 correspondence be-
tween Freebase and YAGO types, but in §3.2 we will
describe how to learn associations between Freebase
paths around entities and their YAGO types. The
snippet and salient words show reasonable overlap
with N(e). In §3.4 we will describe features that
characterize such overlap. In (b), e = 03nmvfz
is reliably typed, but there is no direct match be-
tween N(e) and the snippet. Nevertheless, the snip-
pet can be reliably annotated with 03nmvfz if we
can learn associations between types American foot-
ball player and Athlete (or their approximate YAGO
target types) and several context/salient words (see
§3.3). In (c), e = 0bhbqmm is not reliably typed.
However, there are matches between N(e) and con-
text/salient words. Once the snippet-to-entity asso-
ciation is established, it is easier to assign 0bhbqmm
to suitable types in YAGO.
In this section we will first describe our design of
the target type space, and then the tree signals that
will be used in our joint inference.
3.1 Designing the target type space
By typing two people called John Williams as ac-
tor and footballer, we may also disambiguate their
mentions accurately. Therefore, we need a well-
organized type space where the types
• collectively cover most entities of interest,
• offer reasonable type prediction accuracy, and
• can be selected algorithmically, for any do-
main.
Wikipedia and Freebase have many obscure types
like “people born in 1937” or “artists from On-
tario” which satisfy none of the above requirements.
YAGO, on the other hand, has a clean hierarchy of
over 200,000 types with broad coverage and fine dif-
ferentiation. Most entities in F \ W can be accu-
if t has < Nlow = 5000 member entities then
reject t from our type space
return
if t has > Nhigh = 25000 member entities then
for each immediate child t? ? t do
call ChooseTypes(t?)
else
accept t into our type space (but do not recurse)
Fig. 2: Procedure ChooseTypes(t).
rately attached to one or more YAGO types.
YAGO lists around 37,000 subtypes of person.
To satisfy the three requirements above, we called
ChooseTypes(person) (Fig. 2); this resulted in
130 suitable types being selected. These directly
covered 80% of Freebase people; the rest could
mostly be attached to slightly over-generic types
within our selection.
3.2 Predicting types from entity neighborhood
There will generally not be a simple mapping be-
tween Freebase and target types. E.g., entity e
may be known as a Mayor in Freebase, but the
closest YAGO type may be Politician. Edge and
node labels from the Freebase graph neighborhood
N(e) can embed many clues for assigning e a target
type. E.g., e = 03nmvfz may have an edge labeled
playedFor to a node representing the Wikipedia en-
tity Pittsburgh Steelers, which has a type
link to NFL team. This two-hop link label sequence
would repeat for a large number of players, and can
be used as a feature in a classifier.
0
0.2
0.4
0.6
0.8
1
1 2 3 4Hops
Re
ac
ha
bl
e
Whole
Prune1
Prune2
Prune3
Fig. 3: Freebase has small diameter despite graph thin-
ning. Prune1 removes paths from the whole Free-
base graph that pass through nodes /user/root and /com-
mon/topic, Prune2 also removes node /people/person,
and Prune3 removes several other high degree hubs.
Two further refinements are needed to make this
work. First, we have to collect path labels around
negative instances we well, and submit positive and
negative path labels to a binary classifier to can-
439
cel the effect of frequent but non-informative path
types. Second, indiscriminate expansion around e is
infeasible because the Freebase graph has very small
diameter. Even after substantial pruning, paths of
length 3 and 4 reach over 40% and 96% of all nodes
(Fig. 3). This increases computational burden and
floods us with noisy and spurious paths. We rem-
edy the problem using an idea from PathRank (Lao
and Cohen, 2010). Instead of trying to explore all
paths originating (or terminating) at e, where e may
or may not belong to a target type, we focus on paths
between e and other known members of the target
type.
3.3 Type “language model”
To exploit the second signal, shown in Fig. 1(b), we
need to model the association between target YAGO
types and the mention contexts of Wikipedia entities
known to belong to those types. This model compo-
nent is in the same spirit as (Zheng et al., 2012).
For each target YAGO type t, we sample positive
entities e ? F ?W , and for each e, we collect, from
Wikipedia annotated text, a corpus of snippets men-
tioning e. We remove the mention words and retain
the rest. We also collect salient words from the en-
tire Wikipedia document containing the snippet, as
shown in Fig. 1.
At this point each target type is associated with
a “corpus” of contexts, each represented by snippet
words. We compute the IDF of all words in this
corpus3, and then represent each type as a TFIDF
vector (Salton and McGill, 1983). A test context is
turned into a similar vector, and its score with re-
spect to t is the cosine between these two vectors.
This simple approach was found superior to building
a more traditional smoothed multinomial unigram
model (Zhai, 2008) for each type. Given the output
of this component feeds into an outer discriminative
inference mechanism, a strict probabilistic model is
not necessary.
3.4 Entity neighborhood match with snippet
The third signal is a staple of any disambiguation
work: match the occurrence context against the
neighborhood in the structured representation. In
word sense disambiguation (WSD), support for as-
signing a word in context to a synset comes from
3Generic IDF from Wiki text does not work.
matches between, say, other words in the context and
the WordNet neighborhood of the proposed synset.
As in WSD, many approaches to Wikification mea-
sure some local consistency between a mention m
and the neighborhood N(e) of a candidate entity e.
N(e) is again limited by a maximum path length `.
From snippetmwe extract all phrases P (m) exclud-
ing the mention words. For each phrase p ? P (m),
if p occurs at least once4 in any node of N(e), then
we accumulate a credit of |p|
?
w?p IDF(w), where
w ranges over words in p, IDF(w) is its inverse
document frequency (Salton and McGill, 1983) in
Wikipedia, and |p| is the length of the phrase. This
rewards exact phrase matches.
e
t t’
m2m1
e’
Mentions in context
Candidate
entities
Candidate types
Fig. 4: Tripartite assignment problem.
4 Unified model
Figure 4 abstracts out the signals shown in Figure 1
into a tripartite assignment problem. Each mention
likem1 has to choose at most one entity from among
candidate aliasing entities like e and e?. Each entity
e ? F \W has to choose one type (for simplicity we
ignore zero or more than one types as possibilities)
from candidates like t and t?.
The configuration of thick (green) edges should
be preferred to alternative dotted (red) edges under
these considerations:
• There is high local affinity or compatibility be-
tween e and t, based on associations between t
and N(e) as discussed in §3.2.
• There are better textual matches between N(e)
and m1, as compared to N(e?) and m1.
• In aggregate, the non-mention tokens in the
context of m1,m2 (shown as gray horizontal
lines) match well the language model associ-
ated with mentions of entities of type t (rather
than t?).
4Incorporating term frequency often polluted the score.
440
0…
0t1
0t0
…t2
? (t1)t1
? (t0)t0
…
 
? (e1)e1
? (e0)e0
…
 
? (e1)e1
? (e5)e5
Snippet node
potential
Snippet node
potential
Entity node
potential
Uniform node potential
for dummy entity
………
………
e1
e0
t0
0t0
0t0
Potential of edge
connecting snippet
to dummy entity 
node
………
…
e1
e0
0...
0t0
t0
Potential of edge
connecting snippet
node to entity node
Fig. 5: Illustration of the proposed bipartite graphical model, with tables for node and edge potentials and synthetic
?-entity nodes to implement the reject option.
We now present a unified model that combines the
signals and solves the two proposed tasks jointly.
We model two kinds of decision variables, which
will be represented by nodes in a graphical model.
Associated with each entity e ? F \ W there is a
hidden type variable (node) Te, which can take on a
value from (some subset of) the type catalog T . As-
sociated with each mention m (along with its snip-
pet context and all its observable features) there is
a hidden entity variable Em, which can take on val-
ues from some subset of entities. (For simplicity, we
assume that entities in F ?W have already been an-
notated in the corpus, and no m of interest mentions
such entities.)
We will model the probability of a joint assign-
ment of values to Te, Em as
log Pr(~t,~e) = ?
?
e
?e(te) + ?
?
m
?m(em)
+
?
e,m
?e,m(te, em)? const., (1)
where node log potentials are called ?e, ?m, edge
log potentials are called ?e,m, and ?, ? are tuned
constants. The log partition function, written
“const.” above, will not be of interest in infer-
ence, where we will seek to choose ~t,~e to maximize
Pr(~t,~e). In this section, we will design the node and
edge log potentials.
4.1 Node log potentials
Each node Te is associated with a node potential ta-
ble, mapping from possible types in Te to nonneg-
ative potentials. The potential values are supplied
from §3.2 as the classifier output scores.
Each node Em is associated with a node poten-
tial table, mapping from possible entities in Em to
nonnegative potentials. The potential values are sup-
plied from §3.4.
4.2 Edge log potentials
Suppose we assign Em = e, and Te = t. Then
we would like the non-mention context words of
m to be highly compatible with the type “language
model” developed in §3.3.
If e is among the set of values Em that Em can
take, then nodes Te and Em are joined by an edge.
This edge is associated with an edge potential table
?e,m : Te×Em ? R+. ?e,m(·, e?) will be set to zero
(cells shaded gray in Fig. 5) when e 6= e?. ?e,m(t, e)
is set to the cosine match score described in §3.3.
4.3 The reject (a.k.a. null, nil, NA, ?) option
An algorithm may reject many snippets, i.e., refrain
from annotating them. This could be because the
snippet mentions an entity outside F (and outside
W ), or the system wishes to ensure high precision at
some cost to recall.
Rejection is modeled by adding, for each snippet
m, a pseudo or “null” entity?m (also called “no an-
441
notation” NA, null or nil in the TAC-KBP5 commu-
nity). For simplicity, we assume that ?m and ?m?
are incomparable or distinct for m 6= m?. I.e., we
do not offer to cluster mentions of unknown enti-
ties. These remain separate, unconnected nodes in
the (augmented) Freebase graph.
If we choose Em = ?m, we get zero credit for
matching non-mention text inm toN(?m), because
N(?m) = ? and §3.4 has no information to con-
tribute. I.e., we set ?m(?m) = 0. In general,
?m(·) ? 0, so ?m gets the lowest possible credit
(but this will be modified shortly).
There is also a type variable T?m . To what type
should we assign “entity” ?m? Because ?m has no
connections in the Freebase graph, no hint can come
from §3.2. Put differently, ??m(t) will be constant
(say, zero) for all snippets m and types t.
Even if we do not know the entity mentioned in
m, the non-mention text in m will have differential
affinity to different types, obtained from §3.3. This
means that, if Em = ?m is chosen, T?m will be
argmaxt ??m,m(t,?m), which explains the non-
mention words in m using the best available lan-
guage model associated with some type. For a dif-
ferent entity Em = e0 and type Te0 = t assign-
ment to win, ??m(e0)+??e0(t)+?e0,m(t, e0) must
exceed the null score above. This provides a us-
able recall/precision handle: we modify ?m(?m) to
a tuned number; making it smaller generally gives
higher recall and lower precision.
4.4 Inference and training
The goal of collective inference will be to assign a
type value to each Te and an entity value to each
Em. We seek the maximum a-posteriori (MAP) la-
bel, for which we use tree-reweighted message pass-
ing (TRW-S) (Kolmogorov, 2006). Our graph has
plenty of bipartite cycles, so inference is approxi-
mate. Given the sparsity of data, we preferred to
delexicalize our objective (1), i.e., avoid word-level
features and pre-aggregate their signals via time-
tested aggregators (such as TFIDF cosine). As a re-
sult we have only two free parameters ?, ? in (1),
which we tune via grid search. A more principled
training regimen is left for future work.
5http://www.nist.gov/tac/2013/KBP/
5 Experiments
We focus our experiments on one broad type of en-
tities, people, that is more challenging for disam-
biguators than typical showcase examples of distin-
guishing (Steve) Jobs from employment and Apple
Inc. from fruit.
We report on three sets of experiments. In §5.1,
we restrict to entities from F ? W and Wikipedia
text, for which ground truth annotation is avail-
able. In §5.2, we evaluate TMI and baselines on
ClueWeb12 and entities from Freebase, not lim-
ited to Wikipedia. In §5.3, we compare TMI
with Google’s recently published FACC1 annota-
tions (Gabrilovich et al., 2013).
5.1 Reference corpus CW with F ?W entities
Limited to people, |F | = 2323792, |W | = 807381,
|F \ W | = 1544942, and |F ? W | = 778850. It
is easiest to evaluate TMI and others on Wikipedia
entities. They have known YAGO types. Wikipedia
text has explicit (disambiguated) entity annotations.
For these reasons, the few known systems for
Freebase-based annotation (Zheng et al., 2012) are
evaluated exclusively on F ?W .
5.1.1 Seeding and setup
People in F ?W are known by one or more men-
tion words/phrases. From these, we collect mention
phrases along with the candidate entity set for each
phrase. The number of candidates is the phrase’s de-
gree of ambiguity. We sort phrases by ambiguity and
draw a sample over the ambiguity range. This gives
us seed phrases with representative ambiguity. Then
we collect all entities mentioned by these phrases.
Overall we collect about 1100 entities and 5500 dis-
tinct mentions.
Contrast this with (Zheng et al., 2012), who sam-
ple entities from much fewer than 130, and largely
well-separated types: professional athletes, aca-
demics, actors, films, books, hotels, and tourist at-
tractions. If there were only two namesakes, an ac-
tor and a politician, the politician disappears, leav-
ing a naturally unambiguous alias. I.e., (Zheng et
al., 2012) did not “complete” their entity sets with
aliased entities. For all these reasons, Z0 numbers
here are not directly comparable to those in their pa-
per.
442
5.1.2 Tasks and baselines
The structure of TMI suggests two natural base-
lines to compare against it. TMI solves two tasks si-
multaneously: assign types to entities and entities to
snippets. So the first baseline, T0, is one that solves
the typing task separately, and the second, A0, does
snippet annotation separately. A third baseline, Z0,
from (Zheng et al., 2012) does only snippet annota-
tion; they do not consider typing entities.
While evaluating types output by TMI and T0
against ground truth, we may wish to assign partial
credit for overlapping types, e.g., athlete vs. soccer
player, because our types form an incomplete hierar-
chy. We use the standard “M&W” score of semantic
similarity between types (Milne and Witten, 2008)
for this.
As regards snippet annotation, Z0 (Zheng et al.,
2012) does not specify any mechanism for han-
dling ?. Therefore we run two sets of experiments.
In one we eliminate all snippets with ground truth
?. A0, and TMI are also debarred from returning ?
for any snippet. In the other, snippets marked ? in
ground truth are included. A0 and TMI are enabled
to return ?, but Z0 cannot.
TMI A0 Z0
0/1 snippet accuracy 0.827 0.699 0.627
Fig. 6: Snippet annotation onCW corpus, F ?W entities,
? not allowed.
TMI A0 Z0
0/1 snippet accuracy 0.7307 0.651 0.622
Snippet precision 0.858 0.843 0.622
Snippet recall 0.777 0.692 0.639
Snippet F1 0.815 0.760 0.630
Fig. 7: Snippet annotation onCW corpus, F ?W entities,
? allowed.
5.1.3 Snippet annotation results
Fig. 6 shows snippet annotation accuracy (frac-
tion of snippets labeled with the correct entity) when
? is not allowed as an entity. As two uninformed
refernces, uniform random choice gives an accuracy
of 0.423 and choosing the entity with the largest
prior gives an accuracy of 0.767. TMI is consid-
erably better than A0, which is better than Z0 and
the uninformed references. This is despite training
Z0’s per-type topic models not only on unambiguous
0.
73
0.
93
0.
85
0.
52
0.
74 0.
81
0.
42
0.
75 0
.8
5
0
0.2
0.4
0.6
0.8
1
0…19 20…39 >=40Degree-->
Ac
cu
ra
cy
-->
TMI A0 Z0
0.
73
0.
98
0.
38
0.
75
0.
24
0.
34
0.
18
0.
98
0.
13
0
0.5
0.4
0.7
0.2
6
0…69 50…89 >=40Degree-->
Ac
cu
ea
yT
6-
->
MIZ ? ?
Fig. 8: Bucketed comparison between TMI and baselines,
F ?W , ? allowed.
TMI T0
0/1 type accuracy 0.80 0.81
M&W type accuracy 0.82 0.83
Fig. 9: Type inference, CW corpus, F ?W entities.
snippets, but also on a disjoint fraction ofF?W , as a
surrogate for Wikipedia’s containment in Freebase.
Fig. 7 repeats the experiment while allowing ?.
Here, in 0/1 accuracy, ? is regarded as just another
entity. Again, we see that TMI has a clear advantage.
Z0’s performance here is worse than in (Zheng et
al., 2012). This is explained by our much larger and
difficult-to-separate type system.
We disaggregate the summary results into buck-
ets, shown in Fig. 8. Each bucket covers a range of
degrees of entity nodes in Freebase, while roughly
balacing the number of snippets in each bucket. TMI
generally shows larger gains for low-degree buckets.
5.1.4 Type prediction results
We also compared the type inference accuracy of
TMI and T0; (Zheng et al., 2012) do not infer types.
The summary is in Fig. 9. Two uninformed baselines
are worth mentioning. Uniform random choice over
130 types gave only 2% accuracy. Chossing the type
with largest prior probability gave 28.2% accuracy.
TMI is much better, but offers no significant ben-
efit (or degradation) compared to T0. We verified,
partly by way of debugging, that there do exist enti-
ties e with small degree but a modest number of as-
signed snippets, for which snippet-to-N(e) matches
443
angus mcdonald, chris robinson, christopher henry, elizabeth
cameron, george woods , henry barnes, jack scott, jeremy
robert, john sherman, leonard thomas, marc anthony, mitchell
donald, morrison mark, parker edward, richard andrew , simon
scott, stephen ross, stuart baron, tom clark, whitney john, austin
scott, barbara johnson, brian peterson, carlos rivero, david
berman, david johns, donald fraser, george davies, george fisher,
graham smith, john pepper, jonathan edwards, kevin brown,
kevin hughes, matt johnson, michael davidson, nancy johnson,
paul holmes, pedro martins, peter frank, peter mitchell, peter
mullen, robert stern, roger edwards, stuart walker, terry evans,
tony angelo, tony ward, william jarvis, william sampson
Fig. 10: Seed mentions for confusion clusters for Web
corpus C and entities in F .
provide a boost to type prediction accuracy (about
50%), as compared to T0 (about 20% for these in-
stances). Therefore, the flow of information between
type and entity assignments is, in principle, bidirec-
tional, in the regime of such entities.
5.2 Payload corpus C with entities in F
Recall our main goal is to annotate payload cor-
pus C with entities in all of F . Experience with
F ?W and CW may not be representative of F and
C. Entities in F \W may not come with reference
mentions, and their type and entity neighborhoods
may be sparse. Furthermore, compared to the closed
world of F ?W , evaluating TMI and baselines over
C and F \ W is challenging. Entities in F \ W
do not have ground truth types in the type catalog
T (here, YAGO), nor snippets labeled by humans as
mentioning them. Therefore, we need human edi-
torial judgment, which is scarce. Even though TMI
can be applied at Web scale, the scale if evaluation
is limited by editorial input.
5.2.1 Seeding and data setup
There are about 2.3 million Freebase entities con-
nected to /people/person via type links. Similar to
§5.1.1, we chose phrases (Fig. 10) with diverse de-
gree of ambiguity (Fig. 11), to seed confusion clus-
ters. Then we completed the clusters by including
aliased entities, as before, so as not to artifically re-
duce the degree of ambiguity. Note that entities in
W can and do contend with entities in F \W . The
cluster size distribution is shown in Fig. 12. Limited
by editorial budget, we finished with 634 entities,
238 distinct aliases and 4,500 snippets.
We used the 700-million-page ClueWeb12 Web
corpus. All phrases in the expanded clusters are
0
.0
70
30
90
85
24
85
16
…
>=
54
D5
e…
g1
r4
2-
g
A2
c1
ua
y4
D
ua
y4
D1
A2
c
-=
…
Ag
5T
1M
2=
g
I5
A5
41
c…
Au
=5
aa
A=
2c
y>
1a
52
gy
4Z
u=
4…
>1
42
r…
g>
2g
5Z
-y
4Z
1I
y4
D5
4
>u
2A
A1
y?
>A
…
g
-y
4Z
1A
2g
T
c…
u=
y5
a1
Zy
e…
Z>
2g
5e
yg
>1
A5
44
T
42
85
41
5Z
-y
4Z
>
uy
4a
2>
14
…
e5
42
A=
2c
y>
1u
1a
52
gy
4Z
5Z
-y
4Z
1=
y4
I5
41
Iy
4D
54
y?
>A
…
g1
-y
D5
cy
g1
>u
2A
A
e…
e…
yg
1a
52
gy
4Z
1A
=2
cy
>
>A
?y
4A
1=
1-
ya
D5
4
>u
45
yc
…
g1
>u
2A
A1
>…
c2
g
42
r5
4A
1y
4A
=?
41
c2
4A
2g
I5
A5
41
Z1
c…
Au
=5
aa
gy
gu
T1
M
2=
g>
2g
1>
45
r4
2
cy
4…
gZ
y1
gy
gu
T1
M
2=
g>
2g
cy
4u
1y
gA
=2
gT
1Z
yg
?y
D5
e…
g1
M
1r
42
-g
M
2=
g1
-…
aa
…
yc
1>
=5
4c
yg
M
2=
g1
>=
54
cy
g1
M
4
M
2=
g1
=5
g4
T1
ry
4g
5>
M
2=
g1
51
I5
II5
4
M
54
5c
T1
42
r5
4A
1M
2=
g>
2g
M
yu
D1
Z5
gA
2g
1>
u2
AA
=5
g4
T1
u=
4…
>A
2I
=5
4
84
y=
yc
15
1>
c…
A=
85
24
85
1-
5>
u2
AA
16
…
>=
54
85
24
85
1a
5c
?5
a1
-2
2Z
>
85
24
85
1Z
1-
22
Z>
85
24
85
1r
yr
T1
-2
2Z
>
64
yg
D1
I5
A5
41
a5
=c
yg
g
5Z
-y
4Z
1M
2=
g1
ua
…
6A
2g
Z2
gy
aZ
1c
16
4y
>5
4
Z2
gy
aZ
1Z
1c
…
Au
=5
aa
Zy
4…
5g
18
4y
=y
c1
>c
…
A=
u=
y4
a5
>1
-…
aa
…
yc
1M
y4
e…
>
ry
4r
y4
y1
a1
M
2=
g>
2g
y4
A=
?4
1-
…
aa
…
yc
1>
yc
I>
2g
?gA…2g>?
cr
…
8?
…
AT
??
? 0
.0
70
30
90
??
g…
II5
A>
1?
.0
00
??
??
?
?g…II5A>
?r…8?AT
Fig. 11: Ambiguity distribution for Web corpus C. Un-
ambiguous names are usually fully expanded and very
rare, if at all present, in evaluated snippets.
0
.0
70
30
90
. 8 .. .8 7. 78 3. 38 9. 98
52416…>6=Degr-Accu
ay
4r
Tr
T-
gc
cu
Fig. 12: Confusion cluster size distribution for Web cor-
pus C.
loaded into a trie used by a map-reduce job to ex-
tract documents, then snippets, from the corpus.
Some phrases in Figs. 10 and 11 have overwhelm-
ing numbers of pages with matches. In produc-
tion, we naturally want all of them to be annotated.
But human editorial judgment being the bottleneck,
we sampled 50% or 50,000 snippets, whichever was
smaller. Starting with about 752,450 pages, we ran
the Stanford NER (Finkel et al., 2005) to mark per-
son spans. Pages with fewer than five non-person to-
kens per person were discarded; this effectively dis-
carded long list pages without any informative text
to disambiguate anyone, and left us with 574,135
pages. From these we collected 304,309 snippets
where the mention phrase is marked by the NER as
a person. Each seed phrase leads to one cluster on
which TMI and A0 are run. Note that ? must be
allowed on the open Web.
5.2.2 Editorial judgment
Finally, for each algorithm, about 634 entity-type
and about 4500 snippet-entity assignments are ran-
domly sampled and sent to 20 editors in a commer-
cial search engine company, who judged each as-
signment as correct or incorrect, without knowing
which algorithm produced the annotation, to avoid
444
TMI A0
entity
0/1 accuracy .714 .562
Pooled recall .764 .869
Precision .714 .562
F1 .738 .683
e
?
W
(0
/1
ac
c.
)
e
?
F
\
W
(0
/1
)
TMI .75 .62
A0 .65 .42
Fig. 13: Snippet summary for F and payload corpus C.
bias. Because the editors are trained professionals
(unlike Mechanical Turks), we increased our evalu-
ation coverage by having each type or entity assign-
ment reviewed by one editor.
Pooling: Ideally, editors can be asked to find the
best type or entity for each entity or snippet, but,
given the size and diversity of Freebase, the cogni-
tive burden would be unacceptable. In the Wikipedia
corpus CW , a snippet marked? (no entity) by an al-
gorithm can be judged a loss of recall if Wikipedia
ground truth annotates it with an entity. Unfortu-
nately, this is no longer practical for Web corpus C,
because 8,217 snippets marked ? would have to be
manually inspected and compared with a large num-
ber of candidate entities in Freebase. Therefore, we
adopt pooling as in TREC. (Although the pool is
small, A0 has very high recall.) Recall is evaluated
with respect to the union of snippets annoted with a
non-? entity by at least one competing algorithm,
with agreement in case of more than one.
0/1 Type accuracy: Editors judged each pro-
posed type as correct or not. Unlike in §5.1, where
the true and proposed types could be compared via
M&W (Milne and Witten, 2008), they could not be
asked to objectively estimate relatedness between
types. Therefore we present only their reported post-
hoc 0/1 accuracy for types: T0 and TMI have 0/1
type accuracy of 0.828 and 0.818.
5.2.3 Snippet annotation results
Given the large gap between TMI and Z0 in the
easier setup in §5.1, we no longer consider Z0, and
instead focus on TMI vs. A0. The summary com-
parison of A0 vs. TMI is shown in Fig. 13. Here
TMI’s absolute gains in 0/1 accuracy and F1 are
even larger than in §5.1. To understand TMI’s per-
formance across a diversity of Freebase entity nodes
e, as a function of 1. the size and richness of N(e),
and 2. the number of snippets claimed to mention e,
we disaggregate the data of Fig. 13 into buckets of
0.73
0.98 0.90 0.93
0.52 0.84 0.57
0.75
0.4
0.5
0.8
0.7
0.9
0.1
063 467 962 …>=0Degree--…
Ac
cu
ra
cy
--… TMI
A0
0.73
0.98
0.75
0.92
0.42
0.77
0.71
0.94
0.4
0.7
0.9
0.5
063 867 962 …>10=DegDDrr…
-A
Ac
Du
ay
1r
r… TMI
Z0
Fig. 14: 0/1 accuracy and F1 for snippets, payload corpus
C and entities in F .
Snippet label judgements %
TMI ok, FACC1 ok, neither ? 22
TMI ok, FACC1 wrong, neither ? 6
TMI6= ? ok, FACC1=? wrong 40
TMI6= ? wrong, FACC1=? 23
TMI wrong, FACC1 wrong, neither ? 2
TMI= ? wrong, FACC16= ? correct 4
TMI= ?, FACC16= ?, wrong 3
(TMI= ?, FACC1= ?, not judged) -
Fig. 15: TMI vs. FACC1 comparison.
consecutive degrees, roughly balancing the number
of snippets per bucket, as shown in Fig. 14. At the
very low end of almost disconnected entity nodes,
no algorithm does very well, because these entities
are also hardly ever mentioned. When the entity is
popular and well-connected, TMI’s benefits are rela-
tively modest. TMI’s gains are best in the mid-range
of degrees. The gap narrows for large-degree nodes,
which is expected.
5.3 Comparison with FACC1
After collecting our pool of snippets as in §5.2.2,
we consulted FACC1 (Gabrilovich et al., 2013), and
passed on FACC1 annotations to our editors. As be-
fore, the identity of the algorithm was concealed.
Results are shown in Fig. 15. In a large 40% of
cases, TMI labels correctly while FACC1 backs off.
The converse, where FACC1 backs off and TMI
makes a mistake, is about half as frequent. These
preliminary numbers suggest that TMI is able to
push recall beyond FACC1 while also giving better
precision.
445
6 Conclusion
We presented a formal model for bootstrapping from
YAGO types and entities annotated in Wikipedia to
two tasks, 1. annotating Web snippets with Freebase
entities, and 2. associating Freebase entities with
YAGO types. We presented TMI, a system to solve
the two tasks jointly. Experiments show that TMI’s
snippet annotation accuracy, especially for relatively
weakly-connected Freebase entities, is superior to
baselines. We aim to extend from people to all major
Freebase categories, and larger Web crawls.
Acknowledgment: We are grateful to Shrikant
Naidu, Muthusamy Chelliah, and the editors from
Yahoo! for their generous support. Shashank Gupta
helped process FACC1 data.
References
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In EMNLP Con-
ference, pages 708–716.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In ACL Conference, pages 363–370.
Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag
Subramanya. 2013. FACC1: Freebase annotation
of ClueWeb corpora. http://lemurproject.or
g/clueweb12/, June. Version 1 (Release date 2013-
06-26, Format version 1, Correction level 0).
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR Confer-
ence, pages 267–274. ACM.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in Web text: A graph-based method. In
SIGIR Conference, pages 765–774.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Fu¨rstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum.
2011. Robust disambiguation of named entities in text.
In EMNLP Conference, pages 782–792, Edinburgh,
Scotland, UK, July. SIGDAT.
Vladimir Kolmogorov. 2006. Convergent tree-
reweighted message passing for energy minimization.
IEEE PAMI, 28(10):1568–1583, October.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In SIGKDD Confer-
ence, pages 457–466.
Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine Learning, 81(1):53–67, October.
Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyEngine: Answering entity-relationship queries using
shallow semantics. In CIKM, October. (demo).
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: detecting and typing unlinkable en-
tities. In EMNLP Conference, pages 893–903.
R Mihalcea and A Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM, pages
233–242.
David Milne and Ian H Witten. 2008. Learning to link
with Wikipedia. In CIKM, pages 509–518.
Ndapandula Nakashole, Tomasz Tylenda, and Gerhard
Weikum. 2013. Fine-grained semantic typing of
emerging entities. In ACL Conference.
Patrick Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent
modeling. In ACL Conference, pages 563–571, Jeju
Island, Korea, July.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for
disambiguation to Wikipedia. In ACL Conference,
ACL/HLT, pages 1375–1384, Portland, Oregon.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In EMNLP Conference, pages 1524–
1534, Edinburgh, UK. ACL.
G Salton and M J McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing joint query interpretation and response ranking. In
WWW Conference, Brazil.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In WWWCon-
ference, pages 697–706. ACM Press.
ChengXiang Zhai. 2008. Statistical language models
for information retrieval: A critical review. Founda-
tions and Trends in Information Retrieval, 2(3):137–
213, March.
Zhicheng Zheng, Xiance Si, Fangtao Li, Edward Y.
Chang, and Xiaoyan Zhu. 2012. Entity disambigua-
tion with Freebase. In Web Intelligence Conference,
WI-IAT ’12, pages 82–89.
446
