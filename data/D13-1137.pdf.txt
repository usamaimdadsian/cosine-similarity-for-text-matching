Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Simple Customization of Recursive Neural Networks
for Semantic Relation Classification
Kazuma Hashimoto†, Makoto Miwa††, Yoshimasa Tsuruoka†, and Takashi Chikayama†
†The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp
††The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Abstract
In this paper, we present a recursive neural
network (RNN) model that works on a syn-
tactic tree. Our model differs from previous
RNN models in that the model allows for an
explicit weighting of important phrases for the
target task. We also propose to average param-
eters in training. Our experimental results on
semantic relation classification show that both
phrase categories and task-specific weighting
significantly improve the prediction accuracy
of the model. We also show that averaging the
model parameters is effective in stabilizing the
learning and improves generalization capacity.
The proposed model marks scores competitive
with state-of-the-art RNN-based models.
1 Introduction
Recursive Neural Network (RNN) models are
promising deep learning models which have been
applied to a variety of natural language processing
(NLP) tasks, such as sentiment classification, com-
pound similarity, relation classification and syntactic
parsing (Hermann and Blunsom, 2013; Socher et al.,
2012; Socher et al., 2013). RNN models can repre-
sent phrases of arbitrary length in a vector space of
a fixed dimension. Most of them use minimal syn-
tactic information (Socher et al., 2012).
Recently, Hermann and Blunsom (2013) pro-
posed a method for leveraging syntactic informa-
tion, namely CCG combinatory operators, to guide
composition of phrases in RNN models. While their
models were successfully applied to binary senti-
ment classification and compound similarity tasks,
there are questions yet to be answered, e.g., whether
such enhancement is beneficial in other NLP tasks
as well, and whether a similar improvement can
be achieved by using syntactic information of more
commonly available types such as phrase categories
and syntactic heads.
In this paper, we present a supervised RNN model
for a semantic relation classification task. Our model
is different from existing RNN models in that impor-
tant phrases can be explicitly weighted for the task.
Syntactic information used in our model includes
part-of-speech (POS) tags, phrase categories and
syntactic heads. POS tags are used to assign vec-
tor representations to word-POS pairs. Phrase cate-
gories are used to determine which weight matrices
are chosen to combine phrases. Syntactic heads are
used to determine which phrase is weighted during
combining phrases. To incorporate task-specific in-
formation, phrases on the path between entity pairs
are further weighted.
The second contribution of our work is the intro-
duction of parameter averaging into RNN models.
In our preliminary experiments, we observed that
the prediction performance of the model often fluc-
tuates significantly between training iterations. This
fluctuation not only leads to unstable performance
of the resulting models, but also makes it difficult to
fine-tune the hyperparameters of the model. Inspired
by Swersky et al. (2010), we propose to average the
model parameters in the course of training. A re-
cent technique for deep learning models of similar
vein is dropout (Hinton et al., 2012), but averaging
is simpler to implement.
Our experimental results show that our model per-
1372
Figure 1: A recursive representations of a phrase “a
word vector” with POS tags of the words (DT, NN and
NN respectively). For example, the two word-POS pairs
“word NN” and “vector NN” with a syntactic category
N are combined to represent the phrase “word vector”.
forms better than standard RNN models. By av-
eraging the model parameters, our model achieves
performance competitive with the MV-RNN model
in Socher et al. (2012), without using computation-
ally expensive word-dependent matrices.
2 An Averaged RNN Model with Syntax
Our model is a supervised RNN that works on a bi-
nary syntactic tree. As our first step to leverage in-
formation available in the tree, we distinguish words
with the same spelling but POS tags in the vector
space. Our model also uses different weight ma-
trices dependent on the phrase categories of child
nodes (phrases or words) in combining phrases. Our
model further weights those nodes that appear to be
important.
Compositional functions of our model follow
those of the SU-RNN model in Socher et al. (2013).
2.1 Word-POS Vector Representations
Our model simply assigns vector representations to
word-POS pairs. For example, a word “caused”
can be represented in two ways: “caused VBD” and
“caused VBN”. The vectors are represented as col-
umn vectors in a matrix We ? Rd×|V|, where d is
the dimension of a vector and V is a set of all word-
POS pairs we consider.
2.2 Compositional Functions with Syntax
In construction of parse trees, we associate each of
the tree node with its d-dimensional vector represen-
tation computed from vector representations of its
subtrees. For leaf nodes, we look up word-POS vec-
tor representations in V. Figure 1 shows an example
of such recursive representations. A parent vector
p ? Rd×1 is computed from its direct child vectors
cl and cr? Rd×1:
p = tanh(?lW
Tcl ,Tcr
l cl+?rW
Tcl ,Tcrr cr+bTcl ,Tcr ),
where W Tcl ,Tcrl and W
Tcl ,Tcrr ? Rd×d are weight
matrices that depend on the phrase categories of cl
and cr. Here, cl and cr have phrase categories Tcl
and Tcr respectively (such as N, V, etc.). bTcl ,Tcr ?
Rd×1 is a bias vector. To incorporate the impor-
tance of phrases into the model, two subtrees of a
node may have different weights ?l ? [0, 1] and
?r(= 1 ? ?l), taking phrase importance into ac-
count. The value of ?l is manually specified and
automatically applied to all nodes based on prior
knowledge about the task. In this way, we can com-
pute vector representations for phrases of arbitrary
length. We denote a set of such matrices as Wlr and
bias vectors as b.
2.3 Objective Function and Learning
As with other RNN models, we add on the top of a
node x a softmax classifier. The classifier is used to
predict a K-class distribution d(x) ? RK×1 over a
specific task to train our model:
d(x) = softmax(W labelx+ blabel), (1)
where W label ? RK×d is a weight matrix and
blabel ? RK×1 is a bias vector. We denote t(x) ?
RK×1 as the target distribution vector at node x.
t(x) has a 0-1 encoding: the entry at the correct la-
bel of t(x) is 1, and the remaining entries are 0. We
then compute the cross entropy error between d(x)
and t(x):
E(x) = ?
K
?
k=1
tk(x)logdk(x),
and define an objective function as the sum of E(x)
over all training data:
J(?) =
?
x
E(x) + ?
2
???2,
where ? = (We,Wlr, b,W label, blabel) is the set of
our model parameters that should be learned. ? is a
vector of regularization parameters.
1373
To compute d(x), we can directly leverage any
other nodes’ feature vectors in the same tree. We
denote such additional feature vectors as x?i ? Rd×1,
and extend Eq. (1):
d(x) = softmax(W labelx+
?
i
W addi x?i +blabel),
where W addi ? RK×d are weight matrices for addi-
tional features. We denote these matrices W addi as
W add. We also add W add to ?:
? = (We,Wlr, b,W label,W add, blabel).
The gradient of J(?)
?J(?)
??
=
?
x
?E(x)
??
+ ??
is efficiently computed via backpropagation through
structure (Goller and Ku¨chler, 1996). To minimize
J(?), we use batch L-BFGS1 (Hermann and Blun-
som, 2013; Socher et al., 2012).
2.4 Averaging
We use averaged model parameters
? = 1
T + 1
T
?
t=0
?t
at test time, where ?t is the vector of model parame-
ters after t iterations of the L-BFGS optimization.
Our preliminary experimental results suggest that
averaging ? except We works well.
3 Experimental Settings
We used the Enju parser (Miyao and Tsujii, 2008)
for syntactic parsing. We used 13 phrase categories
given by Enju.
3.1 Task: Semantic Relation Classification
We evaluated our model on a semantic relation clas-
sification task: SemEval 2010 Task 8 (Hendrickx et
al., 2010). Following Socher et al. (2012), we re-
garded the task as a 19-class classification problem.
There are 8,000 samples for training, and 2,717 for
1We used libLBFGS provided at http://www.
chokkan.org/software/liblbfgs/.
Figure 2: Classifying the relation between two entities.
test. For the validation set, we randomly sampled
2,182 samples from the training data.
To predict a class label, we first find the minimal
phrase that covers the target entities and then use the
vector representation of the phrase (Figure 2).
As explained in Section 2.3, we can directly con-
nect features on any other nodes to the softmax clas-
sifier. In this work, we used three such internal fea-
tures: two vector representations of target entities
and one averaged vector representation of words be-
tween the entities2.
3.2 Weights on Phrases
We tuned the weight ?l (or ?r) introduced in Sec-
tion 2.2 for this particular task. There are two fac-
tors: syntactic heads and syntactic path between tar-
get entities. Our model puts a weight ? ? [0.5, 1]
on head phrases, and 1 ? ? on the others. For re-
lation classification tasks, syntactic paths between
target entities are important (Zhang et al., 2006), so
our model also puts another weight ? ? [0.5, 1] on
phrases on the path, and 1 ? ? on the others. When
both child nodes are on the path or neither of them
on the path, we set ? = 0.5. The two weight fac-
tors are summed up and divided by 2 to be the final
weights ?l and ?r to combine the phrases. For ex-
ample, we set ?l = (1??)+?2 and ?r =
?+(1??)
2
when the right child node is the head and the left
child node is on the path.
3.3 Initialization of Model Parameters and
Tuning of Hyperparameters
We initialized We with 50-dimensional word vec-
tors3 trained with the model of Collobert et
2Socher et al. (2012) used richer features including words
around entity pairs in their implementation.
3The word vectors are provided at http://ronan.
collobert.com/senna/. We used the vectors without any
modifications such as normalization.
1374
Method F1 (%)
Our model 79.4
RNN 74.8
MV-RNN 79.1
RNN w/ POS, WordNet, NER 77.6
MV-RNN w/ POS, WordNet, NER 82.4
SVM w/ bag-of-words 73.1
SVM w/ lexical and semantic features 82.2
Table 1: Comparison of our model with other methods on
SemEval 2010 task 8.
Method F1 (%)
Our model 79.4
Our model w/o phrase categories (PC) 77.7
Our model w/o head weights (HW) 78.8
Our model w/o path weights (PW) 78.7
Our model w/o averaging (AVE) 76.9
Our model w/o PC, HW, PW, AVE 74.1
Table 2: Contributions of syntactic and task-specific in-
formation and averaging.
al. (2011), and Wlr with I2 + ?, where I ? Rd×d
is an identity matrix. Here, ? is zero-mean gaussian
random variable with a variance of 0.01. The ini-
tialization of Wlr is the same as that of Socher et
al. (2013). The remaining model parameters were
initialized with 0.
We tuned hyperparameters in our model using the
validation set for each experimental setting. The hy-
perparameters include the regularization parameters
for We, Wlr, W label and W add, and the weights ?
and ?. For example, the best performance for our
model with all the proposed methods was obtained
with the values: 10?6, 10?4, 10?3, 10?3, 0.7 and
0.9 respectively.
4 Results and Discussion
Table 1 shows the performance of our model and that
of previously reported systems on the test set. The
performance of an SVM system with bag-of-words
features was reported in Rink and Harabagiu (2010),
and the performance of the RNN and MV-RNN
models was reported in Socher et al. (2012). Our
model achieves an F1 score of 79.4% and outper-
forms the RNN model (74.8% F1) as well as the
simple SVM-based system (73.1% F1). More no-
Figure 3: F1 vs Training iterations.
tably, the score of our model is competitive with that
of the MV-RNN model (79.1% F1), which is com-
putationally much more expensive. Readers are re-
ferred to Hermann and Blunsom (2013) for the dis-
cussion about the computational complexity of the
MV-RNN model. We improved the performance of
RNN models on the task without much increasing
the complexity. This is a significant practical advan-
tage of our model, although its expressive power is
not the same as that of the MV-RNN model.
Our model outperforms the RNN model with one
lexical and two semantic external features: POS
tags, WordNet hypernyms and named entity tags
(NER) of target word pairs (external features). The
MV-RNN model with external features shows bet-
ter performance than our model. An SVM with rich
lexical and semantic features (Rink and Harabagiu,
2010) also outperforms ours. Note, however, that
this is not a fair comparison because those mod-
els use rich external resources such as WordNet and
named entity tags.
4.1 Contributions of Proposed Methods
We conducted additional experiments to quantify the
contributions of phrase categories, heads, paths and
averaging to our classification score. As shown in
Table 2, our model without phrase categories, heads
or paths still outperforms the RNN model with ex-
ternal features. On the other hand, our model with-
out averaging yields a lower score than the RNN
model with external features, though it is still bet-
1375
ter than the RNN model alone. Without utiliz-
ing these four properties, our model obtained only
74.1% F1. These results indicate that syntactic and
task-specific information and averaging contribute
to the performance improvement. The improvement
is achieved by a simple modification of composi-
tional functions in RNN models.
4.2 Effects of Averaging in Training
Figure 3 shows the training curves in terms of F1
scores. These curves clearly demonstrate that pa-
rameter averaging helps to stabilize the learning and
improve generalization capacity.
5 Conclusion
We have presented an averaged RNN model for se-
mantic relation classification. Our experimental re-
sults show that syntactic information such as phrase
categories and heads improves the performance, and
the task-specific weighting is also beneficial. The
results also demonstrate that averaging the model
parameters not only stabilizes the learning but also
improves the generalization capacity of the model.
As future work, we plan to combine deep learning
models with richer information such as predicate-
argument structures.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful comments.
References
Ronan Collobert, Jason Weston, Le´on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
In JMLR, 12:2493–2537.
Christoph Goller and Andreas Ku¨chler. 1996. Learning
Task-Dependent Distributed Representations by Back-
propagation Through Structure. In ICNN.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ´O Se´aghdha, Sebastian Pado´, Marco
Pennacchiotti, Lorenza Romano and Stan Szpakowicz.
2010. SemEval-2010 Task 8: Multi-Way Classication
of Semantic Relations Between Pairs of Nominals. In
SemEval 2010.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional Se-
mantics. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever and Ruslan R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. In arXiv:1207.0580.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, 34(1):35–80, MIT Press.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical and
Semantic Resources. In SemEval 2010.
Richard Socher, Brody Huval, Christopher D. Manning
and Andrew Y. Ng. 2012. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning and
Andrew Y. Ng. 2013. Parsing with Compositional
Vector Grammars. In ACL.
Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-
itas. 2010. A tutorial on stochastic approximation al-
gorithms for training Restricted Boltzmann Machines
and Deep Belief Nets. In ITA workshop.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006.
A Composite Kernel to Extract Relations between En-
tities with Both Flat and Structured Features. In COL-
ING/ACL.
1376
