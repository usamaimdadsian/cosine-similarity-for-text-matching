EXPLORER: A Natural Language Processing 
System for Oil Exploration 
Wendy G. Lehnert  
Department of Computer and In fo rmat ion  Sc iences 
Graduate Research Center 
University of Massachusetts 
Amherst ,  Mass. 01003 
Steven P. Shwartz 
Cogn i t ive  Systems Inc .  
234 Church St reet  
New Haven, Ct.  06510 
EXPLORER (Lehnert  and Sbwartz ,  1982; 
Shwartz 1982) is  a non- f rag i le ,  'bands -on"  
language ana lys i s  system that  a l lows o i l  
exp lora t ion is ts  w i th  no knowledge of computers 
or  computer progra- . - ing  to c reate  customized 
maps. Users in Tu lsa ,  Denver, and New Or leans 
cur rent ly  have d ia l -up  access  to a DEC-20 
where EXPLORER is  implemented in TLISP. A 
user  converses  wi th  EXI~LORER about • des i red  
map unt i l  both par t ies  have agreed on an 
adequate and unambiguous set  of 
specifications. Another phone link then 
carries EXPLORER's output to an IBM 3033 which 
runs database re t r ieva l  rout ines  on co~aerc ie l  
wel l  data .  When a l l  the in fo rmat ion  has been 
secured from wel l  data ,  a g raph ics  system 
takes  over to perform the actua l  map 
generat ion .  EXPLORER is  cur rent ly  undergo ing 
eva luat ion ,  and i t  is  ta rget ted  for  a 1983 
i ns ta l la t ion  in a l l  reg iona l  o f f i ces  of a 
major o i l  company* for  res t r i c ted  use by 
geo log is ts  and geophys ic i s ts .  
Since our intended user  popu la t ion  is  
naive about computers ,  EXPLORER's in teract ive  
des ign is  dominated by "user-friendly" 
features. EXPLORER processes information 
re t r ieva l  requests  s ta ted  in Eng l i sh ,  w i thout  
imposing vocabu lary  l im i ta t ions  or syntact i c  
res t r i c t ions  on the user .  Using a 7000-word 
d ic t ionary ,  EXPLORER makes in fe rences  about 
what a user is saying and initiates 
i n te ract ive  d ia logues  when m:p spec i f i ca t ions  
are not adequate or potent ia l l y  ambiguous. 
While "user-friendly" system des igns  
o f ten  accomodate novice users  at the expense 
of e f f i c ient  in teract ions  with expert  users ,  
EXPLORER naturally tunes itself to both novice 
and expert users. An expert can state a 
request  very conc ise ly ,  wi thout  a long 
in teract ive  sess ion ,  whi le a novice is  guided 
by a quest ion -and-answer  d ia logue .  Since a l l  
users  are expected to enter  mispe l l ings  and 
typos, an intelligent spelling corrector 
enables the user to correct typographical 
errors quickly and easily. 
*A proprietary agreement forbids us from 
identifying them. 
Once a request  has been analyzed by EXPLORER, 
the user  is  g iven an oppor tun i ty  to ver i fy  
EXI~LORER*s unders tand ing  of that  request  or 
make changes as needed. 
The following example will illustrate the 
complexity of the map requests that EXPLORER 
i s  des igned to hand le .  Fo l lowing the input ,  
EXPLORER d isp lays  i t s  in terpretat ion  of the 
request  fo r  ver i f i ca t ion  by the user .  
EXPLORER Version 02 9/23/81 
READY 
*Show me a map of all tight wells drilled 
*before May I, 1980 but since May I, 1970 
*by texaco that  show o i l  deeper  than 2000", 
*ere themselves  deeper than 5000", are now 
*operated  by she l l ,  are w i ldcat  we l l s  where 
*the operator  repor ted  a dr i l l i ng  problem, 
*and have mechanica l  ogs ,  d r i l l  stem tes ts ,  
*and a commercial  o i l  ana lys i s ,  that  were 
*dr i l l ed  w i th in  the area def ined by la t i tude  
*30 deg 20 min 30 sec to 31:20:30 and 
"80-81. sca le  2000 lest. 
By FEST, do you mean FEET (Y or N) *y 
User requests  a POSTED map 
la t i tude  30.34167 - 31.34167 
long i tude  80 - 81 
output  medium: PAPER 
output device: PHOTODOT 
filters: DRILLING DEPTH > 5000 FEET 
COMPLETION DATE >- 5/I/1970 
COMPLETION DATE < 5/I/1980 
OIL ANALYSIS AVAILABLE 
DRILL STEM TESTS PERFORMED 
MECHANICAL LOG FILE WELL 
DRILLING PROBLEM 
WILDCAT WELL 
SHELL CURRENT OPERATOR 
WELL SHOW OF OIL > 2000" 
TEXACO ORIGINAL OPERATOR 
TIGHT HOLE WELL 
scale of I" = 2000.0" 
69 
As th i s  example shows, EXPLORER can 
handle long and ungrammat ica l  requests ,  make 
in ferences ,  and perform compl icated  word sense 
disambiguation. For example, the word "show" 
has two distinct meanings in the above 
request. The first instance of "show" is 
synonymous with "display", whereas the second 
time "show" is used i t  has a technical meaning 
(i.e., an indicator of oil or gas in a well). 
The sur round ing  conceptua l  context  i s  used to 
determine which sense of "show" i s  appropr ia te  
in each case .  
We suspect expert users who utilize 
EXPLORER regularly will develop a feel for 
EXPLORER's limitations and flexibilities. If 
such a user wants to enter a request as 
efficiently as possible, EXPLORER will handle 
"telegraphic" English which does not have to 
conform to standard syntact i c  conventions: 
EXPLORER Version 02 9/23'/81 
READY 
Please input county or counties by name or 
number. 
*Bibb 
Bibb County is  in  the fo l low ing  s ta tes :  
ALABAMA GEORGIA 
Please input the appropriate state or a new 
county specification. 
*make i t  new haven,  c t .  
A contour  in terva l  was not spec i f ied .  
Would you like the CI from the prev ious  
request? (¥ or N) 
Previous CI = tO0" 
*y 
A scale was not spec i f ied .  
Would you like the sca le  from the prev ious  
request?  (Y or  N) 
P rev ious  sca le  - 2000.0 FT-PER-IN 
*tcg bibb georg ia  ci 100 scale 2000. *y 
User requests a STRUCTURE map 
output-medium: PAPER 
output  device: PHOTODOT 
county:  BIBB, GEORGIA 
on TCG 
CI - I00"  
scale of i" - 2000.0" 
User requests  a STRUCTURE map 
output  medium: PAPER 
output  dev ice :  photodot  
county :  NEW HAVEN, CONNECTICUT 
on TCG 
CI - I00"  
scale of i" = 2000. 9" 
As long as the request is conceptually 
coherent and unambiguous, EXPLORER will be 
able to handle a wide range of stylistic 
constructs. This last request was 
sufficiently specified for EXELORER to 
interpret it without further user interaction, 
but many requests (especially those generated 
by novice users) are likely tO require an 
interactive dialog. For example, the 
following interaction might take place with an 
extremely novice user: 
EXPLORER Version 02 9123/81 
READY 
~Map the to&. 
A map region was not specified. 
Do you want the same geographic region as the 
last request (Y or N)? 
*n 
Do you wish to specify the map region by 
county or by geographic coordinates? 
*c 
EXPLORER will query a user as needed to 
get missing information and resolve any 
ambiguities that may be present. Notice that 
EXPLORER naturally offers the user an option 
of inheriting many specifications from the 
previous map request. Explorationists often 
find it useful to examine a sequence of 
related maps, so our interface has been 
designed to make map sequences easy to 
generate. 
EXPLORER has been undergoing an initial 
test phase since July 13, 1982. During this 
time a variety of oil company employees have 
dialed up the program and entered map 
requests. While we do not yet have enough 
test requests for a comprehensive evaluation 
of the system, we have analyzed EXPLORER's 
performance over the three-week period from 
7/13/82 tO 8/6/82 in an effort to assess its 
strengths and weaknesses. During this time 39 
requests were successfully transmitted to 
EXPLORER by 8 different individuals. In order 
co evaluate EXPLORER's performance, we will 
consider the following categories of 
performance: 
(Al) original input is interpreted correctly 
on the first try - perfect performance. 
70 
TABLE - 1 
REQUEST TYPE NO. OF REQUESTS SURFACE INTERACTIVE CONCEPTUAL 
AI 4 (10%) 
A2 26 (67%) 
A3 9 (23%) 
to ta l  39 (I00%) 
(AZ) or ig ina l  input  is i n te rpreted  cor rec t ly  
a f te r  one or more c la r i fy ing  in ter -  
ac t ions .  These in teract ions  may be due 
to typ ing  er rors ,  spe l l ing  er rors ,  
miss ing  in fo rmat ion ,  or system er rors .  
(A3) o r ig ina l  input  i s  never  in terpreted  
cor rec t ly  due to a system fa i lu re  of 
some sor t .  
I f  a request  can be categor i zed  as an AI or A~ 
request ,  EXPLORER i s  fu l l y  funct iona l  even 
though i t  may make a mis take  at  some po in t  in  
i t s  p rocess ing .  For example ,  i f  EXPLORER does 
not recogn ize  a word, i t  w i l l  query  the user  
for  synonyms. I f  one of the  synonyms is  
recogn ized ,  EZLPORER recovers  from i t s  own 
recogn i t ion  er ror ,  and the request  will be 
categor i zed  as an A2 request .  When a system 
er ror  i s  fa ta l  in  the sense  that  the  user  does 
not or cannot  recover  from i t ,  we categor i ze  
the request  as an A3 request :  an A3 request  
shou ld  not resu l t  in  map generat ion .  We have 
omi t ted  from th i s  ana lys i s  any requests  Chat 
were abor ted  due to t ransmiss ion  er rors  or 
user - in i t ia ted  in ter rupts .  
In  add i t ion  to our th ree  per formance 
categor ies ,  we w i l l  character i ze  the genera l  
complex i ty  of a request  in th ree  ways:  
\[I\] Surface Complexity: 
The number of words in the original 
input request .  
\[2\] In teract ive  Complex i ty :  
The number of complete  in teract ions  
between the user  and EXPLORER dur ing  
a s ing le  request  d ia log .  
\[3\] Conceptual Complexity: 
The number of lines generated in the 
target query language. 
We realize that some users will try to 
maximize efficient communication by minimizing 
the number of complete  interactions. At the 
same time, still other users will find it 
easier to enter a minimal request and let the 
system ask for more information as needed. So 
while there is an apparent trade-off between 
the length of the initial request (surface 
complexity) and the number of interactions 
needed to fully interpret that request 
(interactive complexity), we cannot evaluate 
EXPLORER's effectiveness by trying to minimize 
one or the other. 
19(15-25) 3(3-3) 9(9-10) 
22(1-87) 7(3-14) 11(9-22) 
37(9-57) 8(5-12) 12(10-L4) 
25(1-87) 7(3-14) 11(9-22) 
We must also note  that  conceptual 
complex i ty  as i t  i s  de f ined  here can on ly  g ive  
a very  rough idea  of the  conceptua l  content  
and in fo rmat ion  process ing  invo lved .  It might  
be tempt ing  to look  fo r  conceptua l  complex i ty  
as a function of surface complexity and 
i n te ract ive  complex i ty ,  but any s imp le  
decomposition along these lines will be 
misleading- If a user changes the scale of a 
map I0 times, we will see a large interactive 
complexity with no change in conceptual 
complex i ty .  A more sens i t i ve  set  of 
complex i ty  measures  w i l l  have co be des igned  
be fore  we can expect  to see cor re la t ions  
across  the  var ious  measures .  
The resu l t s  of our  t r ia l  tes t  per iod  are  
sue~ar i zed  in  Tab le  1. We see that  the 
average  sur face  complex i ty  of a l l  requests  i s  
25 words ,  w i th  requests  rang ing  from I to 87 
words in  length .  Each request  averaged 7 
complete  in teract ions ,  w i th  some tak ing  as few 
as 3 and o thers  requ i r ing  as many as 14 
user - in teract ions .  The ta rget  query  language 
requests  averaged l l  l i nes  of code,  w i th  a 
range  between 9 and 22 l ines .  
In  terms of per formance  categor ies ,  fu l l y  
67% of a l l  requests  were A2 requests .  Only 
10% qua l i f i ed  as AI requests ,  w i th  the 
remain ing  23% fa l l ing  in to  the A3 category .  
A~ requests  tended co be s l ight ly  more 
compl i ca ted  on average  than A2 requests ,  but 
i t  i s  impor tant  Co note chat  the most complex 
requests  in  terms of a l l  th ree  measures  were 
never the less  A2 requests .  The re la t ive ly  
smal l  precenCage of AI requests may not be 
significant given the size of our sample, but 
it is likely that the failed A3 requests would 
have been A2 requests  had they been processed  
success fu l l y .  As the sys tem's  h i t  ra te  
improves ,  we expect  to see the A2 ra te  r i se  
while the AI rate remains stable. It is 
i n te res t ing  co note that  the average  sur face  
complexity of the AI requests is very close ~o 
the average surface complexity of the A2 
requests .  
Almost a l l  of the er rors  under ly ing  our 
A3 requests  were programmer er rors  due to an 
imper fec t  unders tand ing  of user  vocabu lary  or 
the ta rget  query language.  This  was expected  
and can on ly  be rec t i f i ed  w i th  cont inued  
tes t ing  by qua l i f i ed  users .  We are ext remely  
p leased  to have a 77% success  ra te  at th i s  
in i t ia l  s tage  of program tes t -deve lopment :  
EXPLORER's e r ror  ra te  shou ld  decrease  over  
time as changes are made to correct the errors 
we uncover .  
71 
Our exper ience  wi th  EXPLORER suggests  
that  i t  is imposs ib le  to complete a system of 
this complexity without some such testing 
phase for feedback purposes. A high degree of 
cooperation between program designers and 
intended users is therefore critical in these 
final stages of system development. 
Our next step is to continue testing 
revised versions of EXPLORER, expanding our 
user  popu la t ion  as the system becomes more 
competent .  At the cur rent  ra te  of user  
feedback,  we pro jec t  a 3-6 month per iod  of 
system rev is ions  be fore  we f reeze  the 
implementat ion  fo r  a f ina l  eva luat ion .  
REFERENCES 
Lehner t ,  W. and $hwartz ,  S. (1982) .  Natura l  
Language Data Base Access w i th  Pear l .  
Proceedings of the Ninth International 
Conference on Computational Linguistics. 
Prague,  Czechos lovak ia .  
Shwartz, $. (1982). Problems with 
domain-independent natural , language 
database  access  sys tems.  Proceedings  of 
the Assoc ia t ion  for  Computat iona l  
L ingu is t i cs .  Toronto ,  Canada. 
7! 
