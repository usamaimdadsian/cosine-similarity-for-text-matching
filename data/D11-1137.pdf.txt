Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Third-order Variational Reranking on Packed-Shared Dependency Forests
Katsuhiko Hayashi†, Taro Watanabe‡, Masayuki Asahara†, Yuji Matsumoto†
†Nara Insutitute of Science and Technology
Ikoma, Nara, 630-0192, Japan
‡National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
{katsuhiko-h,masayu-a,matsu}@is.naist.jp
taro.watanabe@nict.go.jp
Abstract
We propose a novel forest reranking algorithm
for discriminative dependency parsing based
on a variant of Eisner’s generative model. In
our framework, we define two kinds of gener-
ative model for reranking. One is learned from
training data offline and the other from a for-
est generated by a baseline parser on the fly.
The final prediction in the reranking stage is
performed using linear interpolation of these
models and discriminative model. In order to
efficiently train the model from and decode
on a hypergraph data structure representing a
forest, we apply extended inside/outside and
Viterbi algorithms. Experimental results show
that our proposed forest reranking algorithm
achieves significant improvement when com-
pared with conventional approaches.
1 Introduction
Recently, much of research on statistical parsing
has been focused on k-best (or forest) reranking
(Collins, 2000; Charniak and Johnson, 2005; Huang,
2008). Typically, reranking methods first generate
a list of top-k candidates (or a forest) from a base-
line system, then rerank the candidates with arbi-
trary features that are intractable within the baseline
system. In the reranking framework, the baseline
system is usually modeled with a generative model,
and a discriminative model is used for reranking.
Sangati et al. (2009) reversed the usual order of the
two models for dependency parsing by employing
a generative model to rescore the k-best candidates
provided by a discriminative model. They use a vari-
ant of Eisner’s generative model C (Eisner, 1996b;
Eisner, 1996a) for reranking and extend it to capture
higher-order information than Eisner’s second-order
generative model. Their reranking model showed
large improvements in dependency parsing accu-
racy. They reported that the discriminative model is
very effective at filtering out bad candidates, while
the generative model is able to further refine the se-
lection among the few best candidates.
In this paper, we propose a forest generative
reranking algorithm, opposed to Sangati et al.
(2009)’s approach which reranks only k-best candi-
dates. Forests usually encode better candidates more
compactly than k-best lists (Huang, 2008). More-
over, our reranking uses not only a generative model
obtained from training data, but also a sentence spe-
cific generative model learned from a forest. In the
reranking stage, we use linearly combined model
of these models. We call this variational rerank-
ing model. The model proposed in this paper is
factored in the third-order structure, therefore, its
non-locality makes it difficult to perform the rerank-
ing with an usual 1-best Viterbi search. To solve
this problem, we also propose a new search algo-
rithm, which is inspired by the third-order dynamic
programming parsing algorithm (Koo and Collins,
2010). This algorithm enables us an exact 1-best
reranking without any approximation. We summa-
rize our contributions in this paper as follows.
• To extend k-best to forest generative reranking.
• We introduce variational reranking which is a
combination approach of generative reranking
and variational decoding (Li et al., 2009).
• To obtain 1-best tree in the reranking stage, we
1479
propose an exact 1-best search algorithm with
the third-order model.
In experiments on English Penn Treebank data,
we show that our proposed methods bring signif-
icant improvement to dependency parsing. More-
over, our variational reranking framework achieves
consistent improvement, compared to conventional
approaches, such as simple k-best and forest-based
generative reranking algorithms.
2 Dependency Parsing
Given an input sentence x ? X , the task of statis-
tical dependency parsing is to predict output depen-
dencies yˆ for x. The task is usually modeled within a
discriminative framework, defined by the following
equation:
yˆ = argmax
y?Y
s(x, y)
= argmax
y?Y
?? · F(y, x) (1)
where Y is the output space, ? is a parameter vector,
and F() is a set of feature functions.
We denote a set of candidates as G(x). By using
G(x), the conditional probability p(y|x) is typically
derived as follows:
p(y|x) = e
?·s(x,y)
Z(x) =
e?·s(x,y)?
y?G(x) e?·s(x,y)
(2)
where s(x, y) is the score function shown in Eq.1
and ? is a scaling factor to adjust the sharpness of
the distribution and Z(x) is a normarization factor.
2.1 Hypergraph Representation
We propose to encode many hypotheses in a com-
pact representation called dependency forest. While
there may be exponentially many dependency trees,
the forest represents them in polynomial space. A
dependency forest (or tree) can be defined as a hy-
pergraph data strucureHG (Tu et al., 2010).
Figure 1 shows an example of a hypergraph for a
dependency tree. A shaded hyperedge e is defined
as the following form:
e : ?(I1,2, girl3,5,with5,8), saw1,8?.
.
.
.top0,8
..
.
..saw1,8:V
.
.
. .I1,2:N .. . . . .
.
..girl3,5:N
..a3,4:D ..
. .
.
..with5,8:P
..
.
..telescope6,8:N
. ..a6,7:D ..
. .
. .
.e . . . . .
. .
Figure 1: An example of dependency tree for a sentence
“I saw a girl with a telescope”.
The node saw1,8 is a head node of e. The nodes, I1,2,
girl3,5 and with5,8, are tail nodes of e. The hyper-
edge e is an incoming edge for saw1,8 and outgoing
edge for each of I1,2, girl3,5 and with5,81.
More formally, HG(x) of a forest is a pair
?V,E?, where V is a set of nodes and E is a set
of hyperedges. Given a length m sentence x =
(w1 . . . wm), each node v ? V is in the form of
wi,j (= (wi . . . wj+1)) which denotes that a word
w dominates the substring from positions i to j. In
our implementation, each word is paired with POS-
tag tag(w). We denote the root node of dependency
tree y as top. Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the head
and tails(e) ? V + are its dependants. For nota-
tional brevity of algorithmic description, we do not
distinguish left and right tails in the definition, but,
our implementation implicitly distinguishes left tails
tailsL(e) and right tails tailsR(e). We define the set
of incoming edges of a node v as IE(v) and the set
of outgoing edges of a node v as OE(v).
3 Forest Reranking
3.1 Generative Model for Reranking
Given a node v in a dependency tree y, the left and
right children are generated as two separate Markov
sequences, each conditioned on ancestral and sibling
information (context). Like a variation of Eisner’s
generative model C (Eisner, 1996b; Eisner, 1996a),
1In Figure 1, according to custom of dependency tree
description, the direction of hyperedge is written as from
head to tail nodes. However, in this paper, “incoming” and
“outgoing” have the same meanings as those in (Huang, 2006).
1480
Table 1: An event list of tri-sibling model whose event
space is v|h, sib, tsib, dir, extracted from hyperedge e in
Figure 1. EOC is an end symbol of sequence.
event space
I | saw NONE NONE L
EOC | saw I NONE L
girl | saw NONE NONE R
with | saw girl NONE R
EOC | saw with girl R
the probability of our model q is defined as follows:
q(v) =
|tailsL(e)|?
l=1
q(vl|C(vl)) · q(vl)
×
|tailsR(e)|?
r=1
q(vr|C(vr)) · q(vr) (3)
where |tailsL(e)| and |tailsR(e)| are the number of
left and right children of v, vl and vr are the left and
right child of position l and r in each side. C(v) is
a context event space of v. We explain the context
event space later in more detail. The probability of
the entire dependency tree y is recursively computed
by q(y(top)) where y(top) denotes a top node of y.
The probability q(v|C(v)) is dependent on a con-
text space C(v) for a node v. We define two kinds of
context spaces. First, we define a tri-sibling model
whose context space consists of the head node, sib-
ling node, tri-sibling node and direction of a node
v:
q1(v|C(v)) = q1(v|h, sib, tsib, dir) (4)
where h, sib and tsib are head, sibling and tri-sibling
node of v, and dir is a direction of v from h. Table
1 shows an example of an event list of the tri-sibling
model, which is extracted from hyperedge e in Fig-
ure 1. EOC indicates the end of the left or right child
sequence. This is factored in a tri-sibling structure
shown in the left side of Figure 2.
Eq.4 is further decomposed into a product of the
form consisting of three terms:
q1(v|h, sib, tsib, dir) (5)
= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)
= q1(tag(v)|h, sib, tsib, dir)
×q1(wrd(v)|tag(v), h, sib, tsib, dir)
×q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)
where tag(v) and wrd(v) are the POS-tag and word
of v and dist(v, h) is the distance between positions
of v and h. The values of dist(v, h) are partitioned
into 4 categories: 1, 2, 3 ? 6, 7 ??.
Second, following Sangati et al. (2009), we define
a grandsibling model whose context space consists
of the head node, sibling node, grandparent node and
direction of a node v.
q2(v|C(v)) = q2(v|h, sib, g, dir) (6)
where g is a grandparent node of v. Analogous to
Eq.5, Eq.6 is decomposed into three terms:
q2(v|h, sib, g, dir) (7)
= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)
= q2(tag(v)|h, sib, g, dir)
×q2(wrd(v)|tag(v), h, sib, g, dir)
×q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)
where notations are the same as those in Eq.5 with
the exception of tri-sibling tsib and grandparent g.
This model is factored in a grandsibling structure
shown in the right side of Figure 2.
The direct estimation of tri-sibling and grandsib-
ling models from a corpus suffers from serious data
sparseness issues. To overcome this, Eisner (1996a)
proposed a back-off strategy which reduces the con-
ditioning of a model. We show the reductions list
for each term of two models in Table 2. The usage
of reductions list is identical to Eisner (1996a) and
readers may refer to it for further details.
The final prediction is performed using a log-
linear interpolated model. It interpolates the base-
line discriminative model and two (tri-sibling and
grandsibling) generative models.
yˆ = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+ log p(y|x)?base (8)
where ? are parameters to adjust the weight of each
term in prediction. These parameters are tuned using
MERT algorithm (Och, 2003) on development data
using a criterion of accuracy maximization. The rea-
son why we chose MERT is that it effectively tunes
dense parameters with a line search algorithm.
1481
Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,
POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition;
later reductions throw away more and more of this information.
tri-sibling grandsibling
1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd term
wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d
wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d
t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — —wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d
t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — —
..h .tsib .sib .v ..g .h .sib .v
Figure 2: The left side denotes tri-sibling structure and
the right side denotes grandsibling structure.
Table 3: A summarization of the model factorization and
order
first-order McDonald et al. (2005)
second-order Eisner (1996a)
(sibling) McDonald et al. (2005)
third-order tri-sibling model
(tri-sibling) Model 2 (Koo and Collins, 2010)
third-order grandsibling model (Sangati et al., 2009)
(grandsibling) Model 1 (Koo and Collins, 2010)
3.2 Exact Search Algorithm
Our baseline discriminative model uses first- and
second-order features provided in (McDonald et al.,
2005; McDonald and Pereira, 2006). Therefore,
both our tri-sibling model and baseline discrimina-
tive model integrate local features that are factored
in one hyperedge. On the other hand, the grandsib-
ling model has non-local features because the grand-
parent is not factored in one hyperedge. We sum-
marize the order of each model in Table 3. Our
reranking models are generative versions of Koo and
Collins (2010)’s third-order factorization model.
Non-locality of weight function makes it difficult
to perform the search of Eq.8 with an usual exact
Viterbi 1-best algorithm. One solution to resolve
the intractability is an approximate k-best Viterbi
search. For a constituent parser, Huang (2008) ap-
plied cube pruning techniques to forest reranking
with non-local features. Cube pruning is originally
proposed for the decoding of statistical machine
translation (SMT) with an integrated n-gram lan-
guage model (Chiang, 2007). It is an approximate
k-best Viterbi search algorithm using beam search
and lazy computation (Huang and Chiang, 2005).
In the case of a dependency parser, Koo and
Collins (2010) proposed dynamic-programming-
based third-order parsing algorithm, which enumer-
ates all grandparents with an additional loop. Our
hypergraph based search algorithm for Eq.8 share
the same spirit to their third-order parsing algo-
rithm since the grandsibling model is similar to their
model 1 in that it is factored in grandsibling struc-
ture. Algorithm 1 shows the search algorithm. This
is almost the same bottom-up 1-best Viterbi algo-
rithm except an additional loop in line 4. Line 4 ref-
erences outgoing edge e? of node h from a set of out-
going edges OE(h). tails(e) contains a node v, the
sibling node sib and tri-sibling node tsib of v, more-
over, the head of e? (head(e?)) is the grandparent for
v and sib. Thus, in line 5, we can capture tri-sibling
and grandsibling information and compute the cur-
rent inside estimate of Eq.8.
In our actual implementation, each score of com-
ponents in Eq.8 is represented as a cost. This is writ-
ten as a shortest path search algorithm with a tropi-
cal (real) semiring framework (Mohri, 2002; Huang,
2006). Therefore,? denotes the min operater and?
denotes the + operater. The function f is defined as
follows:
f(d(v1, e), . . . , d(v|e|, e))) =
|e|?
i=1
d(vi, e) (9)
where d(vi, e) denotes the current estimate of the
best cost for a pair of node vi and a hyperedge e.? sums the best cost of a pair of a sub span node
and hyperedge e. Each ctsib and cgsib in line 5 and
7 indicates the cost of tri-sibling and grandsibling
1482
Algorithm 1 Exact DP-Search Algorithm(HG(x))
1: for h ? V in bottom-up topological order do
2: for e ? IE(h) do
3: // tails(e) is {v1, . . . , v|e|
}.
4: for e? ? OE(h) do
5: d(h, e?) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e)) ? cgsib(head(e?), h, tails(e))
6: if h == top then
7: d(h) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e))
model. we indicates the cost of hyperedge e com-
puted from a baseline discriminative model. Lines
6-7 denote the calculation of the best cost for a top
node. We do not compute the cost of the grandsib-
ling model when h is top node because top node has
no outgoing edges.
Our baseline k-best second-order parser is imple-
mented using Huang and Chiang (2005)’s algorithm
2 whose time complexity is O(m3+mk log k). Koo
and Collins (2010)’s third-order parser has O(m4)
time complexity and is theoretically slower than our
baseline k-best parser for a long sentence. Our
search algorithm is based on the third-order parsing
algorithm, but, the search space is previously shrank
by a baseline parser’s k-best approximation and a
forest pruning algorithm presented in the next sec-
tion. Therefore, the time efficiency of our reranking
is unimpaired.
3.3 Forest Pruning
Charniak and Johnson (2005) and Huang (2008)
proposed forest pruning algorithms to reduce the
size of a forest. Huang (2008)’s pruning algo-
rithm uses a 1-best Viterbi inside/outside algorithm
to compute an inside probability ?(v) and an out-
side probability ?(v), while Charniak and Johnson
(2005) use the usual inside/outside algorithm.
In our experiments, we use Charniak and Johnson
(2005)’s forest pruning criterion because the varia-
tional model needs traditional inside/outside proba-
bilities for its ML estimation. We prune away all
hyperedges that have score < ? for a threshold ?.
score = ??(e)?(top) . (10)
Following Huang (2008), we also prune away nodes
with all incoming and outgoing hyperedges pruned.
4 Variational Reranking Model
In place of a maximum a posteriori (MAP) decision
based on Eq.2, the minimum Bayes risk (MBR) deci-
sion rule (Titov and Henderson, 2006) is commonly
used and defined as following equation:
yˆ = argmin
y?G(x)
?
y??G(x)
loss(y, y?)p(y?|x) (11)
where loss(y, y?) represents a loss function2. As an
alternative to the MBR decision rule, Li et al. (2009)
proposed a variational decision rule that rescores
candidates with an approximate distribution q? ? Q.
yˆ = argmax
y?G(x)
q?(y) (12)
where q? minimizes the KL divergence KL(p||q)
q? = argmin
q?Q
KL(p||q)
= argmax
q?Q
?
y?G(x)
p log q (13)
where each p and q represents p(y|x) and q(y). For
SMT systems, q? is modeled by n-gram language
model over output strings. While the decoding based
on q? is an approximation of intractable MAP de-
coding3, it works as a rescoring function for candi-
dates generated from a baseline model. Here, we
propose to apply the variational decision rule to de-
pendency parsing. For dependency parsing, we can
choose to model q? as the tri-sibling and grandsib-
ling generative models in section 3.
2In case of dependency parsing, Titov and Henderson (2006)
proposed that a loss function is simply defined using a depen-
dency attachment score.
3In SMT, a marginalization of all derivations which yield
a paticular translation needs to be carried out for each trans-
lation. This makes the MAP decoding NP-hard in SMT. This
variational approximate framework can be applied to other tasks
collapsing spurious ambiguity, such as latent-variable parsing
(Matsuzaki et al., 2005).
1483
Algorithm 2 DP-ML Estimation(HG(x))
1: run inside and outside algorithm onHG(x)
2: for v ? V do
3: for e ? IE(v) do
4: ctsib = pe · ?(v)/?(top)
5: for u ? tails(e) do
6: ctsib = ctsib · ?(u)
7: for e? ? IE(u) do
8: cgsib = pe · pe? · ?(v)/?(top)
9: for u? ? tails(e) \ u do
10: cgsib = cgsib · ?(u?)
11: for u?? ? tails(e?) do
12: cgsib = cgsib · ?(u??)
13: for u?? ? tails(e?) do
14: c2(u??|C(u??))+ = cgsib
15: c2(C(u??))+ = cgsib
16: for u ? tails(e) do
17: c1(u|C(u))+ = ctsib
18: c1(C(u))+ = ctsib
19: MLE estimate q?1 , q?2 using formula Eq.14
4.1 ML Estimation from a Forest
q?(v|C(v)) is estimated from a forest using a max-
imum likelihood estimation (MLE). The count of
events is no longer an integer count, but an expected
count under p, which is formulated as follows:
q?(v|C(v)) = c(v|C(v))c(C(v))
=
?
y p(y|x)cv|C(v)(y)?
y p(y|x)cC(v)(y)
(14)
where ce(y) is the number of event e in y. The es-
timation of Eq.14 can be efficiently performed on a
hypergraph data structureHG(x) of a forest.
Algorithm 2 shows the estimation algorithm.
First, it runs the inside/outside algorithm onHG(x).
We denote inside weight for a node v as ?(v) and
outside weight as ?(v). For each hyperedge e, we
denote ctsib as the posterior weight for computing
expected count c1 of events in the tri-sibling model
q?1 . Lines 16-18 compute c1 for all events occuring
in a hyperedge e.
The expected count c2 needed for the estimation
of grandsibling model q?2 is extracted in lines 7-15.
c2 for a grandsibling model must be extracted over
two hyperedges e and e? because it needs grandpar-
ent information. Lines 8-12 show the algorithm to
compute the posterior weight cgsib of e and e?, which
 92
 93
 94
 95
 96
 97
 98
 99
 100
 0  200  400  600  800  1000  1200  1400
Un
la
be
le
d 
Ac
cu
ra
cy
the number of hyperedges per sentence
p=0.001
k=20
k=100
"kbest"
"forest"
Figure 3: The relationship between tha data size (the
number of hyperedges) and oracle scores on develop-
ment data: Forests encode candidates with high accuracy
scores more compactly than k-best lists.
is similar to that to compute the posterior weight
of rules of tree substitution grammars used in tree-
based MT systems (Mi and Huang, 2008). Lines
13-15 compute expected counts c2 of events occur-
ing over two hyperedges e and e?. Finally, line 19
estimates q?1 and q?2 using the form in Eq.14.
Li et al. (2009) assumes n-gram locality of the
forest to efficiently train the model, namely, the
baseline n-gram model has larger n than that of vari-
ational n-gram model. In our case, grandsibling lo-
cality is not embedded in the forest generated from
the baseline parser. Therefore, we need to reference
incoming hyperedges of tail nodes in line 7.
y? of Eq.12 may be locally appropriate but glob-
ally inadequate because q? only approximates p.
Therefore, we log-linearly combine q? with a global
generative model estimated from the training data
and the baseline discriminative model.
yˆ = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+
2?
n=1
log q?n(top(y))?
?
n
+ log p(y|x)?base (15)
Algorithm1 is also applicable to the decoding of
Eq.15. Note that this framework is a combination of
variational decoding and generative reranking. We
call this framework variational reranking.
1484
Table 4: The statistics of forests and 20-best lists on de-
velopment data: this shows the average number of hyper-
edges and nodes per sentence and oracle scores.
forest 20-best
pruning threshold ? = 10?3 —
ave. num of hyperedges 180.67 255.04
ave. num of nodes 135.74 491.42
oracle scores 98.76 96.78
5 Experiments
Experiments are performed on English Penn Tree-
bank data. We split WSJ part of the Treebank into
sections 02-21 for training, sections 22 for develop-
ment, sections 23 for testing. We use Yamada and
Matsumoto (2003)’s head rules to convert phrase
structure to dependency structure. We obtain k-best
lists and forests generated from the baseline discrim-
inative model which has the same feature set as pro-
vided in (McDonald et al., 2005), using the second-
order Eisner algorithms. We use MIRA for training
as it is one of the learning algorithms that achieves
the best performance in dependency parsing. We set
the scaling factor ? = 1.0.
We also train a generative reranking model from
the training data. To reduce the data sparseness
problem, we use the back-off strategy proposed in
(Eisner, 1996a). Parameters ? are trained using
MERT (Och, 2003) and for each sentence in the de-
velopment data, 300-best dependency trees are ex-
tracted from its forest. Our variational reranking
does not need much time to train the model be-
cause the training is performed over not the train-
ing data (39832 sentences) but the development data
(1700 sentences)4. After MERT was performed un-
til the convergence, the variational reranking finally
achieved a 94.5 accuracy score on development data.
5.1 k-best Lists vs. Forests
Figure 3 shows the relationship between the size of
data structure (the number of hyperedges) and accu-
racy scores on development data. Obviously, forests
can encode a large number of potential candidates
more compactly than k-best lists. This means that
4To generate forests, sentences are parsed only once before
the training. MERT is performed over the forests. We can also
apply a more efficient hypergraph MERT algorithm (Kumar et
al., 2009) to the training than a simple MERT algorithm.
for reranking, there is more possibility of selecting
good candidates in forests than k-best lists.
Table 4 shows the statistics of forests and 20-
best lists on development data. This setting, thresh-
old ? = 10?3 for pruning, is also used for testing.
Forests, which have an average of 180.67 hyper-
edges per sentence, achieve oracle score of 98.76,
which is about 1.0% higher than the 96.78 oracle
score of 20-best lists with 255.04 hyperedges per
sentence. Though the size of forests is smaller than
that of k-best lists, the oracle scores of forests are
much higher than those of k-best lists.
5.2 The Performance of Reranking
First, we compare the performance of variational de-
coding with that of MBR decoding. The results are
shown in Table 5. Variational decoding outperforms
MBR decodings. However, compared with base-
line, the gains of variational and MBR decoding are
small. Second, we also compare the performance of
variational reranking with k-best and forest gener-
ative reranking algorithms. Table 6 shows that our
variational reranking framework achieves the high-
est accuracy scores.
Being different from the decoding framework,
reranking achieves significant improvements. This
result is intuitively reasonable because the rerank-
ing model obtained from training data has the ability
to select a globally consistent candidate, while the
variational approximate model obtained from a for-
est only supports selecting a locally consistent can-
didate. On the other hand, the fact that variational
reranking achieves the best results clearly indicates
that the combination of sentence specific generative
model and that obtained from training data is suc-
cessful in selecting both locally and globally appro-
priate candidate from a forest.
Table 7 shows the parsing time (on 2.66GHz
Quad-Core Xeon) of the baseline k-best, generative
reranking and variational reranking parsers (java im-
plemented). The variational reranking parser con-
tains the following procedures.
1. k-best forest creation (baseline)
2. Estimation of variational model
3. Forest pruning
4. Search with the third-order model
Our reranking parser incurred little overhead to the
1485
Table 5: The comparison of the decoding frameworks:
MBR decoding seeks a candidate which has the high-
est accuracy scores over a forest (Kumar et al., 2009).
Variational decoding is performed based on Eq.8.XXXXXXXXXXDecoding
Eval Unlabeled
baseline 91.9
MBR (8-best forest) 91.99
Variational (8-best forest) 92.17
Table 6: The comparison of the reranking frameworks:
Generative means k-best or forest reranking algorithm
based on a generative model estimated from a corpus.
Variational reranking is performed based on Eq.15.XXXXXXXXXXReranking
Eval Unlabeled
Generative (8-best) 92.66
Generative (8-best forest) 92.72
Variational (8-best forest) 92.87
Table 7: The parsing time (CPU second per sentence) and
accuracy score of the baseline k-best, generative rerank-
ing and variational reranking parsers
k baseline generative variational
2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)
4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)
8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)
16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)
32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)
64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)
Table 8: The comparison of tri-sibling and grandsibling
models: the performance of the grandsibling model out-
performs that of the tri-sibling model.PPPPPPPModel
Eval Unlabeled
tri-sibling 92.63
grandsibling 92.74
baseline parser in terms of runtime. This means that
our reranking parser can parse sentences at reason-
able times.
5.3 The Effects of Third-order Factors and
Error Analysis
From results in section 5.2, our variational rerank-
ing model achieves higher accuracy scores than the
others. To analyze the factors that improve accu-
racy scores, we further investigate whether varia-
tional reranking is performed better with the tri-
sibling or grandsibling model. Table 8 indicates that
grandsibling model achieves a larger gain than that
of tri-sibling model. Table 9 shows the examples
whose accuracy scores improved by the grandsib-
ling model. For example, the dependency relation-
ship from Verb to Noun phrase was corrected by our
proposed model.
On the other hand, many errors remain still in
Table 10: Comparison of our best result (using 16-best
forests) with other best-performing Systems on the whole
section 23
Parser English
McDonald et al. (2005) 90.9
McDonald and Pereira (2006) 91.5
Koo et al. (2008) standard 92.02
Huang and Sagae (2010) 92.1
Koo and Collins (2010) model1 93.04
Koo and Collins (2010) model2 92.93
this work 92.89
Koo et al. (2008) semi-sup 93.16
Suzuki et al. (2009) 93.79
our results. In our experiments, 48% of sentences
which contain errors have Prepositionalword errors.
In fact, well-known PP-Attachment is a problem to
be solved for natural language parsers. Other re-
maining errors are caused by symbols such as .,:“”().
45% sentences contain such a dependency mistake.
Adding features to solve these problems may poten-
tially improve our parser more.
5.4 Comparison with Other Systems
Table 10 shows the comparison of the performance
of variational reranking (16-best forests) with that of
other systems. Our method outperforms supervised
parsers with second-order features, and achieves
comparable results compared to a parser with third-
order features (Koo and Collins, 2010). We can not
directly compare our method with semi-supervised
parsers such as Koo et al. (2008)’s semi-sup and
Suzuki et al. (2009), because ours does not use addi-
tional unlabeled data for training. The model trained
from unlabeled data can be easily incorporated into
our reranking framework. We plan to investigate
semi-supervised learning in future work.
1486
Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational
reranking parsers. The underlined portions show the effect of the grandsibling model.
sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .
correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4
baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4
proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4
sent (No.283) Many called it simply a contrast in styles .
correct 2 0 2 6 6 2 6 7 2
baseline 2 0 2 2 6 2 6 7 2
proposed 2 0 2 6 6 2 6 7 2
6 Related Work
Collins (2000) and Charniak and Johnson (2005)
proposed a reranking algorithm for constituent
parsers. Huang (2008) extended it to a forest rerank-
ing algorithm with non-local features. Our frame-
work is for a dependency parser and the decoding in
the reranking stage is done with an exact 1-best dy-
namic programming algorithm. Sangati et al. (2009)
proposed a k-best generative reranking algorithm for
dependency parsing. In this paper, we use a similar
generative model, but combined with a variational
model learned on the fly. Moreover, our framework
is applicable to forests, not k-best lists.
Koo and Collins (2010) presented third-order de-
pendency parsing algorithm. Their model 1 is de-
fined by an enclosing grandsibling for each sibling
or grandchild part used in Carreras (2007). Our
grandsibling model is similar to the model 1, but
ours is defined by a generative model. The decod-
ing in the reranking stage is also similar to the pars-
ing algorithm of their model 1. In order to capture
grandsibling factors, our decoding calculates inside
probablities for not the current head node but each
pair of the node and its outgoing edges.
Titov and Henderson (2006) reported that the
MBR approach could be applied to a projective de-
pendency parser. In the field of SMT, for an approx-
imation of MAP decoding, Li et al. (2009) proposed
variational decoding and Kumar et al. (2009) pre-
sented hypergraph MBR decoding. Our variational
model is inspired by the study of Li et al. (2009) and
we apply it to a dependency parser in order to select
better candidates with third-order information. We
also propose an efficient algorithm to estimate the
non-local third-order model structure.
7 Conclusions
In this paper, we propose a novel forest reranking
algorithm for dependency parsing. Our reranking
algorithm is a combination approach of generative
reranking and variational decoding. The search al-
gorithm in the reranking stage can be performed
using dynamic programming algorithm. Our vari-
ational reranking is aimed at selecting a candidate
from a forest, which is correct both in local and
global. Our experimental results show more signif-
icant improvements than conventional approaches,
such as k-best and forest generative reranking.
In the future, we plan to investigate more ap-
propriate generative models for reranking. PP-
Attachment is one of the most difficult problems
for a natural language parser. We plan to exam-
ine to model such a complex structure (granduncle)
(Goldberg and Elhadad, 2010) or higher-order struc-
ture than third-order for reranking which is compu-
tationally expensive for a baseline parser. As we
mentioned in Section 5.4, we also plan to incorpo-
rate semi-supervised learning into our framework,
which may potentially improve our reranking per-
formance.
Acknowledgments
Wewould like to thank GrahamNeubig andMasashi
Shimbo for their helpful comments and to the anony-
mous reviewers for their effort of reviewing our pa-
per and giving valuable comments. This work was
supported in part by Grant-in-Aid for Japan Society
for the Promotion of Science (JSPS) Research Fel-
lowship for Young Scientists.
1487
References
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. the CoNLL-
EMNLP, pages 957–961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. the 43rd ACL, pages 173–180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201–228.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. the ICML.
J. M. Eisner. 1996a. An empirical comparison of prob-
ability models for dependency grammar. In Technical
Report, pages 1–18.
J. M. Eisner. 1996b. Three new probabilistic models for
dependency parsing: An exploration. In Proc. the 16th
COLING, pages 340–345.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742–750.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. the IWPT, pages 53–64.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the ACL,
pages 1077–1086.
L. Huang. 2006. Dynamic programming al-
gorithms in semiring and hypergraph frame-
works. Qualification Exam Report, pages 1–19.
http://www.cis.upenn.edu/ lhuang3/wpe2/.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. the 46th ACL,
pages 586–594.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1–11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. the ACL,
pages 595–603.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. the 47th ACL, pages 163–171.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
the 47th ACL, pages 593–601.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic cfg with latent annotations. In Proc. the ACL, pages
75–82.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
the 43rd ACL, pages 91–98.
H. Mi and L. Huang. 2008. Forest-based translation rule
extraction. In Proceedings of EMNLP, pages 206–
214.
M. Mohri. 2002. Semiring framework and algorithms
for shortest-distance problems. Automata, Languages
and Combinatorics, 7:321–350.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160–167.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proc. the
11th IWPT, pages 238–241.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proc. the
EMNLP, pages 551–560.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. In Technical Report,
pages 1–9.
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
pendency forest for statistical machine translation. In
Proc. the 23rd COLING, pages 1092–1100.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195–206.
1488
