TWO S IMPLE  PREDICT ION ALGORITHMS 
TO FACIL ITATE TEXT PRODUCTION 
Lois Boggess 
P.O. Drawer  CS 
Mississ ippi  State University 
Mississ ippi  State, MS 39762 
ABSTRACT 
Several s imple pred ict ion schemes are 
presented for systems intended to fac i l i -  
tate text p roduct ion  for handicapped 
individuals.  The schemes are based on 
s ing le-subject  language models, where the 
system is se l f -adapt ing  to the past 
language use of the subject. Sentence 
pos i t ion,  the immediate ly  preceding one 
or two words,  and init ial  letters of the 
desired word are cues which may be used 
by the systems. 
INTRODUCTION 
For  some years we have been invest i -  
gating the use of a sizeable sample of a 
par t icu lar  indiv idual 's  language habits 
in predict ing future language use for 
that individual .  The research has taken 
two direct ions. 
One of these, the HWYE (Hear What 
You Expect)  system, bui lds a large lan -  
guage model  of the past language h istory 
of the individual ,  with special  emphasis 
on the most frequent words of that 
person, and the result is used in speech 
recognit ion.  In studying the language 
model  developed by the HWYE system, 
several s imple predict ive schemes were 
noted which are capable of ant ic ipat ing,  
dur ing the generat ion of a sentence, a 
small  set of words  f rom which the next 
desired word can be selected. The two 
schemes described here are used for text 
generat ion (not speech recognit ion)  in a 
format  that could be of use to a phys i -  
cal ly hand icapped person; hence the 
schemes have no r ight context  avai lable.  
One of the schemes does use left context ,  
and the other uses only  sentence pos i t ion  
as "context ' .  Both are implemented on 
IBM-PC systems with minimal  memory  
requirements.  
MOTIVAT ION 
One hundred Engl ish words account for 
47 per cent of the Brown corpus (about 
one mi l l ion words of American Engl ish 
text taken from a wide range of sources). 
It seems reasonable to suppose that a 
single indiv idual  might in fact require 
fewer words to account for a large 
propor t ion  of generated text. F rom our 
work  on the HWYE system it was known 
that 75 words accounted for half of all 
the text of Vanity Fair,  a 300,000 word 
Victor ian Engl ish novel by Thackeray  
(which incorporated  a fair ly involved 
syntax,  much embedded quotat ion,  and 
passages in dialect and in French) 
\[Engl ish and Boggess, 1986\]. We further 
found that 50 words accounted for half of 
all the verbiage in a 20,000 word set of 
sentences prov ided by an indiv idual  who 
co l laborated with us. This latter 
corpus,  called the Sherr i  data, is a set 
of texts prov ided by a speech-hand icapped 
indiv idual  who uses a typewr i ter  to 
communicate,  even with her family; it is 
conversat iona l  in nature, as can be seen 
in F igure 1. Most of the work  reported 
in this paper  gives special  at tent ion to 
the set of words required to account for 
half of all the verbiage of a given 
individual.  We refer to this set as the 
set of h igh- f requency  words. 
33 
You said something about  a magazine that <namel> had 
about  computers  that I might l ike to borrow.  
I would some time. 
I th ink we have to pick up the chi ldren whi le <name2> 
is in the hospital .  
I want to visit her in the hospital .  
But you have to lift me up to the w indow for me to see 
the baby. 
Well,  it's May first now. Help! 
I thought  it would not be so busy but it looks  l ike it 
might be now. 
F igure 1. Sample set of cont iguous sentences in Sherr i  data 
It seems reasonable  to suppose that 
for conversat iona l  Engl ish, approx imate ly  
50 words  may account for half of the 
verbiage of most Engl ish users. F rom 
the s tandpo int  of human factors,  an 
argument could be made that one should 
s imply  put the 50 words  up on the screen 
with the a lphabet  and thus be assured 
that half of al l  the words  desired by the 
user were instant ly  avai lable,  in known 
locat ions  that the user would  qu ick ly  
become accustomed to. Constant ly  
changing menus introduce an element of 
user fat igue \ [Gib ler  and Chi ldress,  
1982\]. That  argument may especial ly  make 
sense as larger screens with more lines 
per screen and more characters per line 
become more common. 
If we l imit ourselves to the top 20 
most f requent words  as a constant  menu, 
on ly  about  30 per cent of the user's 
verbiage is accounted for. However,  it 
was observed, whi le work ing  with the 
HWYE system, that if one looked at the 
top 20 words  for any given sentence 
pos i t ion,  one did not see the same set of 
words occurr ing.  C lear ly  the high 
f requency words  (the set that compr ise 
half  of word  use) are mi ld ly sensit ive to 
"context" even when "context" is so 
broad ly  def ined as sentence posit ion.  
Di f ferent subsets of the 50 member set of 
high f requency words  appear  in the set of 
20 most f requent words for a given 
sentence posi t ion.  Moreover ,  after 
processing approx imate ly  2000 sentences 
f rom the user, it was sti l l  the case that 
some of the top 20 words  for a given 
pos i t ion  were not members of the high 
f requency set at all. For  example,  the 
word "they' ,  a member of the menu for the 
first sentence pos i t ion  Isee F igure 2) 
and hence one of the 20 most f requent 
words to start a sentence, is not a 
member of the g lobal  high f requency set. 
A pre l iminary  analys is  by Engl ish 
suggested that, whereas a constant  
"predict ion" of the top 20 most f requent 
words would  yield a success rate of 30 
per cent, predict ing the top 20 most 
f requent words  per pos i t ion  in sentence 
would yield a success rate of 40 per 
cent. 
~CONTEX7"  AS SENTENCE POSIT ION 
The simplest scheme, which has been 
bui lt  as a pro to type  on an IBM PC with 
two f loppy  disk drives, presents the user 
with the top 20 most f requent words  that 
the user has employed at whatever 
pos i t ion  in a sentence is current.  For  
example,  F igure 2 shows the screen 
presented to the user at the beginning of 
p roduct ion  of a sentence. On the left is 
a list of the 20 words which that 
part icu lar  user is known to have used 
most often to begin sentences. On the 
r ight is the a lphabet,  which is normal ly  
avai lable to the user; and in other 
places on the screen are special  
functions. (Selection of words,  letters 
34 
1 but  
2 oan  
3 oou ld  
4 do 
S he 
6 hot*  
? I 
8 I °N  
9 i f  
10  i t  
I I  i t )  s 
12 Lo is  
13 she  
14 that  
15 the  
16 they  
17 we 
18 what  
19 when 
2e uou 
SPELL  
CAP ITAL  
• b o 
PUNCTUATION 
HELP-NENU S h I 
ENDING 
m n o 
NUNDER 
SPECIAL  • t u 
REUlEN 
U z 
HARD-COPY 
SAUE-SENT 
j k 1 
p q r 
v w x 
HEN ERASE qUIT  
HEN SENTENCE:  
Figure 2. Init ial  Screen 
and funct ions  is made by mouse,  though 
the actual se lect ion mechanism is 
separated from the bulk of the code so 
that replacement with another  se lect ion 
mechanism shou ld  be relat ively easy to 
implement.)  The sentence is bui lt  at the 
bot tom of the screen. If the user 
selects a word  from the menu at the left, 
it is placed in first pos i t ion  in the 
sentence, and a second menu, cons ist ing 
of the 20 most  frequent words  that the 
user has used in second place in a 
sentence, appears in the left por t ion  of 
the screen. After a second word  has been 
produced and added to the sentence, a 
third menu, cons is t ing of the 20 most 
frequent words  for that user in third 
place in a sentence, is offered, and so 
on.  
At any time the user may reject the 
lefthand menu by select ing a letter of the 
alphabet.  F igure 3 shows  the screen after 
the user has produced two words  of a 
sentence and has begun to spel l  a third 
word  by select ing the letter "a +. At this 
point ,  the top 20 most f requent ly  used 
words  beginning with +a" have been offered 
at the left. If the desired word  is not  
in the list, the user cont inues  by se lect -  
ing the second letter of the desired word  
(in this case, "n'). The le f t -hand menu 
becomes the 20 most f requent ly  used words  
beginning with the pair of  letters given 
so far. As is shown in F igure 4, there 
are times when fewer than 20 words  of a 
given two- le t te r  start ing combinat ion  have 
been encountered from the user's past 
h istory,  in which case this a lgor i thm 
offers a shortened list. 
In the case i l lustrated, the desired 
word  was on the list. If it were not,  the 
user wou ld  have had to spel l  out  the en-  
tire word,  and it wou ld  have been entered 
into the sentence. In either case, the 
system subsequent ly  returns to of fer ing 
the menu of most - f requent ly -used  words  for 
the fourth pos i t ion ,  and cont inues  in 
s imilar fash ion to the end of the 
sentence. 
L •  
2 ab le  
3 • bout  
4 • £ te r  
5 af ternoon 
6 apa ln  
7 a l l  
8 am 
9 an 
l e  and 
12 app le  
13 Apr i l  
14 are  
15 a~ound 
16 as  
17 ask  
18 asked 
19 at  
2e • un t  
• b o d • F 
g h i J k 1 
• t u v w )< 
NEW SENTENCE:  
I have  
Figure 3: 
1 an imal  
2 an imals  
3 Ani ta  
4 ann iversary  
S AnnM 
6 another  
? &nsuer  
8 answer •  
9 any  
IO  • nvone  
I I  • np th |n •  
User has selected "a" 
• b o d • £ 
• k I J k I 
• t u v ~ x 
NEN SENTENCE:  
I have  
Figure 4: User has selected "a-n" 
35 
The system keeps up with how often a 
word has been used and with how many 
times it has occurred in each pos i t ion  in 
a sentence, so that f rom time to time a 
word is p romoted  to one of the top 20 
a lphabet ic  or top 20 pos i t ion - re la ted  sets 
of words.  For  detai ls on the file o rgan i -  
zat ion scheme that a l lows this to be done 
in real time, see Wei \[1987\]. Detai ls  on 
the mouse-based  implementat ion  for IBM 
PC's are avai lab le  in Chow \[1986\]. 
A SECOND ALGORITHM 
An a l ternat ive predict ive a lgor i thm 
has been implemented which replaces the 
sentence-pos i t ion -based  first menu. It 
pays special  at tent ion to the 50 most 
f requent ly  used words  in the indiv idual 's  
vocabu lary  (the h igh- f requency  words)  and 
to the words most l ike ly  to fo l low them. 
By virtue of their  frequency, these are 
precisely the words  about  which the most 
is known,  with the greatest confidence, 
after a re lat ive ly  small  body of input 
such as a few thousand sentences. 
For  each of the 50 h igh- f requency  
words,  a list is kept  of the top 20 most 
f requent words  to fo l low that word.  Let 
us call  these the f irst order  fo l lowers.  
For  each of the first order  fo l lowers ,  
there is a list of second-order  fo l lowers:  
words known to have fo l lowed the two 
word sequence consist ing of the h igh-  
f requency word and its f irst order  
fo l lower .  
For  example,  the word "I" is a h igh-  
f requency word.  The first order  fo l lowers  
for "I" include the word "wol)ld'.  The 
second-order  fo l lowers  for "I would"  
include the word  "l ike' .  (See F igure 5.) 
The second-order  fo l lowers  for "I would"  
also include many one- t ime-on ly  fo l lowers ,  
as well, so the system maintains a 
threshold  for the number  of oceurrances 
be low which a word  is not included in the 
list of second-order  fo l lowers.  The 
reasoning is that a word 's  having occurred 
only  once in an env i ronment  hat by 
def in i t ion occurs f requent ly  may be taken 
as counter -ev idence  that the word should 
be predicted. 
Rather  than predict  a word with low 
re l iabi l i ty ,  one of two al ternat ives are 
taken. If the f i r s t -o rder  fo l lower  is 
itself a h igh- f requency  word,  then low-  
re l iab i l i ty  second-order  fo l lowers  may be 
replaced with the f i r s t -o rder  fo l lower 's  
own fo l lowers.  ( 'Would"  is a f i r s t -o rder  
I o 
Figure 5. 
..~-! thi,k ,-~--'" 
don ' t  *,-~, 1 
hope ~. ! ' 
i 
was  
wish  
l i ke  
wi l l  
have  
want  
wonder  
got  
r - ,  
~z 
I ' l l  
the  
we 
i t  
I t 'S  
oF  
Vou 
rea l ly  
wan t 
have  
,,. . . . . . . . . .  Q 
F i rs t -  and second-  fo l lowers  
for "I" 
fo l lower  of "I" and is itself a h igh-  
f requency word.  There are re lat ive ly  few 
rel iable second-order  fo l lowers  to "would" 
in the left context  of "I', so the list is 
augmented with f i r s t -o rder  fo l lowers  of 
"would" to round out a list of 20 words.) 
The other  a l ternat ive,  taken when the 
f i r s t -o rder  fo l lower  is not a h igh-  
f requency word,  is to fi l l  out any short  
list of second-order  words  with the h igh-  
f requency words  themselves. 
This a lgor i thm is related to, but 
takes less memory  and is less power fu l  
than a fu l l -b lown second order  Markov  
model.  Each state in a second-order  
(tr igram) Marker  model  is un iquely  
determined by the prev ious two inputs. 
For  an input vocabu lary  of 2000 words,  the 
number  of mathemat ica l ly  poss ib le states 
in a t r igram Marker  model  is 4,000,000, 
with more than 8 b i l l ion  arcs in tercon-  
necting the states. For tunate ly ,  in the 
real wor ld  most of these mathemat ica l ly  
poss ib le states and arcs do not actual ly  
occur, but a t r igram model  for the real 
wor ld  poss ib i l i t ies  is sti l l  quite large. 
We exper imented with abstract ing the 
input vocabu lary  by restr ict ing it to the 
50 h ighest - f requency  words plus the 
pseudo- input  OTHER onto  which all other  
words were mapped. When we did so, the 
number  of states and arcs in the var ious 
order  Markov  models was sti l l  fa i r ly  large 
for the real wor ld  data \[Engl ish and 
Boggess, 1986\]. As F igure 6 shows, for 
example,  the rate of growth  for a four th -  
order  abstract  Markov  model  (just the 50 
h ighest - f requency  words plus OTHER plus 
end-o f - sentence)  is in the ne ighborhood of 
250 new states and 450 new arcs per 1000 
36 
Sher r i  data  Thackeray  data  
words  new s ta tes  new arcs  new s ta tes  new arcs  
1000 527 677 639 830 
2000 469 620 624 818 
3000 471 636 476 705 
4000 399 562 467 716 
5000 397 566 463 714 
6000 391 579 437 668 
7000 337 507 389 642 
8000 311 476 370 628 
9000 323 500 361 612 
10000 285 486 384 629 
11000 329 518 348 601 
12000 278 448 331 588 
13000 276 445 310 543 
14000 240 408 291 530 
15000 248 425 287 529 
16000 244 420 290 533 
17000 243 414 269 497 
18000 259 446 234 468 
F igure 6. Growth  of abstracted four th -order  Marker  models 
new words of text,  after 17000 words of 
input. This was true for both the Sherri  
data (conversat ional  English) and the more 
formal  Thackeray  data. Moreover,  the 
four th -order  Marker  model for the 
abstracted Thackeray  data cont inued to 
grow. After 100,000 words of input,  with 
a model  of approx imate ly  22,000 states and 
approx imate ly  45,000 arcs, the rate of 
growth was sti l l  more than 1,000 states 
and 3,000 ares per 10,000 words of input. 
For  this par t icu lar  implementat ion,  
however,  neither r. fu l l -b lown Markov  
model  using tota l  vocabu lary  nor  an 
abstract  model  using the 50-word  vocabu-  
lary seemed appropr ia te .  On the one hand, 
models of the entire vocabu lary  conf i rmed 
that many mult ip le  word sequences did 
occur regular ly.  Nevertheless, for any 
but the simplest order  Marker  models 
(orders zero and one), the vast bu lk  of 
the networks  were taken by word combina-  
t ions that occurred only  once. On the 
other hand, restr ict ing the predict ive 
mechanism to on ly  the h igh- f requency  words 
obv ious ly  left out some of the regular ly  
occurr ing word combinat ions.  Our f i r s t -  
and second- fo l lower  a lgor i thm described on 
the previous pages a l lows lower f requency 
words to be predicted when they occur 
regular ly  in combinat ion  with h igh-  
f requency words.  
PREDICT IVE  CAPABIL IT IES  
The data used to test the predictive 
capabilities of the system were type- 
scripts provided by the user, who  was 
utilizing a manual typewriter; it follows 
that the results were not biased by the 
user's favoring sentence patterns that the 
system itself provided. The system had 
bccn given 1750 prior scntcnces produced 
by the user and the data collected were 
for the performance of the system over the 
next 97 sentences. The 1750 sentences 
were 14,669 words in length with a vocabu- 
lary of 1512 words. Twelve sentences of 
the 1750 were a single word in length 
{e.g. "yeah", "no" and "gesundheit") and 
51 were of length 20 or greater. Average 
length of sentence for the init ial  body 
was 8.4 words per sentence. The first 200 
sentences included t ranscr ipt ions  of ora l  
sentences, which were much shorter  on 
average, since the user is speech hand i -  
capped. If the first 200 sentences are 
omitted, the average sentence length is 
8.6 for the fo l lowing 1550 sentences. 
Of the next 97 sentences generated, 
the shortest sentence was "Thanks again." 
The longest was "You said something about  
a magazine that Jenni had about  computers  
that I might l ike to borrow."  The 97 
sentences consisted of 884 words (six of 
which were numbers in digital  form), for 
an average length of 9.1 words per 
sentence. 
37 
Of the 884 words,  350 were presented 
on the first menu, 373 were presented on 
the second menu (after one letter had been 
spelled), 109 were presented on the third 
menu (after two letters had been spelled),.  
2 were presented on the fourth  menu (after 
three letters had been spelled, 43 were 
spel led out in their entirety,  and 7 were 
numbers in digital  form, produced using 
the number  screen of the system. 
F rom the above, it is obv ious  that 
the device of predict ing the 20 most 
f requent words  by sentence pos i t ion  is 
successful 39.6 per cent of the time; 
42.2 per cent of the time, the desired 
word is among the 20 most f requent words  
of a given init ial  letter but not in the 
20 most f requent words  by posit ion;  
combin ing these two facts, we see that 
81.8 per cent of the time, this s imple 
pred ict ion scheme presents the desired 
word on a first or second selection. The 
desired word is offered in the first, 
second, or third menu 94.1 per cent of the 
time, and most of the rest of the time 
(5.7 per cent of total),  the desired word 
is unknown to the system and is "spelled 
out ' ,  where "spel l ing" includes produc ing 
numbers. 
A l though the fourth  menu, consist ing 
of words with a th ree- le t te r  init ial  
sequence, present ly  has a low success 
rate, it is precisely this category that 
we expect to see improve as more of the 
user's words  become known to the system 
through spell ing. That  is, as time 
passes, we expect the user to have to 
resort  to complete spel l ing less and less 
because the known vocabu lary  wil l  include 
more and more of the actual  vocabu lary  of 
the user. Many of the new words wil l  be 
low frequency words  that we would expect 
to find on the menu for three- le t ter  com-  
b inat ions after they are known. 
The second a lgor i thm, using f i r s t -  and 
second- fo l lowers  of the h igh- f requency  
words,  was run on i00 sentences, the 
shortest of which was "Help!" (94 of the 
97 test sentences for the first a lgor i thm 
were represented in the test set for the 
second.) There were 895 words in the 
sample, of which 448 were presented on the 
first menu, 280 were presented on the 
second (after one letter had been spel led 
out, 83 on the third (after two letters 
were spelled), 1 on the fourth,  and 83 
were spel led out in their ent i rety (this 
category included numbers). 
Running the second test gave us a 
very quick apprec ia t ion  for the value of 
adding new words to the system as they 
are encountered,  since this imp lementat ion  
of the second a lgor i thm did not. One 
especial ly  str ik ing example  was a word 
beginning with "w-o"  which had never been 
used before, but which occurred five times 
in the 100 test sentences and had to be 
spel led out each time. This was espec ia l -  
ly i r r i tat ing since the "w-o"  menu (third 
menu) had fewer than 20 entries and would  
have accommodated the new word.  A com-  
par ison of the two columns of F igure 7 
suggests that for the text held in common 
by the two tests, approx imate ly  30 words  
had to be spel led out by the second a lgo -  
r ithm, which were selected by menu in the 
first a lgor i thm because it added new words  
to its data sets as they were encountered.  
PROPOSED EXTENSIONS 
We have several plans for the future, 
most of them involv ing the second a lgo -  
r ithm. Our first task is to increase the 
number  of sentences in the Sherr i  data to 
3000 and determine how much (if at all) 
an enlarged base of exper ience improves 
the abi l i ty  of the a lgor i thm to predict  
Sentence pos i t ion  a lgor i thm 
number sentences :  97 
number of words:  884 
frequent word/ lef t  context  a lgor i thm 
number sentences :  100 
number of words:  895 
words % to ta l  
f i r s t  menu: 350 39.6% 39.6% 
second menu: 373 42.2% 81.8% 
th i rd  menu: 109 12.3% 94.1% 
four th  menu: 2 0.2% 94.3% 
spe l led :  43 4.8% 99.2% 
numbers:  7 0.8% 100% 
words % to ta l  
f i r s t  menu: 448 50% 50% 
second menu: 280 31.3% 81.3% 
th i rd  menu: 83 9.3% 90.6% 
four th  menu: 1 0.1% 90.7% 
"spe l led ' :  83 9.3% 100% 
Figure 7. Compar i son  of the predict ive capabi l i t ies.  
38 
the desired word on the first try. 
In its present form, the system is 
rel iable in its predict ions after several 
hundred sentences by the user have been 
processed. We intend to take something 
l ike the Brown corpus for American 
Engl ish and f rom it create a van i l l a -  
f lavored pred ictor  as a s ta r t -up  version 
for a new user, with faci l it ies bui lt  in 
to have the user's own language patterns 
gradual ly  outweigh the Brown corpus 
in i t ia l izat ion as they are input. 
Eventual ly  the Brown corpus would have 
essential ly no effect, or at least no 
effect overr id ing the user's indiv idual  
use of language (it might serve as a 
basic d ic t ionary  for text vocabu lary  not 
yet seen f rom the user). 
We intend to investigate what effect 
generat ing sentences whi le using the 
system has on our co l laborator .  To date, 
she has ob l ig ing ly  been wi l l ing to 
cont inue to use a typewr i ter  to generate 
text, but she does own a personal  computer  
and is able to use a mouse. Our own 
exper ience in enter ing her sentences on 
the system has made it clear that in many 
instances she would have expressed the 
same ideas more rap id ly  on the system with 
a sl ight change in wording.  Since the 
prefer red words and patterns are derived 
by the system from her own language 
history,  they should feel normal  and 
natural  to her and could inf luence her to 
modi fy  her intent ions in generat ing a 
sentence. On the other hand, a dif ferent 
handicapped indiv idual  (a quadr ip legic)  
has informed us that ease of mechanical  
p roduct ion  of a sentence has l itt le or no 
effect on his choice of words,  and that 
would appear  to be the case for our  
co l laborator  whi le she uses the 
typewriter .  
F inal ly ,  we wish to make use of the 
much larger amounts  of memory avai lable 
on personal  computers  by taking account of 
the fo l lowers  for many of the moderate -  
f requency words. For  example,  in the 
sentence "would you be able..." the word 
"able" is not high frequency. Never the-  
less, the system could easi ly deduce what 
fo l lowing word to expect,  since every 
known occurrence of "able" is fo l lowed by 
"to'.  As it happens,  "to" is one of the 
top 20 most frequent words and hence 
for tu i tous ly  is on the default  menu after 
the non-h igh- f requency  word "able' ,  but 
there are many other examples where the 
system is not so lucky. For  instance, 
"pick" is usual ly fo l lowed by "up" in the 
Sherri  data, but "pick" is low frequency 
and "up" is not on the default  first menu. 
S imi lar ly ,  "think" is a h igh- f requency  
word and has a well  developed set of 
fo l lowers.  "Thinks" and "thought" are not 
h igh- f requency  and hence are fo l lowed by 
the default  first menu. Yet v i r tua l ly  
every fo l lower  for "thinks" and "thought" 
in the Sherri  data happens to belong to 
the set of fo l lowers  for "think' .  We 
believe that by stor ing in format ion  on 
moderate f requency words with s t rong ly  
associated fo l lowers  and on clusters of 
verb forms we may s igni f icant ly improve 
the success of the first menu. 
RELATED WORK 
That a small  number  of words account 
for a large propor t ion  of the tota l  ver -  
biage in conversat ion has been known for 
some time \[Kucera and Francis,  1967\]. 
The idea of using the first several 
letters typed by a handicapped indiv idual  
to ant ic ipate the next desired word has 
been used in numerous systems (e.g., 
\ [Gib let  and Chi ldress,  1982\], \ [P icket ing 
et al., 1984\]). The Gib ler  and Chi ldress 
system is typical  in that it uses a few-  
thousand-word  vocabu lary  drawn from the 
general publ ic,  plus a few hundred words 
specific to the user of the system. The 
user must type the first two letters 
before the system provides a menu of 
words beginning with the letter pair. If 
the desired word was not on the menu, the 
user had to spell  the word out. It was 
felt that one letter was not in format ive 
enough to warrant  a menu. Fur thermore ,  
Gi lb ler  and Chi ldress showed that increas -  
ing the system vocabu lary  degraded the 
per formance of their system and they 
recommended l imitat ion of the vocabu lary  
for human factors reasons. 
By contrast,  our  system costs the 
user no more effort  in terms of selecting 
the first two letters - if indeed they 
have needed to go that far; 80 per cent 
of the time, they haven't  needed to pro -  
vide two letters. Further,  there is no 
quest ion that for our  system, a l lowing the 
vocabu lary  to grow is of benefit  both to 
system per formance and to user sat i s -  
faction. 
Gal l iers  \[1987\] describes a dif ferent 
approach  for phys ica l ly  handicapped 
39 
persons conversant in the Bliss communi -  
cations system. Communicat ion with Bliss 
involves a high degree of interpretat ion 
by the "l istener' ,  and Gal l iers reports  an 
impressive 75 per cent success rate in 
automating such interpretat ion.  The 
Gal l iers system is s ingle-subject ,  as ours 
is, and it does use past h istory to 
faci l i tate interpretat ion.  It was, how-  
ever, l imited to a very small domain for 
the exper iment described. 
One statistic cited by this last paper 
was that the same text produced from the 
Bliss communicat ion,  had it been produced 
by typing into a word processing system, 
would have required three times as many 
key-press  operat ions.  Our own rat io of 
key-press  operat ions  to characters 
produced was 45 per cent for the sentence 
pos i t ion algorithm. That is, on average 
it took  45 presses of a mouse button to 
produce 100 characters. Part of the 
reason for such a high rat io has to do 
with punctuat ion,  capita l izat ion,  and 
special screens such as the number screen, 
which requires not only  the same number of 
presses of the button as there are digits, 
for example,  but addi t ional  presses of the 
button to summon the screen and quit the 
menu. But pr imar i ly  the rat io seems to 
derive from the fact that many of the 
words in any text are short - "a', "to',  
"the', "of',  "in', and "on" being examples 
from this very paragraph.  If the first 
menu does not contain a desired two- le t te r  
word,  one has to spell the first letter 
and then make a selection from the second 
menu - requir ing two presses of a button. 
By contrast,  Bliss users commonly use 
a telegraphic style of communicat ion and 
omit funct ion words altogether.  
CONCLUSION 
In summary, evidence exists that for 
a system bui lt  around a single user's 
language, a predict ion scheme that simply 
ant ic ipated f i fty or so words would on 
average be correct  about half the time. 
Limit ing such a system to on ly  the top 20 
most frequent words would give a success 
rate of about 30 per cent. However,  not 
all of the high frequency words are d is -  
tr ibuted evenly by sentence posit ion. A 
system that offers the top 20 most f re -  
quent ly  occurr ing words for each posi t ion 
of a sentence was successful about 40 per 
cent of the time on the next 97 sentences. 
Al lowing a user to reject the first set of 
words by giving the first letter of the 
desired word and offer ing the 20 most 
frequent words beginning with that letter 
resulted in success for the combined first 
and second menus 82 per cent of the time. 
After a training body of 1750 sen-  
tences (14,669 words), with a vocabulary 
of 1512 words, it was stil l the case that 
about six per cent of the desired words 
were unknown to the system. 
An alternat ive a lgor i thm for the first 
offer ing of 20 words, based pr imar i ly  on 
the right hand contexts of the high f re -  
quency words, is successful on the first 
guess 50 per cent of the time. 
REFERENCES 
Boggess, Lois and Thomas M. English, The 
HWYE speech recogni t ion system: a user -  
specific model for expectat ion -based  
recognit ion,  in Proceedings of the 25th 
Southeast Regional  Conference of the 
ACM, Birmingham, 1987. 
Chow, C. L. A mouse-dr iven menu-based 
text prosthesis for the speech 
handicapped, M.C.S. project  report ,  
Mississippi State University, 1986. 
English, T. M. and Lois Boggess, A gram-  
matical approach to reducing the stat is -  
tical sparsity of language models in nat -  
ural domains, Proceedings of the In ter -  
nat ional  Conference on Acoustics, Speech, 
and Signal Processing, Tokyo ,  1986. 
Gall iers, Jul ia, AI for special needs - 
an "intel l igent" communicat ion aid for 
Bliss users, Appl ied Ar t i f i c ia l  
Intel l igence, 1(1):77-86, 1987. 
Gibler,  D. C. and D. S. Childress, Lan-  
guage ant ic ipat ion with a computer  based 
scanning aid, Proceedings of the IEEE 
Computer  Workshop on Computers  
to Aid the Handicavoed,  1982. 
Kucera, H. and W. N. Francis, Computa-  
t ional  analysis of p resent -day  American 
English. Brown University Press, 1967. 
Pickering, J., J. L. Arnott,  J. G. Wolff ,  
and A. L. Swiff in, Predict ion and adap-  
tat ion in a communicat ion aid for the 
disabled, Proceedings of the IFIP 
Conference on Human-Computer  
Interact ion,  London,  1984. 
Wei, Jan -Soong,  Fi le organizat ion  of 
Sherr i  System, M.C.S. project  report ,  
Mississippi State University, 1987. 
40 
