Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718–726,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
Unsupervised Tokenization for Machine Translation
Tagyoung Chung and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Training a statistical machine translation
starts with tokenizing a parallel corpus.
Some languages such as Chinese do not in-
corporate spacing in their writing system,
which creates a challenge for tokenization.
Moreover, morphologically rich languages
such as Korean present an even bigger
challenge, since optimal token boundaries
for machine translation in these languages
are often unclear. Both rule-based solu-
tions and statistical solutions are currently
used. In this paper, we present unsuper-
vised methods to solve tokenization prob-
lem. Our methods incorporate informa-
tion available from parallel corpus to de-
termine a good tokenization for machine
translation.
1 Introduction
Tokenizing a parallel corpus is usually the first
step of training a statistical machine translation
system. With languages such as Chinese, which
has no spaces in its writing system, the main chal-
lenge is to segment sentences into appropriate to-
kens. With languages such as Korean and Hun-
garian, although the writing systems of both lan-
guages incorporate spaces between “words”, the
granularity is too coarse compared with languages
such as English. A single word in these lan-
guages is composed of several morphemes, which
often correspond to separate words in English.
These languages also form compound nouns more
freely. Ideally, we want to find segmentations for
source and target languages that create a one-to-
one mapping of words. However, this is not al-
ways straightforward for two major reasons. First,
what the optimal tokenization for machine trans-
lation should be is not always clear. Zhang et al.
(2008b) and Chang et al. (2008) show that get-
ting the tokenization of one of the languages in
the corpus close to a gold standard does not nec-
essarily help with building better machine trans-
lation systems. Second, even statistical methods
require hand-annotated training data, which means
that in resource-poor languages, good tokenization
is hard to achieve.
In this paper, we explore unsupervised methods
for tokenization, with the goal of automatically
finding an appropriate tokenization for machine
translation. We compare methods that have ac-
cess to parallel corpora to methods that are trained
solely using data from the source language. Unsu-
pervised monolingual segmentation has been stud-
ied as a model of language acquisition (Goldwater
et al., 2006), and as model of learning morphol-
ogy in European languages (Goldsmith, 2001).
Unsupervised segmentation using bilingual data
has been attempted for finding new translation
pairs (Kikui and Yamamoto, 2002), and for finding
good segmentation for Chinese in machine trans-
lation using Gibbs sampling (Xu et al., 2008). In
this paper, further investigate the use of bilingual
information to find tokenizations tailored for ma-
chine translation. We find a benefit not only for
segmentation of languages with no space in the
writing system (such as Chinese), but also for the
smaller-scale tokenization problem of normaliz-
ing between languages that include more or less
information in a “word” as defined by the writ-
ing system, using Korean-English for our exper-
iments. Here too, we find a benefit from using
bilingual information, with unsupervised segmen-
tation rivaling and in some cases surpassing su-
pervised segmentation. On the modeling side,
we use dynamic programming-based variational
Bayes, making Gibbs sampling unnecessary. We
also develop and compare various factors in the
model to control the length of the tokens learned,
and find a benefit from adjusting these parame-
ters directly to optimize the end-to-end translation
quality.
718
2 Tokenization
Tokenization is breaking down text into lexemes
— a unit of morphological analysis. For relatively
isolating languages such as English and Chinese, a
word generally equals a single token, which is usu-
ally a clearly identifiable unit. English, especially,
incorporates spaces between words in its writing
system, which makes tokenization in English usu-
ally trivial. The Chinese writing system does not
have spaces between words, but there is less am-
biguity where word boundaries lie in a given sen-
tence compared to more agglutinative languages.
In languages such as Hungarian, Japanese, and
Korean, what constitutes an optimal token bound-
ary is more ambiguous. While two tokens are usu-
ally considered two separate words in English, this
may be not be the case in agglutinative languages.
Although what is considered a single morpholog-
ical unit is different from language to language,
if someone were given a task to align words be-
tween two languages, it is desirable to have one-
to-one token mapping between two languages in
order to have the optimal problem space. For ma-
chine translation, one token should not necessarily
correspond to one morphological unit, but rather
should reflect the morphological units and writing
system of the other language involved in transla-
tion.
For example, consider a Korean “word” meok-
eoss-da, which means ate. It is written as a sin-
gle word in Korean but consists of three mor-
phemes eat-past-indicative. If one uses morpho-
logical analysis as the basis for Korean tokeniza-
tion, meok-eoss-da would be split into three to-
kens, which is not desirable if we are translat-
ing Korean to English, since English does not
have these morphological counterparts. However,
a Hungarian word szekr´enyemben, which means in
my closet, consists of three morphemes closet-my-
inessive that are distinct words in English. In this
case, we do want our tokenizer to split this “word”
into three morphemes szekr´eny em ben.
In this paper, we use segmentation and to-
kenization interchangeably as blanket terms to
cover the two different problems we have pre-
sented here. The problem of segmenting Chinese
sentences into words and the problem of segment-
ing Korean or Hungarian “words” into tokens of
right granularity are different in their nature. How-
ever, our models presented in section 3 handle the
both problems.
3 Models
We present two different methods for unsuper-
vised tokenization. Both are essentially unigram
tokenization models. In the first method, we try
learning tokenization from word alignments with
a model that bears resemblance to Hidden Markov
models. We use IBMModel 1 (Brown et al., 1993)
for the word alignment model. The second model
is a relatively simpler monolingual tokenization
model based on counts of substrings which serves
as a baseline of unsupervised tokenization.
3.1 Learning tokenization from alignment
We use expectation maximization as our primary
tools in learning tokenization form parallel text.
Here, the observed data provided to the algorithm
are the tokenized English string e
n
1
and the unto-
kenized string of foreign characters c
m
1
. The un-
observed variables are both the word-level align-
ments between the two strings, and the tokeniza-
tion of the foreign string. We represent the tok-
enization with a string s
m
1
of binary variables, with
s
i
= 1 indicating that the ith character is the final
character in a word. The string of foreign words
f
?
1
can be thought of as the result of applying the
tokenization s to the character string c:
f = s ? c where ? =
m
?
i=1
s
i
We use IBM Model 1 as our word-level align-
ment model, following its assumptions that each
foreign word is generated independently from one
English word:
P (f |e) =
?
a
P (f ,a | e)
=
?
a
?
i
P (f
i
| e
a
i
)P (a)
=
?
i
?
j
P (f
i
| e
j
)P (a
i
= j)
and that all word-level alignments a are equally
likely: P (a) =
1
n
for all positions. While Model 1
has a simple EM update rule to compute posteri-
ors for the alignment variables a and from them
learn the lexical translation parameters P (f | e),
we cannot apply it directly here because f itself is
unknown, and ranges over an exponential number
of possibilities depending on the hidden segmenta-
tion s. This can be addressed by applying dynamic
programing over the sequence s. We compute the
719
posterior probability of a word beginning at posi-
tion i, ending at position j, and being generated by
English word k:
P (s
i...j
= (1, 0, . . . , 0, 1), a = k | e)
=
?(i)P (f | e
k
)P (a = k)?(j)
P (c | e)
where f = c
i
. . . c
j
is the word formed by con-
catenating characters i through j, and a is a vari-
able indicating which English position generated
f . Here ? and ? are defined as:
?(i) = P (c
i
1
, s
i
= 1 | e)
?(j) = P (c
m
j+1
, s
j
= 1 | e)
These quantities resemble forward and backward
probabilities of hidden Markov models, and can
be computed with similar dynamic programming
recursions:
?(i) =
L
?
?=1
?(i? ?)
?
a
P (a)P (c
i
i??
| e
a
)
?(j) =
L
?
?=1
?
a
P (a)P (c
j+?
j
| e
a
)?(j + ?)
where L is the maximum character length for a
word.
Then, we can calculate the expected counts of
individual word pairs being aligned (c
j
i
, e
k
) by ac-
cumulating these posteriors over the data:
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)
?(m)
The M step simply normalizes the counts:
˜
P (f | e) =
ec(f, e)
?
e
ec(f, e)
Our model can be compared to a hiddenMarkov
model in the following way: a target word gen-
erates a source token which spans a zeroth order
Markov chain of characters in source sentence,
where a “transition” represents a segmentation and
a “emission” represents an alignment. The model
uses HMM-like dynamic programming to do in-
ference. For the convenience, we refer to this
model as the bilingual model in the rest of the
paper. Figure 1 illustrates our first model with
an small example. Under this model we are not
learning segmentation directly, but rather we are
learning alignments between two sentences. The
c
1
c
2
c
3
c
4
f
1
f
2
e
1
e
2
Figure 1: The figure shows a source sentence
f = f
1
, f
2
= s ? c
1
. . . c
4
where s = (0, 0, 1, 1)
and a target sentence e = e
1
, e
2
. There is a seg-
mentation between c
3
and c
4
; thus c
1
, c
2
, c
3
form
f
1
and c
3
forms f
2
. f
1
is generated by e
2
and f
2
is
generated by e
1
.
segmentation is by-product of learning the align-
ment. We can find the optimal segmentation of
a new source language sentence using the Viterbi
algorithm. Given two sentences e and f ,
a
?
= argmax
a
P (f ,a | e)
and segmentation s
?
implied by alignment a
?
is
the optimal segmentation of f found by this model.
3.2 Learning tokenization from substring
counts
The second tokenization model we propose is
much simpler. More sophisticated unsupervised
monolingual tokenization models using hierarchi-
cal Bayesian models (Goldwater et al., 2006)
and using the minimum description length prin-
ciple (Goldsmith, 2001; de Marcken, 1996) have
been studied. Our model is meant to serve as
a computationally efficient baseline for unsuper-
vised monolingual tokenization. Given a corpus
of only source language of unknown tokenization,
we want to find the optimal s given c — s that
gives us the highest P (s | c). According to Bayes’
rule,
P (s | c) ? P (c | s)P (s)
Again, we assume that all P (s) are equally likely.
Let f = s?c = f
1
. . . f
?
, where f
i
is a word under
some possible segmentation s. We want to find the
s that maximizes P (f). We assume that
P (f) = P (f
1
)× . . .× P (f
?
)
To calculate P (f
i
), we count every possible
720
substring — every possible segmentation of char-
acters — from the sentences. We assume that
P (f
i
) =
count(f
i
)
?
k
count(f
k
)
We can compute these counts by making a sin-
gle pass through the corpus. As in the bilingual
model, we limit the maximum size of f for prac-
tical reasons and to prevent our model from learn-
ing unnecessarily long f . With P (f), given a se-
quence of characters c, we can calculate the most
likely segmentation using the Viterbi algorithm.
s
?
= argmax
s
P (f)
Our rationale for this model is that if a span of
characters f = c
i
. . . c
j
is an independent token, it
will occur often enough in different contexts that
such a span of characters will have higher prob-
ability than other spans of characters that are not
meaningful. For the rest of the paper, this model
will be referred to as the monolingual model.
3.3 Tokenizing new data
Since the monolingual tokenization only uses in-
formation from a monolingual corpus, tokenizing
new data is not a problem. However, with the
bilingual model, we are learning P (f | e). We are
relying on information available from e to get the
best tokenization for f. However, the parallel sen-
tences will not be available for new data we want
to translate. Therefore, for the new data, we have
to rely only on P (f) to tokenize any new data,
which can be obtained by calculating
P (f) =
?
e
P (f | e)P (e)
With P (f) from the bilingual model, we can run
the Viterbi algorithm in the same manner as mono-
lingual tokenization model for monolingual data.
We hypothesize that we can learn valuable infor-
mation on which token boundaries are preferable
in language f when creating a statistical machine
translation system that translates from language f
to language e.
4 Preventing overfitting
We introduce two more refinements to our word-
alignment induced tokenization model and mono-
lingual tokenization model. Since we are consid-
ering every possible token f that can be guessed
from our corpus, the data is very sparse. For the
bilingual model, we are also using the EM algo-
rithm to learn P (f | e), which means there is a
danger of the EM algorithm memorizing the train-
ing data and thereby overfitting. We put a Dirichlet
prior on our multinomial parameter for P (f | e)
to control this situation. For both models, we also
want a way to control the distribution of token
length after tokenization. We address this problem
by adding a length factor to our models.
4.1 Variational Bayes
Beal (2003) and Johnson (2007) describe vari-
ational Bayes for hidden Markov model in de-
tail, which can be directly applied to our bilingual
model. With this Bayesian extension, the emission
probability of our first model can be summarized
as follows:
?
e
| ? ? Dir(?),
f
i
| e
i
= e ? Multi(?
e
).
Johnson (2007) and Zhang et al. (2008a) show
having small ? helps to control overfitting. Fol-
lowing this, we set our Dirichlet prior to be as
sparse as possible. It is set at ? = 10
?6
, the num-
ber we used as floor of our probability.
For the model incorporating the length factor,
which is described in the next section, we do not
place a prior on our transition probability, since
there are only two possible states, i.e. P (s = 1)
and P (s = 0). This distribution is not as sparse as
the emission probability.
Comparing variational Bayes to the traditional
EM algorithm, the E step stays the same but the
M step for calculating the emission probability
changes as follows:
˜
P (f | e) =
exp(?(ec(f, e) + ?))
exp(?(
?
e
ec(f, e) + s?))
where ? is the digamma function, and s is the size
of the vocabulary from which f is drawn. Since
we do not accurately know s, we set s to be the
number of all possible tokens. As can be seen from
the equation, by setting ? to a small value, we are
discounting the expected count with help of the
digamma function. Thus, having lower ? leads to
a sparser solution.
4.2 Token length
We now add a parameter that can adjust the to-
kenizer’s preference for longer or shorter tokens.
721
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.55
lambda=3.16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.58
lambda=2.13
Figure 2: Distribution of token length for (from left to right) Chinese, and Korean. “ref” is the empirical
distribution from supervised tokenization. Two length factors — ?
1
and ?
2
are also shown. For ?
1
, the
parameter to geometric distribution P (s) is set to the value learned from our bilingual model. For ?
2
, ?
is set using the criterion described in the experiment section.
This parameter is beneficial because we want our
distribution of token length after tokenization to
resemble the real distribution of token length. This
parameter is also useful because we also want to
incorporate information on the number of tokens
in the other language in the parallel corpus. This is
based on the assumption that, if tokenization cre-
ates a one-to-one mapping, the number of tokens
in both languages should be roughly the same. We
can force the two languages to have about the same
number of tokens by adjusting this parameter. The
third reason is to further control overfitting. Our
observation is that certain morphemes are very
common, such that they will be always observed
attached to other morphemes. For example, in Ko-
rean, a noun attached with nominative case marker
is very common. Our model is likely to learn a
noun attached with the morpheme — nominative
case marker — rather than noun itself. This is not
desirable when the noun occurs with less common
morphemes; in these cases the morpheme will be
split off creating inconsistencies.
We have experimented with two different length
factors, each with one adjustable parameter:
?
1
(?) = P (s)(1? P (s))
??1
?
2
(?) = 2
??
?
The first, ?
1
, is the geometric distribution, where
l is length of a token and P (s) is probability of
segmentation between two characters. The second
length factor ?
2
was acquired through several ex-
periments and was found to work well. As can
been seen from Figure 2, the second factor dis-
counts longer tokens more heavily than the geo-
metric distribution. We can adjust the value of ?
and P (s) to increase or decrease number of tokens
after segmentation.
For our monolingual model, incorporating these
factors is straightforward. We assume that
P (f) ? P (f
1
)?(?
1
)× . . .× P (f
n
)?(?
n
)
where ?
i
is the length of f
i
. Then, we use the same
Viterbi algorithm to select the f
1
. . . f
n
that max-
imizes P (f), thereby selecting the optimal s ac-
cording to our monolingual model with a length
factor. We pick the value of ? and P (s) that
produces about the same number of tokens in the
source side as in the target side, thereby incorpo-
rating some information about the target language.
For our bilingual model, we modify our model
slightly to incorporate ?
1
, creating a hybrid
model. Now, our forward probability of forward-
backward algorithm is:
?(i) =
L
?
?=1
?(i? l)?
1
(?)
?
a
P (a)P (c
i
i??
| e
a
)
and the expected count of (c
j
i
, e
k
) is
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)?
1
(j ? i)
?(m)
For ?
1
, we can learn P (s) for the geometric dis-
tribution from the model itself:
1
P (s) =
1
m
m
?
i
?(i)?(i)
?(m)
1
The equation is for one sentence, but in practice, we sum
over all sentences in the training data to calculate P (s).
722
We can also fix P (s) instead of learning it through
EM. We incorporate ?
2
into the bilingual model
as follows: after learning P (f) from the bilingual
model, we pick the ? in the same manner as the
monolingual model and run the Viterbi algorithm.
After applying the length factor, what we have
is a log-linear model for tokenization, with two
feature functions with equal weights: the length
factor and P (f) learned from model.
5 Experiments
5.1 Data
We tested our tokenization methods on two differ-
ent language pairs: Chinese-English, and Korean-
English. For Chinese-English, we used FBIS
newswire data. The Korean-English parallel data
was collected from news websites and sentence-
aligned using two different tools described by
Moore (2002) and Melamed (1999). We used sub-
sets of each parallel corpus consisting of about 2M
words and 60K sentences on the English side. For
our development set and test set, Chinese-English
had about 1000 sentences each with 10 reference
translations taken from the NIST 2002 MT eval-
uation. For Korean-English, 2200 sentence pairs
were randomly sampled from the parallel corpus,
and held out from the training data. These were
divided in half and used for test set and develop-
ment set respectively. For all language pairs, very
minimal tokenization — splitting off punctuation
— was done on the English side.
5.2 Experimental setup
We used Moses (Koehn et al., 2007) to train
machine translation systems. Default parameters
were used for all experiments except for the num-
ber of iterations for GIZA++ (Och and Ney, 2003).
GIZA++ was run until the perplexity on develop-
ment set stopped decreasing. For practical rea-
sons, the maximum size of a token was set at three
for Chinese, and four for Korean.
2
Minimum error
rate training (Och, 2003) was run on each system
afterwards and BLEU score (Papineni et al., 2002)
was calculated on the test sets.
For the monolingual model, we tested two ver-
sions with the length factor ?
1
, and ?
2
. We picked
? and P (s) so that the number of tokens on source
side (Chinese, and Korean) will be about the same
2
In the Korean writing system, one character is actually
one syllable block. We do not decompose syllable blocks
into individual consonants and vowels.
as the number of tokens in the target side (En-
glish).
For the bilingual model, as explained in the
model section, we are learning P (f | e), but only
P (f) is available for tokenizing any new data. We
compared two conditions: using only the source
data to tokenize the source language training data
according to P (f) (which is consistent with the
conditions at test time), and using both the source
and English data to tokenize the source language
training data (which might produce better tok-
enization by using more information). For the first
length factor ?
1
, we ran an experiment where the
model learns P (s) as described in the model sec-
tion, and we also had experiments where P (s)was
pre-set at 0.9, 0.7, 0.5, and 0.3 for comparison. We
also ran an experiment with the second length fac-
tor ?
2
where ? was picked as the same manner as
the monolingual model.
We varied tokenization of development set and
test set to match the training data for each ex-
periment. However, as we have implied in the
previous paragraph, in the one experiment where
P (f | e) was used to segment training data, di-
rectly incorporating information from target cor-
pus, tokenization for test and development set is
not exactly consistent with tokenization of train-
ing corpus. Since we assume only source corpus
is available at the test time, the test and the devel-
opment set was tokenized only using information
from P (f).
We also trained MT systems using supervised
tokenizations and tokenization requiring a mini-
mal effort for the each language pair. For Chinese-
English, the minimal effort tokenization is maxi-
mal tokenization where every Chinese character is
segmented. Since a number of Chinese tokeniz-
ers are available, we have tried four different to-
kenizations for the supervised tokenizations. The
first one is the LDC Chinese tokenizer available at
the LDC website
3
, which is compiled by Zhibiao
Wu. The second tokenizer is a maxent-based to-
kenizer described by Xue (2003). The third and
fourth tokenizations come from the CRF-based
Stanford Chinese segmenter described by Chang
et al. (2008). The difference between third and
fourth tokenization comes from the different gold
standard, the third one is based on Beijing Uni-
versity’s segmentation (pku) and the fourth one is
based on Chinese Treebank (ctb). For Korean-
3
http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
723
Chinese Korean
BLEU F-score BLEU
Supervised
Rule-based morphological analyzer 7.27
LDC segmenter 20.03 0.94
Xue’s segmenter 23.02 0.96
Stanford segmenter (pku) 21.69 0.96
Stanford segmenter (ctb) 22.45 1.00
Unsupervised
Splitting punctuation only 6.04
Maximal (Character-based MT) 20.32 0.75
Bilingual P (f | e) with ?
1
P (s) = learned 19.25 6.93
Bilingual P (f) with ?
1
P (s) = learned 20.04 0.80 7.06
Bilingual P (f) with ?
1
P (s) = 0.9 20.75 0.87 7.46
Bilingual P (f) with ?
1
P (s) = 0.7 20.59 0.81 7.31
Bilingual P (f) with ?
1
P (s) = 0.5 19.68 0.80 7.18
Bilingual P (f) with ?
1
P (s) = 0.3 20.02 0.79 7.38
Bilingual P (f) with ?
2
22.31 0.88 7.35
Monolingual P (f) with ?
1
20.93 0.83 6.76
Monolingual P (f) with ?
2
20.72 0.85 7.02
Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of seg-
mentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted.
English, the minimal effort tokenization splitting
off punctuation and otherwise respecting the spac-
ing in the Korean writing system. A Korean mor-
phological analysis tool
4
was used to create the su-
pervised tokenization.
For Chinese-English, since a gold standard for
Chinese segmentation is available, we ran an addi-
tional evaluation of tokenization from each meth-
ods we have tested. We tokenized the raw text
of Chinese Treebank (Xia et al., 2000) using all
of the methods (supervised/unsupervised) we have
described in this section except for the bilingual
tokenization using P (f | e) because the English
translation of the Chinese Treebank data was not
available. We compared the result against the gold
standard segmentation and calculated the F-score.
6 Results
Results from Chinese-English and Korean-English
experiments are presented in Table 1. Note that
nature of data and number of references are dif-
ferent for the two language pairs, and therefore
the BLEU scores are not comparable. For both
language pairs, our models perform equally well
as supervised baselines, or even better. We can
4
http://nlp.kookmin.ac.kr/HAM/eng/main-e.html
observe three things from the result. First, tok-
enization of training data using P (f | e) tested on
a test set tokenized with P (f) performed worse
than any other experiments. This affirms our be-
lief that consistency in tokenization is important
for machine translation, which was alsomentioned
by Chang et al. (2008). Secondly, we are learning
valuable information by looking at the target lan-
guage. Compare the result of the bilingual model
with ?
2
as the length factor to the result of the
monolingual model with the same length factor.
The bilingual version consistently performed bet-
ter than the monolingual model in all language
pairs. This tells us we can learn better token
boundaries by using information from the target
language. Thirdly, our hypothesis on the need
for heavy discount for longer tokens is confirmed.
The value for P (s) learned by the model was 0.55,
and 0.58 for Chinese, and Korean respectively. For
both language pairs, this accurately reflects the
empirical distribution of token length, as can be
seen in Figure 2. However, experiments where
P (s) was directly optimized performed better, in-
dicating that this parameter should be optimized
within the context of a complete system. The sec-
ond length factor ?
2
, which discounts longer to-
kens even more heavily, generally performed bet-
724
English the two presidents will hold a joint press conference at the end of their summit talks .
Untokenized Korean ??????????????????????????????? .
Supervised ???? ??? ??? ???????? ?? ????? ????? ? ?? .
Bilingual P (f | e) with ?
1
??????? ?????????? ??????? ?????? ? .
Bilingual P (f) with ?
2
???? ??? ?????????? ??????? ????? ?? .
Monolingual P (f) with ?
1
??? ? ??? ?????????? ????????????? ? .
Monolingual P (f) with ?
2
???? ??? ?????????? ???????????? ?? .
Figure 3: Sample tokenization results for Korean-English data. The underscores are added to clearly
visualize where the breaks are.
ter than the first length factor when used in con-
junction with the bilingual model. Lastly, F-scores
of Chinese segmentations compared against the
gold standard shows higher segmentation accuracy
does not necessarily lead to higher BLEU score.
F-scores presented in Table 1 are not directly com-
parable for all different experiments because the
test data (Chinese Treebank) is used in training for
some of the supervised segmenters, but these num-
bers do show how close unsupervised segmenta-
tions are to the gold standard. It is interesting to
note that our highest unsupervised segmentation
result does make use of bilingual information.
Sample tokenization results for Korean-English
experiments are presented in Figure 3. We observe
that different configurations produce different tok-
enizations, and the bilingual model produced gen-
erally better tokenizations for translation com-
pared to the monolingual models or the super-
vised tokenizer. In this example, the tokenization
obtained from the supervised tokenizer, although
morphologically correct, is too fine-grained for the
purpose of translation to English. For example,
it correctly tokenized the attributive suffix ? -n
however, this is not desirable since English has no
such counterpart. Both variations of the monolin-
gual tokenization have errors such as incorrectly
not segmenting ??? gyeol-gwa-reul, which is
a compound of a noun and a case marker, into?
? ? gyeol-gwa reul as the bilingual model was
able to do.
6.1 Conclusion and future work
We have shown that unsupervised tokenization for
machine translation is feasible and can outperform
rule-based methods that rely on lexical analysis,
or supervised statistical segmentations. The ap-
proach can be applied both to morphological anal-
ysis of Korean and the segmentation of sentences
into words for Chinese, which may at first glace
appear to be quite different problems. We have
only shown how our methods can be applied to
one language of the pair, where one language is
generally isolating and the other is generally syn-
thetic. However, our methods could be extended
to tokenization for both languages by iterating be-
tween languages. We also used the most simple
word-alignment model, but more complex word
alignment models could be incorporated into our
bilingual model.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 224–232.
Carl de Marcken. 1996. Linguistic structure as compo-
sition and perturbation. In Meeting of the Associa-
tion for Computational Linguistics, pages 335–341.
Morgan Kaufmann Publishers.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the International Conference on Computational Lin-
guistics/Association for Computational Linguistics
(COLING/ACL-06), pages 673–680.
725
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In 2007 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 296–305, Prague, Czech Republic,
June. Association for Computational Linguistics.
Genichiro Kikui and Hirofumi Yamamoto. 2002.
Finding translation pairs from english-japanese un-
tokenized aligned corpora. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02) workshop on
Speech-to-speech translation: algorithms and sys-
tems, pages 23–30. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-07), Demonstration Session, pages 177–
180.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25:107–130.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In AMTA ’02: Pro-
ceedings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, pages
135–144, London, UK. Springer-Verlag.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Conference of the Association for
Computational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Shizhe Huang, Tony
Kroch, and Mitch Marcus. 2000. Developing
Guidelines and Ensuring Consistency for Chinese
Text Annotation. In Proc. of the 2nd International
Conference on Language Resources and Evaluation
(LREC-2000), Athens, Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1017–1024, Manchester, UK, August.
Coling 2008 Organizing Committee.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, volume 8, pages 29–48.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 97–105, Columbus, Ohio.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216–223.
726
