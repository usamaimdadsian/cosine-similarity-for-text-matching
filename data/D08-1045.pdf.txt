Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 429–437,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Online Acquisition of Japanese Unknown Morphemes
using Morphological Constraints
Yugo Murawaki Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
murawaki@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
We propose a novel lexicon acquirer that
works in concert with the morphological ana-
lyzer and has the ability to run in online mode.
Every time a sentence is analyzed, it detects
unknown morphemes, enumerates candidates
and selects the best candidates by comparing
multiple examples kept in the storage. When
a morpheme is unambiguously selected, the
lexicon acquirer updates the dictionary of the
analyzer, and it will be used in subsequent
analysis. We use the constraints of Japanese
morphology and effectively reduce the num-
ber of examples required to acquire a mor-
pheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and
improved the quality of morphological analy-
sis.
1 Introduction
Morphological analysis is the first step for most nat-
ural language processing applications. In Japanese
morphological analysis, segmentation is processed
simultaneously with the assignment of a part of
speech (POS) tag to each morpheme. Segmentation
is a nontrivial task in Japanese because it does not
delimit words by white-space.
Japanese morphological analysis has successfully
adopted dictionary-based approaches (Kurohashi et
al., 1994; Asahara and Matsumoto, 2000; Kudo et
al., 2004). In these approaches, a sentence is trans-
formed into a lattice of morphemes by searching a
pre-defined dictionary, and an optimal path in the
lattice is selected.
This area of research may be considered almost
completed, as previous studies reported the F-score
of nearly 99% (Kudo et al., 2004). When applied
to web texts, however, more errors are made due to
unknown morphemes. In previous studies, exper-
iments were performed on newspaper articles, but
web texts include slang words, informal spelling al-
ternates (Nishimura, 2003) and technical terms. For
example, the verb “???” (gugu-ru, to google) is
erroneously segmented into “??” (gugu) and “?”
(ru).
One solution to this problem is to augment the
lexicon of the morphological analyzer by extracting
unknown morphemes from texts (Mori and Nagao,
1996). In the previous method, a morpheme extrac-
tion module worked independently of the morpho-
logical analyzer and ran in off-line (batch) mode.
It is inefficient because almost all high-frequency
morphemes have already been registered to the pre-
defined dictionary. Moreover, it is inconvenient
when applied to web texts because the web corpus
is huge and diverse compared to newspaper corpora.
It is not necessarily easy to build subcorpora before
lexicon acquisition. Suppose that we want to ana-
lyze whaling-related documents. It is unnecessary
and probably harmful to acquire morphemes that are
irrelevant to the topic. A whaling-related subcorpus
should be extracted from the whole corpus but it is
not clear how large it must be.
We propose a novel lexicon acquirer that works
in concert with the morphological analyzer and has
the ability to run in online mode. As shown in Fig-
ure 1, every time a sentence is analyzed, the lexicon
acquirer detects unknown morphemes, enumerates
429
text
Analyzer
JUMAN
(morph.
analyzer)
KNP
(parser)
analysis
DetectorEnumeratorSelector
accumulated
examples
hand-crafted
dictionary
automatically constructed
dictionary
update
lookup
Lexicon Acquirer
analysis
Figure 1: System architecture
candidates and selects the best candidates by com-
paring multiple examples kept in the storage. When
a morpheme is unambiguously selected, the lexicon
acquirer updates the automatically constructed dic-
tionary, and it will be used in subsequent analysis.
The proposed method is flexible and gives the sys-
tem more control over the process. We do not have
to limit the target corpus beforehand and the system
can stop whenever appropriate.
We use the constraints of Japanese morphology
that have already been coded in the morphological
analyzer. These constraints effectively reduce the
number of examples required to acquire an unknown
morpheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and im-
proved the quality of morphological analysis.
2 Japanese Morphology
In order to understand the task of lexicon acquisi-
tion, we briefly describe the Japanese morpholog-
ical analyzer JUMAN.1 We explain Japanese mor-
phemes in Section 2.1, morphological constraints in
Section 2.2, and unknown morpheme processing in
Section 2.3.
2.1 Morpheme
In JUMAN, the POS tagset consists of four ele-
ments: class, subclass, conjugation type and con-
jugation form. The classes are noun, verb, adjec-
tive and others. Noun has subclasses such as com-
mon noun, sa-group noun, proper noun, organiza-
1
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman.html
tion, place, personal name. Verb and adjective have
no subclasses.
Verbs and adjectives among others change their
form according to the morphemes that occur after
them, which is called conjugation. Conjugable mor-
phemes are grouped by conjugation types such as
vowel verb, ra-row verb, i-type adjective and na-
type adjective. Each conjugable morpheme takes
one of conjugation forms in texts. It has an invari-
ant stem and an ending which changes according to
conjugation type and conjugation form.
In this paper, the tuple of class, subclass and con-
jugation type is referred to as a POS tag. For sim-
plicity, POS tags for nouns are called by their sub-
classes and those for verbs and adjectives by their
conjugation types.
There are two types of morphemes: abstract dic-
tionary entries, and examples or actual occurrences
in texts. An entry consists of a stem and a POS tag
while an example consists of a stem, a POS tag and
a conjugation form. For example, the entry of the
ra-row verb “??” (hashi-ru, to run) can be repre-
sented as
(“?” (hashi), ra-row verb),
and their examples “??” (hashi-ra) and “??”
(hashi-ri) as
(“?” (hashi), ra-row verb, imperfective),
and
(“?” (hashi), ra-row verb, plain continu-
ative)
respectively. As nouns do not conjugate, the entry
of the sa-group noun “??” (kibou, hope) can be
represented as
(“??” (kibou), sa-group noun)
and its sole example form is
(“??” (kibou), sa-group noun, NIL).
2.2 Morphological Constraints
Japanese is an agglutinative language. Depending
on its grammatical roles, a morpheme is followed by
a sequence of grammatical suffixes, auxiliary verbs
and particles, and the connectivity of these elements
is bound by morphological constraints. For exam-
ple, the particle “?” (wo, accusative case) can fol-
low a verb with the conjugation form of plain contin-
uative, as in “???” (hashi-ri-wo, running-ACC),
430
but it cannot follow an imperfective verb (“*???”
(*hashi-ra-wo)).
These constraints are used by JUMAN to reduce
the ambiguity. They can be also used in lexicon ac-
quisition.
2.3 Unknown Morpheme Processing
Given a sentence, JUMAN builds a lattice of mor-
phemes by searching a pre-defined dictionary, and
then selects an optimal path in the lattice. To han-
dle morphemes that cannot be found in the dictio-
nary, JUMAN enumerates unknown morpheme can-
didates using character type-based heuristics, and
adds them to the morpheme lattice. Unknown mor-
phemes are given the special POS tag “undefined,”
which is treated as noun.
Character type-based heuristics are based on the
fact that Japanese is written with several different
character types such as kanji, hiragana and katakana,
and that the choice of character types gives some
clues on morpheme boundaries. For example, a se-
quence of katakana characters are considered as an
unknown morpheme candidate, as in “????”
(guˆguru, Google) out of “?????” (guˆguru-ga,
Google-NOM). Kanji characters are segmented per
character, which is sometimes wrong but prevents
error propagation.
These heuristics are simple and effective, but far
from perfect. They cannot identify mixed-character
morphemes, verbs and adjectives correctly. For ex-
ample, the verb “???” (gugu-ru, to google) is
wrongly divided into the katakana unknown mor-
pheme ”??” (gugu) and the hiragana suffix “?”
(ru).
3 Lexicon Acquisition
3.1 Task
The task of lexicon acquisition is to generate dictio-
nary entries inductively from their examples in texts.
Since the morphological analyzer provides a basic
lexicon, the morphemes to be acquired are limited
to those unknown to the analyzer.
In order to generate an entry, its stem and POS
tag need to be identified. Determining the stem of
an example is to draw the front and rear boundaries
in a character sequence in texts which corresponds
to the stem. The POS tag is selected from the tagset
given by the morphological analyzer.
3.2 System Architecture
Figure 1 shows the system architecture. Each sen-
tence in texts is processed by the morphological an-
alyzer JUMAN and the dependency parser KNP.2
JUMAN consults a hand-crafted dictionary and an
automatically constructed dictionary. KNP is used
to form a phrasal unit called bunsetsu by chunking
morphemes.
Every time a sentence is analyzed, the lexicon
acquirer receives the analysis. It detects examples
of unknown morphemes and keeps them in storage.
When an entry is unambiguously selected, the lex-
icon acquirer updates the automatically constructed
dictionary, and it will be used in subsequent analy-
sis.
3.3 Algorithm Overview
The process of lexicon acquisition has four phases:
detection, candidate enumeration, aggregation and
selection. First the analysis is scanned to detect ex-
amples of unknown morphemes. For each exam-
ple, one or more candidates for dictionary entries are
enumerated. It is added to the storage, and multiple
examples in the storage that share the candidates are
aggregated. They are compared and the best candi-
date is selected from it.
Take the ra-row verb “???” (gugu-ru) for ex-
ample. Its example “???????” (gugu-tte-
mi-ta, to have tried to google) can be interpreted in
many ways as shown in Figure 2. Similarly, multi-
ple candidates are enumerated for another example
“?????” (gugu-ru-no-ha, to google-TOPIC). If
these examples are compared, we can see that the
ra-row verb “???” (gugu-ru) can explain them.
3.4 Suffixes
Morphological constraints are used for candidate
enumeration. Since they are coded in JUMAN, we
first transform them into a set of strings called suf-
fixes. A suffix is created by concatenating the end-
ing of a morpheme (if any) and subsequent ancillary
morphemes. Each POS tag is associated with a set
of suffixes, as shown in Table 1. This means that a
stem can be followed by one of the suffixes specified
2
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html
431
Table 1: Examples of suffixes
POS tag base form stem ending conjugation form1 suffixes
ra-row verb hashi-ru hashi
ra imperfective razu, ranaide
ri plain continuative riwo, riwomo
ru plain ru, rukawo
vowel verb akogare-ru akogare
? imperfective zu, naide
? plain continuative wo, womo
ru plain ru, rukawo
sa-group noun kibou kibou NIL wo wo, womoNIL suru suru, shitara
1 The conjugation form of a noun is substituted with the base form of its immediate
ancillary morpheme because nouns do not conjugate.
suffix
???????
google -CONT try-PAST
stem
stem
stem
suffix
suffix
[POS tags]
• ra-row verb
• wa-row verb
• ta-row verb
• ma-row verb
• vowel verb
• ta-row verb
• (EOB)
stem
Figure 2: Candidate enumeration
by its POS tag and cannot be followed by any other
suffix.
In preparation for lexicon acquisition, suffixes are
acquired from a corpus. We used a web corpus that
was compiled through the procedures proposed by
Kawahara and Kurohashi (2006). Suffixes were ex-
tracted from examples of registered morphemes and
were aggregated per POS tag.
We found that the number of suffixes did not con-
verge even in this large-scale corpus. It was because
ancillary morphemes included the wide variety of
auxiliary verbs and formal nouns. Alternatively, we
used the first five characters as a suffix. In the exper-
iments, we obtained 500 thousand unique suffixes
from 100 million pages. The number of POS tags
that corresponded to a suffix was 1.33 on average.
3.5 Unknown Morpheme Detection
The first step of lexicon acquisition is unknown mor-
pheme detection. Every time the analysis of a sen-
tence was given, the sequence of morphemes are
scanned, and suspicious points that probably repre-
sent unknown morphemes are detected.
Currently, we use the POS tag “undefined” to de-
tect unknown morphemes. For example, the exam-
ple “???????” is detected because “??”
is given “undefined.” This simple method cannot
detect unknown morphemes if they are falsely seg-
mented into combinations of registered morphemes.
We leave the comprehensive detection of unknown
morphemes to future work.
3.6 Candidate Enumeration
For each example, one or more candidates for the
dictionary entry are enumerated. Each candidate is
represented by a combination of a front boundary
and the pair of a rear boundary and a POS tag.
The search range for enumeration is based on bun-
setsu phrases, which is created by chunking mor-
phemes. The range is at most the corresponding
bunsetsu and the two immediately preceding and
succeeding bunsetsu, which we found wide enough
to contain correct candidates.
The candidates for the rear boundary and the POS
tag are enumerated by string matching of suffixes as
shown in Figure 2. If a suffix matches, the start-
ing position of the suffix becomes a candidate for
the rear boundary and the suffix is mapped to one or
more corresponding POS tags.
In addition, the candidates for the front and
rear boundaries are enumerated by scanning the se-
quence of morphemes. The boundary markers we
use are
• punctuations,
• grammatical prefixes such as “?” (go-, hon-
orific prefix), for front boundaries,
432
• grammatical suffixes such as “?” (-sama, hon-
orific title), for rear boundaries, and
• bunsetsu boundaries given by KNP.
Each rear boundary candidate whose correspond-
ing POS tag is not decided is given the special tag
“EOB” (end-of-bunsetsu). This means that no suf-
fix is attached to the candidate. Since nouns, vowel
verbs and na-type adjectives can appear in isolation,
it will be expanded to these POS tags when selecting
the best POS tag.
3.7 Aggregation of Examples
Selection of the best candidate is done by compar-
ing multiple examples. Each example is added to
the storage, and then examples that possibly repre-
sent the same entry with it are extracted from the
storage. Examples aggregated at this phase share the
front boundary but may be unrelated to the example
in question. They are pruned in the next phase.
In order to manage examples efficiently, we im-
plement a trie. The example is added to the trie for
each front boundary candidate. The key is the char-
acter sequence determined by the front boundary
and the leftmost rear boundary. To retrieve examples
that share the front boundary with it, we check every
node in the path from the root to the node where it is
stored, and collect examples stored in each node.
3.8 Selection
The best candidate is selected by identifying the
front boundary, the rear boundary and the POS tag
in this order. Starting from the rightmost front
boundary candidate, multiple rear boundary candi-
dates that share the front boundary are compared and
some are dropped. Then starting from the leftmost
surviving rear boundary candidate, the best POS tag
is selected from the examples that share the stem.
If the selected candidate satisfies simple termination
conditions, it is added to the dictionary and the ex-
amples are removed from the storage.
For each front boundary candidate, some inappro-
priate rear boundary candidates are dropped by ex-
amining the inclusion relation between the examples
of a pair of candidates. The assumption behind this
is that an appropriate candidate can interpret more
examples than incorrect ones. Let p and q be a pair
of the candidates for the rear boundary, and R
p
and
R
q
be the sets of examples for which p and q are
enumerated. If p is a prefix of q and p is the correct
stem, then R
q
must be contained in R
p
. In practice
we loosen this condition, considering possible errors
in candidate enumeration
For each stem candidate, the appropriate POS tag
is identified. Similarly to rear boundary identifica-
tion, POS identification is done by checking inclu-
sion relation.
If the POS tag is successfully disambiguated, sim-
ple termination conditions is checked to prevent the
accidental acquisition of erroneous candidates. The
first condition is that the number of unique conjuga-
tion forms that appear in the examples should be 3 or
more. If the candidate is a noun, it is substituted with
the number of the unique base forms of their imme-
diate ancillary morphemes. The second condition is
that the front boundaries of some examples are de-
cided by clear boundary markers such as punctua-
tions and the beginning of sentence. This prevents
oversegmentation. For example, the stem candidate
“*??” (*sengumi) is always enumerated for exam-
ples of “???” (Shingengumi, a historical organi-
zation) since “?” (shin-, new) is a prefix. This can-
didate is not acquired because “*??” (*sengumi)
does not occur alone and is always accompanied by
“?” (shin-). Thresholds are chosen empirically.
3.9 Decompositionality
Since a morpheme is extracted from a small num-
ber of examples, it is inherently possible that the ac-
quired morpheme actually consists of two or more
morphemes. For example, the noun phrase “??
???” (karyuu-taipu, granular type) may be ac-
quired as a morpheme before “??” (karyuu, gran-
ule) is extracted. To handle this phenomenon, it
is checked at the time of acquisition whether the
new morpheme (kairyuu) can decompose registered
morphemes (kairyuu-taipu). If found, a composite
“morpheme” is removed from the dictionary.
Currently we leave the decompositionality check
to the morphological analyzer. Possible compounds
are enumerated by string matching and temporar-
ily removed from the dictionary. Each candidate
is analyzed by the morphological analyzer and it is
checked whether the candidate is divided into a com-
bination of registered morphemes. If not, the candi-
date is restored to the dictionary.
433
Table 2: Statistical information per query
query
number of number of number of number of number of
sentences affected acquired correct examples1
sentences morphs morphs
(ratio) (precision)
???? 135,379 2,444 293 290 4
(whaling issue) (1.81%) (99.0%)
??????? 74,572 775 107 105 4
(baby hatch) (1.04%) (98.1%)
?????? 195,928 6,259 913 907 4
(JASRAC) (3.19%) (99.3%)
???? 77,962 12,012 243 238 5
(tsundere) (15.4%) (97.4%)
????? 78,922 3,037 114 107 9
(agaricus) (3.85%) (93.9%)
1 The median number of examples used for acquisition.
4 Experiments
4.1 Experimental Design
We used the default dictionary of the morphological
analyzer JUMAN as the initial lexicon. It contained
30 thousand basic morphemes. If spelling variants
were expanded and proper nouns were counted, the
total number of morphemes was 120 thousands.
We used domain-specific corpora as target texts
because efficient acquisition was expected. If target
texts shared a topic, relevant unknown morphemes
were used frequently. In the experiments, we used
search engine TSUBAKI (Shinzato et al., 2008) and
casted the search results as domain-specific corpora.
For each query, our system sequentially read pages
from the top of the result and acquired morphemes.
We terminated the acquisition at the 1000th page
and analyzed the same 1000 pages with the aug-
mented lexicon. The queries used were “???
?” (whaling issue), “???????” (baby hatch),
“??????” (JASRAC, a copyright collective),
“????” (tsundere, a slang word) and “????
?” (agaricus).
4.2 Evaluation Measures
The proposed method is evaluated by measuring the
accuracy of acquired morphemes and their contri-
bution to the improvement of morphological analy-
sis. A morpheme is considered accurate if both seg-
mentation and the POS tag are correct. Note that
segmentation is a nontrivial problem for evaluation.
In fact, the disagreement over segmentation criteria
was considered one of the main reasons for reported
errors by Nagata (1999) and Uchimoto et al. (2001).
It is difficult to judge whether a compound term
should be divided because there is no definite stan-
dard for morpheme boundaries in Japanese. For ex-
ample, “????” (minku-kujira, minke whale) can
be extracted as a single morpheme or decomposed
into “???” and “?.” While segmentation is an
open question in Japanese morphological analysis,
“correct” segmentation is not necessarily important
for applications using morphological analysis. Even
if a noun is split into two or more morphemes in
morphological analysis, they are chunked to form
a phrasal unit called bunsetsu in dependency pars-
ing, and to extract a keyword (Nakagawa and Mori,
2002).
To avoid the decompositionality problem, we
adopted manual evaluation. We analyzed the tar-
get texts with both the initial lexicon and the aug-
mented lexicon. Then we checked differences be-
tween the two analyses and extracted sentences that
were affected by the augmentation. Among these
sentences, we evaluated randomly selected 50 sen-
tences per query. We checked the accuracy of seg-
mentation and POS tagging of each “diff” block,
which is illustrated in Figure 3. The segmentation of
a block was judged correct unless morpheme bound-
aries were clearly wrong.
In the evaluation of POS tagging, we did not dis-
tinguish subclasses of noun3 such as common noun
3In the experiments, we regarded demonstrative pronouns as
434
Table 3: Examples of acquired morphemes
query examples
whaling issue ?????? (moratorium),????? (giant beaked whale),?? (bycatch)
baby hatch ??? (husband),??? (midwife),??? (to abandon),?? (to inquire)
JASRAC ??? (an organization),??? Q (a pop-rock band),?? (geek)
tsundere ??? (abbr. of Akihabara),??? (fujoshi, a slang word),??? (to be popular)
agaricus ??? (abbr. of suppliment),??? (aroma),?? (enhanced nutritional function)
Table 4: Evaluation of “diff” blocks
segmentation POS tagging
query E ? C C ? C E ? E C ? E E ? C C ? C E ? E C ? E total
whaling issue 11 45 0 2 11 45 0 2 58
baby hatch 37 12 0 3 37 12 0 3 52
JASRAC 16 23 1 12 16 23 1 12 52
tsundere 17 39 0 1 17 39 0 1 57
agaricus 22 31 0 0 22 31 0 0 53
(Legend – C: correct; E: erroneous)
???????????
)QQING KVCPFYGYKNNHKPFCNQV
?? WPFGHKPGF MCVCMCPC
? UWHHKZ? XGTDCNUWHHKZ
 ??? XGTD TCTQYXGTD
Figure 3: A “diff” block in a sentence
and proper noun. The special POS tag “undefined”
given by JUMAN was treated as noun.
4.3 Results
Table 2 summarizes statistical information per
query. The number of sentences affected by the
augmentation varied considerably (1.04%–15.4%).
The initial lexicon of the morphological analyzer
lacked morphemes that appeared frequently in some
corpora because morphological analysis had been
tested mainly with newspaper articles.
The precision of acquired morphemes was high
(97.4%–99.3%), and the number of examples used
for acquisition was as little as 4–9. These results are
astonishing considering that Mori and Nagao (1996)
ignored candidates that appeared less than 10 times
(because they were unreliable).
nouns because their morphological behaviors were the same as
those of nouns. Although demonstrative nouns are closed class
morphemes, their katakana forms such as “??” (this) were
acquired as nouns. The morphological analyzer assumed that
demonstrative pronouns were written in hiragana, e.g., “??,”
as they always are in a newspaper.
Table 3 shows some acquired morphemes. As
expected, the overwhelming majority were nouns
(93.0%–100%) and katakana morphemes (80.7%–
91.6%). Some were mixed-character morphemes
(“???” and “???Q”), which cannot be recog-
nized by character-type based heuristics, and slang
words (“???,” “??,” etc.) which did not ap-
pear in newspaper articles. Some morphemes were
spelling variants of those in the pre-defined dictio-
nary. Uncommon kanji characters were used in ba-
sic words (“???” for “???” and “??” for
“??”) and katakana was used to change nuances
(“???” for “???” and “???” for “??”).
Table 4 shows the results of manual evaluation of
“diff” blocks. The overwhelming majority of blocks
were correctly analyzed with the augmented lexicon
(E ? C and C ? C). On the other hand, adverse
effects were observed only in a few blocks (C ?
E). In conclusion, acquired morphemes improve the
quality of morphological analysis.
4.4 Error Analysis
Some short katakana morphemes oversegmented
other katakana nouns. For example, “????”
(saˆbaˆ, server) was wrongly segmented by newly-
acquired “??” (saˆ, sir) and preregistered “??”
(baˆ, bar). Neither the morphological analyzer and
the lexicon acquirer could detect this semantic mis-
match. Curiously, one example of “??” (saˆ) was
actuallly part of “????” (saˆbaˆ), which was erro-
435
 0
 200
 400
 600
 800
 1000
 0  100000  200000
 0
 8000
 16000
 24000
 32000
 40000
nu
m
. o
f a
cq
ui
re
d 
m
or
ph
em
es
nu
m
. o
f e
xa
m
pl
es
num. of sentences
acquired morphemes
stored examples
acquired morphemes in re-analysis
Figure 4: Process of online acquisition
neously segmented when extracting sentences from
HTML.
The katakana adjective “??” (i-i, good), a
spelling variant of the basic morpheme “??,” was
falsely identified as a noun because its ending “?”
was written in katakana. The morphological ana-
lyzer, and hence the lexicon acquirer, assume that
the ending of a verb or adjective is written in hi-
ragana. This assumption is reasonable for stan-
dard Japanese, but does not always hold when we
analyze web texts. In order to recognize uncon-
ventional spellings that are widely used in web
texts (Nishimura, 2003), more flexible analysis is
needed.
4.5 Discussion
It is too costly or impractical to calculate the re-
call of acquisition, or the ratio of the number of ac-
quired morphemes against the total number of un-
known morphemes because it requires human judges
to find undetected unknown morphemes from a large
amount of raw texts.
Alternatively, we examined the ratio against the
number of detected unknown morphemes. Figure 4
shows the process of online acquisition for the query
“JASRAC.” The monotonic increase of the num-
bers of acquired morphemes and stored examples
suggests that the vocabulary size did not converge.
The number of occurrences of acquired morphemes
in re-analysis was approximately the same with the
number of examples kept in the storage during ac-
quisition. This means that, in terms of frequency of
occurrence, about half of unknown morphemes were
acquired. Most unknown morphemes belong to the
“long tail” and the proposed method seems to have
seized a “head” of the long tail.
Although some previous studies emphasized cor-
rect identification of low frequency terms (Nagata,
1999; Asahara and Matsumoto, 2004), it is no longer
necessary because very large scale web texts are
available today. If a small set of texts needs to
be analyzed with high accuracy, we can incorporate
similar texts retrieved from the web, to increase the
number of examples of unknown morphemes. The
proposed method can be modified to check if un-
known morphemes detected in the initial set are ac-
quired and to terminate whenever sufficient acquisi-
tion coverage is achieved.
5 Related Work
Since most languages delimit words by white-space,
morphological analysis in these languages is to seg-
ment words into morphemes. For example, Mor-
pho Challenge 2007 (Kurimo et al., 2007) was eval-
uations of unsupervised segmentation for English,
Finnish, German and Turkish.
While Japanese is an agglutinative language,
other non-segmented languages such as Chinese and
Thai are analytic languages. Among them, Chinese
has been a subject of intensive research. Peng et
al. (2004) integrated new word detection into word
segmentation. They detected new words by comput-
ing segment confidence and re-analyzed the inputs
with detected words as features.
The Japanese language is unique in that it is writ-
ten with several different character types. Heuris-
tics widely used in unknown morpheme process-
ing are based on character types. They were also
used as important clues in statistical methods. Na-
gata (1999) integrated a probabilistic unknown word
models into the word segmentation model. Uchi-
moto et al. (2001) incorporated them as feature func-
tions of a Maximum Entropy-based morphological
analyzer. Asahara and Matsumoto (2004) used them
as a feature of character-based chunking of unknown
words using Support Vector Machines.
Mori (1996) extracted words from texts and esti-
mated their POSs using distributional analysis. The
appropriateness of a word candidate was measured
436
by the distance between probability distributions of
the candidate and a model. In this method, mor-
phological constraints were indirectly represented
by distributions.
Nakagawa and Matsumoto (2006) presented a
method for guessing POS tags of pre-segmented un-
known words that took into consideration all the oc-
currences of each unknown word in a document.
This setting is impractical in Japanese because POS
tagging is inseparable from segmentation.
6 Conclusion
We propose a novel method that augments the lexi-
con of a Japanese morphological analyzer by acquir-
ing unknown morphemes from texts in online mode.
Unknown morphemes are acquired with high accu-
racy and improve the quality of morphological anal-
ysis.
Unknown morphemes are one of the main sources
of error in morphological analysis when we analyze
web texts. The proposed method has the potential
to overcome the unknown morpheme problem, but
it cannot be achieved without recognizing or being
robust over various phenomena such as unconven-
tional spellings and typos. These phenomena are not
observed in newspaper articles but cannot be ignored
in web texts. In the future, we will work on these
phenomena.
Morphological analysis is now very mature. It
is widely applied as preprocessing for NLP appli-
cations such as parsing and information retrieval.
Hence in the future, we aim to use the proposed
method to improve the quality of these applications.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Procs. of COLING 2000, pages 21–27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Procs. of COLING 2004, pages
459–465.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Procs. of LREC-06, pages
1344–1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Procs. of EMNLP 2004,
pages 230–237.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop, pages
19–21.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Procs. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22–38.
Shinsuke Mori and Makoto Nagao. 1996. Word extrac-
tion from corpora and its part-of-speech estimation us-
ing distributional analysis. In Procs. of COLING 1996,
pages 1119–1122.
Masaaki Nagata. 1999. A part of speech estimation
method for Japanese unknown words using a statistical
model of morphology and context. In Procs. of ACL
1999, pages 277–284.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Procs. of COLING-ACL 2006, pages 705–
712.
Hiroshi Nakagawa and Tatsunori Mori. 2002. A sim-
ple but powerful automatic term extraction method. In
COLING-02 on COMPUTERM 2002, pages 29–35.
Yukiko Nishimura. 2003. Linguistic innovations and in-
teractional features of casual online communication in
Japanese. Journal of Computer-Mediated Communi-
cation, 9(1).
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Procs. of COLING
’04, pages 562–568.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access methodology. In
Procs. of IJCNLP-08, pages 189–196.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Procs. of EMNLP 2001, pages 91–99.
437
