Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 2–11,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics
Power-Law Distributions for Paraphrases Extracted from Bilingual
Corpora
Spyros Martzoukos Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{s.martzoukos, c.monz}@uva.nl
Abstract
We describe a novel method that extracts
paraphrases from a bitext, for both the
source and target languages. In order
to reduce the search space, we decom-
pose the phrase-table into sub-phrase-tables
and construct separate clusters for source
and target phrases. We convert the clus-
ters into graphs, add smoothing/syntactic-
information-carrier vertices, and compute
the similarity between phrases with a ran-
dom walk-based measure, the commute
time. The resulting phrase-paraphrase
probabilities are built upon the conversion
of the commute times into artificial co-
occurrence counts with a novel technique.
The co-occurrence count distribution be-
longs to the power-law family.
1 Introduction
Paraphrase extraction has emerged as an impor-
tant problem in NLP. Currently, there exists an
abundance of methods for extracting paraphrases
from monolingual, comparable and bilingual cor-
pora (Madnani and Dorr, 2010; Androutsopou-
los and Malakasiotis, 2010); we focus on the lat-
ter and specifically on the phrase-table that is ex-
tracted from a bitext during the training stage of
Statistical Machine Translation (SMT). Bannard
and Callison-Burch (2005) introduced the pivot-
ing approach, which relies on a 2-step transition
from a phrase, via its translations, to a paraphrase
candidate. By incorporating the syntactic struc-
ture of phrases (Callison-Burch, 2005), the qual-
ity of the paraphrases extracted with pivoting can
be improved. Kok and Brockett (2010) (hence-
forth KB) used a random walk framework to de-
termine the similarity between phrases, which
was shown to outperform pivoting with syntac-
tic information, when multiple phrase-tables are
used. In SMT, extracted paraphrases with asso-
ciated pivot-based (Callison-Burch et al., 2006;
Onishi et al., 2010) and cluster-based (Kuhn et
al., 2010) probabilities have been found to im-
prove the quality of translation. Pivoting has also
been employed in the extraction of syntactic para-
phrases, which are a mixture of phrases and non-
terminals (Zhao et al., 2008; Ganitkevitch et al.,
2011).
We develop a method for extracting para-
phrases from a bitext for both the source and tar-
get languages. Emphasis is placed on the qual-
ity of the phrase-paraphrase probabilities as well
as on providing a stepping stone for extracting
syntactic paraphrases with equally reliable prob-
abilities. In line with previous work, our method
depends on the connectivity of the phrase-table,
but the resulting construction treats each side sep-
arately, which can potentially be benefited from
additional monolingual data.
The initial problem in harvesting paraphrases
from a phrase-table is the identification of the
search space. Previous work has relied on breadth
first search from the query phrase with a depth
of 2 (pivoting) and 6 (KB). The former can be
too restrictive and the latter can lead to excessive
noise contamination when taking shallow syntac-
tic information features into account. Instead, we
choose to cluster the phrase-table into separate
source and target clusters and in order to make this
task computationally feasible, we decompose the
phrase-table into sub-phrase-tables. We propose
a novel heuristic algorithm for the decomposition
of the phrase-table (Section 2.1), and use a well-
established co-clustering algorithm for clustering
2
each sub-phrase-table (Section 2.2).
The underlying connectivity of the source
and target clusters gives rise to a natural graph
representation for each cluster (Section 3.1).
The vertices of the graphs consist of phrases
and features with a dual smoothing/syntactic-
information-carrier role. The latter allow (a) re-
distribution of the mass for phrases with no appro-
priate paraphrases and (b) the extraction of syn-
tactic paraphrases. The proximity among vertices
of a graph is measured by means of a randomwalk
distance measure, the commute time (Aldous and
Fill, 2001). This measure is known to perform
well in identifying similar words on the graph of
WordNet (Rao et al., 2008) and a related measure,
the hitting time is known to perform well in har-
vesting paraphrases on a graph constructed from
multiple phrase-tables (KB).
Generally in NLP, power-law distributions are
typically encountered in the collection of counts
during the training stage. The distances of Sec-
tion 3.1 are converted into artificial co-occurrence
counts with a novel technique (Section 3.2). Al-
though they need not be integers, the main chal-
lenge is the type of the underlying distributions;
it should ideally emulate the resulting count dis-
tributions from the phrase extraction stage of a
monolingual parallel corpus (Dolan et al., 2004).
These counts give rise to the desired probability
distributions by means of relative frequencies.
2 Sub-phrase-tables & Clustering
2.1 Extracting Connected Components
For the decomposition of the phrase-table into
sub-phrase-tables it is convenient to view the
phrase-table as an undirected, unweighted graph
P with the vertex set being the source and target
phrases and the edge set being the phrase-table en-
tries. For the rest of this section, we do not distin-
guish between source and target phrases, i.e. both
types are treated equally as vertices of P . When
referring to the size of a graph, we mean the num-
ber of vertices it contains.
A trivial initial decomposition of P is achieved
by identifying all its connected components (com-
ponents for brevity), i.e. the mutually disjoint
connected subgraphs, {P0, P1, ..., Pn}. It turns
out (see Section 4.1) that the largest component,
say P0, is of significant size. We call P0 giant
and it needs to be further decomposed. This is
done by identifying all vertices such that, upon
removal, the component becomes disconnected.
Such vertices are called articulation points or cut-
vertices. Cut-vertices of high connectivity degree
are removed from the giant component (see Sec-
tion 4.1). For the remaining vertices of the giant
component, new components are identified and
we proceed iteratively, while keeping track of the
cut-vertices that are removed at each iteration, un-
til the size of the largest component is less than a
certain threshold ? (see Section 4.1).
Note that at each iteration, when removing cut-
vertices from a giant component, the resulting col-
lection of components may include graphs con-
sisting of a single vertex. We refer to such ver-
tices as residues. They are excluded from the re-
sulting collection and are considered for separate
treatment, as explained later in this section.
The cut-vertices need to be inserted appropri-
ately back to the components: Starting from the
last iteration step, the respective cut-vertices are
added to all the components of P0 which they
used to ‘glue’ together; this process is performed
iteratively, until there are no more cut-vertices to
add. By ‘addition’ of a cut-vertex to a component,
we mean the re-establishment of edges between
the former and other vertices of the latter. The
result is a collection of components whose total
number of unique vertices is less than the number
of vertices of the initial giant component P0.
These remaining vertices are the residues. We
then construct the graph R which consists of
the residues together with all their translations
(even those that are included in components of
the above collection) and then identify its compo-
nents {R0, ..., Rm}. It turns out, that the largest
component, say R0, is giant and we repeat the de-
composition process that was performed on P0.
This results in a new collection of components
as well as new residues: The components need
to be pruned (see Section 4.1) and the residues
give rise to a new graph R? which is constructed
in the same way asR. We proceed iteratively until
the number of residues stops changing. For each
remaining residue u, we identify its translations,
and for each translation v we identify the largest
component of which v is a member and add u to
that component.
The final result is a collection C = D ? F ,
where D is the collection of components emerg-
ing from the entire iterative decomposition of P0
3
and R, and F = {P1, ..., Pn}. Figure 1 shows
the decomposition of a connected graph G0; for
simplicity we assume that only one cut-vertex is
removed at each iteration and ties are resolved ar-
bitrarily. In Figure 2 the residue graph is con-
structed and its two components are identified.
The iterative insertion of the cut vertices is also
depicted. The resulting two components together
with those from R form the collection D for G0.
The addition of cut-vertices into multiple com-
ponents, as well as the construction method of the
residue-based graph R, can yield the occurrences
of a vertex in multiple components in D. We ex-
ploit this property in two ways:
(a) In order to mitigate the risk of excessive de-
composition (which implies greater risk of good
paraphrases being in different components), as
well as to reduce the size of D, a conserva-
tive merging algorithm of components is em-
ployed. Suppose that the elements of D are
ranked according to size in ascending order as
D = {D1, ..., Dk, Dk+1, ..., D|D|}, where |Di| ?
?, for i = 1, ..., k, and some threshold ? (see Sec-
tion 4.1). Each component Di with i ? {1, ..., k}
is examined as follows: For each vertex of Di the
number of its occurrences inD is inspected; this is
done in order to identify an appropriate vertex b to
act as a bridge between Di and other components
of which b is a member. Note that translations of
a vertex b with smaller number of occurrences in
D are less likely to capture their full spectrum of
paraphrases. We thus choose a vertex b from Di
with the smallest number of occurrences in D ,
resolving ties arbitrarily, and proceed with merg-
ing Di with the largest component, say Dj with
j ? {1, ..., |D| ? 1}, of which b is also a member.
The resulting merged component Dj? contains all
vertices and edges of Di and Dj and new edges,
which are formed according to the rule: if u is a
vertex of Di and v is a vertex of Dj and (u, v) is
a phrase-table entry, then (u, v) is an edge in Dj? .
As long as no connected component has identi-
fied Di as the component with which it should be
merged, then Di is deleted from the collection D.
(b) We define an idf -inspired measure for each
phrase pair (x, x?) of the same type (source or tar-
get) as
idf(x, x?) =
1
log |D|
log
(
2c(x, x?)|D|
c(x) + c(x?)
)
, (1)
where c(x, x?) is the number of components in
which the phrases x and x? co-occur, and equiv-
alently for c(·). The purpose of this measure is
for pruning paraphrase candidates and its use is
explained in Section 3.1. Note that idf(x, x?) ?
[0, 1].
The merging process and the idf measure are
irrelevant for phrases belonging to the compo-
nents of F , since the vertex set of each compo-
nent of F is mutually disjoint with the vertex set
of any other component in C.
  
G0 s1s2s3s4 t1t 2t3 c0={s2 } G11r={t 2 }
s1s4 t1 G12s3s4G12 G21s3 t 4c1={t3} r? r?{s4 }
t 4 s3 t3t 4t3t 4
Figure 1: The decomposition of G0 with vertices
si and tj : The cut-vertex of the ith iteration is de-
noted by ci, and r collects the residues after each
iteration. The task is completed in Figure 2.
  
G s0s1 t 0t2 s0s1 t 0t2s2 t 1 =c3 s3 t3 =c4=c4s2 t 1t2 s2 t2t 1s3s0 t3 s0s2 t2t 1
Figure 2: Top: Residue graph with its components
(no further decomposition is required). Bottom:
Adding cut-vertices back to their components.
2.2 Clustering Connected Components
The aim of this subsection is to generate sep-
arate clusters for the source and target phrases
of each sub-phrase-table (component) C ? C.
For this purpose the Information-Theoretic Co-
Clustering (ITC) algorithm (Dhillon et al., 2003)
is employed, which is a general principled cluster-
ing algorithm that generates hard clusters (i.e. ev-
4
ery element belongs to exactly one cluster) of two
interdependent quantities and is known to per-
form well on high-dimensional and sparse data.
In our case, the interdependent quantities are the
source and target phrases and the sparse data is
the phrase-table.
ITC is a search algorithm similar to K-means,
in the sense that a cost function, is minimized at
each iteration step and the number of clusters for
both quantities are meta-parameters. The number
of clusters is set to the most conservative initial-
ization for both source and target phrases, namely
to as many clusters as there are phrases. At each
iteration, new clusters are constructed based on
the identification of the argmin of the cost func-
tion for each phrase, which gradually reduces the
number of clusters.
We observe that conservative choices for the
meta-parameters often result in good paraphrases
being in different clusters. To overcome this prob-
lem, the hard clusters are converted into soft (i.e.
an element may belong to several clusters): One
step before the stopping criterion is met, we mod-
ify the algorithm so that instead of assigning a
phrase to the cluster with the smallest cost we se-
lect the bottom-X clusters ranked by cost. Addi-
tionally, only a certain number of phrases is cho-
sen for soft clustering. Both selections are done
conservatively with criteria based on the proper-
ties of the cost functions.
The formation of clusters leads to a natural re-
finement of the idf measure defined in eqn. (1):
The quantity c(x, x?) is redefined as the number
of components in which the phrases x and x? co-
occur in at least one cluster.
3 Monolingual Graphs & Counts
We proceed with converting the clusters into di-
rected, weighted graphs and then extract para-
phrases for both the source and target side. For
brevity we explain the process restricted to the
source clusters of a sub-phrase-table, but the same
method applies for the target side and for all sub-
phrase-tables in the collection C.
3.1 Monolingual graphs
Each source cluster is converted into a graph G as
follows: The vertex set consists of the phrases of
the cluster and an edge between s and s? exists, if
(a) s and s? have at least one translation from the
same target cluster, and (b) idf(s, s?) is greater
than some threshold ? (see Section 4.1). If two
phrases that satisfy condition (b) and have trans-
lations in more than one common target cluster,
a distinct such edge is established. All edges are
bi-directional with distinct weights for both direc-
tions.
Figure 3 depicts an example of such a construc-
tion; a link between a phrase si and a target cluster
implies the existence of at least one translation for
si in that cluster. We are not interested in the tar-
get phrases and they are thus not shown. For sim-
plicity we assume that condition (b) is always sat-
isfied and the extracted graph contains the maxi-
mum possible edges. Observe that phrases s3 and
s4 have two edges connecting them, (due to tar-
get clusters Tc and Td) and that the target cluster
Ta is irrelevant to the construction of the graph,
since s1 is the only phrase with translations in it.
This conversion of a source cluster into a graph G
  
s1 s2 s4 s5s3 s8s7s6Ta Tb Tc Td Te Tf
s2
s1 s3 s4s5 s6
s7 s8
Figure 3: Top: A source cluster containing
phrases s1,..., s8 and the associated target clusters
Ta,..., Tf . Bottom: The extracted graph from the
source cluster. All edges are bi-directional.
results in the formation of subgraphs in G, where
each subgraph is generated by a target cluster. In
general, if condition (b) is not always satisfied,
then G need not be connected and each connected
component is treated as a distinct graph.
Analogous to KB, we introduce feature vertices
to G: For each phrase vertex s, its part-of-speech
(POS) tag sequence and stem sequence are iden-
tified and inserted into G as new vertices with
bi-directional weighted edges connected to s. If
phrase vertices s and s? have the same POS tag se-
quence, then they are connected to the same POS
tag feature vertex. Similarly for stem feature ver-
tices. See Figure 4 for an example. Note that we
do not allow edges between POS tag and stem fea-
5
  
s124534
876
Tab Tabcd8e
f?53??
??cd8e
????87?
f?53?
???87?
Figure 4: Adding feature vertices to the extracted
graph (has) ?? (owns) ?? (i have) ?? (i had).
Phrase, POS tag feature and stem feature ver-
tices are drawn in circles, dotted rectangles and
solid rectangles respectively. All edges are bi-
directional.
ture vertices. The purpose of the feature vertices,
unlike KB, is primarily for smoothing and secon-
darily for identifying paraphrases with the same
syntactic information and this will become clear
in the description of the computation of weights.
The set of all phrase vertices that are adja-
cent to s is written as ?(s), and referred to
as the neighborhood of s. Let n(s, t) denote
the co-occurrence count of a phrase-table entry
(s, t) (Koehn, 2009). We define the strength of
s in the subgraph generated by cluster T as
n(s;T ) =
?
t?T
n(s, t), (2)
which is simply a partial occurrence count for s.
We proceed with computing weights for all edges
of G:
Phrase??phrase weights: Inspired by the
notion of preferential attachment (Yule, 1925),
which is known to produce power-law weight dis-
tributions for evolving weighted networks (Barrat
et al., 2004), we set the weight of a directed
edge from s to s? to be proportional to the
strengths of s? in all subgraphs in which both
s and s? are members. Thus, in the random
walk framework, s is more likely to visit
a stronger (more reliable) neighbor. If Ts,s? =
{T |s and s? coexist in subgraph generated by T},
then the weight w(s ? s?) of the directed edge
from s to s? is given by
w(s ? s?) =
?
T?Ts,s?
n(s?;T ), (3)
if s? ? ?(s) and 0 otherwise.
Phrase??feature weights: As mentioned
above, feature vertices have the dual role of car-
rying syntactic information and smoothing. From
eqn. (3) it can be deduced that, if for a phrase
s, the amount of its outgoing weights is close to
the amount of its incoming weights, then this is
an indication that at least a significant part of its
neighborhood is reliable; the larger the strengths,
the more certain the indication. Otherwise, either
s or a significant part of its neighborhood is
unreliable. The amount of weight from s to its
feature vertices should depend on this observation
and we thus let
net(s) =
?
?
?
?
?
?
?
s???(s)
(w(s ? s?)? w(s? ? s))
?
?
?
?
?
?
+ ,
(4)
where  prevents net(s) from becoming 0 (see
Section 4.1). The net weight of a phrase vertex
s is distributed over its feature vertices as
w(s ? fX) =< w(s ? s
?) > +net(s), (5)
where the first summand is the average weight
from s to its neighboring phrase vertices and
X = POS,STEM. If s has multiple POS tag
sequences, we distribute the weight of eqn. (5)
relatively to the co-occurrences of s with the re-
spective POS tag feature vertices. The quantity
< w(s ? s?) > accounts for the basic smoothing
and is augmented by a value net(s) that measures
the reliability of s’s neighborhood; the more unre-
liable the neighborhood, the larger the net weight
and thus larger the overall weights to the feature
vertices.
The choice for the opposite direction is trivial:
w(fX ? s) =
1
|{s? : (fX , s?) is an edge }|
, (6)
where X = POS,STEM. Note the effect of
eqns. (4)–(6) in the case where the neighborhood
of s has unreliable strengths: In a random walk
the feature vertices of s will be preferred and the
resulting similarities between s and other phrase
vertices will be small, as desired. Nonetheless,
if the syntactic information is the same with any
other phrase vertex inG, then the paraphrases will
be captured.
The transition probability from any vertex u to
any other vertex v in G, i.e., the probability of
6
hopping from u to v in one step, is given by
p(u ? v) =
w(u ? v)
?
v? w(u ? v
?)
, (7)
where we sum over all vertices adjacent to u inG.
We can thus compute the similarity between any
two vertices u and v in G by their commute time,
i.e., the expected number of steps in a round trip,
in a random walk from u to v and then back to u,
which is denoted by ?(u, v) (see Section 4.1 for
the method of computation of ?). Since ?(u, v) is
a distance measure, the smaller its value, the more
similar u and v are.
3.2 Counts
We convert the distance ?(u, v) of a vertex pair
u, v in a graph G into a co-occurrence count
nG(u, v) with a novel technique: In order to as-
sess the quality of the pair u, v with respect to G
we compare ?(u, v) with ?(u, x) and ?(v, x) for
all other vertices x in G. We thus consider the av-
erage distance of u with the other vertices of G
other than v, and similarly for v. This quantity is
denoted by ?(u; v) and ?(v;u) respectively, and
by definition it is given by
?(i; j) =
?
x?G
x 6=j
?(i, x)pG(x|i) (8)
where pG(x|i) ? p(x|G, i) is a yet unknown
probability distribution with respect to G. The
quantity (?(u; v)+?(v;u))/2 can then be viewed
as the average distance of the pair u, v to the rest
of the graph G. The co-occurrence count of u and
v in G is thus defined by
nG(u, v) =
?(u; v) + ?(v;u)
2?(u, v)
. (9)
In order to calculate the probabilities pG(·|·) we
employ the following heuristic: Starting with a
uniform distribution p(0)G (·|·) at timestep t = 0,
we iterate
?(t)(i; j) =
?
x?G
x 6=j
?(i, x)p(t)G (x|i) (10)
n(t)G (u, v) =
?(t)(u; v) + ?(t)(v;u)
2?(u, v)
(11)
p(t+1)G (v|u) =
n(t)G (u, v)
?
x?G n
(t)
G (u, v)
(12)
for all pairs of vertices u, v in G until conver-
gence. Experimentally, we find that convergence
is always achieved. After the execution of this it-
erative process we divide each count by the small-
est count in order to achieve a lower bound of 1.
A pair u, v may appear in multiple graphs in the
same sub-phrase-tableC. The total co-occurrence
count of u and v in C and the associated condi-
tional probabilities are thus given by
nC(u, v) =
?
G?C
nG(u, v) (13)
pC(v|u) =
nC(u, v)
?
x?C nC(u, x)
. (14)
A pair u, v may appear in multiple sub-phrase-
tables and for the calculation of the final count
n(u, v) we need to average over the associated
counts from all sub-phrase-tables. Moreover, we
have to take into account the type of the vertices:
For the simplest case where both u and v repre-
sent phrase vertices, their expected count is, by
definition, given by
n(s, s?) =
?
C
nC(s, s
?)p(C|s, s?). (15)
On the other hand, if at least one of u or v is
a feature vertex, then we have to consider the
phrase vertex that generates this feature: Suppose
that u is the phrase vertex s=‘acquire’ and v the
POS tag vertex f=‘NN’ and they co-occur in two
sub-phrase-tables C and C ? with positive counts
nC(s, f) and nC?(s, f) respectively; the feature
vertex f is generated by the phrase vertices ‘own-
ership’ in C and by ‘possession’ in C ?. In that
case, an interpolation of the counts nC(s, f) and
nC?(s, f) as in eqn. (15) would be incorrect and
a direct sum nC(s, f) + nC?(s, f) would provide
the true count. As a result we have
n(s, f) =
?
s?
?
C
nC(s, f(s
?))p(C|s, f(s?)),
(16)
where the first summation is over all phrase ver-
tices s? such that f(s?) = f . With a similar argu-
ment we can write
n(f, f ?) =
?
s,s?
?
C
nC(f(s), f(s
?))×
× p(C|f(s), f(s?)). (17)
7
For the interpolants, from standard probability we
find
p(C|u, v) =
pC(v|u)p(C|u)
?
C? pC?(v|u)p(C
?|u)
, (18)
where the probabilities p(C|u) can be computed
by considering the likelihood function
`(u) =
N?
i=1
p(xi|u) =
N?
i=1
?
C
pC(xi|u)p(C|u)
and by maximizing the average log-likelihood
1
N log `(u), where N is the total number of ver-
tices with which u co-occurs with positive counts
in all sub-phrase-tables.
Finally, the desired probability distributions are
given by the relative frequencies
p(v|u) =
n(u, v)
?
x n(u, x)
, (19)
for all pairs of vertices u, v.
4 Experiments
4.1 Setup
The data for building the phrase-table P
is drawn from DE-EN bitexts crawled from
www.project-syndicate.org, which is
a standard resource provider for the WMT
campaigns (News Commentary bitexts, see,
e.g. (Callison-Burch et al., 2007) ). The filtered
bitext consists of 125K sentences; word align-
ment was performed running GIZA++ in both di-
rections and generating the symmetric alignments
using the ‘grow-diag-final-and’ heuristics. The
resulting P has 7.7M entries, 30% of which are
‘1-1’, i.e. entries (s, t) that satisfy p(s|t) =
p(t|s) = 1. These entries are irrelevant for para-
phrase harvesting for both the baseline and our
method, and are thus excluded from the process.
The initial giant component P0 contains 1.7M
vertices (Figure 5), of which 30% become
residues and are used to construct R. At each it-
eration of the decomposition of a giant compo-
nent, we remove the top 0.5% · size cut-vertices
ranked by degree of connectivity, where size is
the number of vertices of the giant component and
set ? = 2500 as the stopping criterion. The latter
choice is appropriate for the subsequent step of
co-clustering the components, for both time com-
plexity and performance of the ITC algorithm.
100 102 104 106100
101102
103104
105106
107
rank
size 100 102 104 106100
105P0
Figure 5: Log-log plot of ranked components ac-
cording to their size (number of source and target
phrases) for: (a) Components extracted from P .
‘1-1’ components are not shown. (b) Components
extracted from the decomposition of P0.
In the components emerging from the decompo-
sition of R0, we observe an excessive number
of cut-vertices. Note that vertices that consist
these components can be of two types: i) for-
mer residues, i.e., residues that emerged from the
decomposition of P0, and ii) other vertices of
P0. Cut-vertices can be of either type. For each
component, we remove cut-vertices that are not
translations of the former residues of that com-
ponent. Following this pruning strategy, the de-
generacy of excessive cut-vertices does not reap-
pear in the subsequent iterations of decompos-
ing components generated by new residues, but
the emergence of two giant components was ob-
served: One consisting mostly of source type ver-
tices and one of target type vertices. Without go-
ing into further details, the algorithm can extend
to multiple giant components straightforwardly.
For the merging process of the collection D we
set ? = 5000, to avoid the emergence of a giant
component. The sizes of the resulting sub-phrase-
tables are shown in Figure 6. For the ITC algo-
rithm we use the smoothing technique discussed
in (Dhillon and Guan, 2003) with ? = 106.
For the monolingual graphs, we set ? = 0.65
and discard graphs with more than 20 phrase ver-
tices, as they contain mostly noise. Thus, the sizes
of the graphs allow us to use analytical methods
to compute the commute times: For a graph G,
we form the transition matrix P , whose entries
P (u, v) are given by eqn. (7), and the fundamen-
8
100 102 104 106100
101102
103104
105106
rank
size
 
 
before mergingafter merging
Figure 6: Log-log plot of ranked sub-phrase-
tables according to their size (number of source
and target phrases).
tal matrix (Grinstead and Snell, 2006; Boley et al.,
2011) Z = (I?P +1piT )?1, where I is the iden-
tity matrix, 1 denotes the vector of all ones and pi
is the vector of stationary probabilities (Aldous
and Fill, 2001) which is such that piTP = piT
and piT1 = 1 and can be computed as in (Hunter,
2000). The commute time between any vertices u
and v in G is then given by (Grinstead and Snell,
2006)
?(u, v) = (Z(v, v)? Z(u, v))/pi(v) +
+ (Z(u, u)? Z(v, u))/pi(u). (20)
For the parameter of eqn. (4), an appropriate
choice is  = |?(s)| + 1; for reliable neighbor-
hoods, this quantity is insignificant. POS tags and
lemmata are generated with TreeTagger1.
Figure 7 depicts the most basic type of graph
that can be extracted from a cluster; it includes
two source phrase vertices a, b, of different syn-
tactic information. Suppose that both a and
b are highly reliable with strengths n(a;T ) =
n(b;T ) = 40, for some target cluster T . The re-
sulting conditional probabilities adequately repre-
sent the proximity of the involved vertices. On
the other hand, the range of the co-occurrence
counts is not compatible with that of the strengths.
This is because i) there are no phrase vertices with
small strength in the graph, and ii) eqn. (9) is es-
sentially a comparison between a pair of vertices
and the rest of the graph. To overcome this prob-
lem inflation vertices ia and ib of strength 1 with
accompanying feature vertices are introduced to
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
the graph. Figure 8 depicts the new graph, where
the lengths of the edges represent the magnitude
of commute times. Observe that the quality of
the probabilities is preserved but the counts are
inflated, as required.
In general, if a source phrase vertex s has at
least one translation t such that n(s, t) ? 3, then a
triplet (is, f(is), g(is)) is added to the graph as in
Figure 8. The inflation vertex is establishes edges
with all other phrase and inflation vertices in the
graph and weights are computed as in Section 3.1.
The pipeline remains the same up to eqn. (13),
where all counts that include inflation vertices are
ignored.
  
G st =G{c=G{
t =s {
c=s {r =s}G{ 0 123r =t =G{}G{ 0 124r=c =G{}G{ 0 124r =t =s {}G{ 0 1?r=c=s {}G{ 0 1?
? =G? s { 0 213? =G? t =G{{ 0 21?? =G? c=G{{ 0 21?? =G? t =s {{ 0 ??? =G? c =s {{ 0 ??
Figure 7: Top: A graph with source phrase ver-
tices a and b, both of strength 40, with accom-
panying distinct POS sequence vertices f(·) and
stem sequence vertices g(·). Bottom: The result-
ing co-occurrence counts and conditional proba-
bilities for a.
  
G=s{t } 0 122G=c=t }{t } 0 123G=r =t }{t } 0 123G=c=s }{t } 0 14?G=r =s }{t } 0 14?
tsc=t }c=s }
r =t }
r =s }
? t? s c=? t }
r =? t }
c=? s }r =? s }? =t ? s } 0 441?? =t ? c=t }} 0 4?1?? =t ? r =t }} 0 4?1?? =t ? c=s }} 0 31?? =t ? r =s }} 0 31?
Figure 8: The inflated version of Figure 7.
9
4.2 Results
Our method generates conditional probabilities
for any pair chosen from {phrase, POS sequence,
stem sequence}, but for this evaluation we restrict
ourselves to phrase pairs. For a phrase s, the qual-
ity of a paraphrase s? is assessed by
P (s?|s) ? p(s?|s) + p(f1(s
?)|s) + p(f2(s
?)|s),
(21)
where f1(s?) and f2(s?) denote the POS tag se-
quence and stem sequence of s? respectively. All
three summands of eqn. (21) are computed from
eqn. (19). The baseline is given by pivoting (Ban-
nard and Callison-Burch, 2005),
P (s?|s) =
?
t
p(t|s)p(s?|t), (22)
where p(t|s) and p(s?|t) are the phrase-based rel-
ative frequencies of the translation model.
We select 150 phrases (an equal number for
unigrams, bigrams and trigrams), for which we
expect to see paraphrases, and keep the top-10
paraphrases for each phrase, ranked by the above
measures. We follow (Kok and Brockett, 2010;
Metzler et al., 2011) in the evaluation of the ex-
tracted paraphrases: Each phrase-paraphrase pair
is manually annotated with the following options:
0) Different meaning; 1) (i) Same meaning, but
potential replacement of the phrase with the para-
phrase in a sentence ruins the grammatical struc-
ture of the sentence. (ii) Tokens of the paraphrase
are morphological inflections of the phrase’s to-
kens. 2) Samemeaning. Although useful for SMT
purposes, ‘super/substrings of’ are annotated with
0 to achieve an objective evaluation.
Both methods are evaluated in terms of the
Mean Expected Precision (MEP) at k; the Ex-
pected Precision for each selected phrase s at
rank k is computed by Es[p@k] = 1k
?k
i=1 pi,
where pi is the proportion of positive annotations
for item i. The desired metric is thus given by
MEP@k = 1150
?
s Es[p@k]. The contribution
to pi can be restricted to perfect paraphrases only,
which leads to a strict strategy for harvesting para-
phrases. Table 1 summarizes the results of our
evaluation and
we deduce that our method can lead to improve-
ments over the baseline.
An important accomplishment of our method
is that the distribution of counts n(u, v), (as given
Method
Lenient MEP Strict MEP
@1 @5 @10 @1 @5 @10
Baseline .58 .47 .41 .43 .33 .28
Graphs .72 .61 .52 .53 .40 .33
Table 1: Mean Expected Precision (MEP) at k un-
der lenient and strict evaluation criteria.
by eqns. (15)–(17)) for all vertices u and v, be-
longs to the power-law family (Figure 9). This is
evidence that the monolingual graphs can simu-
late the phrase extraction process of a monolin-
gual parallel corpus. Intuitively, we may think of
the German side of the DE–EN parallel corpus as
the ‘English’ approximation to a ‘EN’–EN par-
allel corpus, and the monolingual graphs as the
word alignment process.
100 102 104 106 108100
101
102
103
104
105
rank
co?o
ccur
renc
e co
unt
Figure 9: Log-log plot of ranked pairs of English
vertices according to their counts
5 Conclusions & Future Work
We have described a new method that harvests
paraphrases from a bitext, generates artificial
co-occurrence counts for any pair chosen from
{phrase, POS sequence, stem sequence}, and po-
tentially identifies patterns for the syntactic infor-
mation of the phrases. The quality of the para-
phrases’ ranked lists outperforms that of a stan-
dard baseline. The quality of the resulting condi-
tional probabilities is promising and will be eval-
uated implicitly via an application to SMT.
This research was funded by the European
Commission through the CoSyne project FP7-
ICT- 4-248531.
10
References
David Aldous and James A. Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/?aldous/RWG/
book.html
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135–187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proc.
ACL, pp. 597–604.
Alain Barrat, Marc Barthlemy, and Alessandro Vespig-
nani. 2004. Modeling the Evolution of Weighted
Networks. Phys. Rev. Lett., 92.
Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang. 2011.
Commute Times for a Directed Graph using an
Asymmetric Laplacian. Linear Algebra and its Ap-
plications, Issue 2, pp. 224–242.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora.
Proc. EMNLP, pp. 196–205.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007
(Meta-) Evaluation of Machine Translation. Proc.
Workshop on Statistical Machine Translation, pp.
136–158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006 Improved statistical machine trans-
lation using paraphrases. Proc. HLT/NAACL, pp.
17–24.
Inderjit S. Dhillon and Yuqiang Guan. 2003. Informa-
tion Theoretic Clustering of Sparse Co-Occurrence
Data. Proc. IEEE Int’l Conf. Data Mining, pp. 517–
520.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-Theoretic
Coclustering. Proc. ACM SIGKDD Int’l Conf.
Knowledge Discovery and Data Mining, pp. 89–98.
William Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised construction of large para-
phrase corpora: Exploiting massively parallel news
sources. Proc. COLING, pp. 350-356.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme 2011. Learn-
ing Sentential Paraphrases from Bilingual Paral-
lel Corpora for Text-to-Text Generation. Proc.
EMNLP, pp. 1168–1179.
Charles Grinstead and Laurie Snell. 2006. Introduc-
tion to Probability. Second ed., American Mathe-
matical Society.
Jeffrey J. Hunter. 2000. A Survey of Generalized In-
verses and their Use in Stochastic Modelling. Res.
Lett. Inf. Math. Sci., Vol. 1, pp. 25–36.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Stanley Kok and Chris Brockett. 2010. Hitting the
Right Paraphrases in Good Time. Proc. NAACL,
pp.145–153.
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase Clustering for Smoothing
TM Probabilities: or, how to Extract Paraphrases
from Phrase Tables. Proc. COLING, pp.608–616.
Nitin Madnani and Bonnie Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. Computational Linguistics,
36(3):341–388.
Donald Metzler, Eduard Hovy, and Chunliang
Zhang. 2011. An Empirical Evaluation of Data-
Driven Paraphrase Generation Techniques. Proc.
ACL:Short Papers, pp. 546–551.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. Proc. ACL:Short Papers, pp. 1–5.
Delip Rao, David Yarowsky, and Chris Callison-
Burch. 2008. Affinity Measures based on the Graph
Laplacian. Proc. Textgraphs Workshop on Graph-
based Algorithms for NLP at COLING, pp. 41–48.
George U. Yule. 1925. A Mathematical Theory of
Evolution, based on the Conclusions of Dr. J. C.
Willis, F.R.S. Philos. Trans. R. Soc. London, B 213,
pp. 21–87.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. Proc. ACL, pp.
780–788.
11
