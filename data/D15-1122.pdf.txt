Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1053–1058,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
System Combination for Machine Translation through Paraphrasing  
 
 
Wei-Yun Ma 
Institute of Information science 
Academia Sinica 
Taipei 115, Taiwan 
ma@iis.sinica.edu.tw 
       Kathleen McKeown 
Department of Computer Science 
Columbia University 
New York, NY 10027, USA 
kathy@cs.columbia.edu 
 
  
 
Abstract 
In this paper, we propose a paraphrasing 
model to address the task of system com-
bination for machine translation. We dy-
namically learn hierarchical paraphrases 
from target hypotheses and form a syn-
chronous context-free grammar to guide 
a series of transformations of target hy-
potheses into fused translations. The 
model is able to exploit phrasal and struc-
tural system-weighted consensus and also 
to utilize existing information about word 
ordering present in the target hypotheses. 
In addition, to consider a diverse set of 
plausible fused translations, we develop a 
hybrid combination architecture, where 
we paraphrase every target hypothesis us-
ing different fusing techniques to obtain 
fused translations for each target, and 
then make the final selection among all 
fused translations. Our experimental re-
sults show that our approach can achieve 
a significant improvement over combina-
tion baselines.  
1 Introduction 
In the past several years, many machine transla-
tion (MT) combination approaches have been 
developed. Word-level combination approaches, 
such as the confusion network decoding model, 
have been quite successful (Matusov et al., 2006; 
Rosti et al., 2007a; He et al. 2008; Karakos et al. 
2008; Chen et al. 2009a; Narsale 2010; Leusch 
2011; Freitag et al. 2014).  
In addition to word-level combination ap-
proaches, some phrase-level combination ap-
proaches have also recently been developed; the 
goal is to retain coherence and consistency be-
tween the words in a phrase. The most common 
phrase-level combination approaches are re-
decoding methods: by constructing a new phrase 
table from each MT system’s source-to-target 
phrase alignments, the source sentence can also 
be re-decoded using the new translation table 
(Rosti et al., 2007b; Huang and Papineni, 2007; 
Chen et al., 2007; Chen et al., 2009b). One prob-
lem with these approaches is that, just with a new 
phrase table, existing information about word 
ordering present in the target hypotheses is not 
utilized; thus the approaches are likely to make 
new mistakes of word reordering which do not 
appear in the target hypotheses of MT engines. 
Huang and Papineni (2007) attacked this issue 
through a reordering cost function that encour-
ages search along with decoding paths from all 
MT engines’ decoders.  
Another phrase-level combination approach 
relies on a lattice decoding model to carry out the 
combination (Feng et al 2009; Du and Way 2010; 
Ma and McKeown 2012). In a lattice, each edge 
is associated with a phrase (a single word or a 
sequence of words) rather than a single word. 
The construction of the lattice is based on the 
extraction of phrase pairs from word alignments 
between a selected best MT system hypothesis 
(the backbone) and the other translation hypothe-
ses. One challenge of the lattice decoding model 
is that it is difficult to consider structural consen-
sus among target hypotheses from multiple MT 
engines, i.e, the consensus among occurrences of 
discontinuous words. 
In this paper, we propose another phrase-level 
combination approach – a paraphrasing model 
using hierarchical paraphrases (paraphrases con-
tain subparaphrases), to fuse target hypotheses. 
We dynamically learn hierarchical paraphrases 
from target hypotheses without any syntactic an-
notations and form a synchronous context-free 
grammar (SCFG) (Aho and Ullman 1969) to 
1053
guide a series of transformations of target hy-
potheses into fused translations. Through these 
structural transformations, the paraphrasing 
model is able to exploit phrasal and structural 
system-weighted consensus and also able to uti-
lize existing information about word ordering 
present in the target hypotheses. In addition, to 
consider a diverse set of plausible fused transla-
tions, we develop a hybrid combination architec-
ture, where we paraphrase every target hypothe-
sis using different fusing techniques to obtain 
fused translations for each target, and then make 
the final selection among all fused translations 
through a sentence-level selection-based model.  
In short, compared with other related work, 
our approach features the following advantages: 
 
1. It can consider structural system-weighted 
consensus among target hypotheses from 
multiple MT engines through its hierar-
chical paraphrases, which non-hierarchical 
paraphrases are not able to do. 
 
2. It can utilize existing information about 
word ordering present in the target hy-
potheses. 
 
3. It can retain coherence and consistency 
between the words in a phrase. 
 
4. The hybrid combination architecture ena-
bles us to consider a diverse set of plausi-
ble fused translations produced by differ-
ent fusing techniques. 
2 Hybrid Combination Architecture 
In the context of system combination, discrimi-
native reranking or post editing, MT researchers 
(Rosti et al., 2007a; Huang and Papineni, 2007; 
Devlin and Matsoukas, 2012, Matusov et al., 
2008; Gimpel et al., 2013) have recently shown 
many positive results if more diverse translations 
are considered. Inspired by them, we develop a 
hybrid combination architecture in order to con-
sider more diverse fused translations. We para-
phrase every target hypothesis to obtain the cor-
responding fused translation, and then make the 
final selection among all fused translations 
through a sentence-level selection-based model, 
shown in Figure 1. In the architecture, different 
fusing techniques can be used to generate fused 
translations for the further sentence-level selec-
tion, enabling us to exploit more sophisticated 
information of the whole sentence. 
 
 
 
Figure 1. An example of hybrid combination architecture 
3 Paraphrasing Model 
In this section, we introduce our paraphrasing 
model. For each single target hypothesis, we ex-
tract a set of hierarchical paraphrases from mon-
olingual word alignments between the hypothesis 
and other hypotheses. Each set of hierarchical 
paraphrases forms a synchronous context-free 
grammar to guide a series of transformations of 
that target hypothesis into a fused translation. 
  Any monolingual word aligner can be used to 
produce the monolingual word alignments. In 
our system, we adopt TERp (Snover et al. 2009), 
one of the state-of-the-art alignment tools, to 
serve this purpose. TERp is an extension of TER 
(Snover et al. 2006). Both TERp and TER are 
automatic evaluation metrics for MT, based on 
measuring the ratio of the number of edit opera-
tions between the reference sentence and the MT 
system hypothesis. The edit operations of TERp 
include TER’s Matches, Insertions, Deletions, 
Substitutions and Shifts—as well as three new 
edit operations: Stem Matches, Synonym Match-
es and Paraphrases. A valuable side product of 
TERp is the monolingual word alignment. A 
constructed example is shown in Figure 2. 
3.1 Hierarchical Paraphrase Extraction 
We first introduce our notation. For a given sen-
tence i, we use ihE  to denote the target hypothe-
sis from MT system h, use ihEP  to denote ihE  
attached with related word positions, use ihe  to 
denote a phrase within ihE , and use ihep  to denote 
ihe  attached with related word positions. For in-
stance, If ihE is “you buy the book”, then ihEP  
would be “you1 buy2 the3 book4”. If ihe is “the 
book”, then ihep  is “the
3 book4”.  
For a given sentence i, a MT system h and a 
MT system k, we use a SCFG denoted by 
i khQ ,
to 
represent the set of hierarchical paraphrases 
learned from ihEP  and ikEP . Adapting (Chiang 
Translation from 
MT System 1
…
Fusion
output
Fusion
output
Para-
phrasing
model
Lattice
decoding
Translation from 
MT System 2
Fusion
output
Fusion
output
Para-
phrasing
model
Lattice
decoding
Translation from 
MT System N
Fusion
output
Fusion
output
Para-
phrasing
model
Lattice
decoding
Sentence-level Selection-based Model
Best Translation
Translations
from all MT 
Systems
1054
2007), we design the following rules to obtain 
i khQ ,
, based on the monolingual word alignment, 
obtained by a aligner, such as TERp. 
 
 
? If  ?? ikih epep  ,  is consistent
1 with the mono-
lingual word alignment, then  ??? ikih eepX  ,  
is added to 
i khQ ,
.                     
                                 
? If ??? ??  , X  is in i khQ , , and ?? ikih epep  ,  is 
consistent with monolingual word alignment 
such that 
21 ??? ihep?  and 21 ??? ike? , then
??? 2121  , ???? aa XXX  is added to i khQ , , where 
a is an index. 
 
Please note that for each extracted hierarchical 
paraphrase - ??? ??  , X  ,  ? would include 
information of word positions while ? would not. 
For a certain target hypothesis - ihEP , our goal 
is to paraphrase it to get the fusion output by us-
ing a set of hierarchical paraphrases, denoted by
ihQ . Thus we create the union of all related hier-
archical paraphrases learned from ihEP and other 
target hypotheses. Two special “glue” rules - 
??? 2121 X,X SSS  and ??? 11 X,XS  are also added 
to
ihQ . The process can be represented formally in 
the following: 
 
?? }X,X,X,X{ 1121211 , ??????? ? SSSSQQ Nk i khih
 
 
where N is the total number of MT systems. 
3.1.1 An Example 
 
Figure 2. A constructed example of a sentence - “???? 
(the book that you bought)” and its translations from three 
MT systems – iE1 , iE2  and iE3 , and word alignments be-
tween iE2  and iE1 , and between iE2  and iE3 , obtained 
through TERp. 
 
 
We use a Chinese-to-English example in Figure 
2 to illustrate the extraction process. The extract-
                                                 
1 This means that words in a legal paraphrase are not 
aligned to words outside of the paraphrase, and should 
include at least one pair of words aligned with each 
other. 
ed hierarchical paraphrases to paraphrase iEP2  - 
“you1 buy2 the3 book4” are shown in Table 1. 
Because of limited space, only part of the para-
phrases, i.e, part of the rules of iQ2 , are shown.  
 
 
iQ2 of rules ofpart  
iQ 1,2
in
? 
iQ 2,2
in
? 
iQ 3,2
in
? 
???    you,  youX 1                                                           (a) ? ? ? 
??? buy  you,  buy youX 21                                           (b)  ?  
??? bought  you,  buy youX 21                                     (c)   ? 
??? bring  you,  buy youX 21                                        (d) ?   
??? book , bookX 4                                                         (e) ? ?  
??? books , bookX 4                                                       (f)   ? 
??? book  the, book theX 43                                          (g) ? ?  
??? books  the, book theX 43                                        (h)   ? 
(i)bought  that youbooks  the, book  thebuy youX 4321 ???
 
  ? 
??? 1431 X that books  the, book  theXX                      (j)   ? 
??? bought  that youX  the, X  thebuy youX 11321     (k)   ? 
??? 12231 X that X  the, X  theXX                                (l) ?  ? 
Table 1. Part of extracted hierarchical paraphrases to para-
phrase iEP2 , i.e, part of the rules of iQ2 .  
 
 
Note that, in Table 1, the rules (j), (k) and (l) 
can be regarded as structural paraphrases, and 
they utilize existing information about word or-
dering present in the target hypotheses. Since 
rule (l) is included in both 
iQ 1,2
and
iQ 3,2
, we can 
say that rule (l) has more structural consensus 
than rule (j) and (k). And rule (l) also models the 
word reordering through reversing the order of 
X1 and X2. By the example, we can see the rea-
son why our model is able to exploit structural 
consensus and also to utilize existing information 
about word ordering present in the target hypoth-
eses. 
3.2 Decoding 
Given a certain target hypothesis - ihEP , and its 
set of hierarchical paraphrases - ihQ , the decoder 
aims to paraphrase ihEP  using ihQ by performing a 
search for the single most probable derivation via 
the CKY algorithm with a Viterbi approximation. 
The derivation is the paraphrased result, i.e, the 
fusion result indicated in Figure 1. The single 
most probable derivation can be represented as 
 
 
 
 
 
 
??
? ?? otherwise 0
   if  1),( ,
,
,
i
kh
ji
hji
h
Qqkqf
 
 
the books    that    you     bought
you   buy      the     book
:           
:         
:           
the book that    you     bring
iE1
3
)(*))(log(**
),(*maxarg)|(logmaxarg
1 1
,
i
hw
i
hlp
J
j
N
k
ji
hk
E
i
h
i
h
E
ElengthELMJ
kqfEPEp
i
h
i
h
???
?
???
?
?
??
?
?? ? ?
? ?
1055
where jihq , is the jth paraphrase in ihQ  used to 
generate ihE , J is the number of paraphrases used 
to generate ihE . N is the total number of MT sys-
tems. 
k? is the weight of MT system k, in charge 
of the system-weighted consensus.
p?  is phrase 
penalty. 
l?  is the LM weight and w?  is word 
penalty. All weights are trained discriminatively 
for Bleu score using Minimum Error Rate Train-
ing (MERT) procedure (Och 2004). 
  The ideal result of paraphrasing iEP2  is shown 
in the following, which is supposed to be gener-
ated with a higher chance if, regardless of system 
weights. That is because of the use of the rules 
with higher degree of structural consensus, such 
as (l) and (e). 
 
 
 
 
 
 
4 Sentence-Level Selection-based Model 
For a given sentence i and its M multiple fusion 
outputs - ifE , Mf ??1  generated by the para-
phrasing model or the lattice decoding model, the 
goal here is to select the best one among them, as 
shown in Figure 1 (For the case shown in the 
figure, M is 2N). The idea is to compare system-
weighted consensus among all fusion outputs and 
translations from all MT systems, and then select 
the one with the highest consensus. We adopt 
Minimum Bayes Risk (MBR) decoding (Kumar 
and Byrne, 2004; Sim et al., 2007) to serve our 
purpose and develop the following TER-based 
MBR: 
 
 
 
 
 
where TER is Translation Tdit Ratio. 
m? is the 
fusion weight specific to a certain MT system 
and a certain fusion model, 
k?  is the weight of 
MT system k and 
l?  is the LM weight. All 
weights are trained discriminatively for Bleu 
score using MERT. 
5 Experiments 
Our experiments are conducted and reported on 
three datasets: The first dataset includes Chinese-
English system translations and reference trans-
lations from DARPA GALE 2008 (GALE Chi-
Eng). The second dataset includes Chinese-
English system translations and reference trans-
lations and from NIST 2008 (NIST Chi-Eng). 
And the third dataset includes Arabic-English 
system translations and reference translations 
and from NIST 2008 (NIST Ara-Eng). 
 
 
 MTSystem# TuneSent#   TestSent# 
GALE Chi-Eng 5 422 422 
NIST Chi-Eng 5 524 788 
NIST Ara-Eng 5 592 717 
Table 2. Experimental setting 
 
MT System  Approach Bleu 
nrc phrase-based SMT 30.95     
rwth-pbt-aml phrase-based SMT + 
source reordering 
 
 
 
 
 
 
+ source reordering 
31.83   
rwth-pbt-jx phrase-based SMT + word seg-
mentation 
31.78   
rwth-pbt-sh phrase-based SMT + source reor-
dering + rescoring 
32.63   
sri-hpbt hierarchical phrase-based SMT 32.00   
Table 3: Techniques of top five MT of GALE Chi-Eng Da-
taset 
 
Table 3 lists distinguishing machine transla-
tion approaches of top five MT of GALE Chi-
Eng Dataset. And “rwth-pbt-sh” performs the 
best in Bleu score. 
Two combination baselines are implemented 
for comparison: one is an implementation based 
on confusion network decoding, and the other is 
Lattice Decoding from (Ma and McKeown 2012), 
both of which are using TERp to obtain word 
alignments between a selected backbone hypoth-
esis and other target hypotheses. The former uses 
these word alignments to construct a confusion 
network while the latter extracts phrases which 
are consistent with these word alignments to 
construct a lattice. For both baselines, backbone 
hypotheses are selected sentence by sentence 
based on system-weighted consensus among 
translation of all MT systems.  
5.1 Results 
In Table 4, CN represents confusion network; 
LD represents Lattice Decoding (Ma and McKe-
own 2012); PARA represents paraphrasing mod-
el proposed in this paper; Backbone_* represents 
that * is carried out on selected backbones, in 
contrast with the hybrid combination architecture. 
Arch_LD represents that only lattice decoding is 
carried out using hybrid combination architecture. 
Arch_PARA represents that only paraphrasing 
model is carried out using hybrid combination 
  (e) rule using     bought youbook that   the, book  t euy  you
           (c) rule using             bought  that youX  the, X thebuy  you
                                                       (l) rule using                                      X that X  the, X theX 
                                                                            rule glue using                                                                X ,X 
 S
4321
44
321
344
3
3
22
1
???
??
???
??
??
? ? ? ???
??
???
??
N
k
i
k
i
fk
M
m
i
m
i
fm
i
fl
f
i
f
f
EETEREETER
ELMEp
11
)),(1log(*)),(1log(*
))(log(*maxarg)(logmaxarg
??
?
1056
architecture. Arch_LD_PARA represents that 
LD and PARA are both carried out using hybrid 
combination architecture, which is the example 
shown in Figure 2. 
 
 GALE  
Chi-Eng 
NIST  
Chi-Eng 
NIST  
Ara-Eng 
Best MT system 
32.63 30.16 48.40 
Backbone_CN (baseline) 
33.04 31.21 48.56 
Backbone_LD (baseline) 
33.16 32.65 49.33 
Backbone_PARA 
33.09 32.59 49.46 
Arch_LD 33.24 32.66 50.48 
Arch_PARA 33.32 32.90 50.20 
Arch_LD_PARA 33.72 33.42 50.44 
Table 4. Experimental results in Bleu score 
 
From Table 4, we can first observe that, for 
the three datasets, Backbone_PARA and Back-
bone_LD outperform Backbone_CN, which 
shows the advantage of using phrases over words 
in combination. However, Backbone_PARA 
does not show improvement over Backbone_LD. 
The reason could be that selected backbones al-
ready have a high level of quality and fewer 
words need to be replaced or re-ordered in con-
trast with other target hypotheses. 
We find that Arch_PARA performs better than 
Backbone_PARA, and Arch_LD performs better 
than Backbone_LD. This observation supports 
our claim that it is beneficial to consider more 
diverse sets of plausible fused translations.  
Arch_LD_PARA achieves the best perfor-
mance among all techniques used in this paper. It 
not only supports our claim, but also brings a 
conclusion that the paraphrasing model and lat-
tice decoding can compensate for the weaknesses 
of the other in our architecture.  
Since the paraphrasing model uses hierarchical 
paraphrases to carry out the fusion, it is able to 
make a bigger degree of word-reordering or 
structural change on the input hypothesis in 
comparison with lattice decoding. We suppose 
that when more word-reordering and structural 
changes are needed, paraphrasing model can 
bring more benefits than lattice decoding. Be-
cause the quality of a given translation hypothe-
sis is highly related to word reordering and struc-
tural change, it can be expected that when a 
poorly translated hypothesis is paraphrased, par-
aphrasing model can bring more benefits than 
lattice decoding. In order to obtain the evidence 
to support this hypothesis, we carried out the fol-
lowing experiment on NIST Chi-Eng Dataset. 
For each MT system from the selected top 5 
system A-E, we paraphrase its translations using 
the paraphrasing model and lattice decoding sep-
arately, aiming to compare the performances of 
the two models on each MT system. In other 
words, we do not first do backbone selection. 
Every MT system’s translation is regarded as a 
backbone. The results are shown in Table 5. 
 
 MT Lattice Decod-
ing 
Paraphrasing  
model 
Sys A 30.16 32.17 31.76 
Sys B 30.06 31.93 31.72 
Sys C 28.15 30.66 31.00 
Sys D 29.94 31.86 31.46 
Sys E 29.52 31.52 31.92 
Table 5. The Bleu score of each MT system, the Bleu 
score of paraphrasing each MT system using lattice decod-
ing and the Bleu score of paraphrasing each MT system 
using paraphrasing model. 
 
Among the five MT systems, “Sys C” and 
“Sys D” perform poorer than the other three MT 
systems. When we paraphrase the two systems, 
we find that paraphrasing model outperforms 
lattice decoding. These results support our hy-
pothesis that when more word-reordering and 
structural changes are needed, paraphrasing 
model can bring more benefits than lattice de-
coding. 
6 Conclusion 
We view MT combination as a paraphrasing pro-
cess using a set of hierarchical paraphrases, in 
which more complicated paraphrasing phenome-
na are able to be modeled, such as phrasal and 
structural consensus. Existing information about 
word ordering present in the target hypotheses 
are also considered. The experimental results 
show that our approach can achieve a significant 
improvement over combination baselines.  
There are many possibilities for enriching the 
simple framework. Many ideas from recent 
translation developments can be borrowed and 
modified for combination. Our future work aims 
to incorporate syntactic or semantic information 
into our paraphrasing framework. 
Acknowledgments  
We would like to thank the anonymous review-
ers for their valuable comments and suggestions. 
This work is supported by the National Science 
Foundation via Grant No. 0910778 entitled 
“Richer Representations for Machine Transla-
tion”.  
 
 
1057
Reference 
A. V. Aho and J. D. Ullman. 1969. Syntax directed 
translations and the pushdown assembler. Journal 
of Com-puter and System Sciences, 3:37–56. 
Yu Chen, Andreas Eisele, Christian Federmann, Eva 
Hasler, Michael Jellinghaus, and Silke Theison. 
2007. Multi-engine machine translation with an 
open-source SMT decoder. In Proceedings of 
WMT07 
Boxing Chen, Min Zhang and Aiti Aw. 2009a. A 
Comparative Study of Hypothesis Alignment and 
its Improvement for Machine Translation System 
Combination. In: Proceedings of ACL-IJCNLP.  pp. 
1067-1074. Singapore. August. 
Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi 
Zhang, Sabine Hunsicker, Silke Theison, Christian 
Federmann, Hans Uszkoreit. 2009b. Combining 
Multi-Engine Translations with Moses. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation 
David Chiang. Hierarchical phrase-based translation. 
2007. Computational Linguistics, 33(2):201–228. 
J. Devlin and S. Matsoukas. 2012. Trait-based hy-
pothesis selection for machine translation. In Proc. 
of NAACL 
Jinhua Du and Andy Way. 2010. Using TERp to 
Augment the System Combination for SMT. In 
Proceedings of the Ninth Conference of the Asso-
ciation for Machine Translation (AMTA2010) 
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and 
Yajuan Lu. 2009 Lattice-based System Combina-
tion for Statistical Machine Translation. In Pro-
ceedings of ACL 
Markus Freitag, Matthias Huck, and Hermann Ney. 
2014. Jane: Open source machine translation sys-
tem combination. In Conference of the European 
Chapter of the Association for Computational Lin-
guistics, pages 29–32, Gothenburg, Sweden, April. 
Association for Computational Linguistics. 
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory 
Shakhnarovich. 2013. A Systematic Exploration of 
Diversity in Machine Translation. In Proc. of 
EMNLP 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Ngu-
yen, and Robert Moore. 2008. Indirect-hmm-based 
hypothesis alignment for computing outputs from 
machine translation systems. In Proceedings of 
EMNLP 
Fei Huang and Kishore Papineni. 2007. Hierarchical 
System Combination for Machine Translation. In 
Proceed-ings of EMNLP-CoNLL 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In 
Proceedings of ACL-HLT 
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk 
decoding for statistical machine translation. In Pro-
ceedings of HLT 
Wei-Yun Ma and Kathleen McKeown. 2012. Phrase-
level System Combination for Machine Translation 
Based on Target-to-Target Decoding. In Proceed-
ings of the 10th Biennial Conference of the Asso-
ciation for Ma-chine Translation in the Americas 
(AMTA), San Diego, CA. 
Gregor Leusch, Markus Freitag, and Hermann Ney. 
The RWTH System Combination System for 
WMT 2011. 2011. In Proceedings of the Sixth 
Workshop on Statistical Machine Translation 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney 
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced 
hypotheses alignment. In Proceedings of EACL 
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. 
Dechelotte, M. Federico, M. Kolss, Y. S. Lee, J. B. 
Marino, M. Paulik, S. Roukos, H. Schwenk, and H. 
Ney. 2008. System combination for machine trans-
lation of spoken and written language. IEEE 
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222–1237, September. 
Sushant Narsale. JHU System Combination Scheme 
for WMT 2010. 2010. In Proceedings of the Fifth 
Workshop on Statistical Machine Translation 
Franz Josef Och. 2004. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of ACL 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007a. Improved word-level system 
combination for machine translation. In Proceed-
ings of ACL 
Antti-Veikko I. Rosti, Necip F. Ayan, Bing Xiang, 
Spyros Matsoukas, Richard Schwartz, and Bonnie 
J. Dorr. 2007b. Combining outputs from multiple 
machine translation systems. In Proceedings of 
NAACL-HLT 
Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine 
translation system combination. In Proceedings of 
ICASSP 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
with Targeted Human Annotation. In Proceedings 
of AMTA. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. TER-Plus: Paraphrase, Semantic, and 
Alignment Enhancements to Translation Edit Rate. 
Machine Translation, 23(2–3):117–127. 
1058
