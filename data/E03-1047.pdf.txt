Lexicalized Grammar Acquisition
Yusuke Miyaot	 Takashi Ninomiyatt	 Jun'ichi Tsujiit
f Department of Computer Science, University of Tokyo
Bongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
$CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp
Abstract
This paper presents a formalization
of automatic grammar acquisition that
is based on lexicalized grammar for-
malisms (e.g. LTAG and HPSG). We
state the conditions for the consistent ac-
quisition of a unique lexicalized gram-
mar from an annotated corpus.
1 Introduction
Linguistically motivated and computationally ori-
ented grammar theories take the form of lexical-
ized grammar formalisms; examples include Lex-
icalized Tree Adjoining Grammar (LTAG) (Sch-
abes et al., 1988), Combinatory Categorial Gram-
mar (Steedman, 2000), and Head-driven Phrase
Structure Grammar (HPSG) (Sag and Wasow,
1999). They have been a great success in terms
of linguistic analysis and efficiency in the parsing
of real-world texts. However, such grammars have
not generally been considered suitable for the syn-
tactic analysis within practical NLP systems be-
cause considerable effort is required to develop
and maintain lexicalized grammars that are both
robust and provide broad coverage.
One novel approach to grammar development is
based on the automatic acquisition of lexicalized
grammars from annotated corpora. Since lexical-
ized grammars represent grammatical constraints
with a few grammar rules and a large number of
lexical entries, the rules are quite easy to write but
the construction of a lexicon is unrealistic. The
idea in this study is to automatically obtain the lex-
ical entries from an annotated corpus, which will
greatly reduce the cost of building the grammar.
The approach has the following advantages.
First, the grammars obtained will be robust be-
cause appropriate lexical entries are consistently
acquired even for constructions beyond grammar
developers' intuition. Secondly, the grammar de-
velopers are simply required to annotate the closed
set of the training corpus, where various heuris-
tic and statistical methods are applicable. Con-
sistency between the grammar rules and the ob-
tained lexical entries is assured independently of
the methods of annotation. Lastly, the validity of
the grammar theories is evaluated on real-world
texts. A degree of low coverage by a linguistically
motivated grammar does not necessarily reflect in-
adequacy of the grammar theories; a lack of appro-
priate lexical entries may also be responsible. The
analysis of obtained grammars gives us grounds
for discussing the pros and cons of the theories.
The studies on the extraction of LTAG (Xia,
1999; Chen and Vijay-Shanker, 2000; Chiang,
2000) and CCG (Hockenmaier and Steedman,
2002) represent the first attempts at the acquisition
of linguistically motivated grammars from anno-
tated corpora. Those studies are limited to specific
formalisms, and can be interpreted as instances of
our approach as described in Section 3. This pa-
per does not describe any concrete algorithms for
grammar acquisition that depend on specific gram-
mar formalisms. The contribution of our work is
to formally state the conditions required for the ac-
quisition of lexicalized grammars and to demon-
127
strate that it can be applied to lexicalized gram-
mars other than LTAG and CCG, such as HPSG.
2 Lexicalized Grammars
In this section, we define the general form of lexi-
calized grammars including LTAG (Schabes et al.,
1988) and HPSG (Sag and Wasow, 1999). The
concepts behind a lexicalized grammar are that
i) grammar rules represent general grammatical
constructions in the language while ii) lexical en-
tries describe word-specific lexicallsyntactic con-
straints. Let W be a set of all words and C a set of
linguistic representations (e.g. the tree structures
of LTAG or typed feature structures of HPSG). We
can formally define a lexicalized grammar in the
following way.
Definition 1 (Lexicalized grammar) A lexical-
ized grammar is a tuple G = (L, R), where L
is a lexicon, i.e. Lc Wx C, and R is a set of
grammar rules, i.e. r E R is a partial function:
C x C C.
In what follows, we assume that all grammar rules
are binary for simplicity.
Parsing with a lexicalized grammar is the pro-
cess of applying grammar rules to lexical entries.
Since we assume that the rules are binary, the his-
tory of the process constitutes a binary-branching
tree; we define such structures in this paper as con-
stituent structures' (Miller, 2000).
Definition 2 (Constituent structure) Given a
sentence w, a constituent structure of w is the
least set F c IV satisfying i) w e F and ii)
V7 G F.(171 > 1 ] I71,72 G F.(7 = 7172))
The first condition represents inclusion of the top
node in the structure, and the second constrains the
terminals (words) to a linear order.
The process of parsing is then depicted by a
constituent structure labeled with grammar rules,
which we call a derivation history.
Definition 3 (Derivation history) Given a sen-
tence w, a derivation history is a tuple T = Kr, p),
where F is a constituent structure of w and p is a
function: F R. Derivation history T = F , p)
is a well-formed derivation history iff there exists
a function satisfying the following conditions.
'Note that the constituent structures we define here repre-
sent the process of parsing rather than the result of parsing.
• ew E w. ](2n, e L A	 = c
• V7 E F. 71.72 E F.7 = 7172 A - (7) =
P(7)((71), (72))
Let (xi be "(7i ) or 7i, we denote (7)	 a l ...
when 7 = -a 
The results of parsing (e.g. derived trees of
LTAG and phrasal signs of HPSG) by a lexicalized
grammar are outputs of the application of rules ac-
cording to derivation histories.
Definition 4 (Parse result) Given a sentence w,
c, E C is a parse result of w iff c, w for some
well-formed derivation history.
Given the above definitions, our task is to ob-
tain lexical entries (w, c) E L from a parsed cor-
pus, i.e., parse results C, E C. The idea is to
make derivation histories from a parsed corpus,
and reduce the parse results into lexical entries by
traversing the derivation histories.
3 Grammar Acquisition
Given Definitions 1, 3 and 4, we can deter-
mine unique lexical entries from a parse re-
sult if a derivation history is given and the
grammar rules r E R are injective functions,
i.e., Ve. Ee l , c2 , , c/2 . (c = r(ci, c2) A c
r (ci ci2)) (Cl = c 1 A c2 = c'2 )
Theorem 1 (Grammar acquisition) Given cs
C as the parse result of sentence w, and T as
its derivation history, lexical entries for w are
uniquely determined if all grammar rules r e R
are infective functions. That is, ]!ci, 	 ,c„ E
C. (Vi < n. (wi, ci) C L A cs c1
Proof. We construct lexical entries by inversely
applying the grammar rules to the parse result.
Since the grammar rules are infective, we are able
to uniquely determine the inputs for any rule ap-
plication. That is, V7 E F. ]!c C C. 31/32 E
C 5 . c5 Oic,32 A c 7. We prove this lemma
by the mathematical induction of the length of 7.
I. When 1-y1 =	 = w according to Defini-
tion 2. This case is trivial.
2. When 71 = k, we assume that the lemma
is true for Vi > k. From Definition 2,
C F . y	77 V	 = 7"7.
128
HEAD nounlar--head-filler
LSLASH
NP HEAD verb
LSLASH ,Np>
the girl r HEAD verb
;--NP>
he
talked /PP
"about
-HEAD verb 	 -HEADVerb -
talked: 88%s3r; talked: 88v,t6s ,,pp>
SLASH__SLASH <NP
about: Etirkai	about: EigNEH
Figure 1: A non-injective grammar rule in HPSG
Without loss of generality, we assume ^,1 =
77". By the assumption of the induction,
]!ci cs
 13102 A c' . Since p(71)
is injective, E!c, . c' = p(71 )(c, c"). Hence,
]!c, c". c, 131cc" 132. By Definition 3, c
We thus find that unique c exists for
The lemma is proved by 1 and 2. According to the
lemma, Vw C w. ]!c. c, =- 131 02 A c
Hence, the theorem has been proved. 	 111
The theorem shows that we are able to obtain a
unique sequence of lexical entries that are consis-
tent with the grammar rules, given the constituent
structures and corresponding rule applications.
However, the grammar rules of a lexicalized
grammar are not necessarily injective. For exam-
ple, Figure 1 shows a situation where the SLASH
feature of HPSG causes the same parse result for
distinct inputs. Phrase "talk about" has an NP
in the SLASH feature, but we cannot determine
the origination of the NP. A similar argument can
be made for LTAG on its adjunction rule. When
an auxiliary tree is adjoined into another tree, its
spine is melted into the other tree, which produces
the same result for distinct inputs.
To enable the acquisition of unique lexical en-
tries even when the grammar rules are not injec-
tive, we introduce a further definition that provides
a mark which disambiguates the inputs to gram-
mar rules. This allows us to map non-injective
rules into injective rules.
Definition 5 (Pseudo-injective) Grammar rule
r C R is pseudo-injective when there exists a func-
tion p such that r„ = c2.(r(ci, c2), P(ci , c2))
is an infective function. We call it a marking
function, and use RA to denote a set of r)„.
If all grammar rules are at least pseudo-injective,
unique lexical entries are determined Theorem 1
is now revised into a more general form.
Theorem 2 (Grammar acquisition (revised))
Given a parse result c, C C of a sentence w, a
marking function 11, and a derivation history T of
R„, lexical entries for w are uniquely determined
if all grammar rules r C R are pseudo-infective
functions with respect to ft.
Proof. We omit the proof since it is much the
same as the proof of Theorem].
Non-injective grammar rules in lexicalized
grammars can often be redefined as pseudo-
injective functions by the addition of some infor-
mation, which depends on grammar theories. We
now discuss grammar-theory-dependent issues.
CG The injective condition is apparently pre-
served in a simple CG, which is the class of cat-
egorial grammars composed of the two reduction
rules: Forward Application (FA) and Backward
Application (BA). While these rules take two cat-
egories as inputs and output another category, we
can regard the rules as taking two derived trees as
inputs and outputting a combined tree. With this
insight, we can regard reduction rules as grammar
rules, derived trees as parse results, derivations as
derivation histories, then Theorem 1 can be ap-
plied. The annotation cost will not be problematic
because we only need to annotate FA or BA to the
derived trees by using simple heuristic rules.
This argument is also valid for the extraction
of CCG. In order to annotate the other rules de-
fined by CCG (type-raising, composition, and co-
ordination rules), existing work (Hockenmaier and
Steedman, 2002) exploits the annotations of traces
and coordinations in the Penn Treebank.
LTAG As discussed above, adjoining leads to
the situations where the injective condition is vi-
olated. We can make the grammar rules pseudo-
injective by determining the lengths of the spines,
i.e., defining it as returning the spine length. Exist-
ing studies assume the length as one (Xia, 1999),
or determine the length by using heuristic rules
(Chen and Vijay-Shanker, 2000; Chiang, 2000).
The above discussion was empirically eval-
uated by acquiring LTAG grammars from the
head-subject
head-complement
129
Grammar (a) Grammar (b)
# words 4,514 # words 4,539
# initial trees 1,015 # initial trees 1,068
# aux. trees 1,094 # aux. trees 1,107
coverage 93.4 % coverage 94.0 %
Table 1: Specifications of the LTAG grammars ac-
quired from the Penn Treebank
Penn Treebank (Marcus et al., 1994). We as-
sumed that the length of all spines was one, i.e.,
VG', c2. c2) = 1. 47 heuristic rules were
used for the annotation, which shows the annota-
tion was not so costly. A grammar was success-
fully acquired from Sections 02-21 in 910 sec-
onds on a Xeon 2.0 GHz CPU. As shown in Gram-
mar (a) in Table l , the grammar had the sufficient
coverage (measured against Section 22, 1700 sen-
tences), which shows the robustness of the gram-
mar. We next acquired another grammar (Gram-
mar (b) in Table 1) by including two rules for an-
notating insertions and coordinations. Since the
grammar provides more accurate analyses for in-
sertions and coordinations, the coverage of the
grammar has been slightly improved.
HPSG In HPSG, two points lead to violate the
injective condition. One is ambiguity in terms
of the origin of subcategorization frames as de-
scribed in Section 3. In Figure 1, SLASH fea-
tures in HPSG are represented with a set, and
the mother's SLASH is the union of the daugh-
ters' ones. The union operation is apparently
non-injective, but the grammar rules can be made
pseudo-injective by defining itt as telling the ori-
gin of SLASH features. The other is generaliza-
tion through the use of a type hierarchy. In HPSG,
linguistic generalizations are described by a type
hierarchy, where the entities are placed in gen-
eral/specific relations. Merging of types, unifica-
tion, is inherently not injective. The generaliza-
tion problem is beyond the scope of our study, and
must be left for future research.
4 Conclusion
This study has demonstrated the conditions for ac-
quiring lexicalized grammars from annotated cor-
pora. We proved that a unique lexicalized gram-
mar consistent with the given grammar rules is ac-
quired when derivation histories are given. Our
approach enables the development of lexicalized
grammars that are robust and broad-coverage, and
also lets us compare various grammar theories
with real-world texts. This study provides a start-
ing point for the application of linguistic grammar
theories to real-world NLP systems.
Future work includes an application of our ap-
proach to other grammar theories, such as HPSG.
Although annotation of grammar rules (schemata)
might be more difficult than LTAG since they have
more rules, this will be solved by careful imple-
mentation of heuristic annotation rules.
References
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proc. 6th IWPT.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL 2000, pages 456-463.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with combinatory
categorial grammar. In Proc. 40th ACL, pages 335—
342.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In ARPA Human
Language Technology Workshop.
Philip H. Miller. 2000. Strong Generative Capacity:
The Semantics of Linguistic Formalism. CSLI Pub-
lications.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory —A Formal Introduction. CSLI Lcture Notes no.
92. CLSI Publications.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with `lexicalized gram-
mars': Application to tree adjoining grammars. In
Proc. 12th COLING, pages 578-583.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Fei Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proc. 5th NLPRS.
130
