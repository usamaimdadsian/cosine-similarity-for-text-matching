Comparing a Linguistic and a Stochastic Tagger 
Chr i s ter  Samuelsson  At ro  Vout i la inen  
Lucent Technologies Research Unit  for Mult i l ingu~l Language Techno logy  
Bell Laborator ies  P.O. Box 4 
600 Mounta in  Ave, Room 2D-339 FIN-00014 University of Helsinki  
.Murray Hill, NJ 07974, USA F in land 
christ er©research, bell-labs, tom Afro. Vout ilainen©Helsinki. FI 
Abst ract  
Concerning different approaches to auto- 
matic PoS tagging: EngCG-2, a constraint- 
based morphological tagger, is compared in 
a double-blind test with a state-of-the-art 
statistical tagger on a common disambigua- 
tion task using a common tag set. The ex- 
periments how that for the same amount 
of remaining ambiguity, the error rate of 
the statistical tagger is one order of mag- 
nitude greater than that of the rule-based 
one. The two related issues of priming 
effects compromising the results and dis- 
agreement between human annotators are 
also addressed. 
1 In t roduct ion  
There are currently two main methods for auto- 
matic part-of-speech tagging. The prevailing one 
uses essentially statistical language models automat- 
ically derived from usually hand-annotated corpora. 
These corpus-based models can be represented e.g. 
as collocational matrices (Garside et al. (eds.) 1987: 
Church 1988), Hidden Markov models (cf. Cutting 
et al. 1992), local rules (e.g. Hindle 1989) and neu- 
ral networks (e.g. Schmid 1994). Taggers using these 
statistical language models are generally reported to 
assign the correct and unique tag to 95-97% of words 
in running text. using tag sets ranging from some 
dozens to about 130 tags. 
The less popular approach is based on hand-coded 
linguistic rules. Pioneering work was done in the 
1960"s (e.g. Greene and Rubin 1971). Recently, new 
interest in the linguistic approach as been shown 
e.g. in the work of (Karlsson 1990: Voutilainen et 
al. 1992; Oflazer and Kuru6z 1994: Chanod and 
Tapanainen 1995: Karlsson et al. (eds.) 1995; Vouti- 
lainen 1995). The first serious linguistic competitor 
to data-driven statistical taggers is the English Con- 
straint Grammar parser. EngCG (cf. Voutilainen et 
al. 1992; Karlsson et al. (eds.) 1995). The tagger 
consists of the following sequentially applied mod- 
ules: 
1. Tokenisation 
2. Morphological analysis 
(a) Lexical component 
(b) Rule-based guesser for unknown words 
3. Resolution of morphological mbiguities 
The tagger uses a two-level morphological nal- 
yser with a large lexicon and a morphological 
description that introduces about 180 different 
ambiguity-forming morphological nalyses, as a re- 
sult of which each word gets 1.7-2.2 different analy- 
ses on an average. Morphological analyses are as- 
signed to unknown words with an accurate rule- 
based 'guesser'. The morphological disambiguator 
uses constraint rules that discard illegitimate mor- 
phological analyses on the basis of local or global 
context conditions. The rules can be grouped as 
ordered subgrammars: e.g. heuristic subgrammar 2 
can be applied for resolving ambiguities left pending 
by the more "careful' subgrammar 1.
Older versions of EngCG (using about 1,150 con- 
straints) are reported (~butilainen etal. 1992; Vouti- 
lainen and HeikkiUi 1994; Tapanainen and Vouti- 
lainen 1994; Voutilainen 1995) to assign a correct 
analysis to about 99.7% of all words while each word 
in the output retains 1.04-1.09 alternative analyses 
on an average, i.e. some of the ambiguities remait~ 
unresolved. 
These results have been seriously questioned. One 
doubt concerns the notion 'correct analysis". For 
example Church (1992) argues that linguists who 
manually perform the tagging task using the double- 
blind method disagree about the correct analysis in 
at least 3% of all words even after they have nego- 
tiated about the initial disagreements. If this were 
the case, reporting accuracies above this 97% "upper 
bound' would make no sense. 
However, Voutilainen and J~rvinen (1995) empir- 
ically show that an interjudge agreement virtually 
of 1()0% is possible, at least with the EngCG tag set 
if not with the original Brown Corpus tag set. This 
consistent applicability of the EngCG tag set is ex- 
plained by characterising it as grammatically rather 
than semantically motivated. 
246 
Another main reservation about the EngCG fig- 
ures is the suspicion that, perhaps partly due to the 
somewhat underspecific nature of the EngCG tag 
set, it must be so easy to disambiguate hat also a 
statistical tagger using the EngCG tags would reach 
at least as good results. This argument will be ex- 
amined in this paper. It will be empirically shown 
(i) that the EngCG tag set is about as difficult for a 
probabilistic tagger as more generally used tag sets 
and (ii) that the EngCG disambiguator has a clearly 
smaller error rate than the probabilistic tagger when 
a similar (small) amount of ambiguity is permitted 
in the output. 
A state-of-the-art statistical tagger is trained on 
a corpus of over 350,000 words hand-annotated with 
EngCG tags. then both taggers (a new version 
known as En~CG-21 with 3,600 constraints as five 
subgrammars-, and a statistical tagger) are applied 
to the same held-out benchmark corpus of 55,000 
words, and their performances are compared. The 
results disconfirm the suspected 'easiness' of the 
EngCG tag set: the statistical tagger's performance 
figures are no better than is the case with better 
known tag sets. 
Two caveats are in order. What we are not ad- 
dressing in this paper is the work load required for 
making a rule-based or a data-driven tagger. The 
rules in EngCG certainly took a considerable effort 
to write, and though at the present state of knowl- 
edge rules could be written and tested with less ef- 
fort, it may well be the case that a tagger with an 
accuracy of 95-97% can be produced with less effort 
by using data-driven techniques. 3 
Another caveat is that EngCG alone does not re- 
solve all ambiguities, o it cannot be compared to a 
typical statistical tagger if full disambiguation is re- 
quired. However, "~butilainen (1995) has shown that 
EngCG combined with a syntactic parser produces 
morphologically unambiguous output with an accu- 
racy of 99.3%, a figure clearly better than that of the 
statistical tagger in the experiments below (however. 
the test data was not the same). 
Before examining the statistical tagger, two prac- 
tical points are addressed: the annotation of tile cor- 
pora used. and the modification of the EngCG tag 
set for use in a statistical tagger. 
1An online version of EngCG-2 can be found at, 
ht tp://www.ling.helsinki.fi/"avoutila/engcg-2.ht ml. 
:The first three subgrammars are generally highly re- 
liable and almost all of the total grammar development 
time was spent on them: the last two contain rather 
rough heuristic onstraints. 
3However, for an interesting experiment suggesting 
otherwise, see (Chanod and Tapanainen 1995). 
2 P reparat ion  of  Corpus  Resources  
2.1 Annotat ion  of training corpus 
The stochastic tagger was trained on a sample of 
357,000 words from the Brown University Corpus 
of Present-Day English (Francis and Ku6era 1982) 
that was annotated using the EngCG tags. The cor- 
pus was first analysed with the EngCG lexical anal- 
yser, and then it was fully disambiguated and, when 
necessary, corrected by a human expert. This an- 
notation took place a few years ago. Since then, it 
has been used in the development of new EngCG 
constraints (the present version, EngCG-2, contains 
about 3,600 constraints): new constraints were ap- 
plied to the training corpus, and whenever a reading 
marked as correct was discarded, either the analysis 
in the corpus, or the constraint i self, was corrected. 
In this way, the tagging quality of the corpus was 
continuously improved. 
2.2 Annotat ion  of benchmark corpus 
Our comparisons use a held-out benchmark corpus 
of about 55,000 words of journalistic, scientific and 
manual texts, i.e., no ,training effects are expected 
for either system. The benchmark corpus was an- 
notated by first applying the preprocessor and mor- 
phological aaalyser, but not the morphological dis- 
ambiguator, to the text. This morphologically am- 
biguous text was then independently and fully dis- 
ambiguated by two experts whose task was also to 
detect any errors potentially produced by the pre- 
viously applied components. They worked indepen- 
dently, consulting written documentation f the tag 
set when necessary. Then these manually disam- 
biguated versions were automatically compared with 
each other. At this stage, about 99.3% of all anal- 
yses were identical. When the differences were col- 
lectiyely examined, virtually all were agreed to be 
due to clerical mistakes. Only in the analysis of 21 
words, different (meaning-level) interpretations per- 
sisted, and even here both judges agreed the ambigu- 
ity to be genuine. One of these two corpus versions 
was modified to represent he consensus, and this 
"consensus corpus' was used as a benchmark in the 
evaluations. 
As explained in Voutilainen and J/irvinen (1995). 
this high agreement rate is due to two main factors. 
Firstly, distinctions based on some kind of vague se- 
mantics are avoided, which is not always case with 
better known tag sets. Secondly. the adopted analy- 
sis of most of the constructions where humans tend 
to be uncertain is documented as a collection of tag 
application principles in the form of a grammar- 
inn's manual (for further details, cf. Voutilainen and 
J/irvinen 1995). 
Tile corpus-annotation procedure allows us t.o per- 
form a text-book statistical hypothesis test. Let 
tile null hypothesis be that any two human eval- 
uators will necessarily disagree in at least 3% of 
247 
the cases. Under this assumption, the probability 
of an observed disagreement of less than 2.88% is 
less than 5%. This can be seen as follows: For 
the relative frequency of disagreement, fn, we have 
t - . - - . . -  
that f .  is approximately --, N(p, ~/~) ,  where p 
is the actual disagreement probability and n is the 
number of trials, i.e., the corpus size. This means 
fn -P  v/- ff that P(( ~ < z) ~ ~(x) where ¢b is the 
standard normal distribution function. This in turn 
means that 
P ( f , < p + z P~ - p-----~) ) ,~ ~ ( z ) 
Here n is 55,000 and ~(-1.645) = 0.05. Under the 
null hypothesis, p is at least 3% and thus: 
. /O.O3.0.97 
P( f .  < o.o3- 1.64%/-g,o-g6 ) - 
= P(A  <__ 0.0288) < 0.05 
We can thus discard the null hypothesis at signifi- 
cance level 5% if the observed isagreement is less 
than 2.88%. It was in fact 0.7% before error cor- 
.21)  rection, and virtually zero ( ~  after negotia- 
tion. This means that we can actually discard the 
hypotheses that the human evaluators in average 
disagree in at least 0.8% of the cases before error 
correction, and in at least 0.1% of the cases after 
negotiations, at significance level 5%. 
2.3 Tag set conversion 
The EugCG morphological analyser's output for- 
mally differs from most tagged corpora; consider the 
following 5-ways ambiguous analysis of "'walk": 
walk 
walk <SV> <SVO> V SUBJUNCTIVE VFIN 
walk <SV> <SVO> V IMP VFIN 
walk <SV> <SVG> V INF 
walk <SV> <SVO> V PRES -SG3 VFIN 
walk N NOM SG 
Statistical taggers usually employ single tags to 
indicate analyses (e.g. "'NN" for "'N NOM SG"). 
Therefore a simple conversion program was made for 
producing the following kind of output, where each 
reading is represented as a single tag: 
walk V-SUBJUNCTIVE V-IMP V-INF 
V-PRES-BASE N-NOM-SG 
The conversion program reduces the multipart 
EngCG tags into a set of 80 word tags and 17 punc- 
tuation tags (see Appendix) that retain the central 
linguistic characteristics of the original EngCG tag 
set. 
A reduced version of the benchmark corpus was 
prepared with this conversion program for the sta- 
tistical tagger's use. Also EngCG's output was con- 
verted into this format to enable direct comparison 
with the statistical tagger. 
8 The Stat ist ical  Tagger 
The statistical tagger used in the experiments i a 
classical trigram-based HMM decoder of the kind 
described in e.g. (Church 1988), (DeRose 1988) and 
numerous other articles. Following conventional no- 
tation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn 
and Samuelsson 1996, pp. 42-46), the tagger recur- 
sively calculates the ~, 3, 7 and 6 variables for each 
word string position t = 1 . . . . .  T and each possible 
s ta te  4 s i  : i = 1 , . . . ,n :  
a,(i) = P(W<,;S,  = si) 
.'3,(i) = P(W>,  IS, = s~) 
7t{i) --- 
&(i) = 
Here 
W 
W5t 
W>t 
Sst 
P(W; & = si) 
P(&=s i IW)  = P(W) 
~,(i). 3,(i) 
r6 
y~o~,(i) .  3,(i) 
i=l  
max P(S<t- l ,  S= = si; W<,)  
S<,_t 
= l/V1 = w lq , . . . ,  ~VT = Wkr  
- -  ~'VI = wk~ , . . . , Wt  = wk ,  
"- l~Vt+l = wk,+ t,   . ,  I 'VT = Wkr  
-= S1 = si~ . . . . .  S t  = s i ,  
where St = si is the event of the tth word being 
emitted from state si and Wt = wk, is the event of 
the tth word being the particular word w~, that was 
actually observed in the word string. 
Note that for t = 1 . . . . .  T -1  ; i , j -  l . . . . .  n 
at+~(j) 
3,(0 = ~ 3,+1(j) "Pij .aj~,+~ 
j= l  
where pij = P(St+I = sj I St = si) are the transi- 
tion probabilities, encoding the tag N-gram proba- 
bilities, and 
a jk  = 
= P(Wt=wkIS ,=s j )  = P(Wt=w~l , \ ' t=z j )  
4The N- I  th-order  HMM corresponding to an N-gram 
tagger  is encoded as a first-order HMM,  where each s tate  
cor responds  to a sequence of ,V-I tags, i.e., for a t r igram 
tagger,  each s tate  corresponds to a tag pair. 
248 
are the lexical probabilities. Here X, is the random 
variable of assigning a tag to the tth word and xj is 
the last tag of the tag sequence ncoded as state sj. 
Note that si # sj need not imply zi # zj. 
More precisely, the tagger employs the converse 
lexical probabilities 
P(Xt = zj I Wt = w,) ajk 
a~ k = P(X, = zj) P(W, = wk) 
This results in slight variants a', fl', 7' and 6' of the 
original quantities: 
~,( i )  6,(i) ' 
= = I - \ [  P (Wu = o4(i ) 6;(i) .=1 
~,(i) r 
- H P(W~ =w~=) /3;(i) u=t+l 
and thus Vi, t 
7~(i) = a;(i) ./3;(i) = 
ka ; ( i )  ./3;(i1 
i=1 
~,(i) .~,(i) 
and Vt 
~e, ( i )  ./3t(i) 
i=1 
= 7t(0 
argmax6;(i) = argmax6t(i) 
l< i<n l< i<n 
The rationale behind this is to facilitate estimat- 
ing the model parameters from sparse data. In more 
detail, it is easy to estimate P(tag I word) for a pre- 
viously unseen word by backing off to statistics de- 
rived from words that end with the same sequence 
of letters (or based on other surface cues), whereas 
directly estimating P(word I tag) is more difficult. 
This is particularly useful for languages with a rich 
inflectional and derivational morphology, but also 
for English: for example, the suffix "-tion" is a 
strong indicator that the word in question is a noun; 
the suffix "-able" that it is an adjective. 
More technically, the lexicon is organised as a 
reverse-suffix tree, and smoothing the probability es- 
timates is accomplished by blending the distribution 
at the current node of the tree with that of higher- 
level nodes, corresponding to (shorter) suffixes of the 
current word (suffix). The scheme also incorporates 
probability distributions for the set of capitalized 
words, the set of all-caps words and the set of in- 
frequent words, all of which are used to improve the 
estimates for unknown words. Employing a small 
amount of back-off smoothing also for the known 
words is useful to reduce lexical tag omissions. Em- 
pirically, looking two branching points up the tree 
for known words, and all the way up to the root 
for unknown words, proved optimal. The method 
for blending the distributions applies equally well to 
smoothing the transition probabilities pij, i.e., the 
tag N-gram probabilities, and both the scheme and 
its application to these two tasks are described in de- 
tail in (Samuelsson 1996), where it was also shown 
to compare favourably to (deleted) interpolation, see 
(Jelinek and Mercer 1980), even when the back-off 
weights of the latter were optimal. 
The 6 variables enable finding the most probable 
state sequence under the HMM, from which the most 
likely assignment of tags to words can be directly es- 
tablished. This is the normal modus operandi of an 
HMM decoder. Using the 7 variables, we can calcu- 
late the probability of being in state si at string po- 
sition t, and thus having emitted wk, from this state, 
conditional on the entire word string. By summing 
over all states that would assign the same tag to this 
word, the individual probability of each tag being as- 
signed to any particular input word, conditional on 
the entire word string, can be calculated: 
P(X, = z i lW)  = 
= Z P(S,=sj t W) = E 7,(J) 
8 j : r j=r  i $ j : r j  =~'= 
This allows retaining multiple tags for each word by 
simply discarding only low-probability tags; those 
whose probabilities are below some threshold value. 
Of course, the most probable tag is never discarded, 
even if its probability happens to be less than the 
threshold value. By varying the threshold, we can 
perform a recall-precision, or error-rate-ambiguity, 
tradeoff. A similar strategy is adopted in (de Mar- 
cken 1990). 
4 Exper iments  
The statistical tagger was trained on 357,000 words 
from the Brown corpus (Francis and Ku~era 1982), 
reannotated using the EngCG annotation scheme 
(see above). In a first set of experiments, a 35,000 
word subset of this corpus was set aside and used to 
evaluate the tagger's performance when trained on 
successively arger portions of the remaining 322,000 
words. The learning curve, showing the error rate al- 
ter full disambiguation as a function of the amount 
of training data used, see Figure 1, has levelled off at 
322,000 words, indicating that little is to be gained 
from further training. We also note that the ab- 
solute value of the error rate is 3.51% - -  a typi- 
cal state-of-the-art figure. Here, previously unseen 
words contribute 1.08% to the total error rate, while 
the contribution from lexical tag omissions is 0.08% 
95% confidence intervals for the error rates would 
range from + 0.30% for 30,000 words to + 0.20~c at 
322.000 words. 
The tagger was then trained on the entire set 
of 357,000 words and confronted with the separate 
55,000-word benchmark corpus, and run both in full 
249 
8 
v 
6 
.~ 5 
~ 4 
~ 3 
o 2 
1 
0 
Learning curve 
, 
I I I I I I 
0 50 I00 150 200 250 300 
Train ing set (kWords) 
Figure 1: Learning curve for the statistical tagger 
on the Brown corpus. 
Ambiguity 
(Tags/word) 
1.000 
1.012 
1.025 
1.026 
1.035 
1.038 
1.048 
1.051 
1.059 
1.065 
1.070 
1.078 
1.093 
Error rate (%) 
Statistical Tagger EngCG 
(~) (7) 
4.72 4.68 
4.20 
3.75 
(3.72) 
(3.48) 
3.40 
(3.20) 
3.14 
(2.99) 
2.87 
(2.80) 
2.69 
2.55 
0.43 
0.29 
0.15 
0.12 
0.10 
Table h Error-rate-ambiguity tradeoff or both tag- 
gets on the benchmark corpus. Parenthesized num- 
bers are interpolated. 
and partial disambiguation mode. Table 1 shows 
the error rate as a function of remaining ambiguity 
(tags/word) both for the statistical tagger, and for 
the EngCG-2 tagger. The error rate for full disana- 
biguation using the 6 variables is 4.72% and using 
the 7 variables is 4.68%, both -4-0.18% with confi- 
dence degree 95%. Note that the optimal tag se- 
quence obtained using the 7 variables need not equal 
the optimal tag sequence obtained using the 6 vari- 
ables. In fact, the former sequence may be assigned 
zero probability by the HMM, namely if one of its 
state transitions has zero probability. 
Previously unseen words account for 2.01%, and 
lexical tag omissions for 0.15% of the total error rate. 
These two error sources are together exactly 1.00% 
higher on the benchmark corpus than on the Brown 
corpus, and account for almost the entire difference 
in error rate. They stem from using less complete 
lexical information sources, and are most likely the 
effect of a larger vocabulary overlap between the test 
and training portions of the Brown corpus than be- 
tween the Brown and benchmark corpora. 
The ratio between the error rates of the two tag- 
gets with the same amount of remaining ambiguity 
ranges from 8.6 at 1.026 tags/word to 28,0 at 1.070 
tags/word. The error rate of the statistical tagger 
can be further decreased, at the price of increased 
remaining ambiguity, see Figure 2. In the limit of 
retaining all possible tags, the residual error rate is 
entirely due to lexical tag omissions, i.e., it is 0.15%, 
with in average 14.24 tags per word. The reason 
that this figure is so high is that the unknown words, 
which comprise 10% of the corpus, are assigned all 
possible tags as they are backed off all the way to 
the root of the reverse-suffix tree. 
5 
v 4 
3 
2 
O 
0 
Error - ra te -ambigu i ty  trade-off  
i ! i l i l i 
I I I I i I r- 
2 4 6 8 i0 12 14 
Remain ing ambigui ty  (Tags/Word) 
Figure 2: Error-rate-ambiguity tradeoff or the sta- 
tistical tagger on the benchmark corpus. 
5 D iscuss ion  
Recently voiced scepticisms concerning the superior 
EngCG tagging results boil down to the following: 
 The reported results are due to the simplicity 
of the tag set employed by the EngCG system. 
 The reported results are an effect of trading 
high ambiguity resolution for lower error rate. 
 The results are an effect of so-called priming 
of the huraan annotators when preparing the 
test corpora, compromising the integrity of the 
experimental evaluations. 
In the current article, these points of criticism 
were investigated. A state-of-the-art statistical 
tagger, capable of performing error-rate-ambiguity 
tradeoff, was trained on a 357,000-word portion of 
the Brown corpus reannotated with the EngCG tag 
set, and both taggers were evaluated using a sep- 
arate 55,000-word benchmark corpus new to both 
250 
systems. This benchmark corpus was independently 
disambiguated by two linguists, without access to 
the results of the automatic taggers. The initial 
differences between the linguists' outputs (0.7% of 
all words) were jointly examined by the linguists; 
practically all of them turned out to be clerical er- 
rors (rather than the product of genuine difference 
of opinion). 
In the experiments, the performance of the 
EngCG-2 tagger was radically better than that of 
the statistical tagger: at ambiguity levels common 
to both systems, the error rate of the statistical tag- 
ger was 8.6 to 28 times higher than that of EngCG- 
2. We conclude that neither the tag set used by 
EngCG-2, nor the error-rate-ambiguity tradeoff, nor 
any priming effects can possibly explain the observed 
difference in performance. 
Instead we must conclude that the lexical and con- 
textual information sources at the disposal of the 
EngCG system are superior. Investigating this em- 
pirically by granting the statistical tagger access to 
the same information sources as those available in 
the Constraint Grammar framework constitutes fu- 
ture work. 
Acknowledgements  
Though Voutilainen is the main author of the 
EngCG-2 tagger, the development of the system 
has benefited from several other contributions too. 
Fred Karlsson proposed the Constraint Grammar 
framework in the late 1980s. Juha Heikkil£ and 
Timo J~irvinen contributed with their work on En- 
glish morphology and lexicon. Kimmo Koskenniemi 
wrote the software for morphological nalysis. Pasi 
Tapanainen has written various implementations of 
the CG parser, including the recent CG-2 parser 
(Tapanainen 1996). 
The quality of the investigation and presentation 
was boosted by a number of suggestions to improve- 
ments and (often sceptical) comments from numer- 
ous ACL reviewers and UPenn associates, in partic- 
ular from Mark Liberman. 
References  
J-P Chanod and P. Tapanainen. 1995. Tagging 
French: comparing a statistical and a constraint- 
based method. In Procs. 7th Conference of the 
European Chapter of the Association for Compu- 
tational Lingaistics, pp. 149-157, ACL, 1995. 
K. W. Church. 1988. "'A Stochastic Parts Program 
and Noun Phrase Parser for Unrestricted Text.". 
In Procs. 2nd Conference on Applied Natural Lan- 
guage Processing, pp. 136-143, ACL, 1988. 
K. Church. 1992. Current Practice in Part of 
Speech Tagging and Suggestions for the Future. in 
Simmons (ed.), Sbornik praci: In Honor of Henry 
Ku6era. Michigan Slavic Studies, 1992. 
D. Cutting, J. Kupiec, J. Pedersen and P. Sibun. 
1992. A Practical Part-of-Speech Tagger. In 
Procs. 3rd Conference on Applied Natural Lan- 
guage Processing, pp. 133-140, ACL, 1992. 
S. J. DeRose. 1988. "Grammatical Category 
Disambiguation by Statistical Optimization". In 
Computational Linguistics 14(1), pp. 31-39, ACL, 
1988. 
N. W. Francis and H. Ku~era. 1982. Fre- 
quency Analysis of English Usage, Houghton Mif- 
flin, Boston, 1982. 
R. Garside, G. Leech and G. Sampson (eds.). 1987. 
The Computational Analysis of English. London 
and New York: Longman, 1987. 
B. Greene and G. Rubin. 1971. Automatic gram- 
matical tagging of English. Brown University, 
Providence, 1971. 
D. Hindle. 1989. Acquiring disambiguation rules 
from text. In Procs. 27th Annual Meeting of the 
Association for Computational Linguistics, pp. 
118-125, ACL, 1989. 
F. Jelinek and R. L. Mercer. 1980. "Interpolated 
Estimation of Markov Source Paramenters from 
Sparse Data". Pattern Recognition in Practice: 
381-397. North Holland, 1980. 
F. Karlsson. 1990. Constraint Grammar as a 
Framework for Parsing Running Text. In Procs. 
CoLing'90. In Procs. 14th International Confer- 
ence on Computational Linguistics, ICCL, 1990. 
F. Karlsson, A. Voutilainen, J. Heikkilii and A. 
Anttila (eds.). 1995. Constraint Grammar. A 
Language-Independent System for Parsing Unre- 
stricted Tezt. Berlin and New York: Mouton de 
Gruyter, 1995. 
B. Krenn and C. Samuelsson. The Linguist's 
Guide to Statistics. Version of April 23, 1996. 
http ://coli. uni-sb, de/~christ er. 
C. G. de Marcken. 1990. "Parsing the LOB Cor- 
pus". In Procs. 28th Annual Meeting of the As- 
sociation for Computational Linguistics, pp. 243- 
251, ACL, 1990. 
K. Oflazer and I. KuruSz. 1994. Tagging and 
morphological disambiguation f Turkish text. In 
Procs. 4th Conference on Applied Natural La1~- 
guage Processing. ACL. 1994. 
L. R. Rabiner. 1989. "A Tutorial on Hid- 
den Markov Models and Selected Applications 
in Speech Recognition". In Readings in Speech 
Recognition, pp. 267-296. Alex Waibel and Kai- 
Fu Lee (eds), Morgan I<aufmann, 1990. 
G. Sampson. 1995. English for the Computer, Ox- 
ford University Press. 1995. 
251 
C. Samuelsson. 1996. "Handling Sparse Data by 
Successive Abstraction". In Procs. 16th Interna- 
tional Conference on Computational Linguistics, 
pp. 895-900, ICCL, 1996. 
H. Schmid. 1994. Part-of-speech tagging with neu- 
ral networks. In Procs. 15th International Confer- 
ence on Computational Linguistics, pp. 172-176, 
ICCL, 1994. 
P. Tapanainen. 1996. The Constraint Grammar 
Parser CG-2. Publ. 27, Dept. General Linguistics, 
University of Helsinki, 1996. 
P. Tapanainen and A. Voutilainen. 1994. Tagging 
accurately - don't guess if you know. In Procs. 4th 
Conference on Applied Natural Language Process- 
ing, ACL, 1994. 
A. Voutilainen. 1995. "A syntax-based part of 
speech analyser". In Procs. 7th Conference of the 
European Chapter of the Association for Compu- 
tational Linguistics, pp. 157-164, ACL, 1995. 
A. Voutilainen and J. Heikkil~. 1994. An English 
constraint grammar (EngCG): a surface-syntactic 
parser of English. In Fries, Tottie and Schneider 
(eds.), Creating and using English language cor- 
pora, Rodopi, 1994. 
A. Voutilainen, J. Heikkil~ and A. Anttila. 1992. 
Constraint Grammar of English. A Performance- 
Oriented Introduction. Publ. 21, Dept. General 
Linguistics, University of Helsinki, 1992. 
A. Voutilainen and T. J~irvinen. "Specifying ashal- 
low grammatical representation for parsing pur- 
poses". In Procs. 7th Conference of the Euro- 
pean Chapter of the Association for Computa- 
tional Linguistics, pp. 210-214, ACL, 1995. 
252 
Append ix :  Reduced  EngCG tag  set  
ING 
Punctuation tags: BE-IMP N-GEN-SG/PL 
'~colon BE-INF N-GEN-PL 
@comma BE-ING N-GEN-SG 
:~d~h BE-PAST-BASE N-NOM-SG/PL 
~dotdot BE-PAST-WAS N-NOM-PL 
@dquote BE-PRES-AM N-NOM-SG 
@exclamation BE-PRES-ARE NEG 
@fuUstop BE-PRES-IS NUM-CARD 
@lparen  BE-SUBJUNCTIVE NUM-FRA-PL 
@rparen CC NUM-FRA-SG 
@rparen CCX NUM-ORD 
@rparen CS PREP 
@rparen DET-SG/PL PRON 
@lquote DET-SG PRON-ACC 
@rquote DET-WH PRON-CMP 
@slash DO-EN PRON-DEM-PL 
@newlines DO-IMP PRON-DEM-SG 
@question DO-INF PRON-GEN 
@semicolon DO-ING PRON-INTERR 
Word tags: DO-PAST PRON-NOM-SG/PL 
A-ABS DO-PRES-BASE PRON-NOM-PL 
A-CMP DO-PRES-SG3 PRON-NOM-SG 
A-SUP DO-SUBJUNCTIVE PRON-REL 
ABBR-GEN-SG/PL EN PRON-SUP 
ABBR-GEN-PL HAVE-EN PRON-WH 
ABBR-GEN-SG HAVE-IMP V-AUXMOD 
ABBR-NOM-SG/PL HAVE-INF V-IMP 
ABBR-NOM-PL HAVE-ING V-INF 
ABBR-NOM-SG HAVE-PAST V-PAST 
ADV-ABS HAVE-PRES-BASE V-PRES-BASE 
ADV-CMP HAVE-PRES-SG3 V-PRES-SG1 
ADV-SUP HAVE-SUBJUNCTIVE V-PRES-SG2 
ADV-WH I V-PRES-SG3 
BE-EN INFMARK V-SUBJUNCTIVE 
253 
