Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 159–168,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Cross-Lingual Sentiment Analysis using modified BRAE
Sarthak Jain
Department of Computer Engineering
Delhi Technological University
DL, India
successar@gmail.com
Shashank Batra
Department of Computer Engineering
Indian Institute of technology, Delhi
DL, India
shashankg@gmail.com
Abstract
Cross-Lingual Learning provides a mech-
anism to adapt NLP tools available for la-
bel rich languages to achieve similar tasks
for label-scarce languages. An efficient
cross-lingual tool significantly reduces the
cost and effort required to manually an-
notate data. In this paper, we use the
Recursive Autoencoder architecture to de-
velop a Cross Lingual Sentiment Analysis
(CLSA) tool using sentence aligned cor-
pora between a pair of resource rich (En-
glish) and resource poor (Hindi) language.
The system is based on the assumption
that semantic similarity between different
phrases also implies sentiment similarity in
majority of sentences. The resulting sys-
tem is then analyzed on a newly developed
Movie Reviews Dataset in Hindi with la-
bels given on a rating scale and compare
performance of our system against exist-
ing systems. It is shown that our approach
significantly outperforms state of the art
systems for Sentiment Analysis, especially
when labeled data is scarce.
1 Introduction
Sentiment Analysis is a NLP task that deals with
extraction of opinion from a piece of text on a
topic. This is used by a large number of advertising
and media companies to get a sense of public opin-
ion from their reviews. The ever increasing user
generated content has always been motivation for
sentiment analysis research, but majority of work
has been done for English Language. However, in
recent years, there has been emergence of increas-
ing amount of text in Hindi on electronic sources
but NLP Frameworks to process this data is sadly
miniscule. A major cause for this is the lack of
annotated datasets in Indian Languages.
One solution is to create cross lingual tools be-
tween a resource rich and resource poor language
that exploit large amounts of unlabeled data and
sentence aligned corpora that are widely available
on web through bilingual newspapers, magazines,
etc. Many different approaches have been identi-
fied to perform Cross Lingual Tasks but they de-
pend on the presence of MT-System or Bilingual
Dictionaries between the source and target lan-
guage.
In this paper, we use Bilingually Constrained
Recursive Auto-encoder (BRAE) given by (Zhang
et al., 2014) to perform Cross Lingual sentiment
analysis. Major Contributions of this paper are
as follows: First, We develop a new Rating scale
based Movie Review Dataset for Hindi. Second,
a general framework to perform Cross Lingual
Classification tasks is developed by modifying
the architecture and training procedure for BRAE
model. This model exploits the fact that phrases in
two languages, that share same semantic meaning,
can be used to learn language independent seman-
tic vector representations. These embeddings can
further be fine-tuned using labeled dataset in En-
glish to capture enough class information regard-
ing Resource poor language. We train the resultant
framework on English-Hindi Language pair and
evaluate it against state of the art SA systems on
existing and newly developed dataset.
2 Related Work
2.1 Sentiment Analysis in Hindi
In recent years, there have been emergence of
works on Sentiment Analysis (both monolingual
and cross-lingual) for Hindi. (Joshi et al., 2010)
provided a comparative analysis of Unigram based
In-language, MT based Cross Lingual and Word-
Net based Sentiment classifier, achieving highest
accuracy of 78.14%. (Mittal et al., 2013) described
a system based on Hindi SentiWordNet for assign-
159
ing positive/negative polarity to movie reviews. In
this approach, overall semantic orientation of the
review document was determined by aggregating
the polarity values of the words in the document
assigned using the WordNet. They also included
explicit rules for handling Negation and Discourse
relations during preprocessing in their model to
achieve better accuracies.
For Languages where labeled data is not present,
approaches based on cross-lingual sentiment anal-
ysis are used. Usually, such methods need inter-
mediary machine translation system (Wan et al.,
2011; Brooke et al., 2009) or a bilingual dictionary
(Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge
the language gap. Given the subtle and different
ways in which sentiments can be expressed and the
cultural diversity amongst different languages, an
MT system has to be of a superior quality to per-
form well(Balamurali et al., 2012).
(Balamurali et al., 2012) present an alterna-
tive approach to Cross Lingual Sentiment Analy-
sis (CLSA) using WordNet senses as features for
supervised sentiment classification. A document
in Resource Poor Language was tested for polarity
through a classifier trained on sense marked and
polarity labeled corpora in Resource rich language.
The crux of the idea was to use the linked Word-
Nets of two languages to bridge the language gap.
Recently, (Popat et al., 2013) describes a Cross
Lingual Clustering based SA System. In this ap-
proach, features were generated using syntagmatic
property based word clusters created from unla-
beled monolingual corpora, thereby eliminating
the need for Bilingual Dictionaries. These features
were then used to train a linear SVM to predict
positive or negative polarity on a tourism review
dataset.
2.2 Autoencoders in NLP Tasks
Autoencoders are neural networks that learn a low
dimensional vector representation of fixed-size in-
puts such as image segments or bag-of-word rep-
resentations of documents. They can be used to
efficiently learn feature encodings that are useful
for classification. The Autoencoders were first
applied in a recursive setting by Pollack (1990)
in recursive auto-associative memories (RAAMs).
However, RAAMs needed fixed recursive data
structures to learn vector representations, whereas
RAE given by (Socher et al., 2011) builds recur-
sive data structure using a greedy algorithm. The
RAE can be pre-trainedwith an unsupervised algo-
rithm and then fine-tuned according to the label of
the phrase, such as the syntactic category in pars-
ing(Socher et al., 2013), the polarity in sentiment
analysis, etc. The learned structures are not neces-
sarily syntactically accurate but can capture more
of the semantic information in the word vectors.
3 BRAE Framework
(Zhang et al., 2014) used the RAE along with a
Bilingually Constrained Model to simultaneously
learn phrase embeddings for two languages in se-
mantic vector space. The core idea behind BRAE
is that a phrase and its correct translation should
share the same semantic meaning. Thus, they
can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings. In this
method, a standard recursive autoencoder (RAE)
pre-trains the phrase embedding with an unsuper-
vised algorithm by greedily minimizing the re-
construction error (Socher et al., 2011), while the
bilingually-constrained model learns to finetune
the phrase embedding by minimizing the seman-
tic distance between translation equivalents and
maximizing the semantic distance between non-
translation pairs.
In this section, We will briefly present the struc-
ture and training algorithm for BRAE model. Af-
ter that, we show how this model can be adapted
to perform CLSA.
3.1 Recursive Auto-encoder Framework
In this model, each word w
k
in the vocabulary V
of given language corresponds to a vector x
k
? R
n
and stacked into a single word embedding matrix
L ? R
n×|V |. This matrix is learned using DNN
(Collobert andWeston, 2008; Mikolov et al., 2013)
and serves as input to further stages of RAE.
Using this matrix, a phrase (w
1
w
2
. . . w
m
) is
first projected into a list of vectors (x
1
, x
2
, . . . x
m
).
The RAE learns the vector representation of the
phrase by combining two children vectors recur-
sively in a bottom-up manner. For two children
c
1
= x
1
, c
2
= x
2
, the auto-encoder computes the
parent vector y
1
:
y
1
= f(W
(1)
[c
1
; c
2
] + b
(1)
); y
1
? R
n (1)
To assess how well the parent vector represents
its children, the auto-encoder reconstructs the chil-
160
?? ?? 
good was It ?? ????? ?? 
transformations 
Figure 1: An illustration of BRAE structure
dren :
[c
?
1
; c
?
2
] = W
(2)
p + b
(2) (2)
and tries to minimize the reconstruction error (Eu-
clidean Distance)E
rec
([c
1
; c
2
]) between the inputs
[c
1
; c
2
] and their reconstructions [c?
1
; c
?
2
].
Given y
1
, Eq.1 is used again to compute y
2
by
setting the children to be [c
1
; c
2
] = [y
1
;x
3
]. The
same auto-encoder is re-used until the vector of
the whole phrase is generated. For unsupervised
phrase embedding, the sum of reconstruction er-
rors at each node in binary tree y is minimized:
E
rec
(x; ?) = argmin
y?A(x)
?
k?y
E
rec
([c
1
; c
2
]
k
)
(3)
Where A(x) denotes all the possible binary trees
that can be built from inputs x. A greedy algorithm
is used to generate the optimal binary tree y?. The
parameters ?rec = (?(1), ?(2)) are optimized over
all the phrases in the training data. For further de-
tails, please refer (Socher et al., 2011)
3.2 Semantic Error
The BRAE model jointly learns two RAEs for
source language L
S
and target language L
T
. Each
RAE learn semantic vector representation p
s
and
p
t
of phrases s and t respectively in translation-
equivalent phrase pair (s, t) in bilingual corpora
(shown in Fig.1). The transformation between the
two is defined by:
p
?
t
= f(W
t
s
p
s
+ b
t
s
), p
?
s
= f(W
s
t
p
t
+ b
s
t
) (4)
where ?t
s
= (W
t
s
, b
t
s
), ?
s
t
= (W
s
t
, b
s
t
) are new pa-
rameters introduced.
The semantic error between learned vector rep-
resentations p
s
and p
t
is calculated as :
E
sem
(s, t; ?) = E
?
sem
(t|s; ?
s
t
) + E
?
sem
(s|t; ?
t
s
)
(5)
where E?
sem
(s|t; ?
s
t
) is the semantic distance of
p
s
given p
t
and vice versa. To calculate it, we
first calculate Euclidean distance between origi-
nal p
t
and transformation p?
t
as D
sem
(s|t, ?
t
s
) =
1
2
?p
t
? p
?
t
?
2. The max-semantic-margin distance
between them is then defined as
E
?
sem
(s|t, ?
t
s
) = max{0, D
sem
(s|t, ?
t
s
)
?D
sem
(s|t
?
, ?
t
s
) + 1}
(6)
where we simultaneously minimize the distance
between translation pairs and maximized between
non-translation pairs. Here t? in non-translation
pair (s, t?) is obtained by replacing the words in t
with randomly chosen target language words. We
calculate the E?
sem
(t|s; ?
s
t
) in similar manner.
3.3 BRAE Objective Function
Thus, for the phrase pair (s, t), the joint error be-
comes:
E(s, t, ?) = E(s|t, ?) + E(t|s, ?)
E(s|t, ?) = ?E
rec
(s; ?
rec
s
) + (1 ? ?)E
?
sem
(s|t, ?
t
s
))
E(t|s, ?) = ?E
rec
(t; ?
rec
t
) + (1 ? ?)E
?
sem
(t|s, ?
s
t
))
(7)
The hyper-parameter ? weighs the reconstruction
and semantic errors. The above equation indi-
cates that the Parameter sets ?
t
= (?
s
t
, ?
rec
t
) and
?
s
= (?
t
s
, ?
rec
s
) on each side respectively can be
optimized independently as long as the phrase rep-
resentation of other side is given to compute se-
mantic error.
The final BRAE objective over the phrase pairs
training set (S, T ) becomes:
J
BRAE
=
1
N
?
(s,t)?(S,T )
E(s, t; ?) +
?
BRAE
2
???
2
(8)
3.4 Unsupervised Training of BRAE
The word embedding matrices L
s
and L
t
are pre-
trained using unlabeled monolingual data with
Word2Vec toolkit (Mikolov et al., 2013). All other
parameters are initialized randomly. We use SGD
algorithm for parameter optimization. For full gra-
dient calculations for each parameter set, please
see (Zhang et al., 2014).
1. RAE Training Phase: Apply RAE Frame-
work (Sec. 3.1) to pre-train the source and target
phrase representations p
s
and p
t
respectively by
optimizing ?rec
s
and ?rec
t
using unlabeled monolin-
gual datasets.
2. Cross-Training Phase: Use target-side
phrase representation p
t
to update the source-side
161
parameters ?
s
and obtain source-side phrase repre-
sentation p?
s
, and vice-versa for p
s
. Calculate the
joint error over the bilingual training corpus. On
reaching a local minima or predefined no. of iter-
ations (30 in our case), terminate this phase, other-
wise set p
s
= p
?
s
, p
t
= p
?
t
, and repeat.
4 Adapting Model for Classifying
Sentiments
At the end of previous Training procedure, we ob-
tain high quality phrase embeddings in both source
and target language and transformation function
between them. We now extend that model to per-
form cross lingual supervised tasks, specifically
CLSA.
To achieve this, we need to modify the learned
semantic phrase embeddings such that they can
capture information about sentiment. Since we
only use monolingual labeled datasets from this
point onwards, the supervised learning phases will
occur independently for each RAE as we do not
have any ''phrase pairs'' now. Thus, the new se-
mantic vector space generated for word and phrase
embeddings may no longer be in sync with their
corresponding transformations.
We propose following modifications to the sys-
tem to deal with this problem. Let L
S
and L
T
rep-
resent Resource rich and Resource poor language
respectively in above model.
Modifications in architecture: We first in-
clude a softmax (?) layer on top of each parent
node in RAE for L
S
to predict a K-dimensional
multinomial distribution over the set of output
classes defined by the task (e.g : polarity, Ratings).
d(p; ?
ce
) = ?(W
ce
p) (9)
Given this layer, we calculate cross entropy er-
rorE
ce
(p
k
, t,W
ce
) generated for node p
k
in binary
tree, where t is target multinomial distribution or
one-hot binary vector for target label. We use this
layer to capture and predict actual sentiment in-
formation about the data in both L
S
and L
T
(de-
scribed in next section). We show a node in modi-
fied architecture in Fig.2.
Penalty for Movement in Semantic Vector
space: During subsequent training phases, we in-
clude the euclidean norm of the difference between
the original and new phrase embeddings as penalty
in reconstruction error at each node of the tree.
E
?
rec
([c
1
; c
2
]; ?) = E
rec
([c
1
; c
2
]; ?) +
?
p
2
?p? p
?
?
2
(10)
Resource Rich Language Resource Poor Language 
Reconstruction Reconstruction Cross-Entropy 
Figure 2: An illustration of BRAE segment with
Cross Entropy layer
Here p is the phrase representation we get during
forward propagation of current training iteration
and p? is the representation we get if we apply the
parameters obtained at the end of the Cross training
phase to children [c
1
; c
2
] of that node. The reason
to do this is twofold.
First, during supervised training, the error will
back propagate through RAEs for both languages
affecting their respective weights matrices and
word embeddings. This will modify the semantic
representation of phrases captured during previous
phases of training procedure and adversely affect
the transformations derived from them. Therefore
we need to include some procedure such that the
transformation information learned during Cross-
training phase is not lost.
Secondly, we observe that the information about
the semantic similarity of a word or phrase also im-
plies sentiment similarity between the two. That is
when dealing with bilingual data, words or phrases
that appear near each other in semantic space typi-
cally represent common sentiment information and
we want our model to create a decision boundary
around these vectors instead ofmodifying them too
much.
Disconnecting the RAEs: We fix the trans-
formation weights between the two RAEs, i.e.
in subsequent training steps the transformation
weights(?t
s
, ?
s
t
) are not modified but rather pass
the back propagated error as it is to previous lay-
ers. We observed that on optimizing the objec-
tive along with the penalty term, the transforma-
tion weights are preserved between new seman-
tic/sentiment vector spaces, resulting in slightly
degraded performance, but were still able to
preserve enough information about the semantic
structure of two languages.Also, it reinforced the
penalty imposed on the movement of phrase em-
beddings in semantic vector space.On the other
hand, if the weights were allowed to be updated,
162
the accuracies were affected severely as infor-
mation learned during previous phases was lost
and the weights were not been able to capture
enough information about the modified phrase em-
beddings and generalize well on test phrases not
encountered in labeled training set of Resource
Scarce Language.
4.1 Supervised Training Phases
We now explain supervised training procedure us-
ing only monolingual labeled data for each lan-
guage. These training phases occur at the end of
BRAE training. In each training phase, we use
SGD algorithm to perform parameter optimiza-
tion.
4.1.1 Phase I : Resource Rich language
In this phase, we only modify the parameters of
RAE
L
S
, i.e. ?rec
s
and ?
ce
by optimizing following
objective over (sentence, label) pairs (x, t) in its
labeled corpus.
J
S
=
1
N
?
(x,t)
E(x, t; ?) +
?
S
2
???
2
(11)
whereE(x, t; ?) is the sum over the errors obtained
at each node of the tree that is constructed by the
greedy RAE:
E(x, t; ?) =
?
k?RAE
L
S
(x)
?E
?
rec
([c
1
; c
2
]
k
; ?
s
)
+ (1 ? ?)E
ce
(p
k
, t; ?
ce
)
(12)
To compute this gradient, we first greedily con-
struct all trees and then derivatives for these trees
are computed efficiently via back-propagation
through structure (Goller and Kuchler, 1996). The
gradient for our new reconstruction function (Eq.
10) w.r.t to p at a given node is calculated as
?E
?
rec
?p
=
?E
rec
?p
+ ?
p
(p? p
?
) (13)
The first term ?Erec
?p
is calculated as in standard
RAE model. The partial derivative in above equa-
tion is used to compute parameter gradients in stan-
dard back-propagation algorithm.
4.1.2 Phase II : Resource Poor Language
In this phase, we modify the parameters of
RAE
L
T
and ?
ce
by optimizing Objective J
T
over
(sentence, label) pairs (x, t) in labeled corpus for
L
T
(much smaller than that for L
S
). The equation
for J
T
is similar to Eq.11 and Eq.12 but with ?
t
and
? as parameters instead of ?
s
and ? respectively.
Since cross-entropy layer is only associatedwith
L
S
, we need to traverse the transformation param-
eters to obtain sentiment distribution for each node
(green path in Fig.2). That is, we first transform p
t
to source side phrase p?
s
and then apply the cross
entropy weights to it.
d(p
t
, ?
ce
) = ?(W
ce
.f(W
t
s
p
t
+ b
t
s
)) (14)
We use the similar back-propagation through
structure approach for gradient calculation in
Phase I. During back propagation, 1) we do not
update the transformation weights, 2) we transfer
error signals during back-propagation from Cross-
entropy layer to ?(1)
t
as if the transformation was
an additional layer in the network.
4.1.3 Predicting overall sentiment
To predict overall sentiment associated with the
sentence in L
T
, we use the phrase embeddings p
t
of the top layer of the RAE
L
T
and it transforma-
tion p?
s
. Together, we train a softmax regression
classifier on concatenation of these two vector us-
ing weight matrix W ? RK×2n
5 Experimental Work
We perform experiments on two kind of sentiment
analysis systems : (1) that gives +ve/-ve polarity
to each review and (2) assigns ratings in range 1 -
4 to each review.
5.1 External Datasets Used
For pre-training the word embeddings and RAE
Training, we used HindMonoCorp 0.5(Bojar et al.,
2014) with 44.49M sentences (787M Tokens) and
English Gigaword Corpus.
For Cross Training, we used the bilingual
sentence-aligned data fromHindEnCorp1 (Bojar et
al., 2014) with 273.9k sentence pairs (3.76M En-
glish, 3.88M Hindi Tokens). This dataset contains
sentence pair obtained from Bilingual New Arti-
cles, Wikipedia entries, Automated Translations,
etc. Training and Validation division is 70% and
30% for all above datasets.
In Supervised Phase I, we
used IMDB11 dataset available at
http://ai.stanford.edu/~amaas/data/sentiment/
and first used by (Maas et al., 2011) for +ve/-ve
1http://ufal.mff.cuni.cz/hindencorp
163
system containing 25000 +ve and 25000 -ve
movie reviews.
For 4-ratings system, we use Rotten Toma-
toes Review dataset (scale dataset v1.0) found
at http://www.cs.cornell.edu/People/pabo/movie-
review-data. The dataset is divided into four
author-specific corpora, containing 1770, 902,
1307, and 1027 documents and each document has
accompanying 4-Ratings ({0, 1, 2, 3}) label.
5.2 Rating Based Hindi Movie Review
(RHMR) Dataset
We crawled the Hindi Movie Reviews Website2 to
obtain 2945 movie reviews. Each Movie Review
on this site is assigned rating in range 1 to 4 by
at least three reviewers. We first discard reviews
that whose sum of pairwise difference of ratings is
greater than two. The final rating for each review is
calculated by taking the average of the ratings and
rounding up to nearest integer. The fraction of Re-
views obtained in ratings 1-4 are [0.20, 0.25, 0.35,
0.20] respectively. Average length of reviews is
84 words. For +ve/-ve polarity based system, we
group the reviews with ratings {1, 2} as negative
and {3, 4} as positive.
5.3 Experimental Settings
We used following Baselines for Sentiment Anal-
ysis in Hindi :
Majority class: Assign the most frequent class
in the training set (Rating:3 / Polarity:+ve)
Bag-of-words: Softmax regression on Binary
Bag-of-words
We also compare our system with state of the art
Monolingual and Cross Lingual System for Senti-
ment Analysis in Hindi as described by (Popat et
al., 2013) using the same experimental setup. The
best systems in each category given by them are as
below:
WordNet Based: Using Hindi-SentiWordNet3,
each word in a review was mapped to correspond-
ing synset identifiers. These identifiers were used
as features for creating sentiment classifiers based
onBinary/Multiclass SVM trained on bag ofwords
representation using libSVM library.
Cross Lingual (XL) Clustering Based: Here,
joint clustering was performed on unlabeled bilin-
gual corpora which maximizes the joint likelihood
of monolingual and cross-lingual factors.. For de-
tails, please refer the work of (Popat et al., 2013).
2http://hindi.webdunia.com/bollywood-movie-review/
3http://www.cfilt.iitb.ac.in/
Each word in a reviewwas then mapped to its clus-
ter identifier and used as features in an SVM.
Our approaches
Basic RAE: We use the Semi-Supervised RAE
based classification where we first trained a stan-
dard RAE using Hindi monolingual corpora, then
applied supervised training procedure as described
in (Socher et al., 2011). This approach doesn't use
bilingual corpora, but is dependent on amount of
labeled data in Hindi.
BRAE-U:We neither include penalty term, nor
fix the transformations weights in our proposed
system.
BRAE-P:We only include the penalty term but
allow the transformation weights to be modified in
proposed system.
BRAE-F: We add the penalty term and fix the
transformation weights during back propagation in
proposed system.
5.4 Experimental Setup
We combined the text data from all English
Datasets (English Gigaword + HindEnCorp En-
glish Portion + IBMD11 + Scale Dataset) de-
scribed above to train the word embeddings us-
ing Word2Vec toolkit and RAE. Similarly, we
combined text data from all Hindi Datasets
(HindMonoCorp + HindiEnCorp Hindi Portion +
RHMR) to train word embeddings and RAE for
Hindi.
We used MOSES Toolkit (Koehn et al., 2007)
to obtain high quality bilingual phrase pairs from
HindEnCorp to train our BRAE model. After
removing the duplicates, 364.3k bilingual phrase
pairs were obtained with lengths ranging from 1-
6, since bigger phrases reduced the performance of
the system in terms of Joint Error of BRAEmodel.
We randomly split our RHMR dataset into 10
segments and report the average of 10-fold cross
validation accuracies for each setting for both Rat-
ings and Polarity classifiers.
We also report 5-fold cross validation accuracy
on Standard Movie Reviews Dataset (hereby re-
ferred as SMRD) given by (Joshi et al., 2010)
which contains 125 +ve and 125 -ve reviews
in Hindi. The dataset can be obtained at
http://www.cfilt.iitb.ac.in/Resources.html.
Since this project is about reducing depen-
dence on annotated datasets, we experiment on
how accuracy varies with labeled training dataset
(RHMR) size. To perform this, we train our model
164
in 10% increments (150 examples) of training set
size (each class sampled in proportion of original
set). For each size, we sample the data 10 times
with replacement and trained the model. For each
sample, we calculated 10-fold cross validation ac-
curacy as described above. Final accuracy for each
sizewas calculated by averaging the accuracies ob-
tained on all 10 samples. Similar kind of evalua-
tion is done for all other Baselines explored.
In subsequent section, the word 'significant' im-
plies that the results were statistically significant
(p < 0.05) with paired T-test
5.5 BRAE Hyper Parameters
We empirically set the learning rate as 0.05. The
word vector dimension was selected as 80 from set
[40, 60, 80, 100, 120] using Cross Validation. We
used joint error of BRAE model to select ? as 0.2
from range [0.05, 0.5] in steps of 0.05. Also, ?
L
was set as 0.001 for DNN trained for word embed-
ding and ?
BRAE
as 0.0001.
For semi-supervised phases , we used 5-fold
cross validation on training set to select ? and ? in
range [0.0, 1.0] in steps of 0.05 with optimal value
obtained at ? = 0.2 and ? = 0.35. Parameter ?
p
was selected as 0.01 , ?
S
as 0.1 and ?
T
as 0.04
after selection in range [0.0, 1.0] in steps of 0.01.
5.6 Results
Dataset RHMR SMRD
Classifier Ratings Polarity Polarity
Majority class 35.19 51.83 52.34
Bag-of-Words 51.98 62.52 68.47
WordNet based 55.47 67.29 75.5
XL Clustering 72.34 84.46 84.71
Basic RAE 75.53 79.31 81.06
BRAE-U 76.01 82.66 84.83
BRAE-P 79.70 84.85 87.00
BRAE-F 81.22 90.50 90.21
Table 1: Accuracies obtained for various Exper-
imental Settings. Model are trained on complete
labeled training datasets
Table 1 present the results obtained for both rat-
ings based and polarity classifier on RHMR and
MRD Dataset. Our model gives significantly bet-
ter performance for ratings based classification
than any other baseline system currently used for
SA in Hindi. The margin of accuracy obtained
against next best classifier is about 8%. Also, for
A ? /P ? P-1 P-2 P-3 P-4
A-1 83.19 15.28 1.53 0.00
A-2 12.23 82.20 5.57 0.00
A-3 0.00 9.03 81.26 9.71
A-4 0.00 1.87 19.69 78.44
F1-score 0.83 0.78 0.82 0.80
Table 2: Confusion Matrix for Ratings by BRAE-
F, Across: Predicted Rating, Downward: Actual
Rating
+ve/-ve polarity classifier, the accuracy showed an
improvement of 6% over next highest baseline.
In Table 2, we calculate the confusion matrix for
our model(BRAE-F) for the 4-Ratings case. Value
in a cell (A
i
, P
j
) represents the percentage of ex-
amples in actual rating class i that are predicted
as rating j. We also show the F1 score calcu-
lated for each individual rating class. It clearly
shows that our model has low variation in F1-
scores and thereby its performance among various
rating classes.
In Fig. 3, we show the variation in accuracy
of the classifiers with amount of sentiment labeled
Training data used. We note that our approach con-
sistently outperforms the explored baselines at all
dataset sizes. Also, our model was able to attain
accuracy comparable to other baselines at about
50% less labeled data showing its strength in ex-
ploiting the unlabeled resources.
0 0.2 0.4 0.6 0.8 1
50
60
70
80
90
Majority BOW
WordNet XL
RAE BRAE-U
BRAE-P BRAE-F
Figure 3: Variation of Accuracy (+ve/-ve Polarity)
with Size of labeled Dataset(Hindi), x-axis: Frac-
tion of Dataset Used, y-axis: %age Accuracy Ob-
tained
We also experiment with variation of accuracies
165
New Word/Phrase Similar Words/Phrases Sentiment label
depressing gloomy ???? Rating : 1
????????? discouraging ??????×?? Polarity : -ve
was painful was difficult ???? ?? Rating : 2
??[??? ?? was bad ???? ?? Polarity : -ve
should be awarded was appreciated ?????? ?? ?? Rating : 4
?à????? ???? ???? ????? will get accolades ??????? ????? ????? Polarity : +ve
public won't come no one will come ??? ???? ???? Rating : 1
??? ???? ???? viewers won't come ??[? ???? ???? Polarity : -ve
Table 3: Semantically similar phrases obtained for new phrases and their assigned label
with amount of Unlabeled Bilingual Training Data
used for Cross Lingual models explored. Again
we increase size of bilingual dataset in 10% incre-
ments and calculate the accuracy as described pre-
viously. In Fig. 4, we observed that performance
of the proposed approach steadily increases with
amount of data added, yet even at about 50000
(20%) phrase pairs, our model produces remark-
able gains in accuracy.
0 0.2 0.4 0.6 0.8 1
60
70
80
90
XL BRAE-U
BRAE-P BRAE-F
Figure 4: Variation of Accuracy (+ve/-ve polarity)
with Size of Unlabeled Bilingual Corpora, x-axis:
Fraction of Training Data Used, y-axis: %age Ac-
curacy Obtained
We also observed that the model which restricts
modification to transformation weights during su-
pervised phase II does better than the one which
allows the modification at all dataset sizes. This
result appears to be counterintuitive to normal op-
eration of neural network based models, but sup-
ports our hypothesis as explained in previous sec-
tions.
5.7 Performance and Error Analysis
Analysis on the test results showed that the major
advantage given by our model occurs due to pres-
ence of unknown words (i.e.words not present in
labeled dataset) in test data. Since we restricted
the movement in semantic vector space, our model
was able to infer the sentiment for a unknown
word/phrase by comparing it with semantically
similar words/phrases. In Table 3, we extracted
the Top-2 semantically similar phrases in training
set for small new phrases and sentiment labeled
assigned to them by our model (the phrases are
manually translated from Hindi for reader's under-
standing). As we can see, our model was able to
extract grammatically correct phrases with similar
semantic nature as given phrase and assign correct
sentiment label to it.
Secondly, We found that our model was able to
correctly infer word sense for polysemous words
that adversely affected the quality of sentiment
classifiers in our baselines. This eliminates the
need for manually constructed fine grained lexi-
cal resource like WordNets and development of
automated annotation resources. For example,
to a phrase like "Her acting of a schizophrenic
mother made our hearts weep", the baselines clas-
sifiers assigned negative polarity due to presence
of words like 'weep', yet our model was correctly
able to predict positive polarity and assigned it a
rating of 3.
Error Analysis of test results showed that errors
made by our model can be classified in two major
categories :
1) A review may only give description of the
object in question (in our case , the description of
the film) without actually presenting any individ-
ual sentiments about it or it may express conflict-
ing sentiments about two different aspects about
the same object. This presents difficulty in assign-
166
ing a single polarity/rating to the review.
2) Presence of subtle contextual references af-
fected the quality of predictions made by our clas-
sifier. For example, sentence like ''His poor acting
generally destroys a movie, but this time it didn't''
got a rating of 2 due to presence of phrase with
negative sense (here the phrase doesn't have am-
biguous sense), yet the actual sentiment expressed
is positive due to temporal dependence and gen-
eralization. Also, "This movie made his last one
looked good" makes a reference to entities exter-
nal to the review, which again forces our model to
make wrong prediction of rating 3.
Analyzing these aspects andmaking correct pre-
dictions on such examples needs further work.
6 Conclusion and Future Work
This study focused on developing a Cross Lin-
gual Supervised Classifier based on Bilingually
Constrained Recursive Autoencoder. To achieve
this, our model first learns phrase embeddings for
two languages using Standard RAE, then fine tune
these embeddings using Cross Training procedure.
After imposing certain restrictions on these em-
beddings, we perform supervised training using
labeled sentiment corpora in English and a much
smaller one in Hindi to get the final classifier.
The experimental work showed that our model
was remarkably effective for classification of
Movie Reviews in Hindi on a rating scale and
predicting polarity using least amount of data to
achieve same accuracy as other systems explored.
Moreover it reduces the need for MT System or
lexical resources like Linked WordNets since the
performance is not degraded too much even when
we lack large quantity of labeled data.
In Future, we hope to 1) extend this system to
learn phrase representations among multiple lan-
guages simultaneously, 2) apply this framework to
other cross Lingual Tasks such as Paraphrase de-
tection, Question Answering, Aspect Based Opin-
ion Mining etc and 3) Learning different weight
matrices at different nodes to capture complex re-
lations between words and phrases.
References
Balamurali, Aditya Joshi, and Pushpak Bhattacharyya.
2012. Cross-lingual sentiment analysis for Indian
languages using linked wordnets. In Proceedings of
COLING 2012: Posters, pages 73--82. The COL-
ING 2012 Organizing Committee.
Ond?ej Bojar, Vojt?ch Diatka, Pavel Rychlý, Pavel
Stra?ák, Vít Suchomel, Aleš Tamchyna, and Daniel
Zeman. 2014. HindEnCorp - Hindi-English and
Hindi-only Corpus for Machine Translation. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC'14). Eu-
ropean Language Resources Association (ELRA).
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In RANLP, pages 50--54.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference onMa-
chine learning, pages 160--167. ACM.
Hatem Ghorbel and David Jacot. 2011. Further experi-
ments in sentiment analysis of frenchmovie reviews.
In Advances in Intelligent Web Mastering--3, pages
19--28. Springer.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347--352. IEEE.
Aditya Joshi, AR Balamurali, and Pushpak Bhat-
tacharyya. 2010. A fall-back strategy for sentiment
analysis in hindi: a case study. Proceedings of the
8th ICON.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177--180. Association for Computational Linguis-
tics.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 320--330. Associa-
tion for Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142--150. Asso-
ciation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111--3119.
167
Namita Mittal, Basant Agarwal, Garvit Chouhan, Nitin
Bania, and Prateek Pareek. 2013. Sentiment analy-
sis of hindi review based on negation and discourse
relation. In proceedings of International Joint Con-
ference on Natural Language Processing, pages 45-
-50.
Kashyap Popat, Balamurali A.R, Pushpak Bhat-
tacharyya, and Gholamreza Haffari. 2013. The
haves and the have-nots: Leveraging unlabelled cor-
pora for sentiment analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
412--422. Association for Computational Linguis-
tics.
Richard Socher, Jeffrey Pennington, Eric HHuang, An-
drew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151--161. Association for
Computational Linguistics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Chang Wan, Rong Pan, and Jiefei Li. 2011. Bi-
weighting domain adaptation for cross-language text
classification. In IJCAI Proceedings-International
Joint Conference on Artificial Intelligence, vol-
ume 22, page 1535.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 111--121. Association for Computa-
tional Linguistics.
168
