Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1437–1446,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Extreme Extraction -- Machine Reading in a Week 
 
Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard,  
Gary Kratkiewicz, Nicolas Ward, Ralph Weischedel 
Raytheon BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138 
mfreedma,lramshaw,eboschee,rgabbard,kratkiewicz, 
nward,weischedel@bbn.com 
 
The views expressed are those of the author and do not reflect the official policy or position of the Depart-
ment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.   
  
 
Abstract 
We report on empirical results in extreme 
extraction. It is extreme in that (1) from re-
ceipt of the ontology specifying the target 
concepts and relations, development is li-
mited to one week and that (2) relatively 
little training data is assumed. We are able 
to surpass human recall and achieve an F1 
of 0.51 on a question-answering task with 
less than 50 hours of effort using a hybrid 
approach that mixes active learning, boot-
strapping, and limited (5 hours) manual 
rule writing. We compare the performance 
of three systems: extraction with handwrit-
ten rules, bootstrapped extraction, and a 
combination. We show that while the recall 
of the handwritten rules surpasses that of 
the learned system, the learned system is 
able to improve the overall recall and F1.      
1 Introduction 
Throughout the Automatic Content Extraction 1 
(ACE) evaluations and the Message Understanding 
Conferences2 (MUC), teams typically had a year or 
more from release of the target to submitting sys-
tem results. One exception was MUC-6 (Grishman 
& Sundheim, 1996), in which scenario templates 
for changing positions were extracted given only 
one month. Our goal was to confine development 
to a calendar week, in fact, <50 person hours. This 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://www-nlpir.nist.gov/related_projects/muc/ 
is significant in two ways: the less effort it takes to 
bring up a new domain, (1) the more broadly ap-
plicable the technology is and (2) the less effort 
required to run a diagnostic research experiment. 
Our second goal concerned minimizing training 
data. Rather than approximately 250k words of 
entity and relation annotation as in ACE, only ~20 
example pairs per relation-type were provided as 
training. Reducing the training requirements has 
the same two desirable outcomes: demonstrating 
that the technology can be broadly applicable and 
reducing the overhead for running experiments. 
The system achieved recall of 0.49 and precision 
of 0.53 (for an F1 of 0.51) on a blind test set of 60 
queries of the form Ri(arg1, arg2), where Ri is one 
of the 5 new relations and exactly one of arg1 or 
arg2 is a free variable for each query. 
Key to this achievement was a hybrid of:   
? a variant of (Miller, et al., 2004) to learn two 
new classes of entities via automatically induced 
word classes and active learning (6 hours) 
? bootstrap relation learning (Freedman et al, 
2010) to learn 5 new relation classes (2.5 hours),  
? handwritten patterns over predicate-argument 
structure (5 hours), and 
? coreference (20 hours) 
Our bootstrap learner is initialized with relation 
tuples (not annotated text) and uses LDC‘s Giga-
word and Wikipedia as a background corpus to 
learn patterns for relation detection that are based 
on normalized predicate argument structure as well 
as surface strings.  
These early empirical results suggest the follow-
ing: (1) It is possible to specify a domain, adapt 
our system, and complete manual scoring, includ-
1437
ing human performance, within a month. Experi-
ments in machine reading (and in extraction) can 
be performed much more quickly and cheaply than 
ever before. (2) Through machine learning and 
limited human pattern writing (6 hours), we 
adapted a machine reading system within a week 
(using less than 50 person hours), achieving ques-
tion answering performance with an F1 of 0.5 and 
with recall 11% higher (relative) to a human read-
er. (3) Unfortunately, machine learning, though 
achieving 80% precision,3 significantly lags behind 
a gifted human pattern writer in recall. Thus, boot-
strap learning with much higher recall at minimal 
sacrifice in precision is highly desirable. 
2 Related Work 
This effort is evaluated extrinsically via formal 
questions expressed as a binary relation with one 
free variable. This contrasts with TREC Question 
Answering, 4  where the questions are in natural 
language, and not restricted to a single binary rela-
tion. Like the ?list? queries of TREC QA, the re-
quirement is to find all answers, not just one. 
Though question interpretation is not required in 
our work, interpretation of the text corpus is. 
The goal of rapid adaptation has been tested in 
other contexts. In 2003, a series of experiments in 
adapting to a new language in less than month 
tested system performance on Cebuano and Hindi. 
The primary goal was to adapt to a new language, 
rather than a new domain. The extraction partici-
pants focused on named-entity recognition, not 
relation extraction (May, et al, 2003; Sekine & 
Grishman, 2003; Li & McCallum, 2003; Maynard 
et al, 2003). The scenario templates of MUC-6 
(Grishman & Sundheim, 1996) are more similar to 
our relation extraction task, although the domain is 
quite different. Our experiment allowed for 1 week 
of development time, while MUC-6 allowed a 
month. The core entities in the MUC-6 task 
(people and organizations) had been worked on 
previously. In contrast all of our relations included 
at least one novel class. While MUC-6 systems 
tended to use finite-state patterns, they did not in-
corporate bootstrapping or patterns based on the 
output of a statistical parser.   
                                                          
3 Handwritten patterns achieved 52% precision. 
4 http://trec.nist.gov/data/qamain.html 
For learning entity classes, we follow Miller, et 
al., (2004), using word clustering and active learn-
ing to train a perceptron model, but unlike that 
work we apply the technique not just to names but 
also to descriptions. An alternative approach to 
learning classes, applying structural patterns to 
bootstrap description recognition without active 
learning, is seen in Riloff (1996) and Kozareva et 
al., (2008)   
Much research (e.g. Ramshaw 2001) has fo-
cused on learning relation extractors using large 
amounts of supervised training, as in ACE. The 
obvious weakness of such approaches is the result-
ing reliance on manually annotated examples, 
which are expensive and time-consuming to create.  
Others have explored bootstrap relation learn-
ing from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). Mit-
chell et al. (2009) showed that for macro-reading, 
precision and recall can be improved by learning a 
large set of interconnected relations and concepts 
simultaneously. None use coreference to find train-
ing examples; all use surface (word) patterns. 
Freedman et. al (2010) report improved perfor-
mance from using predicate structure for boot-
strappped relation learning.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in TREC QA, 
where extracting one instance of a relation can be 
sufficient, rather than detecting all instances. Mit-
chell et al. (2009), while demonstrating high preci-
sion, do not measure recall. 
By contrast, our work emphasizes recall, not 
just precision. Our question answering task asks 
list-like questions that require multiple answers.  
We also include the results of a secondary, extrac-
tion evaluation which requires that the system 
identify every mention of the relations in a small 
set of documents. This evaluation is loosely based 
on the relation mention detection task in ACE.  
3 Task Set-Up and Evaluation 
Our effort was divided into four phases. During the 
first phase, a third party produced an ontology and 
the resources, which included: brief (~1 paragraph) 
guidelines for each relation and class in the ontolo-
1438
gy; ~20 examples for each relation in the ontology; 
2K documents that are rich in domain relations. 
Table 1 lists the 5 new relations and number of ex-
amples provided for each. Arguments in italics 
were known by the system prior to the evaluation.  
Relation Ex. 
possibleTreatment(Substance, Condition) 23 
expectedDateOnMarket(Substance, Date) 11 
responsibleForTreatment(Substance, Agent) 19 
studiesDisease(Agent, Condition) 16 
hasSideEffect(Substance, Condition) 27 
Table 1: New Relations and Number of Examples 
In phase two, we spent one week extending our 
extraction system for the new ontology. During the 
third phase, we ran our system over 10K docu-
ments to extract all instances of domain relations 
from those documents. In the fourth phase, our 
question answering system used the extracted in-
formation to answer queries.  
4 Approach to Domain Specialization 
Our approach to extracting domain relations inte-
grated novel relation and class detectors into an 
existing extraction system, designed primarily 
around the ACE tasks. The existing system uses a 
discriminatively trained classifier to detect the enti-
ty and value types of ACE. It also produces a syn-
tactic parse for each sentence; normalizes these 
parses to find logical predicate argument structure; 
and detects and coreferences pronominal, nominal, 
and name mentions for each of the 7 ACE entity 
types (Person, Organization, Geopolitical Entity, 
Location, Facility, Weapon, and Vehicle).5  
The extraction system has three components that 
allow for rapid adaptation to a new domain:  
? Class detectors trained using word classes de-
rived from unsupervised clustering and sentence-
selected training data. 
? A bootstrap relation learner which given a few 
seed examples learns patterns that indicate the 
presence of relations.  
? An expressive pattern language which allows a 
developer to express rules for relation extraction 
in a simple, but fast manner.  
 
 
Component Approach Effort 
Class Recognizer Active Learning 6 hrs 
                                                          
5 The extraction system detects relations and events in the 
ACE ontology, but these were not used in the current work.  
 
Class Recognizer Web-Mined List 1 hrs 
Relation Recognizer 
Semi-supervised 
Bootstrapping 
8.5 hrs 
Relation Recognizer Manual Patterns 5 hrs 
Coreference Heuristics 20 hrs 
Table 2: Effort and Approach for New Domain 
4.1 Class Extraction  
Each of the relations in the new domain included at 
least one argument that was new. While question 
answering requires the system to identify the 
classes only when they appear in a relation, know-
ledge of when a class is present provides important 
information for relation extraction. For example in 
our ontology, Y is a treatment for X only if Y is a 
substance. Thus, ?Group counseling sessions are 
effective treatments for depression’ does not con-
tain an instance of possibleTreatment(), while 
?SSRIs are effective treatments for depression‘ 
does. The bootstrap learner allows constraints 
based on argument type. To use this capability, we 
trained the recognizer at the beginning of the week 
of domain adaptation and used the predicted 
classes during learning.  
We annotated 1064 sentences (~31K words) us-
ing active learning combined with unsupervised 
word clusters (Miller,et al., 2004) for the following 
classes: Substance-Name, Substance-Description, 
Condition-Name, and Condition-Description. Ge-
neric noun-phrases like new drugs, the illness, etc 
were labeled as descriptors. Because of the time 
frame, we did not develop extensive guidelines nor 
measure inter-annotator agreement. Annotation 
took 6 hours. We supplemented our annotation 
with lists of substances and treatments from the 
web, which took 1 hour.  
4.2 Coreference 
Providing a name reference is generally preferable 
to a non-specific string (e.g. the drugs), but not 
always feasible; for instance, reports of new re-
search may appear without a name for the drug. 
Our existing system‘s coreference algorithms op-
erate only on mentions of ACE entity types (per-
sons, organizations, GPEs, other locations, 
facilities, vehicles, and weapons). During the week 
of domain adaption we developed new heuristics 
for coreference over non-ACE types.  Most of our 
heuristics are domain independent (e.g. linking the 
parts of an appositive). Our decision to annotate 
names and descriptions separately was driven par-
1439
tially by the need to select the best reference (i.e. 
name) for co-referent clusters. Adding coreference 
heuristics for the two new entity types was the sin-
gle most time-consuming activity, taking 20 of the 
total 43 hours. 
4.3 Relation Extraction  
For relation extraction, we used both pattern 
learning and handwritten patterns. We initialized 
our bootstrap relation learner with the example 
instances provided with the domain ontology; Ta-
ble 3 includes examples of the instances provided 
to the system as training. Our bootstrap relation 
learner finds instances of the relation argument 
pairs in text and then proposes both predicate-
argument structure and word-based connections 
between the arguments as possible new patterns for 
the relation. The learner automatically prunes po-
tential patterns using information about the number 
of known-to-be true and novel instances matched 
by a proposed pattern. By running the pattern ex-
tractor over a large corpus, the proposed patterns 
generate new seeds which are in turn are used to 
propose new patterns. For this experiment, we in-
corporated a small amount of supervision during 
the bootstrapping process (roughly 1 hour total per 
relation); we also performed ~30 minutes total in 
pruning domain patterns at the end of learning.  
 Relation Arg-1 Arg-2 
possTreatmnt AZT AIDS 
studyDisease Dr Henri Joyeux cancer 
studyDisease Samir Khleif cancer 
Table 3: Sample Instances for Initializing Learner 
We also used a small amount of human effort 
creating rules for detecting the relations. The pat-
tern writer was given the guidelines, the examples, 
and a 2K document background corpus and spent 1 
hour per relation writing rules.  
The learned patterns use a subset of the full pat-
tern language used by the pattern-writer. The lan-
guage operates over surface-strings as well as 
predicate-argument structure. Figure 1 illustrates 
learned and handwritten patterns for the possible-
TreatmentRelation(). The patterns in rectangles 
match surface-string patterns; the tree-like patterns 
match normalized predicate argument structure.  
The –WORD- token indicates a wild card of 1-3 
words.  The blue rectangles at the root of the trees 
in the handwritten patterns are sets of predicates 
that can be matched by the pattern. 
5 Evaluation  
Our question answering evaluation was inspired 
by the evaluation in DARPA‘s machine reading 
program, which requires systems to map the in-
formation in text into a formal ontology and an-
swer questions based on that ontology. Unlike 
ACE, this allows evaluators to measure perfor-
mance without exhaustively annotating documents, 
allows for balance between rare and common rela-
tions, and implicitly measures coreference without 
requiring explicit annotation of answer keys for 
coreference. However because the evaluation only 
measures performance on the set of queries, many 
relation instances will be unscored. Furthermore, 
the system is not rewarded for finding the same 
relation multiple times; finding 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat) is 
the same as finding 1 (or 10) instances.  
 
Figure 1: Sample Patterns for possibleTreatment() 
 
The evaluation included only queries of the type 
Find all instances for which the relation P(X, Z) is 
true where one of X or Z is constant. For example, 
Find possible treatments for diabetes; or What is 
expected date to market for Abilify? There were 60 
queries in the evaluation set to be answered from a 
10K document corpus. To produce a preliminary 
answer key, annotators were given the queries and 
corpus indexed by Google Desktop. Annotators 
were given 1 hour to find potential answers to each 
query. If no answers were found after 1 hour, the 
annotators were given a second hour to look for 
answers. For two queries, both of the form Find 
treatments with an expected date to market of MM-
YYYY, even after two hours of searching the anno-
tators were unable to find any answers.6  
Annotator answers served as the initial gold-
standard. Given this initial answer key, annotators 
reviewed system answers and aligned them with 
gold-standard answers. System output not aligned 
with the initial gold standard was assessed as cor-
rect or incorrect. We assume that the final gold-
standard constitutes a complete answer key, and 
                                                          
6 Evaluators wanted some queries with no answers. 
1440
are thus able to calculate recall for our system and 
for humans7. Because we had only one annotator 
for each query and because we assumed that any 
answer found by an annotator was correct, we 
could not estimate human precision on this task.  
Answers can be specific named concepts (e.g. 
Penicillin) or generic descriptions (e.g. drug, ill-
ness). Given the sentence, ACME produces a wide 
range of drugs including treatments for malaria 
and athletes foot,‘ our reading system would ex-
tract the relations responsibleForTreatment(drugs, 
ACME), possibleTreatment(drugs, malaria), pos-
sibleTreatment(drugs, athletes foot). When a name 
was available in the document, annotators marked 
the answer as correct, but underspecified. We cal-
culated precision and recall treating underspecified 
answers as incorrect and separately calculated pre-
cision and recall counting underspecified answers 
as correct. When treated as correct, there was less 
than a 0.05 absolute increase in both precision and 
recall. Unless otherwise specified, all scores re-
ported here use the stricter condition which treats 
underspecified answers as incorrect.  
We also evaluated extracting all information in a 
small document collection (here human search of 
the 10k documents does not play a role in finding 
answers). Individuals were asked to annotate every 
instance of the 5 relations in a set of 102 docu-
ments. Recall, Precision, and F were calculated by 
aligning system responses to the answer key. Sys-
tem answers that aligned are correct; those that did 
not are incorrect; and answers in the key that were 
not found by the system are misses. Unlike the 
question answering evaluation, this evaluation 
measures the ability to find every instance of a 
fact. If the gold standard includes 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat), re-
call will decrease for each instance missed. The 
?extraction? evaluation does not penalize systems 
for missing coreference.  
6 Results 
6.1 Class Detection 
                                                          
7 The answer key may contain some answers that were found 
neither by the annotator nor by the systems described here, 
since the answer key includes answers pooled from other sys-
tems not reported in this paper. The system reported here was 
the highest performing of all those participating in the experi-
ment. Furthermore, if a system answer is marked as correct, 
but underspecified, the specific  answer is put in the key. 
The recall, precision, and F1 for class detection 
using 10-fold cross validation of the ~1K anno-
tated sentences appear in the 3-5th columns of Table 
4. Given the amount of training, our results are 
lower than in Miller et al (2004) (an F1 of 90 with 
less than 25K words of training). Several factors 
could explain this: Finding boundaries and types 
for descriptions is more complex than for names in 
English. 8  Our classes, pharmaceutical substances 
and physiological conditions, may have been more 
difficult to learn. Our classes are less common in 
news reporting; as such, both word-class clusters 
and active learning may have been less effective. 
Finally, our evaluation was done on a 10-fold split 
of the active-learning selected data; bias in select-
ing the data could explain at least a part of our 
lower performance.  
Type 
# in 
GS 
Without Lists With Lists 
R P F R P F 
Subst-D 789 77 85 80.8 78 85 81.3 
Subst-N 410 70 82 75.5 77 81 78.9 
Cond-D 427 72 78 74.9 72 77 74.4 
Cond-N 963 80 87 83.4 84 83 83.5 
Table 4: Cross Validation:  Condition & Substance 
We noticed that the system frequently reported 
country names to be substance-names. Surprising-
ly, we found that our well-trained name finder 
made the opposite mistake, occasionally reporting 
drugs as geo-political entities.  
We incorporated lists of known substances and 
conditions to improve recall. Performance on the 
same cross-validation split is shown in the final 
three columns of Table 4. Incorporating the lists led 
to recall gains for both substance-name and condi-
tion-name. Because a false-alarm in class recogni-
tion only leads to an incorrect relation extraction if 
it appears in a context indicating a domain relation, 
false alarms of classes may be less important in the 
question answering and extraction evaluations.   
6.2 Question Answering and Extraction 
Figure 2 and Table 6 show system performance 
using only handwritten rules (HW), only learned 
patterns (L), and combining both (C). Figure 2  
includes scores calculated with all of the systems‘ 
answers (in the dotted boxes), and with just those 
answers that were deemed useful (discussed be-
                                                          
8 English names are capitalized; person names have a typical 
form and are frequently signaled by titles; organization names 
frequently have clear signal words, such as Corp. 
1441
low). We include annotator recall. Handwritten 
patterns outperform learned patterns consistently 
with much higher recall. Encouragingly, however, 
1. The combined system‘s recall and F-Score 
are noticeably higher for 3 of the relations.  
2. The learned patterns generate answers not 
found by handwritten patterns.  
3. The learned patterns have high precision.9 
There is variation across the different relations. 
The two best performing relations possibleTreat-
ment() and studiesDisease() have F1 more than 
twice as high as the two worst performing rela-
tions, expectedDateToMarket() and hasSideEf-
fect(). This is primarily due to differences in recall.  
 
Figure 2: Overall Q/A Performance: All answers in  
dotted boxes; 'Useful Answers' unboxed 
The combined system‘s recall (0.49), while low, 
is higher than that of the annotators (0.44). While 
hardly surprising that a machine can process in-
formation much more quickly than a person, it is 
encouraging that higher recall is achieved even 
with only one week‘s effort. In the context of our 
pooled answer-key, the relatively low recall of 
both the system and the annotator suggests that 
there was little overlap between the answers found 
by the annotator and those found by the system.  
As already described, the system answers can 
include both specific references (e.g. Prozac) and 
more generic references (the drug). When a more 
specific answer is present in the document, generic 
references have been treated as incorrect. Howev-
er, sometimes there is not a more specific refer-
ence; for example an article written before a drug 
has been released may never name the drug. Scores 
reported thus far treat such answers as correct. 
These answers would be useful when answering 
more complex queries. For example, given the sen-
                                                          
9 The learned patterns' high precision is to be expected for two 
reasons. First, a few bad patterns were manually removed for 
each relation. More importantly, the learning algorithm strong-
ly favors high precision patterns because it needs to maintain a 
seed set with low noise in order to learn effectively.  
tence ?ACME spent 5 years developing a pill to 
treat the flu which it will release next week,’ ex-
tracting relations involving ?the pill’  would allow 
a system to answer questions that use multiple rela-
tions in the ontology to for example ask about  or-
ganizations developing treatments for the flu, or 
the expected date of release for ACME’s drugs. 
However, in our simple question answering 
framework such generic answers never convey 
novel information and thus were probably ignored 
by human annotators.  
 To measure the impact of treating these generic 
references as correct,10 we did additional annota-
tion on the correct answers, marking answers as 
?useful‘ (specific) and ?not-useful‘ (generic). The 
unboxed bars in Figure 2 show performance when 
?not-useful‘ answers are removed from the answer-
key and the responses. For the four relations where 
there was a change Table 5 provides the relative 
change performance when only ?useful‘ answers 
are considered. The annotator‘s recall increases 
noticeably while the combined system‘s drops. 
This results in the overall recall of annotators sur-
passing that of the combined system.   
Relation 
Recall Precision 
A C H L C H L 
possTreat 12 10 10 14 -10 -11 -3 
respTreat 9 0 -5 8 -4 -4 -1 
studyDis 12 -6 -9 13 -11 -13 0 
hasSidEff 3 4 4 4 0 0 0 
Total 11 -2 -4 6 -9 -10 -2 
Table 5: Relative Change in Recall and Precision When 
Non-Useful Answers are Removed 
Table 7 shows the total number of answers pro-
duced by annotators and by each system, as well as 
the percentage of queries with at least one correct 
answer for each system. For one relation expec-
tedDateOnMarket(), the learned system did not 
find any answers. This relation had far fewer an-
swers found by annotators and occurred far more 
rarely in the fully annotated extraction set (see Ta-
ble 8). Anecdotally, extracting this relation fre-
quently required co-referencing ?it‘ (e.g. ?It will be 
released in March 2011”). Our heuristics for core-
ference of the new classes did not account for pro-
nouns. Learning from such examples would 
require coreference during bootstrapping. Most 
likely, the learned system was unable to generate 
enough novel instances to continue bootstrapping 
                                                          
10 Generic answers were treated as correct only if a more spe-
cific reference was not available in the document.  
1442
and was thus unable to learn the relation.  
Relation Type 
(# Queries; # Correct Ans.) 
Recall Precision F 
A C HW L C HW L C HW L 
possTreatment (10;247) 0.27 0.63 0.50 0.34 0.51 0.47 0.83 0.56 0.48 0.48 
respForTreat (15;134) 0.73 0.33 0.24 0.22 0.66 0.78 0.73 0.44 0.37 0.33 
expectDateMarkt (11;60) 0.90 0.17 0.17 0.00 0.77 0.83 0.00 0.27 0.28 0 
studiesDisease (13;292) 0.23 0.67 0.59 0.09 0.51 0.50 0.79 0.58 0.54 0.16 
hasSideEffect (11;104) 0.80 0.10 0.13 0.02 0.83 0.70 1.00 0.17 0.23 0.04 
Total (60;837) 0.44 0.49 0.42 0.17 0.53 0.52 0.80 0.51 0.46 0.28 
Table 6: Question Answering Results by Relation Type 
Relation Type 
 
Total Number of Answers % Queries with At Least 1 Corr. Ans 
A C HW L A C HW L 
possTreatment  66 303 261 100 100.0% 90.0% 90.0% 90.0% 
respForTreat  98 67 41 40 100.0% 66.7% 60.0% 60.0% 
expectDateMarkt  54 13 12 0 72.7% 45.5% 45.5% 0.0% 
studiesDisease  68 379 347 33 100.0% 61.5% 46.2% 46.2% 
hasSideEffect  83 12 20 2 72.7% 36.4% 45.5% 18.2% 
Total  369 774 681 175 90.0% 60.0% 56.7% 43.3% 
Table 7: Number of Answers and Number of Queries Answered 
Overall, the system did better on relations hav-
ing more correct answers. Bootstrap learning has 
an easier time discovering new instances and new 
patterns when there are more examples to work 
with. Even a human pattern writer will have more 
examples to generalize from for common relations.  
While possibleTreatment() and hasSideEffect() 
have similar F-scores, their performance is very 
different at the query level. The system was able to 
find at least one correct answer to every possible-
Treatment() query; however only 72.7% of the stu-
diesDisease() queries were answered.  
Table 8 presents results from the extraction 
evaluation where a set of ~100 documents were 
annotated for all mentions of the 5 relations. Be-
cause every mention in the document set must be 
found, the system cannot rely on finding the easiest 
answers for common relations. The results in Table 
8 are significantly lower than for the question ans-
wering tasks; yet some of the same trends are 
present. Handwritten rules outperform learned pat-
terns. For at least some relations, the combination 
of the two improves performance. The three rela-
tions for which the learned system has the lowest 
performance on the question-answering task have 
the fewest instances annotated in the document set. 
Fewer instance in the large corpus make bootstrap-
ping more difficult—the learner is less able to gen-
erate novel instances to expand its pattern set.  
7 Discussion 
7.1 Sources of Error 
The most common source of error is pattern cover-
age. In the following figure, the system identified 
responsibleForTreatment(Janssen Pharmaceutical, 
Sporanox), but missed the corresponding relation 
between Novartis and Lamisil.  
 
 
 
 
 
Relation Type # Relations Found Recall Precision F 
GS C HW L C HW L C HW L C HW L 
possibleTreatment 518 225 187 68 0.15 0.10 0.09 0.34 0.28 0.66 0.21 0.15 0.15 
respForTreatment 387 101 77 36 0.10 0.08 0.05 0.41 0.40 0.50 0.17 0.13 0.08 
expDateOnMarket 66 13 13 0 0.06 0.06 0.00 0.31 0.31 0.00 0.10 0.10 0.00 
studiesDisease 136 95 91 4 0.08 0.09 0.00 0.12 0.13 0.00 0.10 0.11 0.00 
hasSideEffect 256 26 25 2 0.04 0.04 0.00 0.39 0.40 0.50 0.07 0.07 0.01 
Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations 
Sporanox is made by Janssen Pharmaceutica Inc., 
of Titusville, N.J. Lamisil is a product of Novartis 
Pharmaceuticals of East Hanover, N.J. 
 
 
1443
Missed class instances contribute to errors, some-
times originating in errors in tokenization (e.g. not 
removing the ?_‘ in each drug name in a bulleted 
list of the form ?_Trovan, an antibiotic...; etc.) 
However, many drug-names are simply missed: 
 
The system correctly identifies Rebif and Aricept 
as drugs, but misses Pregabalin and Serono. In 
both misses, the immediately preceding and fol-
lowing words provide little evidence that the word 
refers to a drug rather than some other product. 
Substance detection might be better served with a 
web-scale, list-learning approach like the doubly 
anchored patterns described in (Kozareva et al., 
2008). Alternatively, our approach may need to be 
extended to include a larger context window. 
7.2 Learned Patterns  
One of the ways in which learned patterns supple-
ment handwritten ones is learning highly specific 
surface-string patterns that are insensitive to errors 
in parsing. Figure 3 illustrates two examples of 
what appear to be easy cases of possibleTreat-
ment(). Because the handwritten patterns are not 
exhaustive and make extensive use of syntactic 
structure, parse errors prevented the system based 
on handwritten rules from firing. Learned surface-
string patterns were able to find these relations.  
Even when the syntactic structure is correct, 
learned patterns capture expressions not common 
enough to have been noticed by the rule writer. For 
example, while the handwritten patterns included 
?withdrew’ as a predicate indicating a company 
was responsible for a drug, they did not include 
?pulled.’ By including ?pulled’, learned patterns 
extracted responsibleForTreatment() from ?Ameri-
can Home Products pulled Duract, a painkiller.’ 
Similarly, the learned patterns include an explicit 
pattern ?CONDITION drug called SUBSTANCE’, 
and thus extracted a possibleTreatment() relation 
from ?newly approved narcolepsy drug called 
modafinil’ without relying on the coreference 
component to link drug to modafinil.  
Handwritten Patterns 
Despite the examples above of successfully learned 
patterns, handwritten patterns perform significantly 
better. In the active-learning context used for these 
experiments, the handwritten rules also required 
less manual effort. This comparison is not entirely 
fair-- while learned patterns required more hours, 
supervising the bootstrapping algorithm requires 
no training. The handwritten patterns, in contrast, 
require a trained expert.  
 
Figure 3: Extractions Missed by Handwritten Rules & 
the Erroneous Parses that Hid them 
While handwritten rules and learned patterns use 
the same language, they make use of it differently. 
The handwritten patterns group similar concepts 
together. A human pattern writer adds relevant 
synonyms, as well as words that are not synonym-
ous but in the pattern context can be used inter-
changeably. In Figure 4, the handwritten patterns 
include three word-sets: (patient*, people, partici-
pant*); (given, taken, took, using); and (report*, 
experience*, develop*, suffer*). The ?*‘ serves as a 
wild-card to further generalize a pattern. The word-
sets in Figure 4 illustrate challenges for a learned 
system: the words are not synonyms, but rather are 
words that can be used to imply the relation.  
A human pattern writer frequently generates 
new classes not in the domain ontology. In Figure 
4, the circled patterns form a class of ?people tak-
ing a substance.‘ The handwritten patterns for stu-
diesDisease() include classes targeting scientists 
and researchers. These classes are not necessarily 
triggered by nouns. Such classes allow the pattern 
writer to include complex patterns as in Figure 4 
and to write relatively precise, but open-ended pat-
terns such as: if there is a single named-drug and a 
named, non-side-effect disease in the same sen-
tence, the drug is a treatment for the disease.  
Pfizer also hopes to introduce Pregabalin next 
year for treatment of neuropathic pain, epilepsy 
and anxiety…Other deals include co-promoting 
Rebif for multiple sclerosis with its discoverer, 
Serono, and marketing Aricept for Alzheimer's 
disease with its developer, Eisai Co. 
1444
 
Figure 4: Learned and Handwritten Patterns for  
hasSideEffect() 
A final difference between handwritten and 
learned patterns is the level of predicate-argument 
complexity used. In general, handwritten patterns 
account for larger spans of predicate argument 
structure while learned patterns tend to limit them-
selves to the connections between the arguments of 
the relation with minor extensions.  
8 Conclusions and Lessons Learned 
First, it is encouraging that the synthesis of learn-
ing algorithms and handwritten algorithms can 
achieve an F1 of 0.51 in a new domain in a week 
(<50 hours of effort). Second, it is exciting that so 
little training data is required: ~20 relation pairs 
out of context (~2.5 hours of effort) and ~6 hours 
of active learning for the new classes.  
Third, the effectiveness of learning algorithms is 
still not competitive with handwritten patterns 
based on predicate-argument structure (~5 hours of 
effort on top of active learning for entities). 
Though the learned patterns have high precision 
(0.80 on average), recall is low (0.17) and varied 
greatly across the relations. Though the dominant 
factor in missing relations is pattern coverage, 
missing instances of classes contributed to low re-
call. Comparing learned patterns to manually writ-
ten patterns, (1) synonyms or other lexical 
alternatives that a human pattern writer would in-
clude, (2) the creation of subclasses for argument 
types, and (3) the scope of patterns11 are each ma-
jor sources of the disparity in coverage. Research 
on learning approaches to raise recall without sig-
nificant sacrifice in precision seems essential.  
Fourth, despite the disparity in performance of 
learned versus manual patterns, and despite the low 
                                                          
11 Learned patterns tend to focus on the structure that appears 
between the two arguments, rather than structure surrounding 
the left and right arguments. 
recall of learned patterns, the combined system‘s 
recall and F-Score are higher for three of the rela-
tions because the learned patterns generated an-
swers not found by handwritten patterns. We found 
examples where highly specific, learned, surface-
level patterns (lexical patterns) occasionally found 
information missed by handwritten patterns due to 
parsing errors or general low coverage. 
Fifth, the effort for coreference was the most 
time-consuming, given that every new relation 
contained at least one of the new argument types. 
While we included this in our estimate of domain 
adaptation, the infrastructure we built is domain 
generic. Improving generic coreference will reduce 
domain specific effort in future.  
Perhaps most significant of all, running a com-
plete experiment from definition of the domain 
through creation of training data and measurement 
of end-to-end performance of the system can be 
completed in a month. The ability to rapidly, 
cheaply, and empirically measure the impact of 
extraction research could prove a significant spur 
to research across the board. 
These experiments suggest three possible direc-
tions for improving the ability to quickly develop 
information extraction technology for a new set of 
relations: (1) reducing the amount of supervision 
provided to the bootstrap-learner; (2) improving 
the bootstrapping approach to reach the level of 
recall achieved by the human pattern writer elimi-
nating the need for a trained expert during domain 
adaptation; and (3) focusing improvements to the 
bootstrapping approach on techniques that allow it 
to find more of the instances missed by the pattern 
writer, thus improving the accuracy of the hybrid 
system.   
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. Distribu-
tion Statement ?A? (Approved for Public Release, 
Distribution Unlimited) Thank you to the review-
ers for your insightful comments and to Michelle 
Franchini for coordinating the assessment effort. 
References 
E. Agichtein and L. Gravano. Snowball: extracting rela-
tions from large plain-text collections. In Proceed-
ings of the ACM Conference on Digital Libraries, pp. 
85-94, 2000.  
1445
A. Blum and T. Mitchell. Combining Labeled and Un-
labeled Data with Co-Training. In Proceedings of the 
1998 Conference on Computational Learning 
Theory, July 1998.  
E. Boschee, V. Punyakanok, R. Weischedel. An Explo-
ratory Study Towards ?Machines that Learn to Read‘. 
Proceedings of AAAI BICA Fall Symposium, No-
vember 2008. 
J. Chen, D. Ji, C. Tan and Z. Niu. (2006). Relation ex-
traction using label propagation based semi-
supervised learning. COLING-ACL 2006: 129-136. 
July 2006. 
M. Freedman, E. Loper, E. Boschee, and R. Weischedel. 
Empirical Studies in Learning to Read. Proceedings 
of NAACL 2010 Workshop on Formalisms and Me-
thodology for Learning by Reading, pp. 61-69, June 
2010. 
W. Li and A. McCallum.  Rapid development of Hindi 
named entity recognition using conditional random 
fields and feature induction. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
R Grishman and B. Sundheim. Message Understanding 
Conference-6 : A Brief History", in COLING-96, 
Proc . of the Int'l Conj. on Computational Linguis-
tics, 1996.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. Human 
Language Technologies: The 2010 Annual Confe-
rence of the North American Chapter of the Associa-
tion for Computational Linguistics, June, 2010, pp. 
618-626. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic 
class learning from the web with hyponym pattern 
linkage graphs. In Proceedings of ACL-08: HLT, 
pages 1048–1056.  
J. May, A. Brunstein, P. Natarajan,  and R. Weischedel.  
Surprise! What's in a Cebuano or Hindi Name? 
Transactions on Asian Language Information 
Processing (TALIP), Volume 2 Issue 3  September, 
2003. 
D. Maynard, V. Tablan, K. Bontcheva, and H. Cun-
ningham. Rapid customization of an information ex-
traction system for a surprise language. Transactions 
on Asian Language Information Processing (TALIP), 
Volume 2 Issue 3  September, 2003. 
S. Miller, J. Guinness, and A. Zamanian, ?Name Tag-
ging with Word Cluster and Discriminative Train-
ing?, Proceedings of HLT/NAACL 2004, pp. 337-
342, 2004 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. ?Populating the Semantic Web by Macro-
Reading Internet Text. Invited paper, Proceedings of 
the 8th International Semantic Web Conference 
(ISWC 2009).  
NIST, ACE 2007: 
http://www.itl.nist.gov/iad/mig/tests/ace/2007/softwa
re.html 
P. Pantel and M. Pennacchiotti. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Se-
mantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia, 2006.  
L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel, A. Zamanian, ?Experiments in 
multi-modal automatic content extraction?, Proceed-
ings of Human Technology Conference, March 2001.  
D. Ravichandran and E. Hovy. Learning surface text 
patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), 
pages 41–47, Philadelphia, PA, 2002.  
E. Riloff. Automatically generating extraction patterns 
from untagged text. In Proceedings of the Thirteenth 
National Conference on Artificial Intelligence, pages 
1044-1049, 1996.  
S. Sekine and R. Grishman.  Hindi-English cross-lingual 
question-answering  system. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
G. Zhou, J. Li, L. Qian, Q. Zhu. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I. 2008. 
 
1446
