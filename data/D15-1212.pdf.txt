Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1847–1856,
Lisbon, Portugal, 17-21 September 2015.
c
©2015 Association for Computational Linguistics.
Multilingual discriminative lexicalized phrase structure parsing
Benoit Crabb
´
e
Alpage – Universit´e Paris Diderot – Inria – IUF
Place Paul Ricoeur 75013 Paris
benoit.crabbe@univ-paris-diderot.fr
Abstract
We provide a generalization of discrimina-
tive lexicalized shift reduce parsing tech-
niques for phrase structure grammar to a
wide range of morphologically rich lan-
guages. The model is efficient and outper-
forms recent strong baselines on almost all
languages considered. It takes advantage
of a dependency based modelling of mor-
phology and a shallow modelling of con-
stituency boundaries.
1 Introduction
Lexicalized phrase structure parsing techniques
were first introduced by Charniak (2000) and
Collins (2003) as generative probabilistic models.
Nowadays most statistical models used in natu-
ral language processing are discriminative: dis-
criminative models provide more flexibility for
modelling a large number of variables and conve-
niently expressing their interactions. This trend is
particularly striking if we consider the literature in
dependency parsing. Most state of the art multi-
lingual parsers are actually weighted by discrimi-
native models (Nivre and Scholz, 2004; McDon-
ald et al., 2005; Fern´andez-Gonz´alez and Martins,
2015).
With respect to multilingual phrase structure
parsing, the situation is quite different. Most
parsers focus on fixed word order languages like
English or Chinese as exemplified by Zhu et al.
(2013). Despite a few exceptions (Collins et al.,
1999), multilingual state of the art results are gen-
erally derived from the generative model of Petrov
et al. (2006). Although more recently Hall et
al. (2014) introduced a conditional random field
parser that clearly improved the state of the art in
the multilingual setting.
Both Petrov et al. (2006) and Hall et al. (2014)
frame their parsing model to model in priority
regular surfacic patterns and word order: Petrov
et al. (2006) crucially infers category refinements
(called category ‘splits‘) in order to specialize
the grammar on recurrent informative patterns ob-
served on input spans. Hall et al. (2014) re-
lies on a similar intuition : the model essentially
aims to capture regularities on the spans of con-
stituents and their immediate neighbourhood, fol-
lowing earlier intuitions of Klein and Manning
(2004). This modelling strategy has two main mo-
tivations. First it reduces the burden of feature en-
gineering, making it easier to generalize to multi-
ple languages. Second it avoids modeling explic-
itly bilexical dependencies for which parameters
are notoriously hard to estimate from small data
sets such as existing treebanks.
On the other hand this strategy becomes less in-
tuitive when it comes to modeling free word or-
der languages where word order and constituency
should in principle be less informative. As such,
the good results reported by Hall et al. (2014)
are surprising. It suggests that word order and
constituency might be more relevant than often
thought for modelling free word order languages.
Nevertheless, free word order languages also
tend to be morphologically rich languages. This
paper shows that a parsing model that can effec-
tively take morphology into account is key for
parsing these languages. More specifically, we
show that an efficient lexicalized phrase structure
parser - modelling both dependencies and mor-
phology - already significantly improves parsing
accuracy. But we also show that an additional
modelling of spans and constituency provides ad-
ditional robustness that contributes to yield state
of the art results on almost all languages consid-
ered, while remaining quite efficient. Moreover,
given the availability of existing multi-view tree-
banks (Bhatt et al., 2009; Seddah et al., 2013; Qiu
et al., 2014), our proposed solution only requires
a lightweight infrastructure to achieve multilin-
1847
gual parsing without requiring costly language-
dependent modifications such as feature engineer-
ing.
The paper is organized as follows. We first re-
view the properties of multiview treebanks (Sec-
tion 2). As these treebanks typically do not pro-
vide directly head annotation, an information re-
quired for lexicalized parsing, we provide an au-
tomated multilingual head annotation procedure
(Section 3). We then describe in section 4 a vari-
ant of lexicalized shift reduce parsing that we use
for the multilingual setting. It provides a way to
integrate morphology in the model. Section 5 fi-
nally describes a set of experiments designed to
test our main hypothesis and to point out the im-
provements over state of the art in multilingual
parsing.
2 Multi-view treebanks
Multi-view treebanks are treebanks annotated both
for constituents and dependencies that have the
property to be token-wise aligned (Bhatt et al.,
2009; Seddah et al., 2013; Qiu et al., 2014) . These
double annotations are typically obtained by con-
verting a constituency or dependency annotation
into the other annotation type. This method was
used for the construction of the dataset for the
SPMRL 2013 shared task (Seddah et al., 2013),
which contains multi-view treebanks for a num-
ber of morphologically rich languages, for which
either constituency or dependency treebanks were
available. The same kind of process was applied
to the Penn TreeBank using the Stanford conver-
sion system to produce dependency annotations
(de Marneffe et al., 2006). In this paper, we use
both of these datasets.
Although in multi-view treebanks each sentence
is annotated both for constituency and depen-
dency, they are not normalized for categories nor
lexical features accross languages such as depen-
dencies in the Google Universal Treebank (Mc-
Donald et al., 2013). What is more, the depen-
dency and constituency structures may sometimes
strongly differ. For some languages, like Hungar-
ian, the conversion has involved some manual re-
annotation (Vincze et al., 2010).
3 Head annotation procedure
Lexicalized phrase structure parsers traditionally
use hand-crafted heuristics for head annotation
(Collins, 2003). Although these heuristics are
available for some languages, for others they are
non existent or non explicit and typically hidden
in conversion procedures. In order to leverage the
burden of managing language specific heuristics,
we first automate head annotation by taking ad-
vantage of the multi-view annotation.
We begin by introducing some notation. As-
suming a sentence W = w
1
. . . w
n
, the depen-
dency annotation of this sentence is assumed to be
a dependency forest (Kuhlmann and Nivre, 2006).
A dependency graph G = ?V,E? where V =
{1 . . . n} is the set of word indexes or vertices and
E ? V × V is a set of dependency links. By
convention, a dependency (i, j) means that i gov-
erns j. A dependency forest is a dependency graph
such that a node has at most a single incoming
edge and where there is no cycle. A node with no
incoming edge is a root of the dependency forest
and a dependency tree is a dependency forest with
a single root. For some languages, such as Ger-
man or Basque, the dependency structures found
in the data set are actually dependency forests.
Lexicalized parsing relies on head annotation,
in other words each node in a constituency tree is
associated with the word index of its head. More
formally, let A be the set of nodes in the c-tree,
head annotation can be represented as a function
h : A 7? {1 . . . n} which maps each node a ? A
to the index of its head in the input sentence. h
is obtained by leveraging head-related information
associated with each rule in the grammar. More
precisely, each rule ? ? ?, with ? = a
1
. . . a
k
,
is associated with a head index i (1 ? i ? k) that
states that the head h(?) of any node labeled ? in a
constituency tree that is built using this rule is the
same as the head of the right-hand side symbol a
i
.
A Naive h function is straightforwardly defined
as the annotation of each local rule part of the tree
in a bottom-up fashion:
1
Base: h(w
i
) = i ?w
i
?W
Recurrence:
h(?) =
{
h(a
i
) if ?
a
j
:a
j
??,i6=j
(h(a
i
), h(a
j
)) ? E
? otherwise
When ?a ? A such that h(a) = ? we say that the
annotation has failed.
However the naive procedure fails in a large
number of cases. Failures fall into the four pat-
terns that are illustrated in Figure 1. For each of
1
The additional case of unary rules is straightforward and
left to the reader.
1848
XX
a
X
b
X
c
a b c
root
– –
X
X
X
a
X
b
X
c
a b c
root
–
–
X
X
a
X
b
X
c
a b c
rootrootroot
X
X
X
a
X
b
X
c
X
d
X
e
a b c d e
root
–
– –
–
Local restructuration I Local restructuration II Forest effect Non projectivity
Figure 1: Patterns of the causes of problems taking place during head annotation
the patterns we have highlighted in bold the sym-
bols for which the Naive procedure currently fails.
Local Restructuration I is where the c-structure is
flatter than the d-structure. Here the Naive pro-
cedure fails because (a, c) 6? E. Local Restruc-
turation II is where the d-structure is flatter than
the c-structure. The procedure fails because nei-
ther (a, b) ? E nor (b, a) ? E. Forest Effect is
where the d-structure is a dependency forest (here
E = ?). And finally Non Projectivity is where the
d-tree is non projective.
We can easily correct the naive procedure for
Local Restructuration I by taking advantage of
E+, the non reflexive transitive closure of E, thus
yielding the following Corrected procedure:
Base: h(w
i
) = i ?w
i
?W
Recurrence:
h(?) =
{
h(a
i
) if ?
a
j
:a
j
??,i6=j
(h(a
i
), h(a
j
)) ? E
+
? otherwise
The three other cases are more problematic, since
their correction would somehow require altering
the structure of either the c-tree or the d-tree.
Refraining from altering the constituency data set
we instead use a catch-all procedure that essen-
tially creates the problematic head annotation by
analogy with the rest of the data, yielding a fully
Robust procedure that is guaranteed to succeed in
any case:
Base: h(w
i
) = i ?w
i
?W
Recurrence:
h(?) =
{
h(a
i
) if ?
a
j
:a
j
??,i6=j
(h(a
i
), h(a
j
)) ? E
+
h(a
KNN(???)
) otherwise
where KNN(? ? ?) is a function returning a
guess for the position of the head in ?, the right
hand side of the rule, based on similarity to suc-
cessfully head annotated rules.
The details are as follows. KNN(? ? ?) sup-
poses a dataset D = (R
i
,H
i
)
N
i=1
of successfully
head annotated rules. In this dataset, each rule
R
i
= ? ? ? is associated with H
i
the posi-
tion of the head in ?. We define the similarity
between two rules R
1
= ?
(1)
? a
(1)
1
. . . a
(1)
k
and R
2
= ?
(2)
? a
(2)
1
. . . a
(2)
k
?
to be the Lev-
enshtein distance between ?
(1)
, a
(1)
1
. . . a
(1)
k
and
?
(2)
, a
(2)
1
. . . a
(2)
k
?
. In practice for a given rule R
the function returns the most frequent H among
the 5 most similar rules in the data set.
The full head annotated data set D is built by
reading off the rules from the trees successfully
annotated in the treebank by the Corrected proce-
dure in a first pass. A second pass yields the final
annotation by running the Robust procedure.
Analysis of the conversion We report in Table 1
an overall quantification of the conversion proce-
dure: % Success (Corrected) reports the number
of trees succesfully annotated by the Corrected
procedure and Silver UAS reports an UAS score
obtained by comparing the reference dependency
trees to the conversion of those obtained from the
Robust conversion of the head-annotated phrase
structure trees back to dependency structures. The
conversion works well apart from four languages
(Arabic, Basque, German and Hungarian) which
cause more difficulties.
Language % Success (Corrected) Silver UAS
ARABIC 61.7 92.0
BASQUE 54.2 82.7
ENGLISH 99.9 98.8
FRENCH 99.9 99.3
GERMAN 98.4 72.1
HEBREW 98.2 99.0
HUNGARIAN 85.9 80.1
KOREAN 100.0 100.0
POLISH 98.6 98.8
SWEDISH 99.6 98.8
Table 1: Quantification of the conversion
In order to better understand the problems faced
by the conversion procedure, we manually in-
spected the errors returned by the Corrected pro-
1849
cedure. For each language, we sampled 20 ex-
amples of failures encountered and we manually
categorized the errors using the four patterns illus-
trated in Figure 1. Across languages, 49.9% of the
errors come from the pattern Local Restructura-
tion II and 50% from the pattern Forest effect and
more suprisingly, we found only one example in
our sample from the pattern Non projectivity in the
Hungarian treebank. This overall average hides
however an important variation across treebanks.
The Forest effect is indeed massively found in the
Basque
2
(100%) and German treebanks and more
marginally in the Hungarian data set. Most of the
time, these are cases of short word sequences (2 to
5 tokens) where all nodes are annoted as roots of
the dependency trees. The Local restructuration II
is mostly found in the Arabic, Hebrew and Polish
treebanks and less frequently in Hungarian. Ara-
bic and Hebrew tend to follow a binary annotation
scheme partially inspired by X-Bar, hence creat-
ing additional constituent structures that are not di-
rectly inferrable from the dependency annotation.
Polish uses this restructuration in patterns involv-
ing coordination. More surprisingly, non projec-
tive patterns, which we expected to be a signifi-
cant feature of these languages, remain marginal
in comparison to annotation related idiosyncrasic
problems.
4 Parsing algorithm
This section provides an overview of the design of
the constituent parsing system. There are three re-
cent proposals for beam-based discriminative shift
reduce parsing for phrase structure grammar with
a structured perceptron and beam search (Zhu et
al., 2013; Crabb´e, 2014; Mi and Huang, 2015). All
three proposals point out that for weighted phrase
structure parsing, the shift reduce algorithm re-
quires a special treatment of unary rules in order to
compare derivations of the same length. They all
provide different management schemes for these
unaries.
The work described here draws on the LR algo-
rithm introduced by Crabb´e (2014), but provides a
simpler algorithm, it precisely describes the man-
agement of unary rules and clarifies how spans and
morphological information is represented (see sec-
tion 5 ).
2
The constituency conversion of the Basque treebank also
contains a recurrent attachment error of the punctuations
which we ignored when computing this statistic.
For each language, the grammar is induced
from a treebank using the following preprocessing
steps. The corpus is first head-annotated with the
Robust head annotation procedure. Second, the
treebank is head-markovized (order 0) and unary
productions that do not emit tokens
3
are collapsed
into unique symbols. Once this has been done
we assume that tokens to be parsed are a list of
couples (tag, wordform). The preprocessing steps
ensure the binarized treebank implicitly encodes
a binary lexicalized grammar whose rules are ei-
ther in Chomsky Normal Form (CNF) like in (a)
X[h]? A[x]B[h], X[h]? A[h]B[x], X[t]? t
or are also of the form (b) X[h]? A[h] t, X[t]?
A[h] t, X[h] ? tB[h], X[t] ? tB[h] where
A,B,X are delexicalized non-terminals, h, x, t
are tokens (terminals) and A[h], A[x] . . . X[t] are
lexicalized non-terminals. Given a grammar in
CNF, we can prove that for a sentence of length
n, the number of derivation steps for a shift reduce
parser is 3n ? 1. However our tagset-preserving
transformation also introduces rules of the form
(b), which explains why the number of derivation
steps may vary from 2n? 1 to 3n? 1.
To ensure that a derivation is of length 3n ? 1,
the parser forces each shift to be followed by either
a unary reduction or an alternative dummy Ghost
Reduction (GR). Given the pre-processed treebank
we infer the set A of actions used by the parser.
Let ? be the set of non-terminal symbols (includ-
ing temporary symbols) read off from the binary
treebank. The set of actions contains one Shift (S),
one Ghost Reduction (GR) a set of |?| unary re-
ductions (RU-X), one for each symbol, a set of |?|
binary left reductions (RL-X) and a set of |?| bi-
nary right reductions (RR-X) (see also Sagae and
Lavie (2006) and Figure 3 for details).
The parser itself is organized around two data
structures: a stack of symbols, S = . . . |s
2
|s
1
|s
0
,
whose topmost element is s
0
. Symbols are lexi-
calized non terminals or tokens of the form A[x].
The second structure is a queue statically filled
with tokens T = t
1
. . . t
n
. Parsing is performed
by sequentially generating configurations C of the
form ?j,S, ·? where S is a stack and j is the index
of the first element of the queue. Given an ini-
tial configuration C
0
= ?1, ,??, a derivation step
C
t?1
a
t?1
? C
t
generates a new configuration C
t
by
applying an action a
t?1
? A as defined in Fig-
ure 3. The derivation is complete and successful
3
In order not to alter the tagset of the treebank.
1850
s2
.c
t
[s
2
.w
t
]
s
1
.c
t
[s
1
.w
t
]
s
1
.c
l
[s
1
.w
l
]
s
1
.lc
s
1
.c
r
[s
1
.w
r
]
s
1
.rc
s
0
.c
t
[s
0
.w
t
]
s
0
.c
l
[s
0
.w
l
]
s
0
.lc
s
0
.c
r
[s
0
.w
r
]
s
0
.rc
q
1
. . .q
j
? ?? ?
stack
? ?? ?
queue
Language gen num case mood aspect other
ARABIC gender number case mood aspect -
BASQUE - NUM KAS MDN ASP DADUDIO,ERL,NOR(I|K)?
ENGLISH - - - - - -
FRENCH g n - m - mwe
GERMAN gender number case mood - -
HEBREW gen num - - - tense
HUNGARIAN - Num Cas Mood - SubPOS
KOREAN - - case-type - - verb-type
POLISH gender number case - aspect post-prepositionality
SWEDISH gender number case verbform - perfectform
Figure 2: Features available for scoring. s
x
denote a position in the stack. Stack positions are local trees of depth 1, features
can access its top, left and right nodes. The suffixes c
p
,w
p
, lc, rc denote respectively the delexicalized category, the head
token, the left corner token, the right corner token of a stack position. For tokens elements accessible from the stack (s
x
.w
x
)
and from the queue (q
x
), features can access the word form, pos tag or any morphological feature m available for that language
as described in the table at the right
INIT ?1, ,?? : 0
GOAL ?n+ 1, ?,?? : w
SHIFT
?j,S,?? :w
?j+1,S | t
j
.tag[t
j
.word]),>? :w+F (S,?j,S?)
RL(X)
?j,S
	
| c
1
[t
1
] c
0
[t
0
],?? :w
?j,S
	
|X[t
1
],?? :w+F (RL(X),?j,S?)
RR(X)
?j,S
	
| c
1
[t
1
] c
0
[t
0
],?? :w
?j,S
	
|X[t
0
],?? :w+F (RR(X),?j,S?)
RU(X)
?j,S
	
| c
0
[t
0
],>? :w
?j,S
	
|X[t
0
],?? :w+F (RU(X),?j,S?)
GR(X)
?j,S
	
| c
0
[t
0
],>? :w
?j,S
	
| c
0
[t
0
],?? :w+F (GR,?j,S?)
Figure 3: Weighted inference rules
once the action C
3n?1
is generated. A derivation
sequence C
0??
is a sequence of derivation steps
C
0
a
0
? . . .
a
??1
? C
?
Weighted prediction The choice of the action
a ? A at each derivation step is naturally non-
deterministic. Determinism is provided by a
weighting function based on a linear model of the
form:
W (C
0??
) =
??1
?
i=0
w ·?(a
i
, C
i
) =
??1
?
i=0
F (a
i
, C
i
)
where w ? R
d
is a weight vector and ?(a
i
, C
i
) ?
{0, 1}
d
is a feature vector. The best parse is then
the successful derivation with the maximum score:
ˆ
C
0?3n?1
= argmax
C
0?3n?1?GEN
3n?1
W (C
0?3n?1
)
In practice, we use a beam of size K at each time
step and lossy feature hashing, which makes the
inference approximative.
For the purpose of computing weights, we ex-
tend the representation of the stack and queue el-
ements such that the feature functions have ac-
cess to a richer context than just simple lexical-
ized symbols of the form A[x]. As described in
Figure 2 (left), features can also access the imme-
diate left and right children of s
0
and s
1
as well as
their left and right corner tokens. This allows us
to encode the span models described in Section 5.
We also use tuple-structured tokens encoding not
only the word-form and the tag but also additional
custom lexical features such as those enumerated
in Figure 2 (right). This allows us to express the
morphological models described in Section 5.
Finally, the parameters w are estimated with a
parallel averaged structured perceptron designed
to cope with inexact inference (beam search):
we specifically rely on max-violation updates of
Huang et al. (2012) and on minibatches to acceler-
ate and parallelize training (Shalev-Shwartz et al.,
2007; Zhao and Huang, 2013).
5 Experiments
The experiments aim to compare the contribution
of span based features approximating some intu-
itions of Hall et al. (2014) for shift reduce parsing
and morphological features for parsing free word
order languages. We start by describing the evalu-
ation protocol and by defining the models used.
We use the standard SPMRL data set (Seddah et
al., 2013). Part of speech tags are generated with
Marmot (M¨uller et al., 2013), a CRF tagger specif-
ically designed to provide tuple-structured tags.
The training and development sets are tagged by
10-fold jackknifing. Head annotation is supplied
by the Robust procedure described in Section 3.
The parser is systematically trained for 25 epochs
with a max violation update perceptron, a beam of
size 8 and a minibatch size of 24.
1851
To enable a comparison with other published re-
sults, the evaluation is performed with a version of
evalb provided by the SPMRL organizers (Sed-
dah et al., 2013) which takes punctuation into ac-
count.
Baseline model (B) The baseline model uses a
set of templates identical to those of Zhu et al.
(2013) for parsing English and Chinese except that
we have no specific templates for unary reduc-
tions.
Span-based model (B+S) This model extends
the B model by modeling spans. The span model
approximates an intuition underlying Hall et al.
(2014): constituent boundaries contain very infor-
mative tokens (typically function words). These
tokens together with the pattern of their neighbor-
hood provide key clues for detecting and (sub-
)typing constituents. Moreover, parameter esti-
mation for frequent functional words should suf-
fer less from data sparseness issues than the esti-
mation of bilexical dependencies on lexical head
words. The model includes conjunctions of non-
terminal symbols on the stack with their left and
right corners (words or tags) and also their imme-
diately adjacent tokens across constituents. Using
the notation given in Figure 2 we specifically in-
cluded the following matrix templates :
s
0
.c
t
&s
0
.lc.word&s
0
.rc.word
s
1
.c
t
&s
1
.lc.word&s
1
.rc.word
s
0
.c
t
&s
0
.lc.word&s
1
.rc.word
q
1
.word&s
0
.lc.word&s
0
.rc.word
q
2
.word&s
0
.lc.word&s
0
.rc.word
from which we derived additional backoff tem-
plates where only a single corner condition is ex-
pressed and/or words are replaced by tags.
Morphological model (B+M) This model ex-
tends the B model by adding morphological fea-
tures. This model aims to approximate the intu-
ition that morphological features such as case are
key for identifying the structure of free word order
languages. As feature engineering may become in
principle quite complex once it comes to morphol-
ogy, we targeted fairly crude models with the goal
of providing a proof of concept. Therefore the
morphologically informed models use as input a
rich set of morphological features specified in Fig-
ure 2 (right) predicted by the CRF tagger (M¨uller
et al., 2013) with the same jackkniffing as before.
The content of Figure 2 provides an explicit indi-
cation of the actual features defined in the original
treebanks (see Seddah et al. (2013) and references
therein for details), while the columns are indica-
tive normalized names. For Basque most of the
additional morphological features further encode
case and verbal subcategorization. For French the
mwe field abbreviates IOB predicted tags derived
from multi-word expression annotations found in
the original dataset.
Now let M be the set of values enumerated for
a language in Figure 2 (right), we systematically
added the following templates to model B:
s
0
.w
t
.m&s
1
.w
t
.m&q
1
.tag ?m ?M
s
0
.w
t
.m&s
1
.c
t
&q
1
.m ?m ?M
s
0
.c
t
.m&s
1
.w
t
.m&q
1
.m ?m ?M
s
0
.w
t
.m&q
1
.m&q
2
.tag ?m ?M
s
0
.w
t
.m&q
1
.tag&q
2
.m ?m ?M
s
0
.c
t
&q
1
.m&q
2
.m ?m ?M
Essentially the model expresses interactions be-
tween morphological features from the constituent
heads on the top of the stack and the morphologi-
cal features from the tokens at the beginning of the
queue.
Mixed model (B+S+M) Our last model is the
union of the span model (B+S) and the morpho-
logical model (B+M).
Results (development) We measured the im-
pact of the model variations on the development
set for c-parsing on the SPMRL data sets (Table
2). We immediately observe that modelling spans
tends to improve the results, in particular for lan-
guages where the head annotation is more prob-
lematic: Arabic
4
, Basque, German and Hungar-
ian and also Swedish however. So the span-based
model seems to improve the parser’s robustness in
cases when dependencies lack precision. For this
model, the average behaviour is similar to that of
Hall et al. (2014) although the variance is high.
On the other hand, the morphological model
tends to be most important for languages where
head annotation is easier: French, Korean, Polish
and Swedish. It is key for very richly inflected lan-
guages such as Basque and Hungarian even though
our head annotation is more approximative
5
. A
4
Although not detailed in the paper, we also observe that
for Arabic, the morphological features are generally pre-
dicted with a lower accurracy by the tagger than for other
languages.
5
As annotation schemes are not normalized across lan-
guages, it is important to stress that these observations are
very unlikely to be representative of the linguistic properties
of these languages.They are more likely to be a result of an-
notation choices. For example Korean is a strongly aggluti-
native language for which much of the morphology is already
encoded in the tag set.
1852
Model Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
1 Base 79.46 74.67 79.66 82.61 90.43 84.34 81.96 91.68 75.60 82.26
2 Base+S 80.59 76.39 80.15 83.63 90.63 85.62 82.21 91.75 77.49 83.16
3 Base+M 80.17 83.69 81.05 83.66 90.40 87.75 82.79 92.72 77.50 84.41
4 Base+S+M 81.25 84.01 80.87 84.08 90.69 88.27 83.09 92.78 77.87 84.77
5 Hall-Klein 14 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30
F1-scores provided by evalb-spmrl (Seddah et al., 2013). Takes punctuation into account and penalizes unparsed sentences.
Table 2: Development F-scores
comparison with Hall et al. (2014) also reveals
that for Basque, Hungarian and Swedish, taking
into account morphological information largely
explains our improved results.
Results (test) We observe in Table 3 that our
joint B+S+M model yields a state of the art c-
parser on almost all languages considered
6
. It is
quite clear that both our span and morphology en-
hanced models could be dramatically improved,
but it shows that with reasonable feature engi-
neering, these two sub-models are largely suffi-
cient to improve the state of the art in c-parsing
for these languages over strong baselines. Al-
though in principle the Berkeley parsers (Petrov
et al., 2006; Hall et al., 2014) are designed to be
language-generic with an underlying design that
is surprisingly accurate for free word order lan-
guages end up suffering from a lack of sensitiv-
ity to morphological information. Finally we also
observe that our phrase structure parser clearly
outperforms the TurboParser setup described by
Fern´andez-Gonz´alez and Martins (2015) in which
an elaborate output conversion procedure gener-
ates c-parses from d-parses.
Comparison with related work We conclude
with a few comparisons with related work. This
will enable us to show that our approach is not
only accurate but also efficient. A comparison
with dependency parsers will also allow us to bet-
ter identify the properties of our proposal.
In order to test efficiency, we compared our
parser to c-parsers trained on Penn Treebank
(PTB) for which we have running times reported
6
For Basque, our problem comes from a recurrent incon-
sistency in the SPMRL data set. As annotated in the c-trees,
the punctuation induces a modification of the d-structure: c-
trees encode a different governor for punctuation marks than
d-trees. This not only causes problem to our head annotation
procedure but also for the parser to solving these attachments.
A simple correction results in a significant improvement of
these parsing results. However we decided to leave the data
untouched in order to preserve fair comparisons with other
systems.
by Fern´andez-Gonz´alez and Martins (2015). This
required first assigning heads, for which we used
the Stanford tool for converting PTB to Basic De-
pendencies, and then used our Robust conversion
method. We performed a simple test using the
PTB standard split with the same experimental set-
ting as before, except that we use the standard
evalb scorer (Table 5). Although the time com-
System (single parsers) F1 (EVALB) (Toks/sec)
Hall-Klein 14 88.6 12
StanfordSR 89.1 655
Charniak 00 89.5 -
This paper (B+S) 89.7 2150

This paper (B+S) [Collins] 90.0 2150

Petrov 06 90.1 169
Fernandez-Martins 15 90.2 957
Zhu et al. 13 90.4 1290
Alls scores and times except  are measured by Fern´andez-
Gonz´alez and Martins (2015) on an intel Xeon 2.3Ghz.  de-
notes the use of a different architecture (2.4Ghz intel).
Table 5: Penn treebank test (WSJ 23)
parison remains indicative, it is clear that the pars-
ing framework described in this paper is not only
reasonably accurate on a fixed word order lan-
guage such as English but it is also quite efficient.
Parsing accuracies might be different with other
head annotation schemes (See e.g. Elming et al.
(2013) for illustrations). In our case, we compare
the (B+S) model with automated head annotation
to the Collins head annotation as implemented in
the Standord CORE NLP library (Manning et al.,
2014), where we can see that the Collins hand-
crafted head annotation yields better results than
the automated one on English
7
.
The question is now to which extend c-trees en-
code meaningful dependencies? As lexicalized
c-trees encode unlabeled dependency trees, our
parser also directly outputs unlabeled d-trees by
7
This pattern does not seem to be systematic: on French
we could also compare with head annotations described in
(Arun and Keller, 2005) and we observed a slight improve-
ment when using the automated procedure.
1853
Parser (single) Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
Petrov 06 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.19 78.45
Petrov 06 + tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 81.17
Hall-Klein 14 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.72
Fernandez-Martins 15 - 85.90 78.75 78.66 88.97 88.16 79.28 91.20 82.80 84.22
This paper (B+S+M) 81.31 84.94 80.84 79.26 89.65 90.14 82.65 92.66 83.24 85.42
Best semi/ensemble 81.32 88.24 82.53 81.66 89.80 91.72 83.81 90.50 85.50 86.72
F-scores provided by evalb-spmrl (Seddah et al., 2013). It takes punctuation into account and penalizes unparsed sentences. The average ignores Arabic
for comparison with TurboParser. Petrov 06 + tags is the Berkeley parser with externally predicted pos tags (Seddah et al., 2013)
Table 3: Multilingual test (F-scores, phrase structure parsing)
System English French Korean Hebrew Polish Swedish Arabic Basque German Hungarian
This paper (B+S+M) 91.75 86.68 87.22 85.28 88.61 86.22 80.64 73.68 67.20 74.46
Best d-parser (single) 91.95 85.80 85.84 81.05 88.12 84.54 84.57 84.33 87.65 83.71
Best semi/ensemble - 89.19 89.10 87.41 91.75 88.48 88.32 89.96 91.64 89.81
Unlabeled Accuracy Scores. Best other is the best single parser UAS result reported either in SPMRL 13 or SPMRL 14 shared tasks.
Best ensemble is the best semi-supervised or ensemble system from either SPMRL 13 or SPMRL 14 (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014).
Table 4: Multilingual test (UAS, dependency parsing)
simply reading them off from the lexicalized c-
structure. We report in Table 4 the UAS evalua-
tion of those dependencies and we compare them
to the best results obtained by dependency parsers
in both SPMRL13 and SPMRL14 shared tasks.
For each language, the comparison is made with
the best single dependency parsing system
8
. For
English we compare against Standard TurboParser
- which seems to be the most similar to our
system- when parsing to Basic Stanford dependen-
cies. The comparison with semi-supervised and
ensemble parsers still provides a reasonable upper-
line (Bj¨orkelund et al., 2013).
As can be seen in Table 4, our results partly
generalize the observation summarized by Cer et
al. (2010) and Kong and Smith (2014) that phrase
structure parsers tend to provide better dependen-
cies than genuine dependency parsers for parsing
to Stanford Dependencies. For English, our UAS
is similar to that of TurboParser, but in a broader
multilingual framework, the left side of the table
shows that the unlabeled dependencies are clearly
better than those of genuine dependency parsers.
On the right side of the table are languages for
which our dependencies are actually worse. This
is not a surprise, since these are also the languages
for which head annotation was more problematic
in the first place. This last observation suggests
that a lexicalized c-parser can also provide very
accurate dependencies. A way to further gen-
8
In practice it turns out that these are either DYALOG-
SR (de La Clergerie, 2013) or sometimes MALTOPTIMIZER
(Ballesteros and Nivre, 2012)
eralize this observation to problematic languages
would be either to design a less immediate post-
processing conversion scheme or to further nor-
malize the data set to obtain the correct heads from
the outset.
6 Conclusion
Lexicalized phrase structure parsing of morpho-
logically rich languages used to be difficult since
existing implementations targeting essentially En-
glish or Chinese do not allow a straightforward
integration of morphology. Given multi-view
treebanks, we achieve multilingual parsing with
a language-agnostic head annotation procedure.
Once this procedure has created the required data
representation for lexicalized parsing, only mod-
est and weakly language dependent feature engi-
neering is required to achieve state-of-the-art ac-
curacies on all languages considered: a minimal
interface with morphology already contributes to
improving accuracy, and this is specifically the
case when heads are accurately identified. When
heads are only approximatively identified, span-
based configurational modelling tends to correct
the approximation.
Leaving aside details concerning conversion
and data normalization, we generally found that
the unlabeled dependencies modelled by the lex-
icalized c-parser also tend to be highly accu-
rate. For languages where c-annotations and d-
annotations are less compatible, additional lan-
guage renormalizations would help to get better
comparisons.
1854
As suggested in this paper, future work for pars-
ing morphologically rich languages will require to
focus both on feature selection and on the interface
between syntax and morphology, which means in
our case the interface between the segmenter, the
tagger and the parser.
Acknowledgments
The author wishes to thank Djam´e Seddah for in-
sigthful discussions regarding the work reported in
this paper as well as R. Bawden, M. Coavoux and
B. Sagot for their careful proofreading.
References
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Association for Computational Linguis-
tics.
Miguel Ballesteros and Joakim Nivre. 2012. Maltopti-
mizer: An optimization tool for maltparser. In 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL).
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009.
A multi-representational and multi-layered treebank
for hindi/urdu. In Proceedings of the Third Linguis-
tic Annotation Workshop (LAW III).
Anders Bj¨orkelund, Ozlem Cetinoglu, Rich´ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the SPMRL 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages.
Anders Bj¨orkelund,
¨
Ozlem Cetino?glu, Agnieszka
Fale´nska, Rich´ard Farkas, Thomas M¨uller, Wolf-
gang Seeker, and Zsolt Sz´ant´o. 2014. The ims-
wrocaw-szeged-cis entry at the spmrl 2014 shared
task: Reranking and morphosyntax meet unlabeled
data. In Fifth Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Daniel M. Cer, Marie-Catherine de Marneffe, Daniel
Jurafsky, and Christopher D. Manning. 2010. Pars-
ing to stanford dependencies: Trade-offs between
speed and accuracy. In Proceedings of the Language
Ressources and Evaluation Conference (LREC).
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In ANLP, pages 132–139.
Michael Collins, Jan Hajic, Lance A. Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In 27th Annual Meeting of the Association
for Computational Linguistics (ACL).
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637.
Benoit Crabb´e. 2014. An LR-inspired generalized
lexicalized phrase structure parser. In 25th Inter-
national Conference on Computational Linguistics
(COLING).
Eric Villemonte de La Clergerie. 2013. Exploring
beam-based shift-reduce dependency parsing with
dyalog: Results from the spmrl 2013 shared task.
In 4th Workshop on Statistical Parsing of Morpho-
logically Rich Languages (SPMRL2013).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Language Ressources and Evaluation
Conference (LREC).
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez, and Anders
Sogaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proceedings of
NAACL-HLT.
Daniel Fern´andez-Gonz´alez and Andr´e F. T. Martins.
2015. Parsing as reduction. In Proceeding of the As-
sociation of Computational Linguistics (ACL), 2015.
David Hall, Greg Durrett, and Dan Klein. 2014. Less
grammar, more features. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, pages 228–237.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Hu-
man Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics (NAACL-HLT).
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 478–485.
Lingpeng Kong and Noah A. Smith. 2014. An em-
pirical comparison of parsing methods for stanford
dependencies. CoRR, abs/1404.4314.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (ACL/COLING).
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations.
1855
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov,
and Jan Hajic. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP).
Ryan McDonald, J. Nivre, Y. Quirmbach-Brundage,
Y. Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,
H. Zhang, O. Tackstrom, C. Bedini, N. Bertomeu
Castello, and J. Lee. 2013. Universal dependency
annotation for multilingual parsing. In Proceed-
ing of the Association of Computational Linguistics
(ACL).
Haitao Mi and Liang Huang. 2015. Shift-reduce con-
stituency parsing with dynamic programming and
pos tag lattice. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT).
Thomas M¨uller, Helmut Schmid, and Hinrich Sch¨utze.
2013. Efficient higher-order crfs for morphological
tagging. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
2004, 20th International Conference on Computa-
tional Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, Proceedings of the Conference
(ACL/COLING).
Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.
2014. Multi-view chinese treebanking. In 25th In-
ternational Conference on Computational Linguis-
tics (COLING).
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL/COLING).
Djam´e Seddah, Reut Tsarfaty, Sandra Kubler, Marie
Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, Spence Green, Nizar Habash,
Marco Kuhlmann, Wolfgang Maier, Yuval Mar-
ton, Joakim Nivre, Adam Przepi´orkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Woliski, Alina Wr´oblewska, and
Eric Villemonte de la Clergerie. 2013. Overview
of the spmrl 2013 shared task: A cross-framework
evaluation of parsing morphologically rich lan-
guages. In Proceedings of the Fourth SPMRL Work-
shop, Seattle, USA.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for SVM. In Machine Learning, Proceed-
ings of the Twenty-Fourth International Conference
(ICML).
Veronika Vincze, Dora Szauter, Attila Almasi, Gy-
orgy Mora, Zoltan Alexin, and Janos Csirik. 2010.
Hungarian dependency treebank. In Proceedings of
Language Ressources and Evalutation Conference
(LREC).
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Human Language Technologies: Conference
of the North American Chapter of the Association of
Computational Linguistics (NAACL-HLT).
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, (ACL).
1856
