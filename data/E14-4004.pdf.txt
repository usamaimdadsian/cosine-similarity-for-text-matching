Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17–21,
Gothenburg, Sweden, April 26-30 2014.
c©2014 Association for Computational Linguistics
Temporal Text Ranking and Automatic Dating of Texts
Vlad Niculae
1
, Marcos Zampieri
2
, Liviu P. Dinu
3
, Alina Maria Ciobanu
3
Max Planck Institute for Software Systems, Germany
1
Saarland University, Germany
2
Center for Computational Linguistics, University of Bucharest, Romania
3
vniculae@mpi-sws.org, marcos.zampieri@uni-saarland.de,
ldinu@fmi.unibuc.ro, alina.ciobanu@my.fmi.unibuc.ro
Abstract
This paper presents a novel approach to
the task of temporal text classification
combining text ranking and probability for
the automatic dating of historical texts.
The method was applied to three histor-
ical corpora: an English, a Portuguese
and a Romanian corpus. It obtained per-
formance ranging from 83% to 93% ac-
curacy, using a fully automated approach
with very basic features.
1 Introduction
Temporal text classification is an underexplored
problem in NLP, which has been tackled as a
multi-class problem, with classes defined as time
intervals such as months, years, decades or cen-
turies. This approach has the drawback of having
to arbitrarily delimit the intervals, and often leads
to a model that is not informative for texts written
within such a window. If the predefined window is
too large, the output is not useful for most systems;
if the window is too small, learning is impractical
because of the large number of classes. Particu-
larly for the problem of historical datasets (as the
one we propose here), learning a year-level classi-
fier would not work, because each class would be
represented by a single document.
Our paper explores a solution to this drawback
by using a ranking approach. Ranking amounts to
ordering a set of inputs with respect to some mea-
sure. For example, a search engine ranks returned
documents by relevance. We use a formalization
of ranking that comes from ordinal regression, the
class of problems where samples belong to inher-
ently ordered classes.
This study is of interest to scholars who deal
with text classification and NLP in general; his-
torical linguists and philologists who investigate
language change; and finally scholars in the dig-
ital humanities who often deal with historical
manuscripts and might take advantage of temporal
text classification applications in their research.
2 Related Work
Modelling temporal information in text is a rele-
vant task for a number of NLP tasks. For example,
in Information Retrieval (IR) research has been
concentrated on investigating time-sensitivity doc-
ument ranking (Dakka and Gravana, 2010). Even
so, as stated before, temporal text classification
methods were not substantially explored as other
text classification tasks.
One of the first studies to model temporal infor-
mation for the automatic dating of documents is
the work of de Jong et al. (2005). In these exper-
iments, authors used unigram language models to
classify Dutch texts spanning from January 1999
to February 2005 using normalised log-likelihood
ratio (NLLR) (Kraaij, 2004). As to the features
used, a number of approaches proposed to auto-
matic date take into account lexical features (Dalli
and Wilks, 2006; Abe and Tsumoto, 2010; Ku-
mar et al., 2011) and a few use external linguistic
knowledge (Kanhabua and Nørv?ag, 2009).
A couple of approaches try to classify texts not
only regarding the time span in which the texts
were written, but also their geographical location
such as (Mokhov, 2010) for French and, more re-
cently, (Trieschnigg et al., 2012) for Dutch. At the
word level, two studies aim to model and under-
stand how word usage and meaning change over
time (Wijaya and Yeniterzi, 2011), (Mihalcea and
Nastase, 2012).
The most recent studies in temporal text classifi-
cation to our knowledge are (Ciobanu et al., 2013)
for Romanian using lexical features and (
?
Stajner
and Zampieri, 2013) for Portuguese using stylistic
and readability features.
17
3 Methods
3.1 Corpora
To evaluate the method proposed here we used
three historical corpora. An English historical
corpus entitled Corpus of Late Modern English
Texts (CLMET)
1
(de Smet, 2005), a Portuguese
historical corpus entitled Colonia
2
(Zampieri and
Becker, 2013) and a Romanian historical corpus
(Ciobanu et al., 2013).
CLMET is a collection of English texts derived
from the Project Gutenberg and from the Oxford
Text Archive. It contains around 10 million to-
kens, divided over three sub-periods of 70 years.
The corpus is available for download as raw text
or annotated with POS annotation.
For Portuguese, the aforementioned Colonia
(Zampieri and Becker, 2013) is a diachronic col-
lection containing a total of 5.1 million tokens and
100 texts ranging from the 16
th
to the early 20
th
century. The texts in Colonia are balanced be-
tween European and Brazilian Portuguese (it con-
tains 52 Brazilian texts and 48 European texts) and
the corpus is annotated with lemma and POS in-
formation. According to the authors, some texts
presented edited orthography prior to their com-
pilation but systematic spelling normalisation was
not carried out.
The Romanian corpus was compiled to portrait
different stages in the evolution of the Romanian
language, from the 16
th
to the 20
th
century in a
total of 26 complete texts. The methodology be-
hind corpus compilation and the date assignment
are described in (Ciobanu et al., 2013).
3.2 Temporal classification as ranking
We propose a temporal model that learns a linear
function g(x) = w · x to preserve the temporal or-
dering of the texts, i.e. if document
3
x
i
predates
document x
j
, which we will henceforth denote as
x
i
? x
j
, then g(x
i
) < g(x
j
). Such a problem is
often called ranking or learning to rank. When the
goal is to recover contiguous intervals that corre-
spond to ordered classes, the problem is known as
ordinal regression.
We use a pairwise approach to ranking that re-
duces the problem to binary classification using a
1
https://perswww.kuleuven.be/
˜
u0044428/clmet
2
http://corporavm.uni-koeln.de/
colonia/
3
For brevity, we use x
i
to denote both the document itself
and its representation as a feature vector.
linear model. The method is to convert a dataset
of the form D = {(x, y) : x ? R
d
, y ? Y} into a
pairwise dataset:
D
p
= {((x
i
, x
j
), I[y
i
< y
j
]) :
(x
i
, y
i
), (x
j
, y
j
) ? D}
Since the ordinal classes only induce a partial or-
dering, as elements from the same class are not
comparable, D
p
will only consist of the compara-
ble pairs.
The problem can be turned into a linear classifi-
cation problem by noting that:
w · x
i
< w · x
j
?? w · (x
i
? x
j
) < 0
In order to obtain probability values for the or-
dering, we use logistic regression as the linear
model. It therefore holds that:
P(x
i
? x
j
;w) =
1
1 + exp(?w · (x
i
? x
j
))
While logistic regression usually fits an inter-
cept term, in our case, because the samples consist
of differences of points, the model operates in an
affine space and therefore gains an extra effective
degree of freedom. The intercept is therefore not
needed.
The relationship between pairwise ranking and
predicting the class from an ordered set {r
1
, ...r
k
}
is given by assigning to a document x the class r
i
such that
?(r
i?1
) ? g(x) < ?(r
i
) (1)
where ? is an increasing function that does not
need to be linear. (Pedregosa et al., 2012), who
used the pairwise approach to ordinal regression
on neuroimaging prediction tasks, showed using
artificial data that ? can be accurately recovered
using non-parametric regression. In this work, we
use a parametric estimation of ? that can be used
in a probabilistic interpretation to identify the most
likely period when a text was written, as described
in section 3.3.
3.3 Probabilistic dating of uncertain texts
The ranking model described in the previous sec-
tion learns a direction along which the temporal
order of texts is preserved as much as possible.
This direction is connected to the chronological
axis through the ? function. For the years t for
18
which we have an unique attested document x
t
,
we have that
x ? x
t
?? g(x) < g(x
t
) < ?(t)
This can be explained by seeing that equation 2
gives ?(t) as an upper bound for the projections of
all texts written in year t, and by transitivity for all
previous texts as well.
Assuming we can estimate the function ? with
another function
ˆ
?, the cumulative densitiy func-
tion of the distribution of the time when an unseen
document was written can be expressed.
P (x ? t) ?
1
1 + exp(w · x?
ˆ
?(t))
(2)
Setting the probability to
1
2
provides a point es-
timate of the time when x was written, and confi-
dence intervals can be found by setting it to p and
1? p.
3.4 Features
Our ranking and estimation model can work with
any kind of numerical features. For simplicity
we used lexical and naive morphological features,
pruned using ?
2
feature selection with tunable
granularity.
The lexical features are occurrence counts of all
words that appear in at least p
lex
documents. The
morphological features are counts of character n-
grams of length up to w
mph
in final positions of
words, filtered to occur in at least n
mph
documents.
Subsequently, a non-linear transformation ? is
optionally applied to the numerical features. This
is one of ?
sqrt
(z) =
?
z, ?
log
(z) = log(z) or
?
id
(z) = z (no transformation).
The feature selection step is applied before gen-
erating the pairs for classification, in order for the
?
2
scoring to be applicable. The raw target val-
ues used are year labels, but to avoid separating
almost every document in its own class, we in-
troduce a granularity level that transforms the la-
bels into groups of n
gran
years. For example, if
n
gran
= 10 then the features will be scored ac-
cording to how well they predict the decade a doc-
ument was written in. The features in the top p
fsel
percentile are kept. Finally, C is the regulariza-
tion parameter of the logistic regression classifier,
as defined in liblinear (Fan et al., 2008).
0.2 0.4 0.6 0.8 1.00.72
0.74
0.76
0.78
0.80
0.82
0.84
RidgeRanking
0.6 0.7 0.8 0.9 1.00.78
0.79
0.80
0.81
0.82
0.83
RidgeRanking
Figure 1: Learning curves for English (top) and
Portuguese (bottom). Proportion of training set
used versus score.
4 Results
Each corpus is split randomly into training and test
sets with equal number of documents. The best
feature set is chosen by 3-fold cross-validated ran-
dom search over a large grid of possible configu-
rations. We use random search to allow for a more
efficient exploration of the parameter space, given
that some parameters have much less impact to the
final score than others.
The evaluation metric we used is the percentage
of non-inverted (correctly ordered) pairs, follow-
ing (Pedregosa et al., 2012).
We compare the pairwise logistic approach to
a ridge regression on the same feature set, and
two multiclass SVMs, at century and decade level.
While the results are comparable with a slight ad-
vantage in favour of ranking, the pairwise ranking
system has several advantages. On the one hand, it
provides the probabilistic interpretation described
in section 3.3. On the other hand, the model can
naturally handle noisy, uncertain or wide-range la-
bels, because annotating whether a text was writ-
ten before another can be done even when the texts
do not correspond to punctual moments in time.
While we do not exploit this advantage, it can lead
to more robust models of temporal evolution. The
learning curves in Figure 1 further show that the
pairwise approach can better exploit more data and
nonlinearity.
The implementation is based on the scikit-learn
machine learning library for Python (Pedregosa et
al., 2011) with logistic regression solver from (Fan
et al., 2008). The source code will be available.
4.1 Uncertain texts
We present an example of using the method from
Section 3.3 to estimate the date of uncertain, held-
out texts of historical interest. Figure 2 shows the
process used for estimating ? as a linear, and in
the case of Portuguese, quadratic function. The
19
size p
lex
n
mph
w
mph
? n
gran
p
fsel
C score ridge century decade MAE
en 293 0.9 0 3 ?
log
100 0.15 2
9
0.838 0.837 0.751 0.813 22.8
pt 87 0.9 25 4 ?
sqrt
5 0.25 2
?5
0.829 0.819 0.712 0.620 58.7
ro 42 0.8 0 4 ?
log
5 0.10 2
28
0.929 0.924 0.855 0.792 28.8
Table 1: Test results of the system on the three datasets. The score is the proportion of pairs of docu-
ments ranked correctly. The column ridge is a linear regression model used for ranking, while century
and decade are linear SVMs used to predict the century and the decade of each text, but scored as pair-
wise ranking, for comparability. Chance level is 0.5. MAE is the mean absolute error in years. The
hyperparameters are described in section 3.4.
1650 1700 1750 1800 1850 1900 1950Year
300
200
100
0
100
200
w·x
Linear (33.54)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
40
20
0
20
40
60
w·x
Linear (17.27)Quadratic (15.44)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
100
50
0
50
100
w·x
Linear (1.87)TrainTest
Figure 2: Estimating the function ? that defines the relationship between years and projections of docu-
ments to the direction of the model, for English, Portuguese and Romanian (left to right). In parantheses,
the normalized residual of the least squares fit is reported on the test set.
154
0
156
0
158
0
160
0
162
0
164
0
166
0
168
0
170
0
172
0
174
0
176
0
178
0
180
0
182
0
184
0
186
0
188
0
190
0
192
0
194
0
196
0
198
0
200
0
202
00.2
0.00.2
0.40.6
0.81.0
1.2
Figure 3: Visualisation of the probability esti-
mation for the dating of C. Cantacuzino’s Isto-
ria T
,
?arii Rumˆanes
,
ti. The horizontal axis is the
time, the points are known texts with a height
equal to the probability predicted by the classifier.
The dashed line is the estimated probability from
Equation 2.
estimation is refit on all certain documents prior to
plugging into the probability estimation.
The document we use to demonstrate the pro-
cess is Romanian nobleman and historian Con-
stantin Cantacuzino’s Istoria T
,
?arii Rumˆanes
,
ti.
The work is believed to be written in 1716, the
year of the author’s death, and published in sev-
eral editions over a century later (Stahl, 2001).
This is an example of the system being reasonably
close to the hypothesis, thus providing linguistic
support to it. Our system gives an estimated dat-
ing of 1744.7 with a 90% confidence interval of
1736.2 ? 1753.2. As publications were signifi-
cantly later, the lexical pull towards the end of 18
th
century that can be observed in Figure 3 could be
driven by possible editing of the original text.
5 Conclusion
We propose a ranking approach to temporal mod-
elling of historical texts. We show how the model
can be used to produce reasonable probabilistic
estimates of the linguistic age of a text, using a
very basic, fully-automatic feature extraction step
and no linguistic or historical knowledge injected,
apart from the labels, which are possibly noisy.
Label noise can be atenuated by replacing un-
certain dates with intervals that are more certain,
and only generating training pairs out of non-
overlapping intervals. This can lead to a more
robust model and can use more data than would
be possible with a regression or classification ap-
proach. The problem of potential edits that a text
has suffered still remains open.
Finally, better engineered and linguistically-
motivated features, such as syntactic, morphologi-
cal or phonetic patterns that are known or believed
to mark epochs in the evolution of a language, can
be plugged in with no change to the fundamental
method.
20
References
H. Abe and S. Tsumoto. 2010. Text categorization
with considering temporal patterns of term usages.
In Proceedings of ICDM Workshops, pages 800–
807. IEEE.
A. Ciobanu, A. Dinu, L. Dinu, V. Niculae, and
O. Sulea. 2013. Temporal text classification for
romanian novels set in the past. In Proceedings of
RANLP2013, Hissar, Bulgaria.
W. Dakka and C. Gravana. 2010. Answering gen-
eral time-sensitive queries. IEEE Transactions on
Knowledge and Data Engineering.
A. Dalli and Y. Wilks. 2006. Automatic dating of doc-
uments and temporal text classification. In Proceed-
ings of the Workshop on Annotating and Reasoning
about Time and Events, pages 17–22, Sidney, Aus-
tralia.
F. de Jong, H. Rode, and D. Hiemstra. 2005. Temporal
language models for the disclosure of historical text.
In Proceedings of AHC 2005 (History and Comput-
ing).
H. de Smet. 2005. A corpus of late modern english.
ICAME-Journal.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
N. Kanhabua and P. Nørv?ag. 2009. Using tem-
poral language models for document dating. In
ECML/PKDD, pages 738–741.
W. Kraaij. 2004. Variations on language modeling
for information retrieval. Ph.D. thesis, University
of Twente.
A. Kumar, M. Lease, and J. Baldridge. 2011. Super-
vised language modelling for temporal resolution of
texts. In Proceedings of CIKM11 of the 20th ACM
international conference on Information and knowl-
edge management, pages 2069–2072.
R. Mihalcea and V. Nastase. 2012. Word epoch dis-
ambiguation: Finding how words change over time.
In Proceedings of ACL, pages 259–263. Association
for Computational Linguistics.
S. Mokhov. 2010. A marf approach to deft2010. In
Proceedings of TALN2010, Montreal, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Fabian Pedregosa, Alexandre Gramfort, Ga¨el Varo-
quaux, Elodie Cauvet, Christophe Pallier, and
Bertrand Thirion. 2012. Learning to rank from
medical imaging data. CoRR, abs/1207.3598.
H.H. Stahl. 2001. Gˆanditori s¸i curente de istorie
social?a romˆaneasc?a. Biblioteca Institutului Social
Romˆan. Ed. Univ. din Bucures¸ti.
S.
?
Stajner and M. Zampieri. 2013. Stylistic changes
for temporal text classification. In Proceedings of
the 16th International Conference on Text Speech
and Dialogue (TSD2013), Lecture Notes in Artificial
Intelligence (LNAI), pages 519–526, Pilsen, Czech
Republic. Springer.
D. Trieschnigg, D. Hiemstra, M. Theune, F. de Jong,
and T. Meder. 2012. An exploration of lan-
guage identification techniques for the dutch folktale
database. In Proceedings of LREC2012.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In Proc. of
the Workshop on Detecting and Exploiting Cultural
Diversity on the Social Web (DETECT).
M. Zampieri and M. Becker. 2013. Colonia: Corpus of
historical portuguese. ZSM Studien, Special Volume
on Non-Standard Data Sources in Corpus-Based Re-
search, 5.
21
