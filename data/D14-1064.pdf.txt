Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599,
October 25-29, 2014, Doha, Qatar.
c
©2014 Association for Computational Linguistics
Learning to Translate: A Query-Specific Combination Approach for
Cross-Lingual Information Retrieval
Ferhan Ture
Raytheon BBN Technologies
10 Moulton St
Cambridge, MA, 02138 USA
fture@bbn.com
Elizabeth Boschee
Raytheon BBN Technologies
10 Moulton St
Cambridge, MA, 02138 USA
eboschee@bbn.com
Abstract
When documents and queries are pre-
sented in different languages, the com-
mon approach is to translate the query into
the document language. While there are
a variety of query translation approaches,
recent research suggests that combining
multiple methods into a single ”structured
query” is the most effective. In this pa-
per, we introduce a novel approach for
producing a unique combination recipe for
each query, as it has also been shown
that the optimal combination weights dif-
fer substantially across queries and other
task specifics. Our query-specific combi-
nation method generates statistically sig-
nificant improvements over other combi-
nation strategies presented in the litera-
ture, such as uniform and task-specific
weighting. An in-depth empirical anal-
ysis presents insights about the effect of
data size, domain differences, labeling and
tuning on the end performance of our ap-
proach.
1 Introduction
Cross-lingual information retrieval (CLIR) is a
special case of information retrieval (IR) in which
documents and queries are presented in different
languages. In order to overcome the language
barrier, the most commonly adopted method is
to translate queries into the document language.
Many methods have been introduced for translat-
ing queries for CLIR, ranging from word-by-word
dictionary lookups (Xu and Weischedel, 2005;
Darwish and Oard, 2003) to sophisticated use of
machine translation (MT) systems (Magdy and
Jones, 2011; Ma et al., 2012). Previous research
has shown that combining evidence from differ-
ent translation approaches is superior to any sin-
gle query translation method (Braschler, 2004;
Herbert et al., 2011). While there are numer-
ous combination-of-evidence techniques for both
mono-lingual and cross-lingual IR, recent work
suggests that there is no one-size-fits-all solution.
In fact, the optimal combination weights (i.e.,
weights assigned to each piece of evidence in a
linear combination) differ greatly across queries,
tasks, languages, and other variants (Ture et al.,
2012; Berger and Savoy, 2007).
In this paper, we introduce a novel method for
learning optimal combination weights when build-
ing a linear combination of existing query transla-
tion approaches. From standard query-document
relevance judgments we train a set of classifiers,
which produce a unique combination recipe for
each query, based on a large set of features ex-
tracted from the query and collection. Experi-
mental results show that the effectiveness of our
method is significantly higher than state-of-the-art
query translation methods and other combination
strategies.
2 Related Work
The earliest approaches to query translation for
CLIR used machine-readable bilingual dictio-
naries (Hull and Grefenstette, 1996; Balles-
teros and Croft, 1996), achieving around up to
60% of monolingual IR effectiveness. Xu and
Weischedel (2005) showed that effectiveness can
be increased to around 80% by weighting each
translation proportional to its rank in the dictio-
nary. The practice of weighting translation candi-
dates was later formulated as a “structured query”,
in which each query term is represented by a prob-
ability distribution over its translations in the doc-
ument language (Pirkola, 1998; Kwok, 1999; Dar-
wish and Oard, 2003). Our approach is based on
the structured query formulation.
Some of the earliest studies in IR discovered
that with different underlying models, the re-
trieved document set would vary substantially, al-
589
though the effectiveness was similar (McGill et
al., 1979). Later studies showed that combining
different representations of the query and/or doc-
ument often produced superior output (Rajashekar
and Croft, 1995; Turtle and Croft, 1990; Fox,
1983). This intuitive idea was supported theoret-
ically by Pearl (1988), concluding that multiple
pieces of evidence estimates relevance more accu-
rately, but that the benefit strongly depends on the
quality and independence of each piece. Experi-
ments by Belkin et al. (1995) indicated the need to
properly weight each representation with respect
to its effectiveness. These so-called “combination-
of-evidence” techniques became more powerful
with the introduction of Indri, a probabilistic re-
trieval framework specifically designed for com-
bining multiple query and document representa-
tions (Metzler and Croft, 2005). Croft (2000) pro-
vides a detailed summary of earlier query combi-
nation approaches in IR, while Peters et al. (2012)
cites more recent related work.
The benefits of combination-of-evidence trans-
fer to the cross-lingual case especially well, since
the inherent ambiguity of translation readily pro-
vides a diverse set of representations. Most CLIR
approaches implement a post-retrieval merging
of ranked lists, each generated from different
query (Hiemstra et al., 2001; Savoy, 2001; Gey
et al., 2001; Chen and Gey, 2004) or docu-
ment (Lopez and Romary, 2009) representations,
also called “data fusion”. In contrast, we focus on
a pre-retrieval combination at the modeling stage,
so that a single complex query is used in retrieval,
instead of multiple simpler ones. Two advantages
of the former are easier implementation (since
the approach requires no changes to the modeling
side) and the possibly greater diversity that can be
achieved by having separate retrieval runs. How-
ever, each ranked list needs to be limited in size,
which might cause some potentially useful docu-
ments not to be considered in the combination at
all. Since the focus of this paper is on the model-
ing end of retrieval, pre-retrieval combination was
a more suitable choice, though we think that the
two approaches have complementary benefits.
The idea of combining query translations
before retrieval has been explored previously.
Braschler (2004) combines three translation ap-
proaches: output of an MT system, a novel trans-
lation approach based on a similarity thesaurus
built automatically from a comparable corpus,
and a dictionary-based translation. The main
reason that this combination does not provide
much benefit is due to the lower coverage of
the thesaurus-based and dictionary-based trans-
lation methods. A similar approach by Herbert
et al. (2011) uses Wikipedia to provide transla-
tions of certain phrases and entities, and combin-
ing that with the Google Translate MT sys-
tem yields statistically significant improvements
in English-to-German retrieval. More recently,
Ture et al. (2012) presented a more sophisti-
cated translation approach using the internal rep-
resentation of an MT system, and reported sta-
tistically significant improvements when a pre-
retrieval combination was performed.
All of the previously cited approaches either
use uniform weights for combination, or select
weights based on collection-level information.
However, as stated previously, numerous stud-
ies suggest that certain methods work better on
certain queries, collections, languages. In fact,
when weights are optimized separately on each
collection, they differ substantially across differ-
ent collections (Ture et al., 2012). For monolin-
gual retrieval, there has been a series of learning-
to-rank (LTR) papers that determine weights for
query concepts (Bendersky et al., 2011), such
that retrieval effectiveness is maximized. A re-
cent study extends this idea to the cross-lingual
case, by learning how to weight each translated
word for English-Persian CLIR (Azarbonyad et
al., 2013). In contrast, we extract translated word
weights from diverse and sophisticated translation
methods, then learn how to weight each trans-
lated structured query, We call this “learning-to-
translate” (LTT), which can be formulated as a
simpler learning problem. In CLIR, both LTR and
LTT are under-explored problems, with a common
goal of applying machine learning techniques to
improve query translation, yet with complemen-
tary benefits.
To our knowledge, there has been one prior LTT
approach: a classifier was trained to predict ef-
fectiveness of each query translation, using fea-
tures based on statistics of the query terms (Berger
and Savoy, 2007). Instead of weighting, the
translations with highest classifier scores were
concatenated, yielding statistically significant im-
provements over using the single-best translation
method. However, the translation methods ex-
plored in this paper are all based on one-best MT
590
systems, making it difficult to draw strong conclu-
sions.
3 Query Translation
The primary contribution of this paper is to show
how a diverse set of query translation (QT) meth-
ods can be combined effectively into a single
weighted structured query, with improved retrieval
effectiveness. While our approach can applied to
any set of translation methods, we focus on three
methods that have complementary strengths and
that have shown promise in CLIR: word-based
probabilistic translation, one-best MT, and n-best
probabilistic MT. We briefly present our imple-
mentation of each method; more details can be
found in earlier work (Darwish and Oard, 2003;
Ture et al., 2012).
Each QT method generates a representation of
the query in the document language. In the case of
word-based and n-best MT approaches, the repre-
sentation is a structured query itself, where each
query word is represented by a probability distri-
bution over translation alternatives. For one-best
MT, the query is represented by a bag of translated
words.
3.1 One-Best MT
A query translation approach that has become
more popular recently is to simply run the query
through an MT system, and use the best output as
the query:
t
1
t
2
. . . t
l
= MT(s
1
s
2
. . . s
k
) (1)
where s = s
1
s
2
. . . s
k
is the query and t =
t
1
t
2
. . . t
l
is the translated query.
Since modern statistical MT systems generate
high-quality translations for many language pairs,
this one-best strategy works reasonably well for
retrieval and provides a competitive baseline. A
practical advantage of this approach is the ease of
implementation – one can simply use any MT in-
terface (e.g., Google Translate) as a black
box in their CLIR system.
3.2 Probabilistic n-best MT
The top translation might sometimes be incorrect,
or might lack some of the alternative representa-
tions that are very useful in retrieval. Therefore,
considering the n highest scored translations (also
referred to as the n-best list in MT literature) has
become increasingly popular in CLIR approaches.
In order to benefit from the diversity amongst
the n-best translations, one can simply concate-
nate them together, forming a large list of query
terms. However, statistical MT systems also
assign probabilities to each translation, which
can be incorporated into the query representation
for better effectiveness, as suggested by Ture et
al. (2012).
In this approach, each of the top n transla-
tion candidates from the MT system are processed
one by one. For each translation candidate, the
MT system provides a translation probability, and
alignments between words in the query and its
translation. As we process each of the n transla-
tions, for each query word s
i
, we accumulate prob-
abilities on each translated word t
ij
aligned to s
i
.
Finally, we normalize the translation probabilities
to get Pr
nbest
(t
ij
|s
i
).
3.3 Word-based
One of the most widely used approaches in CLIR
is based on translating each query word s
i
in-
dependently, with probabilities assigned to each
translation candidate t
ij
. Translations are de-
rived automatically from a bilingual corpus using
statistical word alignment techniques, which are
used as part of the training of statistical MT sys-
tems (Brown et al., 1993). These probabilities can
be exploited for retrieval based on the technique
of Darwish and Oard (2003) for “projecting” text
into the document language. After cleaning up the
automatically learned translation probabilities (de-
tails omitted for space considerations), we end up
with the translation probabilities Pr
word
(t
ij
|s
i
).
4 Combination of Evidence
Once we have multiple ways to represent the query
q in the document language (QT
i
(q), i = 1 . . .m),
it is possible to combine these “pieces of evi-
dence” into a single representation as follows:
QT(q) =
m
?
i=1
w
i
(q)QT
i
(q)
and each combination-of-evidence approach dif-
fers by how the combination weights w
i
are com-
puted:
Uniform In this baseline method, we ignore any
information we have about the collection or query
and assign equal weights to each method (i.e.,
w
i
(q) = 1/m). In our case, this means a weight
591
of 33.3% to each of the one-best, probabilistic n-
best, and word-based QT methods.
Task-specific We can optimize the combination
weights by overall effectiveness on a specific re-
trieval task. Given a query set and collection,
we perform a grid search on combination weights
(with a step interval of 0.1) and select the weights
that maximize retrieval effectiveness. The training
is performed in a leave-one-out manner: weights
for test query q are optimized on all queries except
for q.
Query-specific We propose a novel method to
compute combination weights specifically for
each query, resulting in a more customized op-
timization that can take into account how effec-
tiveness of each translation method varies across
queries.
In the remainder of this section, we describe
the details of our novel query-specific combina-
tion method.
4.1 Overview of Query-Specific Combination
We present a novel approach for determining
query-specific combination weights by training a
classifier for each QT method. Prior to train-
ing the classifier, we first run retrieval using each
QT method, and evaluate the effectiveness of the
retrieved documents. The effectiveness of the
i
th
method on query q (i.e., f
i
(q)) is then con-
verted into a binary label (further described in
Section 4.2). Treating each query as a separate
instance, a classifier is trained for each method,
generating classifiers C
1
, . . . , C
m
. During re-
trieval (i.e., at test time), for each query q, each
trained classifier C
i
is applied to the query, re-
sulting in a predicted label l
i
(q) and the classi-
fier’s confidence in a positive label, C
i
(q).
1
These
values are then used to determine combination
weights w
1
(q), . . . , w
m
(q) that are custom-fit for
the query.
4.2 Labeling
First of all, we discard queries in which the dif-
ference between the best and worst performing
methods is small (specifically, the worst perform-
ing method scores at least k
1
% of the best per-
forming one). For such queries, generating fair
training labels is more difficult and therefore more
1
The confidence in a negative label is 1? C
i
(q).
likely to introduce noise into the process.
2
More-
over, these are exactly the queries where choos-
ing optimal combination weights is less important
(since all methods perform relatively similarly), so
it is reasonable to exclude them from training. In
fact, a high number of such queries would indi-
cate lower potential for combination-of-evidence
approaches.
For each QT method i, we create training in-
stances per query, per retrieval task. Since our
goal is to select the best among existing methods,
the training label should reflect the effectiveness
of method i relative to other methods. A strategy
that we call best-by-measure assigns a label of 1
if the effectiveness of the i
th
method (i.e., f
i
(q)) is
at least k
2
% of the maximum effectiveness for that
query, and 0 otherwise. While this directly corre-
lates with retrieval effectiveness, labels might be
distributed in an unbalanced manner, which might
affect the training process negatively. A balanced
labeling requires sorting all training instances by
how much better the i
th
method is than other meth-
ods (max
i
?
6=i
(f
i
?
(q)/f
i
(q))), and then assigning a
label of 1 to the lower half and 0 to the higher half.
This strategy is called best-by-rank.
4.3 Features
We introduce a diverse set of features, in order to
train a robust classifier for predicting when each
QT method performs better and worse than others.
We split the feature set into four meaningful cate-
gories, so that we can measure the impact of each
subset separately:
Surface features These features do not require
a deep analysis of the query: (a) Number of words
in query and the translated query, (b) Type of
query that we automatically classify based on pre-
defined templates (e.g., fact question, cause-effect,
etc.), and (c) Number of stop words in the query
and the translated query.
Parse-based features These features are ex-
tracted from a deeper syntactic analysis of the
query text: (a) Number of related names found in
a named entity database, and (b) Existence of syn-
tactic constituents in query and its translation (e.g.,
“is there a VVB in the query parse tree”).
2
We also experimented with including these queries with
a third label (e.g., “same”) and train a ternary classifier. Hav-
ing more labels requires more training data, which is not easy
to obtain for this task. Also, obtaining a balanced label dis-
tribution becomes even more difficult with three labels.
592
Translation-based features These features
consist of statistics computed from the query and
its translation: (a) Number of query words that
were unaligned in at least half of the n-best query
translations, (b) Number of query words that were
aligned to multiple target words in at least half
of the n-best query translations, (c) Number of
query words that were self-aligned (i.e., target
word is exactly same string) in at least half of the
n-best query translations, (d) Average / Standard
deviation / Maximum / Minimum of entropy of
Pr
nbest
of each query word, and (e) Average /
Standard deviation / Maximum / Minimum of
entropy of Pr
word
of each query word.
Index-based features These features are based
on frequency statistics from a representative col-
lection:
3
(a) Average / Standard deviation / Max-
imum / Minimum of document frequency (df) of
query words and their translations, (b) Average /
Standard deviation / Maximum / Minimum of term
frequency averaged across query words and their
translations, and (c) Sum / Maximum / Minimum
of total probability assigned to words that do not
appear in the collection (df = 0).
Additionally, the target language is a default
feature in all of our experiments. For each clas-
sification task, we train a separate classifier on
each subset of these four feature categories, so that
there are 16 different sets (including the empty
set). After we select which categories to pull fea-
tures from, we optionally perform feature selec-
tion to reduce the number of features by a pre-
defined percentage.
In our experimentation, we observed that
collection-based features were most useful for
classifying the one-best method, whereas parse-
based features were most discriminative for prob-
abilistic 10-best. For the word-based QT method,
the translation-based features were most effective
in our experiments. We further analyze the effect
of various features in Section 5.
4.4 Training and Tuning Classifiers
The scikit-learn package was used for the
training pipeline (Pedregosa et al., 2012). Using
an established toolkit allowed us to experiment
with many options for classification, such as the
learner type (support vector machine, maximum
entropy, decision tree), feature set (16 subsets of
3
We used the BOLT collection in our experiments.
the four categories described earlier) and two fea-
ture selection methods (recursive elimination or
selection based on univariate statistical tests). In
the end, we get 96 different parameter combina-
tions while training a classifier for a particular QT
method, resulting in the need for tuning — picking
the parameters that produce highest accuracy on a
representative tuning set.
Given that we have a set of queries for testing
purposes, there are few strategies for selecting a
training and tuning set. One approach is to apply a
leave-one-out strategy, so that a classifier is trained
and tuned on all but one of the test queries, and
then applied on the remaining query to predict its
label. We call this the fully-open setting.
In a more realistic scenario, there will not be
relevance judgments for the test queries, yet there
might be a small amount of labeled data similar to
the test task (e.g., different queries on same col-
lection) that can be utilized for tuning purposes,
and a larger set of training queries from different
collections. We call this the half-blind setting.
If testing in a new domain, queries of similar
type are not available for training and tuning pur-
poses. This is a more challenging scenario than
the previous two, yet it is important for real-world
applications. In order to demonstrate the effec-
tiveness of the training pipeline in this case, we
hold out test queries entirely, then train and tune
on queries from a completely different task (i.e.,
different queries and collection). We call this the
fully-blind setting.
4.5 Retrieval
Once we have classifiers trained for all QT meth-
ods, we can apply them to a given query on-the-fly,
and compute query-specific combination weights.
One approach is hard weighting, putting all weight
onto a single method — when there are more than
one methods classified with label 1, we can ei-
ther pick one randomly or use the classifier con-
fidence value as a tie-breaker. An alternative is
soft weighting, where the weight of the i
th
method
can be computed either using classifier confidence
C
i
(i.e., how confident the model is that the i
th
method will perform well), precision on tuning set
precision
i
(i.e., how precise the model is at its pre-
593
dictions for the i
th
method), or both:
w
s1
i
(q) =C
i
(q)
w
s2
i
(q) =precision
i
(1)× l
i
(q)
+(1?precision
i
(0))× (1? l
i
(q))
w
s3
i
(q) =precision
i
(1)× C
i
(q)
+(1?precision
i
(0))× (1? C
i
(q))
The intuition behind all of these weighting
schemes is to produce a weight for each QT
method, by taking into account the confidence of
the classifier, and/or the precision of the classifier
on tuning instances.
The computed weights are normalized before
constructing the final query for retrieval:
w
final
i
(q) = w
i
(q)/
?
m
j=1
w
j
(q)
When compared empirically, we noticed that
soft weighting is more effective than hard weight-
ing, as the latter is more sensitive to classifier er-
rors. Among the three soft weighting functions,
differences were mostly negligible in our exper-
iments. Hence, we decided to use the simplest
weighting function w
s1
.
4.6 Analytical Model
It is time-consuming to implement various
combination-of-evidence approaches and run re-
trieval experiments. Therefore, it is useful to have
an analytical model of the process that can pro-
vide a rough estimate of how fruitful it would be
to spend this effort, given certain details about the
task. The model we present in this section esti-
mates the effectiveness of combining QT methods
1 . . .m on a query set Q, given (1) the effective-
ness of each method on Q and (2) error rate of
binary classifiers C
1
. . . C
m
on Q. Using this for-
mulation, one can assess the benefit of combina-
tion without running retrieval, based only on er-
ror rates — this saves precious time during de-
velopment. Moreover, even without trained clas-
sifiers, this model can be used to estimate poten-
tial benefits by plugging in hypothetical error val-
ues. In other words, one can ask the question “If
I had classifiers with x% error on this query set,
what would be the benefit of using these classi-
fiers to combine QT methods?” before developing
any combination approach at all.
The analytical model considers a special case of
weighted combination: for each query q, we pick a
single QT method i = 1 . . .m, for which the clas-
sifier predicts a label of 1. If there are more than
one such method, one of them is picked randomly.
This simplified version allows us to compute ex-
pected effectiveness for q as follows:
E[f(q)] =
?
method i
Pr(pick i|q)f
i
(q)
While f
i
(q) is an observed value (the effective-
ness of the i
th
method on query q), Pr(pick i|q)
needs to be estimated (the probability of selecting
the i
th
method). Since this depends on the pre-
dicted labels, we consider all possible scenarios
l = l
1
l
2
. . . l
m
, where each value is the prediction
of a classifier. For instance, “l=010” means that
classifiers C
1
and C
3
predicted a label of 0, while
C
2
predicted a positive label. Marginalizing over
the 2
m
possible scenarios gives us the following
estimate:
Pr(pick i|q)
=
?
?
1
?
l
1
=0
. . .
1
?
l
m
=0
Pr(l|q)
?
?
× Pr(pick i|l, q)
=
?
?
1
?
l
1
=0
. . .
1
?
l
m
=0
m
?
i=1
Pr(l
i
|q)
?
?
× Pr(i|l, q)
In the final step, we assumed that classifiers make
predictions independent of each other, which is
a desired property for successful combination.
Pr(l
i
|q) can be estimated using classifier error
statistics:
Pr(l
i
|q) ?
count(predicted = l
i
, true = l
q
)
count(true = l
q
)
where l
q
is the true label of q. If l
i
= l
q
, this ex-
pression becomes the true positive or true negative
rate, depending on the value. Similarly, if l
i
6= l
q
,
it is either the false positive or false negative rate.
Finally, the probability that the i
th
method is se-
lected in a particular scenario depends solely on
the predicted labels, since it is a random selection:
Pr(pick i|l) = l
i
/
?
m
j=1
l
j
This concludes the derivation of the analytical
model of query evidence combination, which we
use in Section 5.1 to evaluate the effectiveness of
labeling approaches.
5 Evaluation
We evaluated our approach on four different CLIR
tasks: TREC 2002 English-Arabic CLIR, NTCIR-
8 English-Chinese Advanced Cross-Lingual Infor-
594
mation Access (ACLIA), and two forum post re-
trieval tasks as part of the DARPA Broad Oper-
ational Language Technologies (BOLT) program:
English-Arabic (BOLT
ar
) and English-Chinese
(BOLT
ch
). The query language is English in all
cases, and we preprocess the queries using BBN’s
information extraction toolkit SERIF (Ramshaw et
al., 2011). State-of-the-art English-Arabic (En-
Ar) and English-Chinese (En-Ch) MT systems
were trained on parallel corpora released in NIST
OpenMT 2012, in addition to parallel forum data
collected as part of the BOLT program (10m En-
Ar words; 30m En-Ch words). From these data,
word alignments were learned with GIZA++ (Och
and Ney, 2003), using five iterations of each of
IBM Models 1–4 and HMM.
3-gram Chinese and 5-gram Arabic Kneser-Ney
language models were trained from the Gigaword
corpus (1b words each) and non-English side of
the training corpus. Chinese and English parallel
text were preprocessed through the Treebank Tok-
enizer,
4
while no special treatment was performed
on Arabic.
For retrieval, we used Indri, a state-of-the-
art probabilistic relevance model that supports
weighted query representations through operators
#combine and #weight (Metzler and Croft,
2005). A character-based index was built for
Chinese collections, whereas Arabic text was
stemmed using Lucene before indexing.
5
En-
glish text was preprocessed by Indri’s imple-
mentation of the Porter stemmer (Porter, 1997).
Statistics for each collection and query set are
summarized in Table 1.
Before performing any combination, we first
ran the three baseline QT methods individually
and evaluated the retrieved documents. Mean
average precision (MAP) was used to measure
retrieval effectiveness, which is a widely used
and stable metric, estimating the area under the
precision-recall curve. We set n = 10 for the
n-best probabilistic translation method. Baseline
scores are reported in Table 2. The average preci-
sion (AP) of each query in these tasks was used to
label the query and construct training data accord-
ingly.
In subsequent sections, we evaluate the effect of
several variants in the training pipeline.
4
http://www.cis.upenn.edu/˜treebank
5
http://lucene.apache.org
5.1 Effect of Labeling
In Section 4.2, we introduced two ways to label
instances. In our evaluation, we set the free pa-
rameters k
1
= k
2
= 90, which filters out 33% of
queries from the training set of the BOLT
ar
task;
this percentage is 29% in BOLT
ch
, 44% in TREC,
and 27% in NTCIR.
Labeling determines which query translation
method is considered effective or not, which con-
sequently determines what the “learning problem”
is (since the objective of the classifier is to sep-
arate differently labeled instances). As a result,
there are two dimensions to consider when com-
paring labeling strategies. One is the accuracy of
the classifiers on held-out data, and the other is
how well the trained classifier reflects this accu-
racy when used in retrieval. To clarify the dis-
tinction, consider a case where every instance is
labeled 1. This generates a trivial learning prob-
lem with no test errors, yet this does not entail that
using these classifiers in retrieval will be more ef-
fective than other labeling strategies. If, even with
high classifier accuracy, the retrieval effectiveness
is low, that indicates a bad choice for labeling.
We can theoretically analyze how suitable each
labeling method is by applying the analytical
model to each CLIR task, setting parameters based
on a perfect classifier: true positive/negative rate
of 1 and false positive/negative rate of 0 (see Sec-
tion 4.6). Table 2 shows these results in the “Per-
fect” column, since these scores represent what
could be achieved if classifiers were trained to pre-
dict labels perfectly (no training or retrieval is ac-
tually performed). There are two values in each
row of the “Perfect” column, one for each labeling
strategy. In each row, we found these two values to
be statistically significantly higher than any of the
baseline scores. This shows that both labeling ap-
proaches have the potential to improve effective-
ness significantly.
We also made an empirical comparison of the
two labeling approaches by actually training clas-
sifiers with each labeling, and then using the clas-
sifiers to combine query translations in retrieval.
The “Trained” column in Table 2 shows the MAP
we get on each CLIR task (and average classifier
accuracies), using either labeling.
6
Based on these results, we conclude that best-
6
For a fair comparison, we fixed the train-tune setting to
fully-open, trained classifiers on the test collection and re-
ported leave-one-out accuracies.
595
Lang
Collection
Topics
MT Training data
Source Size (docs) Source (domain) Size (words)
Arabic TREC-02 383,872 50 OpenMT-12 (news/web)
10m
Arabic BOLT 12,258,904 45 BOLT (forum)
Chinese NTCIR-8 388,589 100 OpenMT-12 (news/web)
30m
Chinese BOLT 6,693,951 45 BOLT (forum)
Table 1: Summary of the CLIR tasks in our evaluations.
Task
Baseline Perfect Trained
one-best ten-best word measure rank measure rank
BOLT
ar
0.296 0.311 0.318 0.341 0.341 0.342 (74) 0.330 (72)
BOLT
ch
0.370 0.406 0.407 0.458 0.462 0.438 (68) 0.426 (60)
TREC 0.292 0.298 0.301 0.327 0.330 0.305 (59) 0.316 (59)
NTCIR 0.146 0.152 0.141 0.180 0.177 0.163 (56) 0.162 (61)
Table 2: Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison of
labeling strategies is provided on the right side. All numbers represent MAP values, except for classifier
accuracy shown in percentage values (in parantheses). Analytically computed values are shown in italics.
by-measure labeling is more useful in practice,
supported by typically higher accuracy and effec-
tiveness. Best-by-rank yields better results only
on TREC, but a closer look reveals that the in-
crease in MAP is due to only two outlier queries.
For BOLT
ar
, on the other hand, retrieval with best-
by-measure labeling is more effective (statistically
significant) than best-by-rank; hence, the former is
used in remaining parts of our evaluation.
5.2 Effect of Train-Tune Setting
In Section 4.4, we introduced three major train-
tune settings: fully-open, half-blind, and fully-
blind. In order to implement these settings, we
treat each of the three query sets (BOLT, TREC,
NTCIR) as a separate training dataset and experi-
ment with a variety of combinations.
For simplicity, let us demonstrate the variety of
experiments assuming the test collection is BOLT.
For the fully-open case, the default training data is
all of the BOLT queries (this training set is referred
to as b). Additionally, one can include queries
from TREC (referred to as t) and NTCIR (referred
to as n) into the training data. This gives us four
different training datasets for the fully-open case:
b, b + n, b + t, b + t + n. Similarly, each of the
half-blind and fully-blind settings can be applied
to three different training sets: For BOLT, these
are t, n, t + n.
7
This results in ten different ex-
periments run for each task — in each experiment,
7
In the case of half-blind, b is split into two: 20% is used
for tuning and the remainder is used for testing.
we train a classifier for each QT method, select the
best meta-parameters on the tuning set, and then
compute combination weights for retrieval using
the classifiers.
Each cell on the left side of Table 3 (under col-
umn “Query-specific Combination”) shows the re-
sults of the most effective experiment for a partic-
ular task and train-tune setting. Accuracy values
for classifiers varied widely across these experi-
ments. Still, even when accuracies dropped close
to or below 50% (i.e. random baseline), combined
retrieval was always more effective than any single
QT approach, which emphasizes the robustness of
our approach. For instance, in the fully-blind set-
ting for the NTCIR task, the individual classifiers
had accuracies of only 56%, 49%, and 44% but
MAP was 0.163, which is higher than the MAP of
any individual method for that collection (0.146,
0.152, or 0.141).
Another key observation in Table 3 is that
the domain effect (i.e., training and/or tuning on
queries similar to test queries) is only noticeable
on the two BOLT tasks. For NTCIR and TREC,
we do not observe a boost in MAP when queries
from the same task are included in training (i.e.,
fully-open setting). This can be explained by the
BOLT-centric nature of our system components:
the text analysis tool and MT systems are tuned
mainly for forum data, and the collection-based
features are extracted from BOLT. Due to this bias,
BOLT queries were most useful in our experi-
ments, supported by the fact that BOLT is always
596
Task
Query-specific Combination
Uniform
Task-
Max
fully-open half-blind fully-blind specific
BOLT
ar
0.342
?†
b 0.330 t+n 0.329 t+n 0.324
12
0.329
1
0.346
BOLT
ch
0.438
?†
b 0.428 n 0.426 t+n 0.422
1
0.431
1
0.466
TREC 0.321 b+t 0.324
?†
b+n 0.321 b+n 0.314
1
0.318
1
0.332
NTCIR 0.164
?
b+n 0.163 b+t 0.163 b 0.162
13
0.162
13
0.182
Table 3: A comparison of query combination approaches. For query-specific combination, MAP and
training data are shown for the most effective experiment of each train-tune setting. For each task, the
highest MAP achieved with our approach is shown in bold. Superscripts 1, 2, and 3 indicate statisti-
cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based,
whereas * indicates improvements over all three. Superscript † indicates results significantly better than
uniform and task-specific combination methods.
included in the train set when testing on TREC or
NTCIR (see lowest two rows in Table 3). Also,
when there is no domain effect (i.e., half-blind and
fully-blind ), more data yields higher effectiveness
in 6 out of 8 cases (see two right columns on the
left side of Table 3).
5.3 Retrieval Effectiveness
In this section, we compare our novel query-
specific combination-of-evidence approach to the
baseline CLIR approaches, as well as comparable
combination methods (uniform and task-specific
combination) in terms of retrieval effectiveness.
Based on a randomized significance test (Smucker
et al., 2007), the best query-specific combina-
tion method (shown in boldface in Table 3) out-
performs all baseline QT methods in all tasks
with 95% confidence (indicated by superscript *
in Table 3). This is not the case for uniform or
task-specific query combination, which are statis-
tically indistinguishable from at least one of the
QT methods, depending on the task (indicated by
superscripts 1, 2, and 3 for one-best, probabilis-
tic 10-best, and word-based QT methods, respec-
tively). When we directly compare our query-
specific combination approach to other combina-
tion methods, the differences are statistically sig-
nificant for all tasks but NTCIR (indicated by su-
perscript †).
For reference, we also computed effectiveness
for a hypothetical system (denoted by “Max” in
Table 3) that could select the best QT method for
each query and use only that for retrieval. This is
not a strict upper bound, since correctly weight-
ing each method can produce better results, but
it is still a reasonable target for effectiveness. In
our experiments, Arabic retrieval runs were very
close to this target with our combination approach,
while the gap for Chinese is still substantial, which
is worth further exploration.
6 Conclusions and Future Work
In this paper, we introduced a novel combination-
of-evidence approach for CLIR, which learns a
custom combination recipe for each query. We for-
mulate this as a set of binary classification prob-
lems, and show that trained classifiers can be used
to produce query-specific combination weights ef-
fectively. Our deep exploration of many variants
(e.g., labeling, training-tuning, weight computa-
tion, analytical formulation) and extensive empiri-
cal analysis on four different tasks provide insights
for future research on the under-studied problem
of combining translations for CLIR.
Our approach advances the state of the art of
CLIR, yielding higher effectiveness than three ad-
vanced query translation approaches, all based
on state-of-the-art MT systems. Furthermore, on
three of the four tasks, our combination strategy is
statistically significantly better than two compara-
ble combination techniques. Experimental results
also suggest that even a uniform combination of
query translations is consistently better than any
individual method. While it is known that com-
bining translations helps CLIR, we confirm this on
a set of modern CLIR tasks, including two target
languages and a variety of text domains.
Having a simple linear learning problem allows
us to train robust models with relatively simpler
features. Nevertheless, we are interested in ex-
perimenting with more sophisticated learning ap-
proaches. In terms of non-linear classifiers, our
experience with decision trees in this paper indi-
cated a higher tendency to overfit. In terms of
597
combining queries in a non-linear fashion, our fu-
ture plans include integrating our approach into a
LTR framework, and directly optimize MAP. This
will also allow us to explore more complex fea-
tures extracted from query and document text, as
well as external sources.
Another possible future endeavor is to extend
these ideas to (i) other query translation ap-
proaches and (ii) document translation. While the
exact same problem can be formulated for learning
to translate documents effectively, a more compli-
cated infrastructure and longer running times are
two challenges that need to be considered.
Finally, we hope this to be a significant step to-
wards more context-dependent and robust CLIR
models, by taking advantage of modern translation
technologies, as well as machine learning tech-
niques.
Acknowledgments
This work was supported by DARPA/I2O Con-
tract No. HR0011-12-C-0014 under the BOLT
program (Approved for Public Release, Distribu-
tion Unlimited). The views, opinions, and/or find-
ings contained in this article are those of the author
and should not be interpreted as representing the
official views or policies, either expressed or im-
plied, of the Defense Advanced Research Projects
Agency or the Department of Defense.
References
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. 2013. Exploiting multiple translation re-
sources for english-persian cross language informa-
tion retrieval. In Proceedings of the Cross-Language
Evaluation Forum on Cross-Language Information
Retrieval and Evaluation, CLEF ’13, pages 93–99.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791–801.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information
retrieval. Information Processing & Management,
31(3):431–448, May.
Michael Bendersky, Donald Metzler, and W. Bruce
Croft. 2011. Parameterized concept weighting in
verbose queries. In Proceedings of the 34th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’11,
pages 605–614, New York, NY, USA. ACM.
Pierre-Yves Berger and Jacques Savoy. 2007. Se-
lecting automatically the best query translations. In
Large Scale Semantic Access to Content (Text, Im-
age, Video, and Sound), RIAO ’07, pages 287–300,
Paris, France, France. Le Centre de Hautes Etudes
Internationales D’Informatique Documentaire.
Martin Braschler. 2004. Combination approaches for
multilingual text retrieval. Information Retrieval,
7(1-2):183–204, January.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.
Aitao Chen and Fredric C. Gey. 2004. Multilingual in-
formation retrieval using machine translation, rele-
vance feedback and decompounding. Inf. Retr., 7(1-
2):149–182, January.
W. Bruce Croft. 2000. Combining approaches to in-
formation retrieval. In W. Bruce Croft, editor, Ad-
vances in Information Retrieval, volume 7 of The
Information Retrieval Series, pages 1–36. Springer.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings of
the 26th Annual International ACM SIGIR Confer-
ence on Research and Development in Informaion
Retrieval, SIGIR ’03, pages 338–344.
Edward A. Fox. 1983. Extending the Boolean
and Vector Space Models of Information Retrieval
with P-norm Queries and Multiple Concept Types.
Ph.D. thesis, Cornell University, Ithaca, NY, USA.
AAI8328584.
Fredric C. Gey, Hailing Jiang, Vivien Petras, and
Aitao Chen. 2001. Cross-language retrieval for
the clef collections - comparing multiple meth-
ods of retrieval. In Revised Papers from the
Workshop of Cross-Language Evaluation Forum on
Cross-Language Information Retrieval and Evalua-
tion, CLEF ’00, pages 116–128, London, UK, UK.
Springer-Verlag.
Benjamin Herbert, Gy¨orgy Szarvas, and Iryna
Gurevych. 2011. Combining query transla-
tion techniques to improve cross-language informa-
tion retrieval. In Proceedings of the 33rd Euro-
pean Conference on Advances in Information Re-
trieval, ECIR’11, pages 712–715, Berlin, Heidel-
berg. Springer-Verlag.
Djoerd Hiemstra, Wessel Kraaij, Ren´ee Pohlmann,
and Thijs Westerveld. 2001. Translation re-
sources, merging strategies, and relevance feedback
for cross-language information retrieval. In Revised
Papers from the Workshop of Cross-Language Eval-
uation Forum on Cross-Language Information Re-
trieval and Evaluation, CLEF ’00, pages 102–115,
London, UK, UK. Springer-Verlag.
598
David A. Hull and Gregory Grefenstette. 1996. Query-
ing across languages: a dictionary-based approach
to multilingual information retrieval. In Proceed-
ings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ’96, pages 49–57.
Kui-Lam Kwok. 1999. English-Chinese cross-
language retrieval based on a translation package.
In Workshop on Machine Translation for Cross Lan-
guage Information Retrieval, Machine Translation
Summit VII, pages 8–13.
Patrice Lopez and Laurent Romary. 2009. Patatras:
Retrieval model combination and regression mod-
els for prior art search. In Proceedings of the 10th
Cross-language Evaluation Forum Conference on
Multilingual Information Access Evaluation: Text
Retrieval Experiments, CLEF’09, pages 430–437,
Berlin, Heidelberg. Springer-Verlag.
Yanjun Ma, Jian-Yun Nie, Hua Wu, and Haifeng Wang.
2012. Opening machine translation black box for
cross-language information retrieval. In Information
Retrieval Technology, pages 467–476. Springer.
Walid Magdy and Gareth J. F. Jones. 2011. Should
MT systems be used as black boxes in CLIR? In
Proceedings of the 33rd European Conference on In-
formation Retrieval, ECIR ’11, pages 683–686.
Michael McGill, Matthew Koll, and Terry Noreault.
1979. An Evaluation of Factors Affecting Document
Ranking by Information Retrieval Systems. ERIC
reports. School of Information Studies, Syracuse
University.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In Pro-
ceedings of the 28th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ’05, pages 472–479.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Judea Pearl. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Carol Peters, Martin Braschler, and Paul Clough. 2012.
Multilingual Information Retrieval - From Research
To Practice. Springer.
Ari Pirkola. 1998. The effects of query struc-
ture and dictionary-setups in dictionary-based cross-
language information retrieval. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’98, pages 55–63.
M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages
313–316. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
T. B. Rajashekar and W. Bruce Croft. 1995. Com-
bining automatic and manual index representations
in probabilistic retrieval. J. Am. Soc. Inf. Sci.,
46(4):272–283, May.
Lance Ramshaw, Elizabeth Boschee, Marjorie Freed-
man, Jessica MacBride, Ralph Weischedel, and Alex
Zamanian. 2011. Serif language processing — ef-
fective trainable language understanding. In J. Olive
et al., editor, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global
Autonomous Language Exploitation, pages 626–
631. Springer.
Jacques Savoy. 2001. Report on CLEF-2001 exper-
iments: Effective combined query-translation ap-
proach. In CLEF, pages 27–43.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the 16th ACM conference on Conference on In-
formation and Knowledge Management, CIKM ’07,
pages 623–632.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING ’12, pages
2685–2702.
Howard Turtle and W. Bruce Croft. 1990. Inference
networks for document retrieval. In Proceedings of
the 13th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’90, pages 1–24, New York, NY,
USA. ACM.
Jinxi Xu and Ralph Weischedel. 2005. Empirical stud-
ies on the impact of lexical resources on CLIR per-
formance. Information Processing & Management,
41(3):475–487, May.
599
