Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Online Learning of Interpretable Word Embeddings
Hongyin Luo
1
, Zhiyuan Liu
1,2 ?
, Huanbo Luan
1
, Maosong Sun
1,2
1
Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2
Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China
Abstract
Word embeddings encode semantic mean-
ings of words into low-dimension word
vectors. In most word embeddings, one
cannot interpret the meanings of specific
dimensions of those word vectors. Non-
negative matrix factorization (NMF) has
been proposed to learn interpretable word
embeddings via non-negative constraints.
However, NMF methods suffer from scale
and memory issue because they have to
maintain a global matrix for learning. To
alleviate this challenge, we propose on-
line learning of interpretable word embed-
dings from streaming text data. Exper-
iments show that our model consistently
outperforms the state-of-the-art word em-
bedding methods in both representation a-
bility and interpretability. The source code
of this paper can be obtained from http:
//github.com/skTim/OIWE.
1 Introduction
Word embeddings (Turian et al., 2010) aim to
encode semantic meanings of words into low-
dimensional dense vectors. As compared with tra-
ditional one-hot representation and distributional
representation, word embeddings can better ad-
dress the sparsity issue and have achieved success
in many NLP applications recent years.
There are two typical approaches for word em-
beddings. The neural-network (NN) approach
(Bengio et al., 2006) employs neural-based tech-
niques to learn word embeddings. The matrix fac-
torization (MF) approach (Pennington et al., 2014)
builds word embeddings by factorizing word-
context co-occurrence matrices. The MF approach
requires a global statistical matrix, while the N-
N approach can flexibly perform learning from
?
Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)
streaming text data, which is efficient in both com-
putation and memory. For example, two recen-
t NN methods, Skip-Gram and Continuous Bag-
of-Word Model (CBOW) (Mikolov et al., 2013a;
Mikolov et al., 2013b), have achieved impressive
impact due to their simplicity and efficiency.
For most word embedding methods, a critical
issue is that, we are unaware of what each dimen-
sion represent in word embeddings. Hence, the
latent dimension for which a word has its largest
value is difficult to interpret. This makes word em-
beddings like a black-box, and prevents them from
being human-readable and further manipulation.
People have proposed non-negative matrix fac-
torization (NMF) for word representation, denoted
as non-negative sparse embedding (NNSE) (Mur-
phy et al., 2012). NNSE realizes interpretable
word embeddings by applying non-negative con-
straints for word embeddings. Although NNSE
learns word embeddings with good interpret-
abilities, like other MF methods, it also requires a
global matrix for learning, thus suffers from heavy
memory usage and cannot well deal with stream-
ing text data.
Inspired by the characteristics of NMF meth-
ods (Lee and Seung, 1999), we note that, non-
negative constraints only allow additive combi-
nations instead of subtractive combinations, and
lead to a parts-based representation. Hence, the
non-negative constraints derive interpretabilities
of word embeddings. In this paper, we aim to de-
sign an online NN method to efficiently learn in-
terpretable word embeddings. In order to achieve
the goal of interpretable embeddings, we design
projected gradient descent (Lin, 2007) for opti-
mization so as to apply non-negative constraints
on NN methods such as Skip-Gram. We also em-
ploy adaptive gradient descent (Sun et al., 2012)
to speedup learning convergence. We name the
proposed models as online interpretable word em-
beddings (OIWE).
1687
For experiments, we implement OIWE based
on Skip-Gram. We evaluate the representation
performance of word embedding methods on the
word similarity computation task. Experiment re-
sults show that, our OIWE models are signifi-
cantly superior to other baselines including Skip-
Gram, RNN and NNSE. We also evaluate the in-
terpretability performance on the word intrusion
detection task. The results demonstrate the effec-
tiveness of OIWE as compared to NNSE.
2 Our Model
In this section, we first introduce Skip-Gram and
then introduce the proposed online interpretable
word embeddings based on Skip-Gram.
2.1 Skip-Gram
Skip-Gram (Mikolov et al., 2013b) is simple and
effective to learn word embeddings. The objec-
tive of Skip-Gram is to make word vectors good
at predicting its context words. More specifically,
given a word sequence {w
1
, w
2
, . . . , w
T
}, Skip-
Gram aims to maximize the average log probabil-
ity
1
T
T
?
1
(
?
?k?j?k,j 6=0
log Pr(w
t+j
|w
t
)
)
, (1)
where k is the context window size, and
Pr(w
t+j
|w
t
) indicates the probability of seeing
w
t+j
in the context of w
t
, which are measured
with softmax function
Pr(w
t+j
|w
t
) =
exp
(
w
t+j
·w
t
)
?
w?W
exp
(
w ·w
t
)
, (2)
where w
t+j
and w
t
are word embeddings of w
t+j
and w
t
, and W is the vocabulary size. Since the
computation of full softmax is time consuming,
the techniques of hierarchical softmax and nega-
tive sampling (Mikolov et al., 2013b) are proposed
for approximation.
Take negative sampling for example. The log
probability Pr(w
t+j
|w
t
) can be approximate by
log ?
(
w
t+j
·w
t
)
+
?
w?N
t
log ?
(
w ·w
t
)
, (3)
where ?(x) = 1/(1 + exp(?x)), and N
t
is the
set of negative samples as compared to the cor-
responding context word w
t+j
. The task can be
regarded as to distinguish the context word w
t+j
from negative samples.
For Skip-Gram with negative sampling, we can
perform stochastic gradient descent for learning.
The update rule for the positive/negative context
words u ? {w
t+j
} ?N
t
is
u
i+1
= u
i
+ ?
[
I
w
t
(u)? ?(u ·w
t
)
]
w
i
t
, (4)
where I
w
t
(u) = 1 when w is the positive contex-
t word of w
t
and I
w
t
(u) = 0 when w is nega-
tive, i is the iteration number, and ? is the learning
rate. Correspondingly, the update rule for the in-
put word w
t
is
w
i+1
t
= w
i
t
+?
?
u?{w
t
}?N
t
[
I
w
t
(u)??(u ·w
t
)
]
u
i
t
.
(5)
We note that, the learning rate ? in Skip-Gram is
shared by all word embeddings.
2.2 OIWE
In order to learn interpretable word embeddings,
we have to make the word embeddings learned in
Skip-Gram keep non-negative. In order to achieve
this goal, we have to constrain the update rules in
Equation (4) and (5) as follows:
x
i+1
k
= P
[
x
i
k
+ ??f(x
k
)
]
, (6)
where xmay be u orw
t
, k is the corresponding di-
mension in word embedding x, ?f(x
k
) indicates
the gradient corresponding to x
k
, and P [·] is de-
fined as
P [x] =
{
x if x > 0,
0 if x ? 0.
(7)
Motivated by the projected gradient descent meth-
ods for NMF (Lin, 2007), in this paper we pro-
pose two methods for Skip-Gram to realize the
constraint in Equation (6).
Naive Projected Gradient (NPG). In NPG, we
consider the most straightforward update strategy
by simply setting
x
i+1
k
= max
(
0,x
i
k
+ ??f(x
k
)
)
. (8)
The method has been used for NMF (Lin, 2007)
although the details are not discussed.
The NPG method only constrains the violated
dimensions without taking the update consisten-
cy among dimensions of a word embedding into
account. For example, if many dimensions en-
counter x
i
k
+ ??f(x
k
) < 0 at the same time,
which are set to 0 with Equation (8) with other
1688
dimensions unchanged, the updated word embed-
ding may heavily deviate from its semantic mean-
ing. Hence, NPG may suffer from instable updat-
ing results. To address this issue, we propose to
employ the following improved projected gradient
method.
Improved Projected Gradient (IPG). In order
to make the non-negative update more consistent
among dimensions, we design an improved pro-
jected gradient by iteratively finding the most ap-
propriate learning rate ?. The basic idea is that,
we will find a good learning rate ? to make less
dimensions violate the non-negative constraint.
More specifically, in Equation (6), for a learning
rate ?, we define the violation ratio as
R(?) =
?
?
{k|x
i
k
> 0,x
i
k
+ ??f(x
k
) < 0}
?
?
K
, (9)
where K is the dimension size of word embed-
dings. The violation ratio indicates how many di-
mensions violate the non-negative constraint and
require to be set to 0. When the learning rate ? de-
creases, the violation ratio will also decrease, and
the zero-setting in Equation (8) will bring less de-
viation to word embeddings.
We set a threshold ? for the violation ratio R(?)
and a lower bound ?
L
for the learning rate ?. S-
tarting from an initial learning rate ?
0
, we will re-
peatedly decrease the learning rate by
?
m+1
= ?
m
· ? (10)
with 0 < ? < 1 until
R(?
m+1
) < ? or ?
m+1
? ?
L
, (11)
and then update with Equation (8) using ?
m+1
.
In nature, the updating constraint of learning rate
in Equation (11) play a similar role to Equation
(13) in (Lin, 2007), which aims to prevent the pro-
jection operation from heavily deviating the word
embeddings.
2.3 More Optimization Details
In experiments, we explore many optimization
methods and find the following two strategies
are important: (1) Adaptive Gradient Descen-
t. Following the idea from (Sun et al., 2012),
we maintain different learning rates ?
w
for each
word w, and the learning rates for those high-
frequency words may decrease faster than those
low-frequency words. This will speedup the con-
vergence of word embedding learning. (2) Unified
Word Embedding Space. Different from original
Skip-Gram (Mikolov et al., 2013b) which learn
embeddings of w
t
and its context words w
t+j
in
two separate spaces, in this paper both w
t
and it-
s context words w
t+j
share the same embedding
space. Hence, a word embedding may get more
opportunities for learning.
3 Experiments
In this section, we investigate the representation
performance and interpretability of our OIWE
models with other baselines including typical N-
N and MF methods.
The representation performance is evaluated
with the word similarity computation task, and the
interpretability is evaluated with the word intru-
sion detection task. For the both tasks, we train our
OIWE models using the text8 corpus obtained
from word2vec website
1
, and the OIWE models
achieve the best performance by setting the dimen-
sion number K = 300, ? = 0.6, ? = 1/60, and
?
L
= 2.5× 10
?6
.
3.1 Word Similarity Computation
Following the settings in (Murphy et al., 2012),
we also select the following three sets for word
similarity computation: (1) WS-203, the strict-
similarity subset of 203 pairs (Agirre et al., 2009)
selected from the wordsim-353 (Finkelstein et al.,
2001), (2) RG-65, 65 concrete word pairs built
by (Rubenstein and Goodenough, 1965) and (3)
MEN, 3, 000 word pairs built by (Bruni et al.,
2014). The performance is evaluated with the S-
pearman coefficient between human judgements
and similarities calculated using word embed-
dings.
We select three baselines including Skip-Gram
(Mikolov et al., 2013b), recurrent neural networks
(RNN) (Mikolov et al., 2011) and NNSE (Mur-
phy et al., 2012). For Skip-Gram, we report the
result we learned using word2vec on text8 cor-
pus. The result of RNN is from (Faruqui and Dyer,
2014) and the one of NNSE is from (Murphy et al.,
2012).
The evaluation results of word similarity com-
putation are shown in Table 1. We can ob-
serve that: (1) The OIWE models consistently
outperform other baselines. (2) IPG generally
achieves better representation performance than
1
https://code.google.com/p/word2vec/
1689
Model WS-203 RG-65 MEN
Skip-Gram 67.35 50.49 52.56
RNN 49.28 50.19 43.44
NNSE 51.06 56.48 -
OIWE-NPG 63.71 56.85 57.60
OIWE-IPG 71.74 57.16 56.68
Table 1: Spearman coefficient results (%) on word
similarity computation.
NPG. This indicates consistent updates are im-
portant for learning of word embeddings. One
can refer to http://github.com/skTim/
OIWE for the evaluation results on more evalua-
tion datasets.
3.2 Word Intrusion Detection
We evaluate interpretability of word embeddings
with the task of word intrusion detection proposed
by (Murphy et al., 2012). In this task, for each
dimension we create a word set containing top-5
words in this dimension, and intruce a noisy word
from the bottom half of this dimension which
ranks high in other dimensions. Human editors
are asked to check each word set and try to pick
out the intrusion words, and the detection preci-
sion indicates the interpretability of word embed-
ding models. Note that, for this task we do not
perform normalization for word vectors.
Model Precision
Skip-Gram 32.62
NNSE 92.00
OIWE-NPG 61.40
OIWE-IPG 94.80
Table 2: Experiment results (%) on word intrusion
detection.
The evaluation results are shown in Table 2. We
can observe that: (1) Skip-Gram performs poor
in word intrusion detection without doubt since it
is uninterpretable in nature. (2) The OIWE-NPG
model achieves better interpretability as compared
to Skip-Gram, but performs much worse than
the OIWE-IPG model. The OIWE-IPG model
achieves competitive interpretability with NNSE.
This indicates that reducing violation rations in
word embedding learning is crucial for preserving
interpretability.
In Table 3, we show top-5 words for some
dimensions, which clearly demonstrate semantic
meanings of these dimensions. One can also
refer to http://github.com/skTim/OIWE
to find top-5 words for all dimensions.
No. Top Words
1 type, form, way, kind, manner
2 translates, describes, combines, includ-
ed, includes
3 gospel, baptism, jesus, faith, judaism
4 Franz, Johann, Wilhelm, Friedrich, von
25 prominent, famous, important, influen-
tial, popular
Table 3: Top words of some dimensions in word
embeddings.
3.3 Influence of Dimension Numbers
The dimension number is an important configura-
tion in word embeddings. In Fig. 1 we show the
performance of OIWE and Skip-Gram on word
similarity computation with varying dimension
numbers.
From the figure, we can observe that: (1) The
both models achieve their best performance un-
der the same dimension number. This indicates
that OIWE, to some extent, inherits the represen-
tation power of Skip-Gram. (2) The performance
of OIWE seems to be more sensitive to dimension
numbers. When the dimension number changes
from 300 to 200 or 400, the performance drops
much quickly than Skip-Gram. The reason may
be as follows. OIWE has to concern about both
representation ability of word embeddings and in-
terpretability of each dimension. An appropri-
ate dimension number is critical to make each di-
mension interpretable, just like the cluster num-
ber is important for clustering. On the contrary,
Skip-Gram is much free to learn word embeddings
only concerning about representation ability. (3)
The performance of OIWE with various dimen-
sions also varies on different evaluation dataset-
s. For example, OIWE-IPG with K = 400 get-
s 68.74 on MEN, which is much better than that
with K = 300. In future work, we will exten-
sively investigate the characteristics of OIWE with
respect to dimension numbers and other hyperpa-
rameters.
4 Conclusion and Future Work
In this paper, we present online interpretable word
embeddings. The OIWE models perform project-
1690
Figure 1: Influence of Dimension Number on
Words Similarity
ed gradient descent to apply non-negative con-
straints on NN methods such as Skip-Gram. Ex-
periment results on word similarity computation
and word intrusion detection demonstrate the ef-
fectiveness and efficiency of our models in both
representation ability and interpretability. We al-
so note that, our models can be easily extended to
other NN methods.
In future, we will explore the following re-
search issues: (1) We will extensively investigate
the characteristics of OIWE with respect to var-
ious hyperparameters including dimension num-
bers. (2) We will evaluate the performance of
our OIWE models in various NLP applications.
(3) We will also investigate possible extensions of
our OIWE models, including multiple-prototype
models for word sense embeddings (Huang et al.,
2012; Chen et al., 2014), semantic composition-
s for phrase embeddings (Zhao et al., 2015) and
knowledge representation (Bordes et al., 2013; Lin
et al., 2015).
Acknowledgments
Zhiyuan Liu and Maosong Sun are supported by
National Key Basic Research Program of Chi-
na (973 Program 2014CB340500) and Nation-
al Natural Science Foundation of China (NSFC
No. 62102140). Huanbo Luan is supported by
the National Natural Science Foundation of Chi-
na (NSFC No. 61303075). This research is al-
so supported by the Singapore National Research
Foundation under its International Research Cen-
tre@Singapore Funding Initiative and adminis-
tered by the IDM Programme.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19–27.
Yoshua Bengio, Holger Schwenk, Jean-Sebastien
Senecal, Frederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137–186.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS, pages
2787–2795.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. JAIR,
49:1–47.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of EMNLP, pages
1025–1035.
Manaal Faruqui and Chris Dyer. 2014. Community
evaluation and exchange of word vectors at word-
vectors.org. In Proceedings of ACL System Demon-
strations.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406–
414. ACM.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873–882.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of AAAI.
Chuan-bi Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural computa-
tion, 19(10):2756–2779.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
Jan Honza Cernocky, and Sanjeev Khudanpur.
2011. Extensions of recurrent neural network lan-
guage model. In Proceedings of ICASSP, pages
5528–5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
1691
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Brian Murphy, Partha Pratim Talukdar, and Tom M
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In Proceedings of COLING, pages 1933–
1950.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of EMNLP, 12:1532–
1543.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of ACL, pages 253–262.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384–394.
Yu Zhao, Zhiyuan Liu, and Maosong Sun. 2015.
Phrase type sensitive tensor indexing model for se-
mantic composition. In Proceedings of AAAI.
1692
