Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Recurrent Continuous Translation Models
Nal Kalchbrenner Phil Blunsom
Department of Computer Science
University of Oxford
{nal.kalchbrenner,phil.blunsom}@cs.ox.ac.uk
Abstract
We introduce a class of probabilistic con-
tinuous translation models called Recur-
rent Continuous Translation Models that are
purely based on continuous representations
for words, phrases and sentences and do not
rely on alignments or phrasal translation units.
The models have a generation and a condi-
tioning aspect. The generation of the transla-
tion is modelled with a target Recurrent Lan-
guage Model, whereas the conditioning on the
source sentence is modelled with a Convolu-
tional Sentence Model. Through various ex-
periments, we show first that our models ob-
tain a perplexity with respect to gold transla-
tions that is > 43% lower than that of state-
of-the-art alignment-based translation models.
Secondly, we show that they are remarkably
sensitive to the word order, syntax, and mean-
ing of the source sentence despite lacking
alignments. Finally we show that they match a
state-of-the-art system when rescoring n-best
lists of translations.
1 Introduction
In most statistical approaches to machine transla-
tion the basic units of translation are phrases that are
composed of one or more words. A crucial com-
ponent of translation systems are models that esti-
mate translation probabilities for pairs of phrases,
one phrase being from the source language and the
other from the target language. Such models count
phrase pairs and their occurrences as distinct if the
surface forms of the phrases are distinct. Although
distinct phrase pairs often share significant similari-
ties, linguistic or otherwise, they do not share statis-
tical weight in the models’ estimation of their trans-
lation probabilities. Besides ignoring the similar-
ity of phrase pairs, this leads to general sparsity is-
sues. The estimation is sparse or skewed for the
large number of rare or unseen phrase pairs, which
grows exponentially in the length of the phrases, and
the generalisation to other domains is often limited.
Continuous representations have shown promise
at tackling these issues. Continuous representations
for words are able to capture their morphological,
syntactic and semantic similarity (Collobert and We-
ston, 2008). They have been applied in continu-
ous language models demonstrating the ability to
overcome sparsity issues and to achieve state-of-the-
art performance (Bengio et al., 2003; Mikolov et
al., 2010). Word representations have also shown
a marked sensitivity to conditioning information
(Mikolov and Zweig, 2012). Continuous repre-
sentations for characters have been deployed in
character-level language models demonstrating no-
table language generation capabilities (Sutskever et
al., 2011). Continuous representations have also
been constructed for phrases and sentences. The rep-
resentations are able to carry similarity and task de-
pendent information, e.g. sentiment, paraphrase or
dialogue labels, significantly beyond the word level
and to accurately predict labels for a highly diverse
range of unseen phrases and sentences (Grefenstette
et al., 2011; Socher et al., 2011; Socher et al., 2012;
Hermann and Blunsom, 2013; Kalchbrenner and
Blunsom, 2013).
Phrase-based continuous translation models were
first proposed in (Schwenk et al., 2006) and re-
1700
cently further developed in (Schwenk, 2012; Le et
al., 2012). The models incorporate a principled way
of estimating translation probabilities that robustly
extends to rare and unseen phrases. They achieve
significant Bleu score improvements and yield se-
mantically more suggestive translations. Although
wide-reaching in their scope, these models are lim-
ited to fixed-size source and target phrases and sim-
plify the dependencies between the target words tak-
ing into account restricted target language modelling
information.
We describe a class of continuous translation
models called Recurrent Continuous Translation
Models (RCTM) that map without loss of generality
a sentence from the source language to a probabil-
ity distribution over the sentences in the target lan-
guage. We define two specific RCTM architectures.
Both models adopt a recurrent language model for
the generation of the target translation (Mikolov et
al., 2010). In contrast to other n-gram approaches,
the recurrent language model makes no Markov as-
sumptions about the dependencies of the words in
the target sentence.
The two RCTMs differ in the way they condi-
tion the target language model on the source sen-
tence. The first RCTM uses the convolutional sen-
tence model (Kalchbrenner and Blunsom, 2013) to
transform the source word representations into a rep-
resentation for the source sentence. The source sen-
tence representation in turn constraints the genera-
tion of each target word. The second RCTM intro-
duces an intermediate representation. It uses a trun-
cated variant of the convolutional sentence model to
first transform the source word representations into
representations for the target words; the latter then
constrain the generation of the target sentence. In
both cases, the convolutional layers are used to gen-
erate combined representations for the phrases in a
sentence from the representations of the words in the
sentence.
An advantage of RCTMs is the lack of latent
alignment segmentations and the sparsity associated
with them. Connections between source and target
words, phrases and sentences are learnt only implic-
itly as mappings between their continuous represen-
tations. As we see in Sect. 5, these mappings of-
ten carry remarkably precise morphological, syntac-
tic and semantic information. Another advantage is
that the probability of a translation under the models
is efficiently computable requiring a small number
of matrix-vector products that is linear in the length
of the source and the target sentence. Further, trans-
lations can be generated directly from the probabil-
ity distribution of the RCTM without any external
resources.
We evaluate the performance of the models in four
experiments. Since the translation probabilities of
the RCTMs are tractable, we can measure the per-
plexity of the models with respect to the reference
translations. The perplexity of the models is signifi-
cantly lower than that of IBM Model 1 and is> 43%
lower than the perplexity of a state-of-the-art variant
of the IBM Model 2 (Brown et al., 1993; Dyer et
al., 2013). The second and third experiments aim to
show the sensitivity of the output of the RCTM II
to the linguistic information in the source sentence.
The second experiment shows that under a random
permutation of the words in the source sentences,
the perplexity of the model with respect to the refer-
ence translations becomes significantly worse, sug-
gesting that the model is highly sensitive to word
position and order. The third experiment inspects
the translations generated by the RCTM II. The
generated translations demonstrate remarkable mor-
phological, syntactic and semantic agreement with
the source sentence. Finally, we test the RCTMs
on the task of rescoring n-best lists of translations.
The performance of the RCTM probabilities joined
with a single word penalty feature matches the per-
formance of the state-of-the-art translation system
cdec that makes use of twelve features including
five alignment-based translation models (Dyer et al.,
2010).
We proceed as follows. We begin in Sect. 2 by
describing the general modelling framework under-
lying the RCTMs. In Sect. 3 we describe the RCTM
I and in Sect. 4 the RCTM II. Section 5 is dedicated
to the four experiments and we conclude in Sect. 6.1
2 Framework
We begin by describing the modelling framework
underlying RCTMs. An RCTM estimates the proba-
bility P (f|e) of a target sentence f = f1, ..., fm being
a translation of a source sentence e = e1, ..., ek. Let
1Code and models available at nal.co
1701
us denote by fi:j the substring of words fi, ..., fj . Us-
ing the following identity,
P (f|e) =
m?
i=1
P (fi|f1:i?1, e) (1)
an RCTM estimates P (f|e) by directly computing
for each target position i the conditional probability
P (fi|f1:i?1, e) of the target word fi occurring in the
translation at position i, given the preceding target
words f1:i?1 and the source sentence e. We see that
an RCTM is sensitive not just to the source sentence
e but also to the preceding words f1:i?1 in the target
sentence; by doing so it incorporates a model of the
target language itself.
To model the conditional probability P (f|e), an
RCTM comprises both a generative architecture for
the target sentence and an architecture for condition-
ing the latter on the source sentence. To fully cap-
ture Eq. 1, we model the generative architecture with
a recurrent language model (RLM) based on a re-
current neural network (Mikolov et al., 2010). The
prediction of the i-th word fi in a RLM depends on
all the preceding words f1:i?1 in the target sentence
ensuring that conditional independence assumptions
are not introduced in Eq. 1. Although the predic-
tion is most strongly influenced by words closely
preceding fi, long-range dependencies from across
the whole sentence can also be exhibited. The con-
ditioning architectures are model specific and are
treated in Sect. 3-4. Both the generative and con-
ditioning aspects of the models deploy continuous
representations for the constituents and are trained
as a single joint architecture. Given the modelling
framework underlying RCTMs, we now proceed to
describe in detail the recurrent language model un-
derlying the generative aspect.
2.1 Recurrent Language Model
A RLM models the probability P (f) that the se-
quence of words f occurs in a given language. Let
f = f1, ..., fm be a sequence of m words, e.g. a sen-
tence in the target language. Analogously to Eq. 1,
using the identity,
P (f) =
m?
i=1
P (fi|f1:i?1) (2)
the model explicitly computes without simpli-
fying assumptions the conditional distributions
R
I O
f
i
P(f   )
i+1
h
R
f
i-1
P(f )
i
f
i+1
P(f   )
i+2
OI
h h h
i-1 i i+1
Figure 1: A RLM (left) and its unravelling to depth 3
(right). The recurrent transformation is applied to the hid-
den layer hi?1 and the result is summed to the represen-
tation for the current word fi. After a non-linear transfor-
mation, a probability distribution over the next word fi+1
is predicted.
P (fi|f1:i?1). The architecture of a RLM comprises
a vocabulary V that contains the words fi of the
language as well as three transformations: an in-
put vocabulary transformation I ? Rq×|V |, a re-
current transformation R ? Rq×q and an output
vocabulary transformation O ? R|V |×q. For each
word fk ? V , we indicate by i(fk) its index in V
and by v(fk) ? R|V |×1 an all zero vector with only
v(fk)i(fk) = 1.
For a word fi, the result of I · v(fi) ? Rq×1 is
the input continuous representation of fi. The pa-
rameter q governs the size of the word representa-
tion. The prediction proceeds by successively ap-
plying the recurrent transformation R to the word
representations and predicting the next word at each
step. In detail, the computation of each P (fi|f1:i?1)
proceeds recursively. For 1 < i < m,
h1 = ?(I · v(f1)) (3a)
hi+1 = ?(R · hi + I · v(fi+1)) (3b)
oi+1 = O · hi (3c)
and the conditional distribution is given by,
P (fi = v|f1:i?1) =
exp (oi,v)
?V
v=1 exp(oi,v)
(4)
In Eq. 3, ? is a nonlinear function such as tanh. Bias
values bh and bo are included in the computation. An
illustration of the RLM is given in Fig. 1.
The RLM is trained by backpropagation through
time (Mikolov et al., 2010). The error in the pre-
dicted distribution calculated at the output layer is
1702
backpropagated through the recurrent layers and cu-
mulatively added to the errors of the previous predic-
tions for a given number d of steps. The procedure is
equivalent to standard backpropagation over a RLM
that is unravelled to depth d as in Fig. 1.
RCTMs may be thought of as RLMs, in which
the predicted distributions for each word fi are con-
ditioned on the source sentence e. We next define
two conditioning architectures each giving rise to a
specific RCTM.
3 Recurrent Continuous Translation
Model I
The RCTM I uses a convolutional sentence model
(CSM) in the conditioning architecture. The CSM
creates a representation for a sentence that is pro-
gressively built up from representations of the n-
grams in the sentence. The CSM embodies a hierar-
chical structure. Although it does not make use of an
explicit parse tree, the operations that generate the
representations act locally on small n-grams in the
lower layers of the model and act increasingly more
globally on the whole sentence in the upper layers
of the model. The lack of the need for a parse tree
yields two central advantages over sentence models
that require it (Grefenstette et al., 2011; Socher et
al., 2012). First, it makes the model robustly appli-
cable to a large number of languages for which accu-
rate parsers are not available. Secondly, the transla-
tion probability distribution over the target sentences
does not depend on the chosen parse tree.
The RCTM I conditions the probability of each
target word fi on the continuous representation of the
source sentence e generated through the CSM. This
is accomplished by adding the sentence representa-
tion to each hidden layer hi in the target recurrent
language model. We next describe the procedure in
more detail, starting with the CSM itself.
3.1 Convolutional Sentence Model
The CSM models the continuous representation of
a sentence based on the continuous representations
of the words in the sentence. Let e = e1...ek be
a sentence in a language and let v(ei) ? Rq×1 be
the continuous representation of the word ei. Let
Ee ? Rq×k be the sentence matrix for e defined by,
Ee:,i = v(ei) (5)
(K    M)
*
:,1
M M M
:,1
:,2
:,3
the cat
sat
on the
mat
e
E 
e
K 
2
K 
3
L  
3
K 
i
   :,1
K 
i
   :,2
K 
i
   :,3
i
Figure 2: A CSM for a six word source sentence e and the
computed sentence representation e. K2,K3 are weight
matrices and L3 is a top weight matrix. To the right, an
instance of a one-dimensional convolution between some
weight matrix Ki and a generic matrix M that could for
instance correspond to Ee2. The color coding of weights
indicates weight sharing.
The main component of the architecture of the CSM
is a sequence of weight matrices (Ki)2?i?r that cor-
respond to the kernels or filters of the convolution
and can be thought of as learnt feature detectors.
From the sentence matrix Ee the CSM computes a
continuous vector representation e ? Rq×1 for the
sentence e by applying a sequence of convolutions
to Ee whose weights are given by the weight matri-
ces. The weight matrices and the sequence of con-
volutions are defined next.
We denote by (Ki)2?i?r a sequence of weight
matrices where each Ki ? Rq×i is a matrix of i
columns and r = d
?
2Ne, where N is the length of
the longest source sentence in the training set. Each
row of Ki is a vector of i weights that is treated as
the kernel or filter of a one-dimensional convolution.
Given for instance a matrix M ? Rq×j where the
number of columns j ? i, each row of Ki can be
convolved with the corresponding row in M, result-
ing in a matrix Ki ?M, where ? indicates the con-
volution operation and (Ki ?M) ? Rq×(j?i+1). For
i = 3, the value (Ki ?M):,a is computed by:
Ki:,1M:,a+K
i
:,2M:,a+1 +K
i
:,3M:,a+2 (6)
where  is component-wise vector product. Ap-
plying the convolution kernel Ki yields a matrix
(Ki?M) that has i?1 columns less than the original
matrix M.
Given a source sentence of length k, the CSM
convolves successively with the sentence matrix Ee
1703
the sequence of weight matrices (Ki)2?i?r, one af-
ter the other starting with K2 as follows:
Ee1 = E
e (7a)
Eei+1 = ?(K
i+1 ?Eei ) (7b)
After a few convolution operations, Eei is either a
vector in Rq×1, in which case we obtained the de-
sired representation, or the number of columns in
Eei is smaller than the number i + 1 of columns in
the next weight matrix Ki+1. In the latter case, we
equally obtain a vector in Rq×1 by simply apply-
ing a top weight matrix Lj that has the same num-
ber of columns as Eei . We thus obtain a sentence
representation e ? Rq×1 for the source sentence e.
Note that the convolution operations in Eq. 7b are
interleaved with non-linear functions ?. Note also
that, given the different levels at which the weight
matrices Ki and Li are applied, the top weight
matrix Lj comes from an additional sequence of
weight matrices (Li)2?i?r distinct from (Ki)2?i?r.
Fig. 2 depicts an instance of the CSM and of a one-
dimensional convolution.2
3.2 RCTM I
As defined in Sect. 2, the RCTM I models the condi-
tional probability P (f|e) of a sentence f = f1, ..., fm
in a target language F being the translation of a sen-
tence e = e1, ..., ek in a source language E. Accord-
ing to Eq. 1, the RCTM I explicitly computes the
conditional distributions P (fi|f1:i?1, e). The archi-
tecture of the RCTM I comprises a source vocabu-
lary V E and a target vocabulary V F, two sequences
of weight matrices (Ki)2?i?r and (Li)2?i?r that
are part of the constituent CSM, transformations
I ? Rq×|V
F|, R ? Rq×q and O ? R|V
F|×q that are
part of the constituent RLM and a sentence transfor-
mation S ? Rq×q. We write e = csm(e) for the
output of the CSM with e as the input sentence.
The computation of the RCTM I is a simple mod-
ification to the computation of the RLM described in
Eq. 3. It proceeds recursively as follows:
s = S · csm(e) (8a)
h1 = ?(I · v(f1) + s) (8b)
hi+1 = ?(R · hi + I · v(fi+1) + s) (8c)
oi+1 = O · hi (8d)
2For a formal treatment of the construction, see (Kalchbren-
ner and Blunsom, 2013).
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. ? is a nonlinear func-
tion and bias values are included throughout the
computation. Fig. 3 illustrates an RCTM I.
Two aspects of the RCTM I are to be remarked.
First, the length of the target sentence is predicted
by the target RLM itself that by its architecture has
a bias towards shorter sentences. Secondly, the rep-
resentation of the source sentence e constraints uni-
formly all the target words, contrary to the fact that
the target words depend more strongly on certain
parts of the source sentence and less on other parts.
The next model proposes an alternative formulation
of these aspects.
4 Recurrent Continuous Translation
Model II
The central idea behind the RCTM II is to first es-
timate the length m of the target sentence indepen-
dently of the main architecture. Given m and the
source sentence e, the model constructs a represen-
tation for the n-grams in e, where n is set to 4. Note
that each level of the CSM yields n-gram represen-
tations of e for a specific value of n. The 4-gram
representation of e is thus constructed by truncat-
ing the CSM at the level that corresponds to n = 4.
The procedure is then inverted. From the 4-gram
representation of the source sentence e, the model
builds a representation of a sentence that has the
predicted length m of the target. This is similarly
accomplished by truncating the inverted CSM for a
sentence of length m.
We next describe in detail the Convolutional n-
gram Model (CGM). Then we return to specify the
RCTM II.
4.1 Convolutional n-gram model
The CGM is obtained by truncating the CSM at the
level where n-grams are represented for the chosen
value of n. A column g of a matrix Eei obtained
according to Eq. 7 represents an n-gram from the
source sentence e. The value of n corresponds to
the number of word vectors from which the n-gram
representation g is constructed; equivalently, n is
the span of the weights in the CSM underneath g
(see Fig. 2-3). Note that any column in a matrix
Eei represents an n-gram with the same span value
n. We denote by gram(Eei ) the size of the n-grams
1704
RCTM IIRCTM I
P( f | e ) 
P( f | m, e )
e 
e 
e 
F 
T 
S 
S 
csm 
cgm 
icgm 
E 
F 
g 
g 
Figure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations while lines are
vector transformations corresponding to columns of weight matrices.
represented by Eei . For example, for a sufficiently
long sentence e, gram(Ee2) = 2, gram(E
e
3) = 4,
gram(Ee4) = 7. We denote by cgm(e, n) that matrix
Eei from the CSM that represents the n-grams of the
source sentence e.
The CGM can also be inverted to obtain a repre-
sentation for a sentence from the representation of
its n-grams. We denote by icgm the inverse CGM,
which depends on the size of the n-gram represen-
tation cgm(e, n) and on the target sentence length
m. The transformation icgm unfolds the n-gram
representation onto a representation of a target sen-
tence with m words. The architecture corresponds
to an inverted CGM or, equivalently, to an inverted
truncated CSM (Fig. 3). Given the transformations
cgm and icgm, we now detail the computation of the
RCTM II.
4.2 RCTM II
The RCTM II models the conditional probability
P (f|e) by factoring it as follows:
P (f|e) = P (f|m, e) · P (m|e) (9a)
=
m?
i=1
P (fi+1|f1:i,m, e) · P (m|e) (9b)
and computing the distributions P (fi+1|f1:i,m, e)
and P (m|e). The architecture of the RCTM II
comprises all the elements of the RCTM I together
with the following additional elements: a translation
transformation Tq×q and two sequences of weight
matrices (Ji)2?i?s and (Hi)2?i?s that are part of
the icgm3.
The computation of the RCTM II proceeds recur-
sively as follows:
Eg = cgm(e, 4) (10a)
Fg:,j = ?(T ·E
g
:,j) (10b)
F = icgm(Fg,m) (10c)
h1 = ?(I · v(f1) + S · F:,1) (10d)
hi+1 = ?(R · hi + I · v(fi+1) + S · F:,i+1) (10e)
oi+1 = O · hi (10f)
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. Note how each re-
constructed vector F:,i is added successively to the
corresponding layer hi that predicts the target word
fi. The RCTM II is illustrated in Fig. 3.
3Just like r the value s is small and depends on the length
of the source and target sentences in the training set. See
Sect. 5.1.2.
1705
For the separate estimation of the length of the
translation, we estimate the conditional probability
P (m|e) by letting,
P (m|e) = P (m|k) = Poisson(?k) (11)
where k is the length of the source sentence e and
Poisson(?) is a Poisson distribution with mean ?.
This concludes the description of the RCTM II. We
now turn to the experiments.
5 Experiments
We report on four experiments. The first experiment
considers the perplexities of the models with respect
to reference translations. The second and third ex-
periments test the sensitivity of the RCTM II to the
linguistic aspects of the source sentences. The fi-
nal experiment tests the rescoring performance of
the two models.
5.1 Training
Before turning to the experiments, we describe the
data sets, hyper parameters and optimisation algo-
rithms used for the training of the RCTMs.
5.1.1 Data sets
The training set used for all the experiments com-
prises a bilingual corpus of 144953 pairs of sen-
tences less than 80 words in length from the news
commentary section of the Eighth Workshop on Ma-
chine Translation (WMT) 2013 training data. The
source language is English and the target language
is French. The English sentences contain about
4.1M words and the French ones about 4.5M words.
Words in both the English and French sentences
that occur twice or less are substituted with the
?unknown? token. The resulting vocabularies V E
and V F contain, respectively, 25403 English words
and 34831 French words.
For the experiments we use four different test sets
comprised of the Workshop on Machine Transla-
tion News Test (WMT-NT) sets for the years 2009,
2010, 2011 and 2012. They contain, respectively,
2525, 2489, 3003 and 3003 pairs of English-French
sentences. For the perplexity experiments unknown
words occurring in these data sets are replaced with
the ?unknown? token. The respective 2008 WMT-
NT set containing 2051 pairs of English-French sen-
tences is used as the validation set throughout.
5.1.2 Model hyperparameters
The parameter q that defines the size of the En-
glish vectors v(ei) for ei ? V E, the size of the hid-
den layer hi and the size of the French vectors v(fi)
for v(fi) ? V F is set to q = 256. This yields a
relatively small recurrent matrix and corresponding
models. To speed up training, we factorize the target
vocabulary V F into 256 classes following the proce-
dure in (Mikolov et al., 2011).
The RCTM II uses a convolutional n-gram model
CGM where n is set to 4. For the RCTM I, the num-
ber of weight matrices r for the CSM is 15, whereas
in the RCTM II the number r of weight matrices for
the CGM is 7 and the number s of weight matrices
for the inverse CGM is 9. If a test sentence is longer
than all training sentences and a larger weight matrix
is required by the model, the larger weight matrix is
easily factorized into two smaller weight matrices
whose weights have been trained. For instance, if a
weight matrix of 10 weights is required, but weight
matrices have been trained only up to weight 9, then
one can factorize the matrix of 10 weights with one
of 9 and one of 2. Across all test sets the proportion
of sentence pairs that require larger weight matrices
to be factorized into smaller ones is < 0.1%.
5.1.3 Objective and optimisation
The objective function is the average of the sum
of the cross-entropy errors of the predicted words
and the true words in the French sentences. The En-
glish sentences are taken as input in the prediction
of the French sentences, but they are not themselves
ever predicted. An l2 regularisation term is added to
the objective. The training of the model proceeds by
back-propagation through time. The cross-entropy
error calculated at the output layer at each step is
back-propagated through the recurrent structure for
a number d of steps; for all models we let d = 6.
The error accumulated at the hidden layers is then
further back-propagated through the transformation
S and the CSM/CGM to the input vectors v(ei) of
the English input sentence e. All weights, includ-
ing the English vectors, are randomly initialised and
inferred during training.
The objective is minimised using mini-batch
adaptive gradient descent (Adagrad) (Duchi et al.,
2011). The training of an RCTM takes about 15
hours on 3 multicore CPUs. While our experiments
1706
WMT-NT 2009 2010 2011 2012
KN-5 218 213 222 225
RLM 178 169 178 181
IBM 1 207 200 188 197
FA-IBM 2 153 146 135 144
RCTM I 143 134 140 142
RCTM II 86 77 76 77
Table 1: Perplexity results on the WMT-NT sets.
are relatively small, we note that in principle our
models should scale similarly to RLMs which have
been applied to hundreds of millions of words.
5.2 Perplexity of gold translations
Since the computation of the probability of a trans-
lation under one of the RCTMs is efficient, we can
compute the perplexities of the RCTMs with respect
to the reference translations in the test sets. The per-
plexity measure is an indication of the quality that
a model assigns to a translation. We compare the
perplexities of the RCTMs with the perplexity of the
IBM Model 1 (Brown et al., 1993) and of the Fast-
Aligner (FA-IBM 2) model that is a state-of-the-art
variant of IBM Model 2 (Dyer et al., 2013). We add
as baselines the unconditional target RLM and a 5-
gram target language model with modified Kneser-
Nay smoothing (KN-5). The results are reported in
Tab. 1. The RCTM II obtains a perplexity that is
> 43% lower than that of the alignment based mod-
els and that is 40% lower than the perplexity of the
RCTM I. The low perplexity of the RCTMs suggests
that continuous representations and the transforma-
tions between them make up well for the lack of ex-
plicit alignments. Further, the difference in perplex-
ity between the RCTMs themselves demonstrates
the importance of the conditioning architecture and
suggests that the localised 4-gram conditioning in
the RCTM II is superior to the conditioning with the
whole source sentence of the RCTM I.
5.3 Sensitivity to source sentence structure
The second experiment aims at showing the sensi-
tivity of the RCTM II to the order and position of
words in the English source sentence. To this end,
we randomly permute in the training and testing sets
WMT-NT PERM 2009 2010 2011 2012
RCTM II 174 168 175 178
Table 2: Perplexity results of the RCTM II on the WMT-
NT sets where the words in the English source sentences
are randomly permuted.
the words in the English source sentence. The re-
sults on the permuted data are reported in Tab. 2. If
the RCTM II were roughly comparable to a bag-of-
words approach, there would be no difference under
the permutation of the words. By contrast, the dif-
ference of the results reported in Tab. 2 with those
reported in Tab. 1 is very significant, clearly indicat-
ing the sensitivity to word order and position of the
translation model.
5.3.1 Generating from the RCTM II
To show that the RCTM II is sensitive not only to
word order, but also to other syntactic and semantic
traits of the sentence, we generate and inspect can-
didate translations for various English source sen-
tences. The generation proceeds by sampling from
the probability distribution of the RCTM II itself and
does not depend on any other external resources.
Given an English source sentence e, we let m be
the length of the gold translation and we search the
distribution computed by the RCTM II over all sen-
tences of length m. The number of possible target
sentences of length m amounts to |V |m = 34831m
where V = V F is the French vocabulary; directly
considering all possible translations is intractable.
We proceed as follows: we sample with replace-
ment 2000 sentences from the distribution of the
RCTM II, each obtained by predicting one word at
a time. We start by predicting a distribution for the
first target word, restricting that distribution to the
top 5 most probable words and sampling the first
word of a candidate translation from the restricted
distribution of 5 words. We proceed similarly for
the remaining words. Each sampled sentence has a
well-defined probability assigned by the model and
can thus be ranked. Table 3 gives various English
source sentences and some candidate French trans-
lations generated by the RCTM II together with their
ranks.
The results in Tab. 3 show the remarkable syn-
tactic agreements of the candidate translations; the
1707
English source sentence French gold translation RCTM II candidate translation Rank
the patient is sick . le patient est malade . le patient est insuffisante . 1
le patient est mort . 4
la patient est insuffisante . 23
the patient is dead . le patient est mort . le patient est mort . 1
le patient est de´passe´ . 4
the patient is ill . le patient est malade . le patient est mal . 3
the patients are sick . les patients sont malades . les patients sont confronte´s . 2
les patients sont corrompus . 5
the patients are dead . les patients sont morts . les patients sont morts . 1
the patients are ill . les patients sont malades . les patients sont confronte´s . 5
the patient was ill . le patient e´tait malade . le patient e´tait mal . 2
the patients are not dead . les patients ne sont pas morts . les patients ne sont pas morts . 1
the patients are not sick . les patients ne sont pas malades . les patients ne sont pas ?unknown? . 1
les patients ne sont pas mal . 6
the patients were saved . les patients ont e´te´ sauve´s . les patients ont e´te´ sauve´es . 6
Table 3: English source sentences, respective translations in French and candidate translations generated from the
RCTM II and ranked out of 2000 samples according to their decreasing probability. Note that end of sentence dots (.)
are generated as part of the translation.
WMT-NT 2009 2010 2011 2012
RCTM I + WP 19.7 21.1 22.5 21.5
RCTM II + WP 19.8 21.1 22.5 21.7
cdec (12 features) 19.9 21.2 22.6 21.8
Table 4: Bleu scores on the WMT-NT sets of each RCTM
linearly interpolated with a word penalty WP. The cdec
system includes WP as well as five translation models and
two language modelling features, among others.
large majority of the candidate translations are fully
well-formed French sentences. Further, subtle syn-
tactic features such as the singular or plural ending
of nouns and the present and past tense of verbs are
well correlated between the English source and the
French candidate targets. Finally, the meaning of
the English source is well transferred on the French
candidate targets; where a correlation is unlikely or
the target word is not in the French vocabulary, a se-
mantically related word or synonym is selected by
the model. All of these traits suggest that the RCTM
II is able to capture a significant amount of both
syntactic and semantic information from the English
source sentence and successfully transfer it onto the
French translation.
5.4 Rescoring and BLEU Evaluation
The fourth experiment tests the ability of the RCTM
I and the RCTM II to choose the best translation
among a large number of candidate translations pro-
duced by another system. We use the cdec sys-
tem to generate a list of 1000 best candidate trans-
lations for each English sentence in the four WMT-
NT sets. We compare the rescoring performance of
the RCTM I and the RCTM II with that of the cdec
itself. cdec employs 12 engineered features includ-
ing, among others, 5 translation models, 2 language
model features and a word penalty feature (WP). For
the RCTMs we simply interpolate the log probabil-
ity assigned by the models to the candidate transla-
tions with the word penalty feature WP, tuned on the
validation data. The results of the experiment are
reported in Tab. 4.
While there is little variance in the resulting Bleu
scores, the performance of the RCTMs shows that
their probabilities correlate with translation qual-
ity. Combining a monolingual RLM feature with
the RCTMs does not improve the scores, while re-
ducing cdec to just one core translation probability
and language model features drops its score by two
to five tenths. These results indicate that the RCTMs
have been able to learn both translation and language
modelling distributions.
1708
6 Conclusion
We have introduced Recurrent Continuous Transla-
tion Models that comprise a class of purely contin-
uous sentence-level translation models. We have
shown the translation capabilities of these models
and the low perplexities that they obtain with respect
to reference translations. We have shown the ability
of these models at capturing syntactic and semantic
information and at estimating during reranking the
quality of candidate translations.
The RCTMs offer great modelling flexibility due
to the sensitivity of the continuous representations to
conditioning information. The models also suggest
a wide range of potential advantages and extensions,
from being able to include discourse representations
beyond the single sentence and multilingual source
representations, to being able to model morpholog-
ically rich languages through character-level recur-
rences.
References
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7–12. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of ibm model 2. In Proc. of NAACL.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics. Forthcoming.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Convolutional Neural Networks for Discourse Com-
positionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In HLT-NAACL, pages 39–48.
Tomas Mikolov and Geoffrey Zweig. 2012. Context de-
pendent recurrent neural network language model. In
SLT, pages 234–239.
Tomas Mikolov, Martin Karafia´t, Lukas Burget, Jan Cer-
nocky´, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Takao
Kobayashi, Keikichi Hirose, and Satoshi Nakamura,
editors, INTERSPEECH, pages 1045–1048. ISCA.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky´, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model. In
ICASSP, pages 5528–5531. IEEE.
Holger Schwenk, Daniel De´chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In ACL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071–1080.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801–809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositional-
ity Through Recursive Matrix-Vector Spaces. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Ilya Sutskever, James Martens, and Geoffrey E. Hinton.
2011. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer, editors, ICML,
pages 1017–1024. Omnipress.
1709
