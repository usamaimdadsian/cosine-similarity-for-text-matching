Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580–1590,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
A convex relaxation for weakly supervised relation extraction
´
Edouard Grave
EECS Department
University of California, Berkeley
grave@berkeley.edu
Abstract
A promising approach to relation extrac-
tion, called weak or distant supervision,
exploits an existing database of facts as
training data, by aligning it to an unla-
beled collection of text documents. Using
this approach, the task of relation extrac-
tion can easily be scaled to hundreds of
different relationships. However, distant
supervision leads to a challenging multi-
ple instance, multiple label learning prob-
lem. Most of the proposed solutions to this
problem are based on non-convex formu-
lations, and are thus prone to local min-
ima. In this article, we propose a new
approach to the problem of weakly su-
pervised relation extraction, based on dis-
criminative clustering and leading to a
convex formulation. We demonstrate that
our approach outperforms state-of-the-art
methods on the challenging dataset intro-
duced by Riedel et al. (2010).
1 Introduction
Information extraction refers to the broad task
of automatically extracting structured information
from unstructured documents. An example is the
extraction of named entities and the relations be-
tween those entities from natural language texts.
In the age of the world wide web and big data,
information extraction is quickly becoming perva-
sive. For example, in 2013, more than 130, 000
scientific articles were published about cancer.
Keeping track with that quantity of information
is almost impossible, and it is thus of utmost im-
portance to transform the knowledge contained in
this massive amount of documents into structured
databases.
Traditional approaches to information extrac-
tion relies on supervised learning, yielding high
Knowledge base
r e
1
e
2
BornIn Lichtenstein New York City
DiedIn Lichtenstein New York City
Sentences Latent labels
Roy Lichtenstein was born in
New York City, into an upper-
middle-class family.
BornIn
In 1961, Leo Castelli started
displaying Lichtenstein’s work
at his gallery in New York.
None
Lichtenstein died of pneumonia
in 1997 in New York City.
DiedIn
Figure 1: An example of a knowledge database
comprising two facts and training sentences ob-
tained by aligning this database to unlabeled text.
precision and recall results (Zelenko et al.,
2003). Unfortunately, these approaches need large
amount of labeled data, and thus do not scale well
to the great number of different types of fact found
on the Web or in scientific articles. A promising
approach, called distant or weak supervision, is
to exploit an existing database of facts as training
data, by aligning it to an unlabeled collection of
text documents (Craven and Kumlien, 1999).
In this article, we are interested in weakly super-
vised extraction of binary relations. A challenge
pertaining to weak supervision is that the obtained
training data is noisy and ambiguous (Riedel et
al., 2010). Let us start with an example: if the
fact Attended(Turing, King
?
s College) exists
in the knowledge database and we observe the sen-
tence
Turing studied as an undergraduate from
1931 to 1934 at King’s College, Cambridge.
which contains mentions of both entities Turing
1580
and King
?
s College, then this sentence might ex-
press the fact that Alan Turing attended King’s
College, and thus, might be a useful example for
learning to extract the relation Attended. How-
ever, the sentence
Celebrations for the centenary of Alan Tur-
ing are being planned at King’s College.
also contains mentions of Turing and
King
?
s College, but do not express the re-
lation Attended. Thus, weak supervision lead
to noisy examples. As noted by Riedel et al.
(2010), such negative extracted sentences for
existing facts can represent more than 30% of
the data. Moreover, a given pair of entities,
such as (Roy Lichtenstein, New York City),
car verify multiple relations, such as BornIn
and DiedIn. Weak supervision thus lead to
ambiguous examples.
This challenge is illustrated in Fig. 1. A solution
to address it is to formulate the task of weakly su-
pervised relation extraction as a multiple instance,
multiple label learning problem (Hoffmann et al.,
2011; Surdeanu et al., 2012). However, these for-
mulations are often non-convex and thus suffer
from local minimum.
In this article, we make the following contribu-
tions:
• We propose a new convex relaxation for the
problem of weakly supervised relation ex-
traction, based on discriminative clustering,
• We propose an efficient algorithm to solve the
associated convex program,
• We demonstrate that our approach obtains
state-of-the-art results on the dataset intro-
duced by Riedel et al. (2010).
To our knowledge, this paper is the first to propose
a convex formulation for solving the problem of
weakly supervised relation extraction.
2 Related work
Supervised learning. Many approaches based
on supervised learning have been proposed to
solve the problem of relation extraction, and the
corresponding literature is to large to be summa-
rized here. One of the first supervised method for
relation extraction was inspired by syntactic pars-
ing: the system described by Miller et al. (1998)
combines syntactic and semantic knowledge, and
thus, part-of-speech tagging, parsing, named en-
tity recognition and relation extraction all happen
at the same time. The problem of relation ex-
traction was later formulated as a classification
problem: Kambhatla (2004) proposed to solve this
problem using maximum entropy models using
lexical, syntactic and semantic features. Kernel
methods for relation extraction, based on shallow
parse trees or dependency trees were introduced
by Zelenko et al. (2003), Culotta and Sorensen
(2004) and Bunescu and Mooney (2005).
Unsupervised learning. The open information
extraction paradigm, simultaneously proposed by
Shinyama and Sekine (2006) and Banko et al.
(2007), does not rely on any labeled data or even
existing relations. Instead, open information ex-
traction systems only use an unlabeled corpus, and
output a set of extracted relations. Such systems
are based on clustering (Shinyama and Sekine,
2006) or self-supervision (Banko et al., 2007).
One of the limitations of these systems is the fact
that they extract uncanonicalized relations.
Weakly supervised learning. Weakly super-
vised learning refers to a broad class of meth-
ods, in which the learning system only have ac-
cess to partial, ambiguous and noisy labeling.
Craven and Kumlien (1999) were the first to pro-
pose a weakly supervised relation extractor. They
aligned a knowledge database (the Yeast Protein
Database) with scientific articles mentioning a par-
ticular relation, and then used the extracted sen-
tences to learn a classifier for extracting that rela-
tion.
Later, many different sources of weak label-
ings have been considered. Bellare and McCallum
(2007) proposed a method to extract bibliographic
relations based on conditional random fields and
used a database of BibTex entries as weak super-
vision. Wu and Weld (2007) described a method
to learn relations based on Wikipedia infoboxes.
Knowledge databases, such as Freebase
1
(Mintz et
al., 2009; Sun et al., 2011) and YAGO
2
(Nguyen
and Moschitti, 2011) were also considered as a
source of weak supervision.
Multiple instance learning. The methods we
previously mentionned transform the weakly su-
pervised problem into a fully supervised one, lead-
ing to noisy training datasets (see Fig. 1). Mul-
1
www.freebase.com
2
www.mpi-inf.mpg.de/yago-naga/yago
1581
tiple instance learning (Dietterich et al., 1997) is
a paradigm in which the learner receives bags of
examples instead of individual examples. A pos-
itively labeled bag contains at least one positive
example, but might also contains negative exam-
ples. In the context of relation extraction, Bunescu
and Mooney (2007) introduced a kernel method
for multiple instance learning, while Riedel et al.
(2010) proposed a solution based on a graphical
model.
Both these methods allow only one label per
bag, which is an asumption that is not true for
relation extraction (see Fig. 1). Thus, Hoffmann
et al. (2011) proposed a multiple instance, multi-
ple label method, based on an undirected graphical
model, to solve the problem of weakly supervised
relation extraction. Finally, Surdeanu et al. (2012)
also proposed a graphical model to solve this prob-
lem. One of their main contributions is to cap-
ture dependencies between relation labels, such as
the fact that two labels cannot be generated jointly
(e.g. the relations SpouseOf and BornIn).
Discriminative clustering. Our approach is
based on the discriminative clustering framework,
introduced by Xu et al. (2004). The goal of dis-
criminative clustering is to find a labeling of the
data points leading to a classifier with low classifi-
cation error. Different formulations of discrimina-
tive clustering have been proposed, based on sup-
port vector machines (Xu et al., 2004), the squared
loss (Bach and Harchaoui, 2007) or the logistic
loss (Joulin et al., 2010). A big advantage of dis-
criminative clustering is that weak supervision or
prior information can easily be incorporated. Our
work is closely related to the method proposed by
Bojanowski et al. (2013) for learning the names of
characters in movies.
3 Weakly supervised relation extraction
In this article, our goal is to extract binary
relations between entities from natural lan-
guage text. Given a set of entities, a binary
relation r is a collection of ordered pairs of
entities. The statement that a pair of entities
(e
1
, e
2
) belongs to the relation r is denoted by
r(e
1
, e
2
) and this triple is called a fact or relation
instance. For example, the fact that Ernest
Hemingway was born in Oak Park is denoted
by BornIn(Ernest Hemingway, Oak Park).
A given pair of entities, such as
(Edouard Manet, Paris), can belong to
different relations, such as BornIn and DiedIn.
An entity mention is a contiguous sequence of
tokens refering to an entity, while a pair mention
or relation mention candidate is a sequence of text
in which a pair of entities is mentioned. In the
following, relation mention candidates will be re-
stricted to pair of entities that are mentioned in the
same sentence. For example, the sentence:
Ernest Hemingway was born in Oak Park.
contains two entity mentions, corresponding
to two relation mention candidates. In-
deed, the pairs (Hemingway, Oak Park) and
(Oak Park, Hemingway) are two distinct pairs of
entities, where only the first one verifies the rela-
tion BornIn.
Given a text corpus, aggregate extraction corre-
sponds to the task of extracting a set of facts, such
that each extracted fact is expressed at least once in
the corpus. On the other hand, the task of senten-
tial extraction corresponds to labeling each rela-
tion mention candidate by the relation it expresses,
or by a None label if it does not express any rela-
tion. Given a solution to the sentential extraction
problem, it is possible to construct a solution for
the aggregate extraction problem by returning all
the facts that were detected. We will follow this
approach, by building an instance level classifier,
and aggregating the results by extracting the facts
that were detected at least once in the corpus.
In the following, we will describe a method to
learn such a classifier using a database of facts in-
stead of a set of labeled sentences. This setting
is known as distant supervision or weak supervi-
sion, since we do not have access to labeled data
on which we could directly train a sentence level
relation extractor.
4 General approach
In this section, we propose a two step procedure to
solve the problem of weakly supervised relation
extraction:
1. First, we describe a method to infer the re-
lation labels corresponding to each relation
mention candidate of our training set,
2. Second, we train a supervised instance level
relation extractor, using the labels infered
during step 1.
In the second step of our approach, we will simply
use a multinomial logistic regression model. We
1582
(Lichtenstein, New York City)
Roy Lichtenstein was
born in New York City.
Lichtenstein left New
York to study in Ohio.
BornIn
DiedIn
N relation mention candidates
represented by vectors x
n
I pairs of entities p
i
K relations
E
in
R
ik
Figure 2: Instance of the weakly supervised relation extraction problem, with notations used in the text.
now describe the approach we propose for the first
step.
4.1 Notations
Let (p
i
)
1?i?I
be a collection of I pairs of entities.
We suppose that we have N relation mention can-
didates, represented by the vectors (x
n
)
1?n?N
.
LetE ? R
I×N
be a matrix such thatE
in
= 1 if the
relation mention candidate n corresponds to the
pair of entities i, and E
in
= 0 otherwise. The ma-
trix E thus indicates which relation mention can-
didate corresponds to which pair of entities. We
suppose that we have K relations, indexed by the
integers {1, ...,K}. Let R ? R
I×K
be a matrix
such that R
ik
= 1 if the pair of entities i verifies
the relation k, and R
ik
= 0 otherwise. The matrix
R thus represents the knowledge database. See
Fig. 2 for an illustration of these notations.
4.2 Problem formulation
Our goal is to infer a binary matrix
Y ? {0, 1}
N×(K+1)
, such that Y
nk
= 1 if
the relation mention candidate n express the
relation k and Y
nk
= 0 otherwise (and thus, the
integer K + 1 represents the relation None).
We take an approach inspired by the discrimi-
native clustering framework of Xu et al. (2004).
We are thus looking for a (K + 1)-class indicator
matrix Y, such that the classification error of an
optimal multiclass classifier f is minimum. Given
a multiclass loss function ` and a regularizer ?,
this problem can be formulated as:
min
Y
min
f
N
?
n=1
`(y
n
, f(x
n
)) + ?(f),
s.t. Y ? Y
where y
n
is the nth line of Y. The constraints
Y ? Y are added in order to take into account
the information from the weak supervision. We
will describe in the next section what kind of con-
straints are considered.
4.3 Weak supervision by constraining Y
In this section, we show how the information
from the knowledge base can be expressed as con-
straints on the matrix Y.
First, we suppose that each relation mention
candidate express exactly one relation (including
the None relation). This means that the matrix Y
contains exactly one 1 per line, which is equivalent
to the constraint:
?n ? {1, ..., N},
K
?
k=1
Y
nk
= 1.
Second, if the pair i of entities verifies the rela-
tion k we suppose that at least one relation men-
tion candidate indeed express that relation. Thus
we want to impose that for at least one relation
mention candidate n such that E
in
= 1, we have
Y
nk
= 1. This is equivalent to the constraint:
?(i, k) such that R
ik
= 1,
N
?
n=1
E
in
Y
nk
? 1.
Third, if the pair i of entities does not verify the re-
lation k, we suppose that no relation mention can-
didate express that relation. Thus, we impose that
for all mention candidate n such that E
in
= 1, we
have Y
nk
= 0. This is equivalent to the constraint:
?(i, k) such that R
ik
= 0,
N
?
n=1
E
in
Y
nk
= 0.
Finally, we do not want too many relation men-
tion candidates to be classified as None. We thus
impose
?i ? {1, ..., I},
N
?
n=1
E
in
Y
n(K+1)
? c
N
?
n=1
E
in
,
where c is the proportion of relation mention can-
didates that do not express a relation, for entity
pairs that appears in the knowledge database.
1583
We can rewrite these constraints using only ma-
trix operations in the following way:
Y1 = 1
(EY) ? S ?
˜
R, (1)
where ? is the Hadamard product (a.k.a. the ele-
mentwise product), the matrix S ? R
I×(K+1)
is
defined by
S
ik
=
{
1 if R
ik
= 1
?1 if R
ik
= 0 or k = K + 1,
and the matrix
˜
R ? R
I×(K+1)
is defined by
˜
R = [R,?cE1].
The set Y is thus defined as the set of matrices
Y ? {0, 1}
N×(K+1)
that verifies those two linear
constraints. It is important to note that besides the
boolean constraints, the two other constraints are
convex.
5 Squared loss and convex relaxation
In this section, we describe the problem we ob-
tain when using the squared loss, and its associated
convex relaxation. We then introduce an efficient
algorithm to solve this problem, by computing its
dual.
5.1 Primal problem
Following Bach and Harchaoui (2007), we use lin-
ear classifiers W ? R
D×(K+1)
, the squared loss
and the squared `
2
-norm as the regularizer. In that
case, our formulation becomes:
min
Y,W
1
2
?Y ?XW?
2
F
+
?
2
?W?
2
F
,
s.t. Y ? {0, 1}
N×(K+1)
Y1 = 1,
(EY) ? S ? R.
where ? · ?
F
is the Frobenius norm and the ma-
trix X = [x
1
, ...,x
N
]
>
? R
N×D
represents the
relation mention candidates. Thanks to using the
squared loss, we have a closed form solution for
the matrix W:
W = (X
>
X + ?I
D
)
?1
X
>
Y.
Replacing the matrix W by its optimal solution,
we obtain the following cost function:
min
Y
1
2
Y
>
(I
N
?X(X
>
X + ?I
D
)
?1
X
>
)Y.
Then, by applying the Woodbury matrix identity
and relaxing the constraint Y ? {0, 1}
N×(K+1)
into Y ? [0, 1]
N×(K+1)
, we obtain the following
convex quadratic problem in Y:
min
Y
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
,
s.t. Y ? 0,
Y1 = 1,
(EY) ? S ? R.
Since the inequality constraints might be in-
feasible, we add the penalized slack variables
? ? R
I×(K+1)
, finally obtaining:
min
Y,?
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
+ µ???
1
s.t. Y ? 0, ? ? 0,
Y1 = 1,
(EY) ? S ? R? ?.
This convex problem is a quadratic program. In
the following section, we will describe how to
solve this problem efficiently, by exploiting the
structure of its dual problem.
5.2 Dual problem
The matrix Q = (XX
>
+ ?I
N
) appearing in the
quadratic program is an N by N matrix, where
N is the number of mention relation candidates.
Computing its inverse is thus expensive, since N
can be large. Instead, we propose to solve the
dual of this problem. Introducing dual variables
? ? R
I×(K+1)
, ? ? R
N×(K+1)
and ? ? R
N
,
the dual problem is equal to
min
?,?,?
1
2
tr
(
Z
>
QZ
)
? tr
(
?
>
R
)
? ?
>
1
s.t. 0 ? ?
ik
? µ, 0 ? ?
nk
,
where
Z = E
>
(S ? ?) + ? + ?1
>
.
The derivation of this dual problem is given in Ap-
pendix A.
Solving the dual problem instead of the primal
has two main advantages. First, the dual does not
depend on the inverse of the matrix Q, while the
primal does. Since traditional features used for re-
lation extraction are indicators of lexical, syntactic
and named entities properties of the relation men-
tion candidates, the matrix X is extremely sparse.
1584
Using the dual problem, we can thus exploit the
sparsity of the matrix X in the optimization pro-
cedure. Second, the constraints imposed on dual
variables are simpler than constraints imposed on
primal variables. Again, we will exploit this struc-
ture in the proposed optimization procedure.
Given a solution of the dual problem, the asso-
ciated primal variable Y is equal to:
Y = (XX
>
+ ?I
N
)Z.
Thus, we do not need to compute the inverse of the
matrix (XX
>
+ ?I
N
) to obtain a solution to the
primal problem once we have solved the dual.
5.3 Optimization of the dual problem
We propose to solve the dual problem using
the accelerated projected gradient descent algo-
rithm (Nesterov, 2007; Beck and Teboulle, 2009).
Indeed, computing the gradient of the dual cost
function is efficient, since the matrix X is sparse.
Moreover, the constraints on the dual variables are
simple and it is thus efficient to project onto this
set of constraints. See Appendix B for more de-
tails.
Complexity. The overall complexity of one step
of the accelerated projected gradient descent al-
gorithm is O(NFK), where F is the average
number of features per relation mention candi-
date. This means that the complexity of solving
the quadratic problem corresponding to our ap-
proach is linear with respect to the number N of
relation mention candidates, and thus our algo-
rithm can scale to large datasets.
5.4 Discussion
Before moving to the experimental sections of this
article, we would like to discuss some properties
of our approach.
Kernels. First of all, one should note that our
proposed formulation only depends on the (lin-
ear) kernel matrix XX
T
. It is thus possible to re-
place this matrix by any other kernel. However,
in the case of a general kernel, the optimization
algorithm presented in the previous section has a
quadratic complexity O(KN
2
) with respect to the
number N of relation mention candidates, and it
is thus not applicable as is. We plan to explore the
use of kernels in future work.
Rounding. Given a continuous solution Y ?
[0, 1]
N×(K+1)
of the relaxed problem, a very sim-
ple way to obtain a relation label for each relation
mention candidate of the training set is to com-
pute the orthogonal projection of the matrix Y on
the set of indicator matrices
{
M ? {0, 1}
N×(K+1)
|M1 = 1
}
.
This projection consists in taking the maximum
value along the rows of the matrix Y. It should
be noted that the obtained matrix does not neces-
sarily verify the inequality constraints defined in
Eq. 1. In the following, we will use this rounding,
refered to as argmax rounding, to obtain relation
labels for each relation mention candidate.
6 Dataset and features
In this section, we describe the dataset used in the
experimental section and the features used to rep-
resent the data.
6.1 Dataset
We consider the dataset introduced by Riedel et
al. (2010). This dataset consists of articles from
the New York Times corpus (Sandhaus, 2008),
from which named entities where extracted and
tagged using the Stanford named entity recog-
nizer (Finkel et al., 2005). Consecutive tokens
with the same category were treated as a single
mention. These named entity mentions were then
aligned with the Freebase knowledge database, by
using a string match between the mentions and the
canonical names of entities in Freebase.
6.2 Features
We use the features extracted by Riedel et al.
(2010), which were first introduced by Mintz et
al. (2009). These features capture how two en-
tity mentions are related in a given sentence, based
on syntactic and lexical properties. Lexical fea-
tures include: the sequence of words between the
two entities, a window of k words before the first
entity and after the second entity, the correspond-
ing part-of-speech tags, etc.. Syntactic features are
based on the dependency tree of the sentence, and
include: the path between the two entities, neigh-
bors of the two entities that do not belong to the
path. The OpenNLP
3
part-of-speech tagger and
the Malt parser (Nivre et al., 2007) were used to
extract those features.
3
opennlp.apache.org
1585
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Recall
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Mintz et al. (2009)
Hoffmann et al. (2011)
Surdeanu et al. (2012)
This work
Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of
aggregate extraction.
6.3 Implementation details
In this section, we discuss some important imple-
mentation details.
Kernel normalization. We normalized the ker-
nel matrix XX
>
, so that its diagonal coefficients
are equal to 1. This corresponds to normalizing
the vectors x
n
so that they have a unit `
2
-norm.
Choice of parameters. We kept 20% of the ex-
amples from the training set as a validation set, in
order to choose the parameters of our method. We
then re-train a model on the whole training set, us-
ing the chosen parameters.
7 Experimental evaluation
In this section, we evaluate our approach to weakly
supervised relation extraction by comparing it to
state-of-the art methods.
7.1 Baselines
We now briefly present the different methods we
compare to.
Mintz et al. This baseline corresponds to the
method described by Mintz et al. (2009). We
use the implementation of Surdeanu et al. (2012),
which slightly differs from the original method:
each relation mention candidate is treated inde-
pendently (and not collapsed across mentions for
a given entity pair). This strategy allows to predict
multiple labels for a given entity pair, by OR-ing
the predictions for the different mentions.
Hoffmann et al. This method, introduced by
Hoffmann et al. (2011), is based on probabilis-
tic graphical model of multi-instance multi-label
learning. They proposed a learning method
for this model, based on the perceptron algo-
rithm (Collins, 2002) and a greedy search for the
inference. We use the publicly available code of
Hoffmann et al.
4
.
Surdeanu et al. Finally, we compare our
method to the one described by Surdeanu et al.
(2012). This method is based on a two-layer
graphical model, the first layer corresponding to
4
www.cs.washington.edu/ai/raphaelh/mr/
1586
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
0.0
0.2
0.4
0.6
0.8
1.0
Pr
ec
isi
on
/location/location/contains
/people/person/place_lived
/person/person/nationality
/people/person/place_of_birth
/business/person/company
Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the
task of aggregate extraction.
a relation classifier at the mention level, while the
second layer is aggregating the different predic-
tion for a given entity pair. In particular, this sec-
ond layer capture dependencies between relation
labels, such as the fact that two labels cannot be
generated jointly (e.g. the relations SpouseOf and
BornIn). This model is trained by using hard
discriminative Expectation-Maximization. We use
the publicly available code of Surdeanu et al.
5
.
7.2 Precision / recall curves
Following standard practices in relation extrac-
tion, we report precision/recall curves for the dif-
ferent models. In order to rank aggregate extrac-
tions for our model, the score of an extracted fact
r(e
1
, e
2
) is set to the maximal score of the differ-
ent extractions of that fact. This is sometimes ref-
ered to as the soft-OR function.
7.3 Discussion
Comparison with the state-of-the-art. We re-
port results for the different methods on the dataset
5
nlp.stanford.edu/software/mimlre.shtml
introduced by Riedel et al. (2010) in Fig. 3. We
observe that our approach generally outperforms
the state of the art. Indeed, at equivalent recall,
our method achieves better (or similar) precision
than the other methods, except for very low re-
call (smaller than 0.05). The improvement over
the methods proposed by Hoffmann et al. (2011)
and Surdeanu et al. (2012), which are currently
the best published results on this dataset, can be
as high as 5 points in precision for the same recall
point. Moreover, our method achieves a higher re-
call (0.30) than these two methods (0.25).
Performance per relation. The dataset in-
troduced by Riedel et al. (2010) is highly
unbalanced: for example, the most common
relation, /location/location/contains, rep-
resents almost half of the positive relations, while
some relations are mentioned less than ten times.
We thus decided to also report precision/recall
curves for the five most common relations of
that dataset in Fig. 4. First, we observe that the
perfomances vary a lot from a relation to another.
The frequence of the different relations is not the
1587
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Recall
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Prec
ision
Hoffmann et al. (2011)This work
Figure 5: Precision/recall curves for the task
of sentential extraction, on the manually labeled
dataset of Hoffmann et al. (2011).
only factor in those discrepancies. Indeed, the
relation /people/person/place lived and the
relation /people/person/place of birth
are more frequent than the relation
/business/person/company, but the ex-
traction of the later works much better than the
extraction of the two first.
Upon examination of the data, this can
partly be explained by the fact that al-
most no sentences extracted for the relation
/people/person/place of birth in fact
express this relation. In other words, many
facts present in Freebase are not expressed in
the corpus, and are thus impossible to extract.
On the other hand, most facts for the relation
/people/person/place lived are missing in
Freebase. Therefore, many extractions produced
by our system are considered false, but are in
fact true positives. The problem of incomplete
knowledge base was studied by Min et al. (2013).
Sentential extraction. We finally report preci-
sion/recall curves for the task of sentential extrac-
tion, in Fig. 5, using the manually labeled dataset
of Hoffmann et al. (2011). We observe that for
most values of recall, our method achieves simi-
lar precision that the one proposed by Hoffmann
et al. (2011), while extending the highest recall
from 0.52 to 0.68. Thanks to this higher recall, our
method achieves a highest F1 score of 0.66, com-
pared to 0.61 obtained by the method proposed by
Hoffmann et al. (2011).
Method Runtime
Mintz et al. (2009) 7 min
Hoffmann et al. (2011) 2 min
Surdeanu et al. (2012) 3 hours
This work 3 hours
Table 1: Comparison of running times for the dif-
ferent methods compared in the experimental sec-
tion.
8 Conclusion
In this article, we introduced a new formulation
for weakly supervised relation extraction. Our
method is based on a constrained discriminative
formulation of the multiple instance, multiple la-
bel learning problem. Using the squared loss,
we obtained a convex relaxation of this formula-
tion, allowing us to obtain an approximate solu-
tion to the initial integer quadratic program. Thus,
our method is not sensitive to initialization. We
demonstrated the competitiveness of our approach
on the dataset introduced by Riedel et al. (2010),
on which our method outperforms the state of the
art methods for weakly supervised relation extrac-
tion, on both aggregate and sentential extraction.
As noted earlier, another advantage of our
method is the fact that it is easily kernelizable.
We would like to explore the use of kernels, such
as the ones introduced by Zelenko et al. (2003),
Culotta and Sorensen (2004) and Bunescu and
Mooney (2005), in future work. We believe that
such kernels could improve the relatively low re-
call obtained so far by weakly supervised method
for relation extraction.
Acknowledgments
The author is supported by a grant from Inria
(Associated-team STATWEB) and would like to
thank Armand Joulin for helpful discussions.
References
Francis Bach and Za¨?d Harchaoui. 2007. DIFFRAC: a
discriminative and flexible framework for clustering.
In Adv. NIPS.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI.
Amir Beck and Marc Teboulle. 2009. A fast iterative
shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1).
1588
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth international workshop on in-
formation integration on the web.
Piotr Bojanowski, Francis Bach, Ivan Laptev, Jean
Ponce, Cordelia Schmid, and Josef Sivic. 2013.
Finding actors and actions in movies. In Proceed-
ings of ICCV.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT-EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In Proceedings of the ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In ISMB, volume 1999.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL.
Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial in-
telligence, 89(1).
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the ACL.
Armand Joulin, Jean Ponce, and Francis Bach. 2010.
Efficient optimization for discriminative latent class
models. In Adv. NIPS.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proceedings
of the ACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1998. Algorithms that learn to
extract information. In Proceedings of MUC-7.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
ACL-IJCNLP.
Yurii Nesterov. 2007. Gradient methods for minimiz-
ing composite objective function.
Truc-Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the HLT-NAACL.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP-CoNLL.
Fei Wu and Daniel S Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on informa-
tion and knowledge management.
Linli Xu, James Neufeld, Bryce Larson, and Dale
Schuurmans. 2004. Maximum margin clustering.
In Adv. NIPS.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3.
1589
Appendix A Derivation of the dual
In this section, we derive the dual problem of the
quadratic program of section 5. We introduce dual
variables ? ? R
I×(K+1)
, ? ? R
N×(K+1)
,
? ? R
I×(K+1)
and ? ? R
N
, such that ? ? 0,
? ? 0 and ? ? 0.
The Lagrangian of the problem is
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
+ µ
?
i,k
?
ik
? tr
(
?
>
((EY) ? S?R + ?)
)
? tr(?
>
Y)? tr(?
>
?)? ?
>
(Y1? 1).
To find the dual function g we minimize the La-
grangian over Y and ?. Minimizing over ?, we
find that the dual function is equal to ?? unless
µ??
ik
??
ik
= 0, in which case, we are left with
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
? tr((? ? S)
>
EY)? tr(?
>
Y)? tr(1?
>
Y)
+ tr(?
>
R) + ?
>
1.
Minimizing over Y, we then obtain
Y = (XX
>
+ ?I
N
)(E
>
(S ? ?) + ? + ?1
>
).
Replacing Y by its optimal value, we then obtain
the dual function
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1.
where
Q = (XX
>
+ ?I
N
),
Z = E
>
(S ? ?) + ? + ?1
>
.
Thus, the dual problem is
max
?,?,?
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1
s.t. 0 ? ?
ik
, 0 ? ?
nk
, 0 ? ?
ik
,
µ? ?
ik
? ?
ik
= 0.
We can then eliminate the dual variable ?, since
the constraints ?
ik
= µ ? ?
ik
and ?
ik
? 0 are
equivalent to µ ? ?
ik
. We finally obtain
max
?,?,?
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1
s.t. 0 ? ?
ik
? µ, 0 ? ?
nk
.
Appendix B Optimization details
Gradient of the dual cost function. The gradi-
ent of the dual cost function f with respect to the
dual variables ?, ? and ? is equal to
?
?
f = (XX
>
+ ?I
N
)Z,
?
?
f =
(
(XX
>
+ ?I
N
)ZE
>
)
? S?R,
?
?
f = (XX
>
+ ?I
N
)Z1? 1.
The most expensive step to compute those gra-
dients is to compute the matrix product XX
>
Z.
Since the matrix X is sparse, we efficiently com-
pute this product by first computing the product
X
>
Z, and then by left multiplying the result by
X. The complexity of these two operations is
O(NFK), where F is the average number of fea-
tures per relation mention candidate.
Projecting ? and ?. The componentwise pro-
jection operators associated to the constraints on
? and ? are defined by:
proj
?
(?
nk
) = max(0,?
nk
),
proj
?
(?
ik
) = max(0,min(µ,?
ik
)).
The complexity of projecting ? and ? is O(NK).
Thus, the cost of those operations is ne gligible
compared to the cost of computing the gradients
of the dual cost function.
1590
