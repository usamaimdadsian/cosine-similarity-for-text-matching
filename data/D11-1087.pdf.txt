Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 941–948,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Entire Relaxation Path for Maximum Entropy Problems
Moshe Dubiner
Google
moshe@google.com
Yoram Singer
Google
singer@google.com
Abstract
We discuss and analyze the problem of find-
ing a distribution that minimizes the relative
entropy to a prior distribution while satisfying
max-norm constraints with respect to an ob-
served distribution. This setting generalizes
the classical maximum entropy problems as
it relaxes the standard constraints on the ob-
served values. We tackle the problem by in-
troducing a re-parametrization in which the
unknown distribution is distilled to a single
scalar. We then describe a homotopy between
the relaxation parameter and the distribution
characterizing parameter. The homotopy also
reveals an aesthetic symmetry between the
prior distribution and the observed distribu-
tion. We then use the reformulated problem to
describe a space and time efficient algorithm
for tracking the entire relaxation path. Our
derivations are based on a compact geomet-
ric view of the relaxation path as a piecewise
linear function in a two dimensional space
of the relaxation-characterization parameters.
We demonstrate the usability of our approach
by applying the problem to Zipfian distribu-
tions over a large alphabet.
1 Introduction
Maximum entropy (max-ent) models and its dual
counterpart, logistic regression, is a popular and ef-
fective tool in numerous natural language process-
ing tasks. The principle of maximum entropy was
spelled out explicitly by E.T. Jaynes (1968). Ap-
plications of maximum entropy approach to natural
language processing are numerous. A notable ex-
ample and probably one of the earliest usages and
generalizations of the maximum entropy principle
to language processing is the work of Berger, Della
Pietra×2, and Lafferty (Berger et al., 1996, Della
Pietra et al., 1997). The original formulation of
max-ent cast the problem as the task of finding the
distribution attaining the highest entropy subject to
equality constraints. While this formalism is aes-
thetic and paves the way to a simple dual in the form
of a unique Gibbs distribution (Della Pietra et al.,
1997), it does not provide sufficient tools to deal
with input noise and sparse representation of the
target Gibbs distribution. To mitigate these issues,
numerous relaxation schemes of the equality con-
straints have been proposed. A notable recent work
by Dudik, Phillips, and Schapire (2007) provided a
general constraint-relaxation framework. See also
the references therein for an in depth overview of
other approaches and generalizations of max-ent.
The constraint relaxation surfaces a natural param-
eter, namely, a relaxation value. The dual form of
this free parameter is the regularization value of pe-
nalized logistic regression problems. Typically this
parameter is set by experimentation using cross val-
idation technique. The relaxed maximum-entropy
problem setting is the starting point of this paper.
In this paper we describe and analyze a frame-
work for efficiently tracking the entire relaxation
path of constrained max-ent problems. We start in
Sec. 2 with a generalization in which we discuss the
problem of finding a distribution that minimizes the
relative entropy to a given prior distribution while
satisfying max-norm constraints with respect to an
observed distribution. In Sec. 3 we tackle the prob-
lem by introducing a re-parametrization in which the
941
unknown distribution is distilled to a single scalar.
We next describe in Sec. 4 a homotopy between the
relaxation parameter and the distribution character-
izing parameter. This formulation also reveals an
aesthetic symmetry between the prior distribution
and the observed distribution. We use the reformu-
lated problem to describe in Secs. 5-6 space and time
efficient algorithms for tracking the entire relaxation
path. Our derivations are based on a compact ge-
ometric view of the relaxation path as a piecewise
linear function in a two dimensional space of the
relaxation-characterization parameters. In contrast
to common homotopy methods for the Lasso Os-
borne et al. (2000), our procedure for tracking the
max-ent homotopy results in an uncharacteristically
low complexity bounds thus renders the approach
applicable for large alphabets. We provide prelim-
inary experimental results with Zipf distributions in
Sec. 8 that demonstrate the merits of our approach.
Finally, we conclude in Sec. 9 with a brief discus-
sion of future directions.
2 Notations and Problem Setting
We denote vectors with bold face letters, e.g. v.
Sums are denoted by calligraphic letters, e.g. M =?
j mj . We use the shorthand [n] to denote the set
of integers {1, . . . , n}. The n’th dimensional sim-
plex, denoted ?, consists of all vectors p such that,?n
j=1 pj = 1 and for all j ? [n], pj ? 0. We gen-
eralize this notion to multiplicity weighted vectors.
Formally, we say that a vector p with multiplicity m
is in the simplex, (p,m) ? ?, if ?nj=1 mjpj = 1,
and for all j ? [n], pj ? 0, and mj ? 0.
The generalized relaxed maximum-entropy prob-
lem is concerned with obtaining an estimate p, given
a prior distribution u and an observed distribution q
such that the relative entropy between p and u is as
small as possible while p and q are within a given
max-norm tolerance. Formally, we cast the follow-
ing constrained optimization problem,
min
p
n?
j=1
mjpj log
(pj
uj
)
, (1)
such that (p,m) ? ? ; ?p ? q?? ? 1/?. The
vectors u and q are dimensionally compatible with
p, namely, (q,m) ? ? and (u,m) ? ?. The scalar
? is a relaxation parameter. We use 1/? rather than
? itself for reasons that become clear in the sequel.
We next describe the dual form of (1). We derive
the dual by introducing Lagrange-Legendre multi-
pliers for each of the constraints appearing in (1).
Let ?+j ? 0 denote the multiplier for the constraint
qj ? pj ? 1/? and ??j ? 0 the multiplier for the
constraint qj ? pj ? ?1/?. In addition, we use ? as
the multiplier for the constraint
?
j mjpj = 1. fter
some routine algebraic manipulations we get that the
Lagrangian is,
?n
j=1 mi
(
pj log
(
pj
uj
)
+ ?j(qj ? pj) + |?j |?
)
+ ?
(?n
j=1 mjpj ? 1
)
. (2)
To find the dual form we take the partial derivative
of the Lagrangian with respect to each pj , equate to
zero, and get that log
(
pj
uj
)
+1??j +? = 0, which
implies that pj ? uje?j . We now employ the fact
that (p,m) ? ? to get that the exact form for pj is
pj =
uje?j?n
i=1 miuie?i
. (3)
Using (3) in the compact form of the Lagrangian we
obtain the following dual problem
max
?
?
?
?
?log (Z)?
n?
j=1
mjqj?j +
n?
j=1
mj
? |?j |
?
?
? ,
(4)
where Z = ?nj=1mjuje?j . We make rather little
use of the dual form of the problem. However, the
complementary slackness conditions that are neces-
sary for optimality to hold play an important role in
the next section in which we present a reformulation
of the relaxed maximum entropy problem.
3 Problem Reformulation
First note that the primal problem is a strictly con-
vex function over a compact convex domain. Thus,
its optimum exists and is unique. Let us now charac-
terize the form of the solution. We partition the set
of indices in [n] into three disjoint sets depending on
whether the constraint |pj ? qj| ? 1/? is active and
its form. Concretely, we define
I? = {1 ? j ? n | pj = qj ? 1/?}
I0 = {1 ? j ? n | |pj ? qj| < 1/?} (5)
I+ = {1 ? j ? n | pj = qj + 1/?} .
942
F(1,1)
(-1,-1)
Figure 1: The capping function F .
Recall that Z =
?n
j=1 mjuje?j . Thus, from
(3) we can rewrite pj = uje?j/Z . We next use
the complementary slackness conditions (see for in-
stance (Boyd and Vandenberghe, 2004)) to further
characterize the solution. For any j ? I? we must
have ??j = 0 and ?+j ? 0 therefore ?j ? 0, which
immediately implies that pj ? uj/Z . By definition
we have that pj = qj ? 1/? for j ? I?. Combin-
ing these two facts we get that uj/Z ? qj ? 1/? for
j ? I?. Analogous derivation yields that uj/Z ?
qj + 1/? for j ? I+. Last, if the set I0 is not empty
then for each j in I0 we must have ?+j = 0 and
??j = 0 thus ?j = 0. Resorting again to the def-
inition of p from (3) we get that pj = uj/Z for
j ? I0. Since |pj ? qj| < 1/? for j ? I0 we
get that |uj/Z ? qj| < 1/?. To recap, there ex-
ists Z > 0 such that the optimal solution takes the
following form,
pj =
?
?
?
qj ? 1/? uj/Z ? qj ? 1/?
uj/Z |uj/Z ? qj| < 1/?
qj + 1/? uj/Z ? qj + 1/?
. (6)
We next introduce an key re-parametrization,
defining µ = ?/Z . We also denote by F (·) the
capping function F (x) = max {?1,min {1, x}}. A
simple illustration of the capping function is given
in Fig. 1. Equipped with these definition we can
rewrite (6) as follows,
pj = qj +
1
? F (µuj ? ?qj) . (7)
Given u, q, and ?, the value of µ can be found by
using
?
j mjpj =
?
j mjqj = 1, which implies
G(?, µ) def=
n?
j=1
mjF (µuj ? ?qj) = 0 . (8)
We defer the derivation of the actual algorithm for
computing µ (and in turn p) to the next section. In
the meanwhile let us continue to explore the rich
structure of the general solution. Note that µ,u are
interchangeable with ?,q. We can thus swap the
roles of the prior distribution with the observed dis-
tribution and obtain an analogous characterization.
In the next section we further explore the depen-
dence of µ on ?. The structure we reveal shortly
serves as our infrastructure for deriving efficient al-
gorithms for following the regularization path.
4 The function µ(?)
In order to explore the dependency of µ on ? let us
introduce the following sums
M =
?
j?I+
mj ?
?
j?I?
mj
U =
?
j?I0
mj uj
Q =
?
j?I0
mj qj . (9)
Fixing ? and using (9), we can rewrite (8) as follows
µU ? ?Q+M = 0 . (10)
Clearly, so long as the partition of [n] into the sets
I+, I?, I0 is intact, there is a simple linear relation
between µ and ?. The number of possible subsets
I?, I0, I+ is finite. Thus, the range 0 < ? < ?
decomposes into a finite number of intervals each
of which corresponds to a fixed partition of [n] into
I+, I?, I0. In each interval µ is a linear function of
?, unless I0 is empty. Let ?? be the smallest ? value
for which I0 is empty. Let µ? be its corresponding
µ value. If I0 is never empty for any finite value of ?
we define ?? = µ? =?. Clearly, replacing (?, µ)
with (??, ?µ) for any ? ? 1 and ? ? ?? yields
the same feasible solution as I+(??) = I+(?),
I?(??) = I?(?). Hence, as far as the original prob-
lem is concerned there is no reason to go past ??
during the process of characterizing the solution. We
recap our derivation so far in the following lemma.
Lemma 4.1 For 0 ? ? ? ??, the value of µ as
defined by (7) is a unique. Further, the function µ(?)
is a piecewise linear continuous function in ?. When
? ? ?? letting µ = µ??/?? keeps (7) valid.
We established the fact that µ(?) is a piecewise lin-
ear function. The lingering question is how many
943
linear sub-intervals the function can attain. To study
this property, we take a geometric view of the plane
defined by (?, µ). Our combinatorial characteriza-
tion of the number of sub-intervals makes use of the
following definitions of lines in R2,
?+j = {(?, µ) | ujµ? qj? = +1} (11)
??j = {(?, µ) | ujµ? qj? = ?1} (12)
?0 = {(?, µ) | µU ? ?Q+M = 0} , (13)
where?? < ? <? and j ? [n]. The next theorem
gives an upper bound on the number of linear seg-
ments the function µ() may attain. While the bound
is quadratic in the dimension, for both artificial data
and real data the bound is way too pessimistic.
Theorem 4.2 The piecewise linear function µ(?)
consists of at most n2 linear segments for ? ? R+.
Proof Since we showed that that µ(?) is a piece-
wise linear function, it remains to show that it
has at most n2 linear segments. Consider the
two dimensional function G(?, µ) from (8). The
(?, µ) plane is divided by the 2n straight lines
?1, ?2, . . . , ?n, ??1, ??2, . . . , ??n into at most 2n2+1
polygons. The latter property is proved by induc-
tion. It clearly holds for n = 0. Assume that it holds
for n ? 1. Line ?n intersects the previous 2n ? 2
lines at no more than 2n ? 2 points, thus splitting
at most 2n ? 1 polygons into two separate polygo-
nal parts. Line ??n is parallel to ?n, again adding
at most 2n ? 1 polygons. Recapping, we obtain at
most 2(n ? 1)2 + 1 + 2(2n ? 1) = 2n2 + 1 poly-
gons, as required per induction. Recall that µ(?) is
linear inside each polygon. The two extreme poly-
gons where G(?, µ) = ±?nj=1 mj clearly disallow
G(?, µ) = 0, hence µ(?) can have at most 2n2 ? 1
segments for ?? < ? < ?. Lastly, we use the
symmetry G(??,?µ) = ?G(?, µ) which implies
that for ? ? R+ there are at most n2 segments.
This result stands in contrast to the Lasso homotopy
tracking procedure (Osborne et al., 2000), where the
worst case number of segments seems to be expo-
nential in n. Moreover, when the prior u is uniform,
uj = 1/
?n
j=1 mj for all j ? [n], the number of
segments is at most n + 1. We defer the analysis of
the uniform case to a later section as the proof stems
from the algorithm we describe in the sequel.
0 20 40 60 80 1000
10
20
30
40
50
?
µ
Figure 2: An illustration of the function µ(?) for a syn-
thetic 3 dimensional example.
10 20 30 40 50 60
?1.5
?1
?0.5
0
0.5
1
1.5
µ
G
Figure 3: An illustration of the function G(µ) for a syn-
thetic 4 dimensional example and a ? = 17.
5 Algorithm for a Single Relaxation Value
Suppose we are given u,q,m and a specific relax-
ation value ?˜. How can we find p? The obvious
approach is to solve the one dimensional monotoni-
cally nondecreasing equation G(µ) def= G(?˜, µ) = 0
by bisection. In this section we present a more effi-
cient and direct procedure that is guaranteed to find
the optimal solution p in a finite number of steps.
Clearly G(µ) is a piecewise linear function with
at most 2n easily computable change points of the
slope. See also Fig. (5) for an illustration of G(·).
In order to find the slope change points we need to
calculate the point (?, µj) for all the lines ?±j where
1 ? j ? n. Concretely, these values are
µj =
?q|j| + sign(j)
u|j|
. (14)
We next sort the above values of µj and denote the
resulting sorted list as µpi1 ? µpi2 ? · · · ? µpi2n . For
any 0 ? j ? 2n letMj ,Uj ,Qj be the sums, defined
944
in (9), for the line segment µpij?1 < µ < µpij (de-
noting µpi0 = ??, µpi2n+1 = ?). We compute
the sums Mj,Uj ,Qj incrementally, starting from
M0 = ?
?n
i=1 mi, U0 = Q0 = 0. Once the
values of j?1’th sums are known, we can compute
the next sums in the sequence as follows,
Mj = Mj?1 + m|pij|
Uj = Uj?1 ? sign(?j)m|pij | u|pij |
Qj = Qj?1 ? sign(?j)m|pij | q|pij| .
From the above sums we can compute the value of
the function G(?, µ) at the end point of the line seg-
ment (µpij?1 , µpij), which is the same as the start
point of the line segment (µpij , µpij+1),
Gj = Mj?1 + Uj?1 µj ?Qj?1 ?
= Mj + Uj µj ?Qj ? .
The optimal value of µ resides in the line segment
for which G(·) attains 0. Such a segment must exist
since G0 = M0 = ?
?n
i=1 mi < 0 and G2n =
?M0 > 0. Therefore, there exists an index 1 ?
j < 2n, where Gj ? 0 ? Gj+1. Once we bracketed
the feasible segment for µ, the optimal value of µ is
found by solving the linear equation (10),
µ = (Qj ? ? Mj) /Uj . (15)
From the optimal value of µ it is straightforward to
construct p using (7). Due to the sorting step, the al-
gorithm’s run time is O(n log(n)) and it takes linear
space. The number of operations can be reduced to
O(n) using a randomized search procedure.
6 Homotopy Tracking
We now shift gears and focus on the main thrust
of this paper, namely, an efficient characterization
of the entire regularization path for the maximum
entropy problem. Since we have shown that the
optimal solution p can be straightforwardly ob-
tained from the variable µ, it suffices to efficiently
track the function µ(?) as we traverse the plane
(?, µ) from ? = 0 through the last change point
which we denoted as (??, µ?). In this section
we give an algorithm that traverses µ(?) by lo-
cating the intersections of ?0 with the fixed lines
??n, ??n+1, . . . , ??1, ?1, . . . , ?n and updating ?0 af-
ter each intersection.
More formally, the local homotopy tracking fol-
lows the piecewise linear function µ(?), segment by
segment. Each segment corresponds to a subset of
the line ?0 for a given triplet (M,U ,Q). It is simple
to show that µ(0) = 0, hence we start with
? = 0, M = 0, U = Q = 1 . (16)
We now track the value of µ as ? increases, and the
relaxation parameter 1/? decreases. The character-
ization of ?0 remains intact until ?0 hits one of the
lines ?j for 1 ? |j| ? n. To find the line intersect-
ing ?0 we need to compute the potential intersection
points (?j, µj) = ?0 ? ?j which amounts to calculat-
ing ??n, ??n+1, . . . , ??1, ?1, ?2, · · · , ?n where
?j =
Mu|j| + Usign(j)
Qu|j| ? Uq|j|
. (17)
The lines for which the denominator is zero cor-
respond to infeasible intersection and can be dis-
carded. The smallest value ?j which is larger than
the current traced value of ? corresponds to the next
line intersecting ?0.
While the above description is mathematically
sound, we devised an equivalent intersection in-
spection scheme which is more numerically stable
and efficient. We keep track of partition I?, I0, I1
through the vector,
sj =
?
?
?
?1 j ? I?
0 j ? I0
+1 j ? I+
.
Initially s1 = s2 = · · · = sn = 0. What kind of
intersection does ?0 have with ?j? Recall that QU is
the slope of ?0 while
q|j|
u|j| is the slope of ?j . Thus
Q
U >
q|j|
u|j| means that the |j|’th constraint is moving
“up” from I? to I0 or from I0 to I+. When QU <
q|j|
u|j|
the |j|’th constraint is moving “down” from I+ to I0
or from I0 to I?. See also Fig. 4 for an illustration
of the possible transitions between the sets. For in-
stance, the slope of µ(?) on the bottom left part of
the figure is larger than the slope the line it inter-
sects. Since this line defines the boundary between
I? and I0, we transition from I? to I0. We need
only consider 1 ? |j| ? n of the following types.
Moving “up” from I? to I0 requires
s|j| = ?1 j < 0 Qu|j| ? Uq|j| > 0 .
945
Figure 4: Illustration of the possible intersections be-
tween µ(?) and ?j and the corresponding transition be-
tween the sets I±, I0.
Similarly, moving “down” from I+ to I0 requires
s|j| = 1 j > 0 Qu|j| ? Uq|j| < 0 .
Finally, moving “up” or “down” from I0 entails
s|j| = 0 j(Qu|j| ? Uq|j|) > 0 .
If there are no eligible ?j’s, we have finished travers-
ing µ(). Otherwise let index j belong to the the
smallest eligible ?j . Infinite accuracy guarantees
that ?j ? ?. In practice we perform the update
? ? max(?, ?j)
M ? M+ sign(Qu|j| ? Uq|j|)m|j|
U ? U +
(
2
??s|j|
??? 1
)
m|j| u|j|
Q ? Q+
(
2
??s|j|
??? 1
)
m|j| q|j|
sj ? sj + sign(Qu|j| ? Uq|j|) .
We are done with the tracking process when I0 is
empty, i.e. for all j sj 6= 0.
The local homotopy algorithm takes O(n) mem-
ory and O(nk) operations where k is the number of
change points in the function µ(?). This algorithm
is simple to implement, and when k is relatively
small it is efficient. An illustration of the tracking
result, µ(?), along with the lines ?±j , that provide a
geometrical description of the problem, is given in
Fig. 5.
7 Uniform Prior
We chose to denote the prior distribution as u to un-
derscore the fact that in the case of no prior knowl-
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
?4
?2
0
2
4
6
8
10
?
µ
Figure 5: The result of the homotopy tracking for a 4
dimensional problem. The lines ?j for j < 0 are drawn in
blue and for j > 0 in red. The function µ(?) is drawn in
green and its change points in black. Note that although
the dimension is 4 the number of change points is rather
small and does not exceed 4 either in this simple example.
edge u is the uniform distribution,
u def= uj =
( n?
i=1
mi
)?1
.
In this case the objective function amounts to the
negative entropy and by flipping the sign of the ob-
jective we obtain the classical maximum entropy
problem. The fact that the prior probability is the
same for all possible observations infuses the prob-
lem with further structure which we show how to
exploit in this section. Needless to say though that
all the results we obtained thus far are still valid.
Let us consider a point (?, µ) on the boundary be-
tween I0 and I+, namely, there exist a line ?+i such
that,
µui ? ?qi = µu? ?qi = 1 .
By definition, for any j ? I0 we have
µuj ? ?qj = µu? ?qj < 1 = µu? ?qi .
Thus, qi < qj for all j ? I0 which implies that
mj u qj > mj u qi . (18)
Summing over j ? I0 we get that
Qu =
?
j?I0
mj qj u >
?
j?I0
mj u qi = Uqi ,
hence,
qi
ui
= qiu <
Q
U
946
and we must be moving “up” from I0 to I+ when the
line ?0 hits ?i. Similarly we must be moving “down”
from when ?0 hits on the boundary between I0 and
I?. We summarize these properties in the following
theorem.
Theorem 7.1 When the prior distribution u is uni-
form, I?(?) and I+(?) are monotonically nonde-
creasing and I0(?) is monotonically nonincreasing
in ? > 0 . Further, the piecewise linear function
µ(?) consists of at most n + 1 line segments.
The homotopy tracking procedure when the prior
is uniform is particularly simple and efficient. In-
tuitively, there is a sole condition which controls
the order in which indices would enter I± from I0,
which is simply how “far” each qi is from u, the sin-
gle prior value. Therefore, the algorithm starts by
sorting q. Let qpi1 > qpi2 > · · · > qpin denote the
sorted vector. Instead of maintaining the vector of
set indicators s, we merely maintain two indices j?
and j+ which designate the size of I? and I+ that
were constructed thus far. Due to the monotonic-
ity property of the sets I± as ? grows, the two sets
can be written as, I? = {?j | 1 ? j < j?} and
I+ = {?j | j+ < j ? n}. The homotopy track-
ing procedure starts as before with ? = 0,M = 0,
U = Q = 1. We also set j? = 1 and j+ = n which
by definition imply that I± are empty and I0 = [n].
In each tracking iteration we need to compare only
two values which we compactly denote as,
?± =
Mu ± U
Qu ? Uqpij±
.
When ?? ? ?+ we just encountered a transition
from I0 to I? and as we encroach I? we perform
the updates, ? ? ??, M ?M ? mpij? , U ?
U ?mpij?u, Q? Q?mpij?qpij? , j? ? j? + 1.
Similarly when ?? > ?+ we perform the updates
? ? ?+, M?M + mpij+ , U ? U ? mpij+u,
Q? Q ? mpij+ qpij+ , j+ ? j+ ? 1.
The tracking process stops when j? > j+ as we
exhausted the transitions out of the set I0 which be-
comes empty. Homotopy tracking for a uniform
prior takes O(n) memory and O(n log(n)) opera-
tions and is very simple to implement.
We also devised a global homotopy tracking algo-
rithms that requires a priority queue which facilitates
insertions, deletions, and finding the largest element
100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sample Size / Dimensions
# 
Ch
an
ge
 P
oi
nt
s 
/ D
im
en
sio
ns
Figure 6: The number of line-segments in the homotopy
as a function of the number of samples used to build the
observed distribution q.
in the queue in O(log(n)) time. The algorithm re-
quires O(n) memory and O(n2 log(n)) operations.
Clearly, if the number of line segments constituting
µ(?) is greater than n log(n) (recall that the upper
bound is O(n2)) then the global homotopy proce-
dure is faster than the local one. However, as we
show in Sec. 8, in practice the number of line seg-
ments is merely linear and it thus suffices to use the
local homotopy tracking algorithm.
8 Number of line segments in practice
The focus of the paper is the design and analysis
of a novel homotopy method for maximum entropy
problems. We thus left with relatively little space
to discuss the empirical aspects of our approach. In
this section we focus on one particular experimental
facet that underscores the usability of our apparatus.
We briefly discuss current natural language applica-
tions that we currently work on in the next section.
The practicality of our approach hinges on the
number of line segments that occur in practice. Our
bounds indicate that this number can scale quadrat-
ically with the dimension, which would render the
homotopy algorithm impractical when the size of the
alphabet is larger than a few thousands. We there-
fore extensively tested the actual number of line seg-
ments in the resulting homotopy when u and q are
Zipf (1949) distributions. We used an alphabet of
size 50, 000 in our experiments. The distribution u
was set to be the Zipf distribution with an offset pa-
rameter of 2, that is, ui ? 1/(i + 2). We defined
a “mother” distribution for q, denoted q¯, which is
947
a plain Zipf distribution without an offset, namely
q¯i ? 1/i. We then sampled n/2l letters according
to the distribution q¯ where l ? ?3, . . . , 3. Thus the
smallest sample was n/23 = 6, 250 and the largest
sample was n/3?3 = 40, 000. Based on the sample
we defined the observed distribution q such that qi
is proportional to the number of times the i’th let-
ter appeared in the sample. We repeated the process
100 times for each sample size and report average
results. Note that when the sample is substantially
smaller than the dimension the observed distribution
q tends to be “simple” as it consists of many zero
components. In Fig. 6 we depict the average num-
ber line segments for each sample size. When the
sample size is one eighth of the dimension we aver-
age st most 0.1n line segments. More importantly,
even when the size of the sample is fairly large, the
number of lines segments is linear in the dimension
with a constant close to one. We also performed
experiments with large sample sizes for which the
empirical distribution q is very close to the mother
distribution q¯. We seldom found that the number of
line segments exceeds 4n and the mode is around
2n. These findings render our approach usable even
in the very large natural language applications.
9 Conclusions
We presented a novel efficient apparatus for tracking
the entire relaxation path of maximum entropy prob-
lems. We currently study natural language process-
ing applications. In particular, we are in the process
of devising homotopy methods for domain adapta-
tion Blitzer (2008) and language modeling based
on context tree weighting (Willems et al., 1995).
We also examine generalization of our approach in
which the relative entropy objective is replaced with
a separable Bregman (Censor and Zenios, 1997)
function. Such a generalization is likely to distill
further connections to the other homotopy methods,
in particular the least angle regression algorithm of
Efron et al. (2004) and homotopy methods for the
Lasso in general (Osborne et al., 2000). We also plan
to study separable Bregman functions in order to de-
rive entire path solutions for less explored objectives
such as the Itakura-Saito spectral distance (Rabiner
and Juang, 1993) and distances especially suited for
natural language processing.
References
A.L. Berger, S.A. Della Pietra, and V. J. Della Pietra.
A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22
(1):39–71, 1996.
John Blitzer. Domain Adaptation of Natural Lan-
guage Processing Systems. PhD thesis, University
of Pennsylvania, 2008.
S. Boyd and L. Vandenberghe. Convex Optimiza-
tion. Cambridge University Press, 2004.
Y. Censor and S.A. Zenios. Parallel Optimization:
Theory, Algorithms, and Applications. Oxford
University Press, New York, NY, USA, 1997.
S. Della Pietra, V. Della Pietra, and J. Lafferty. In-
ducing features of random fields. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 5:179–190, 1997.
M. Dud?´k, S. J. Phillips, and R. E. Schapire. Maxi-
mum entropy density estimation with generalized
regularization and an application to species distri-
bution modeling. Journal of Machine Learning
Research, 8:1217–1260, June 2007.
Bradley Efron, Trevor Hastie, Iain Johnstone, and
Robert Tibshirani. Least angle regression. Annals
of Statistics, 32(2):407–499, 2004.
Edwin T. Jaynes. Prior probabilities. IEEE Transac-
tions on Systems Science and Cybernetics, SSC-4
(3):227–241, September 1968.
Michael R. Osborne, Brett Presnell, and Berwin A.
Turlach. On the lasso and its dual. Journal
of Computational and Graphical Statistics, 9(2):
319–337, 2000.
L. Rabiner and B.H. Juang. Fundamentals of Speech
Recognition. Prentice Hall, 1993.
F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens.
The context tree weighting method: basic proper-
ties. IEEE Transactions on Information Theory,
41(3):653–664, 1995.
George K. Zipf. Human Behavior and the Principle
of Least Effort. Addison-Wesley, 1949.
948
