Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 471–481,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Learning a Deep Hybrid Model for Semi-Supervised Text Classification
Alexander G. Ororbia II, C. Lee Giles, David Reitter
College of Information Sciences and Technology
The Pennsylvania State University, University Park, PA
{ago109, giles, reitter}@psu.edu
Abstract
We present a novel fine-tuning algorithm
in a deep hybrid architecture for semi-
supervised text classification. During
each increment of the online learning pro-
cess, the fine-tuning algorithm serves as
a top-down mechanism for pseudo-jointly
modifying model parameters following a
bottom-up generative learning pass. The
resulting model, trained under what we
call the Bottom-Up-Top-Down learning al-
gorithm, is shown to outperform a vari-
ety of competitive models and baselines
trained across a wide range of splits be-
tween supervised and unsupervised train-
ing data.
1 Introduction
Recent breakthroughs in learning expressive neu-
ral architectures have addressed challenging prob-
lems in domains such as computer vision, speech
recognition, and natural language processing. This
success is owed to the representational power af-
forded by deeper architectures supported by long-
standing theoretical arguments (Hastad, 1987).
These architectures efficiently model complex,
highly varying functions via multiple layers of
non-linearities, which would otherwise require
very “wide” shallow models that need large quan-
tities of samples (Bengio, 2012). However, many
of these deeper models have relied on mini-batch
training on large-scale, labeled data-sets, either us-
ing unsupervised pre-training (Bengio et al., 2007)
or improved architectural components (such as ac-
tivation functions) (Schmidhuber, 2015).
In an online learning problem, samples are pre-
sented to the learning architecture at a given rate
(usually with one-time access to these data points),
and, as in the case of a web crawling agent, most
of these are unlabeled. Given this, batch training
and supervised learning frameworks are no longer
applicable. While incremental approaches such
as co-training have been employed to help these
models learn in a more update-able fashion (Blum
and Mitchell, 1998; Gollapalli et al., 2013), neural
architectures can naturally be trained in an online
manner through the use of stochastic gradient de-
scent (SGD).
Semi-supervised online learning does not only
address practical applications, but it also reflects
some challenges of human category acquisition
(Tomasello, 2001). Consider the case of a child
learning to discriminate between object categories
and mapping them to words, given only a small
amount of explicitly labeled data (the mother
pointing to the object), and a large portion of un-
supervised learning, where the child comprehends
an adult’s speech or experiences positive feedback
for his or her own utterances regardless of their
correctness. The original argument in this respect
applied to grammar (e.g., Chomsky, 1980; Pullum
& Scholz, 2002). While neural networks are not
necessarily models of actual cognitive processes,
semi-supervised models can show learnability and
illustrate possible constraints inherent to the learn-
ing process.
The contribution of this paper is the develop-
ment of the Bottom-Up-Top-Down learning al-
gorithm for training a Stacked Boltzmann Ex-
perts Network (SBEN) (Ororbia II et al., 2015)
hybrid architecture. This procedure combines
our proposed top-down fine-tuning procedure for
jointly modifying the parameters of a SBEN with
a modified form of the model’s original layer-wise
bottom-up learning pass (Ororbia II et al., 2015).
We investigate the performance of the constructed
deep model when applied to semi-supervised text
classification problems and find that our hybrid ar-
chitecture outperforms all baselines.
471
2 Related Work
Recent successes in the domain of connection-
ist learning stem from the expressive power af-
forded by models, such as the Deep Belief Net-
work (DBN) (Hinton et al., 2006; Bengio et al.,
2007) or Stacked Denoising Autoencoder (Vincent
et al., 2010), that greedily learn layers of stacked
non-linear feature detectors, equivalent to levels of
abstraction of the original representation. In a va-
riety of language-based problems, deep architec-
tures have outperformed popular shallow models
and classifiers (Salakhutdinov and Hinton, 2009;
Liu, 2010; Socher et al., 2011; Glorot et al.,
2011b; Lu and Li, 2013; Lu et al., 2014). How-
ever, these architectures often operate in a multi-
stage learning process, where a generative archi-
tecture is pre-trained and then used to initialize pa-
rameters of a second architecture that can be dis-
criminatively fine-tuned (using back-propagation
of errors or drop-out: Hinton et al., 2012). Sev-
eral ideas have been proposed to help deep mod-
els deal with potentially uncooperative input dis-
tributions or encourage learning of discriminative
information earlier in the process, many leverag-
ing auxiliary models in various ways (Bengio et
al., 2007; Zhang et al., 2014; Lee et al., 2014). A
few methods for adapting deep architecture con-
struction to an incremental learning setting have
also been proposed (Calandra et al., 2012; Zhou
et al., 2012). Recently, it was shown in (Oror-
bia II et al., 2015) that deep hybrid architectures,
or multi-level models that integrate discriminative
and generative learning objectives, offer a strong
viable alternative to multi-stage learners and are
readily usable for categorization tasks.
For text-based classification, a dominating
model is the support vector machine (SVM)
(Cortes and Vapnik, 1995) with many useful in-
novations to yet further improve its discrimina-
tive performance (Subramanya and Bilmes, 2008).
When used in tandem with prior human knowl-
edge to hand-craft good features, this simple ar-
chitecture has proven effective in solving practical
text-based tasks, such as academic document clas-
sification (Caragea et al., 2014). However, while
model construction may be fast (especially when
using a linear kernel), this process is costly in
that it requires a great deal of human labor to an-
notate the training corpus. Our approach, which
builds on that of (Ororbia II et al., 2015), provides
a means for improving classification performance
when labeled data is in scarce supply, learning
structure and regularity within the text to reduce
classification error incrementally.
3 A Deep Hybrid Model for
Semi-Supervised Learning
To directly handle the problem of discriminative
learning when labeled data is scarce, (Ororbia II
et al., 2015) proposed deep hybrid architectures
that could effectively leverage small amounts of
labeled and large amounts of unlabeled data. In
particular, the best-performing architecture was
the Stacked Boltzmann Experts Network (SBEN),
which is a variant of the DBN. In its construction
and training, the SBEN design borrows many re-
cent insights from efficiently learning good DBN
models (Hinton et al., 2006) and is essentially a
stack of building block models where each layer
of model parameters is greedily modified while
freezing the parameters of all others. In con-
trast to the DBN, which stacks restricted Boltz-
mann machines (RBM’s) and is often used to ini-
tialize a deep multi-layer perceptron (MLP), the
SBEN model is constructed by composing hybrid
restricted Boltzmann machines and can be directly
applied to the discriminative task in a single learn-
ing phase.
The hybrid restricted Boltzmann machine
(HRBM) (Schmah et al., 2008; Larochelle and
Bengio, 2008; Larochelle et al., 2012) building
block of the SBEN is itself an extension of the
RBM meant to ultimately perform classification.
The HRBM graphical model is defined via pa-
rameters ? = (W,U,b, c,d) (where W is the
input-to-hidden weight matrix, U the hidden-to-
class weight matrix, b is the visible bias vector, c is
the hidden unit bias vector, and d is the class unit
bias vector), and is a model of the joint distribu-
tion of a binary feature vector x = (x
1
, · · · , x
D
)
and its label y ? {1, · · · , C} that makes use of a
latent variable set h = (h
1
, · · · , h
H
). The model
assigns a probability to the triplet (y,x,h) using:
p(y, x,h) =
e
?E(y,x,h)
Z
, (1)
p(y, x) =
1
Z
?
h
e
?E(y,x,h)
(2)
where Z is known as the partition function. The
model’s energy function is defined as
472
E(y, x,h) = ?h
T
Wx?b
T
x?c
T
h?d
T
e
y
?h
T
Ue
y
.
(3)
where e
y
= (1
i=y
)
C
i=1
is the one-hot vector en-
coding of y. It is often not possible to compute
p(y, x,h) or the marginal p(y, x) due to the in-
tractable normalization constant. However, ex-
ploiting the model’s lack of intra-layer connec-
tions, block Gibbs sampling may be used to draw
samples of the HRBM’s latent variable layer given
the current state of the visible layer and vice versa.
This yields the following equations:
p(h|y, x) =
?
j
p(h
j
|y, x),
p(h
j
= 1|y, x) = ?(c
j
+ U
jy
+
?
i
W
ji
x
i
)
(4)
p(x|h) =
?
i
p(x
i
|h),
p(x
i
= 1|h) = ?(b
i
+
?
j
W
ji
h
j
)
(5)
p(y|h) =
e
d
y
+
?
j
U
jy
h
j
?
y
?
e
d
y
?+
?
j
U
jy
?h
j
(6)
where ?(v) = 1/(1 + e
?v
). Classification may
be performed directly with the HRBM by using its
free energy function F (y, x) to compute the con-
ditional distribution
p(y|x) =
e
?F (y,x)
?
y
?
?{1,··· ,C}
e
?F (y
?
,x)
(7)
where the free energy is formally defined as
?F (y, x) = (d
y
+
?
j
?(c
j
+ U
jy
+
?
W
ji
x
i
))
(8)
and ? is the softplus activation function ?(v) =
log(1 + e
v
).
To construct an N-layer SBEN (or N-SBEN), as
was shown in (Ororbia II et al., 2015), one may
learn a stack of HRBMs in one of two ways: (1)
in a strict greedy, layer-wise manner, where lay-
ers are each trained in isolation on all of the data
samples one at a time from the bottom-up; or (2)
in a more relaxed disjoint fashion, where all layers
are trained together on all of the data but still in a
Figure 1: Architecture of the SBEN model. The
model in feedforward mode can be viewed as a
directed model, however, during training, connec-
tions are bi-directional.
layer-wise bottom-up pass. To properly compute
intermediate data representations during training
and prediction in the SBEN, one must combine
Equations 4 and 7. (The specific procedure for do-
ing this can be found in the computeLayerwiseS-
tatistics sub-routine in Algorithm 1.) This gives
rise to the full SBEN architecture, which is de-
picted in Figure 1.
3.1 Ensembling of Layer-Wise Experts
The SBEN may be viewed as a natural vertical en-
semble of layer-wise “experts”, where each layer
maps latent representations to predictions, which
differs from standard methods such as boosting
(Schapire, 1990). Traditional feedforward neural
models propagate data through the final network
to obtain an output prediction y
t
from a penulti-
mate layer for a given x
t
. In contrast, this hybrid
model is capable of a producing a label y
n
t
at each
level n for x
t
.
To vertically aggregate layer-wise expert out-
puts, we compute a simple mean predictor,
p(y|x)
ensemble
, as follows:
p(y|x)
ensemble
=
1
N
N
?
n=1
p(y|x)
n
(9)
This ensembling scheme provides a simple way to
incorporate acquired discriminative knowledge of
different levels of abstraction into the model’s fi-
nal prediction. We note that the SBEN’s inherent
layer-wise discriminative ability stands as an alter-
native to coupling helper classifiers (Bengio et al.,
2007) or the “companion objectives” (Lee et al.,
2014).
473
3.2 The Bottom-Up-Top-Down Learning
Algorithm
With the SBEN architecture defined, we next
present its simple two-step training algorithm,
or the Bottom-Up-Top-Down procedure (BUTD),
which combines a greedy, bottom-up pass with a
subsequent top-down fine-tuning step. At every
iteration of training, the model makes use of a sin-
gle labeled sample (taken from an available, small
labeled data subset) and an example from either a
large unlabeled pool or a data-stream. We describe
each of the two phases in Sections 3.2.1 and 3.2.2.
3.2.1 Bottom-Up Layer-wise Learning (BU)
The first phase of N-SBEN learning consists of
a bottom-up pass where each layerwise HRBM
can be trained using a compound objective func-
tion. Data samples are propagated up the model
to the layer targeted for layer-wise training using
the feedforward schema described above. Each
HRBM layer of the SBEN is greedily trained us-
ing the frozen latent representations of the one be-
low, which are generated by using the lower level
expert’s input and prediction. The loss function
for each layer balances a discriminative objective
L
disc
, a supervised generative objective L
gen
, and
an unsupervised generative objectiveL
unsup
, fully
defined as follows:
L
semi
(D
train
,D
unlab
) = ?L
disc
(D
train
)
+?L
gen
(D
train
)
+?L
unsup
(D
unlab
)
(10)
Unlike generative pre-training of neural architec-
tures (Bengio et al., 2007), the additional free pa-
rameters ?, ?, and ? offer explicit control over
the extent to which the final parameters discovered
are influenced by generative learning (Larochelle
et al., 2012; Ororbia II et al., 2015). More im-
portantly, the generative objectives may be viewed
as providing data-dependent regularization on the
discriminative learning gradient of each layer.
The objectives themselves are defined as:
L
disc
(D
train
) = ?
|D
train
|
?
t=1
log p(y|x
t
), (11)
L
gen
(D
train
) = ?
|D
train
|
?
t=1
log p(y
t
, x
t
), and (12)
L
unsup
(D
unlab
) = ?
|D
unlab
|
?
t=1
log p(x
t
) (13)
where D
train
= {(x
t
, y)} is the labeled training
data-set and D
unlab
= {(u
t
)} is the unlabeled
training data-set. The gradient for L
disc
may be
computed directly, which follows the general form
? log p(y
t
|x)
??
= ?E
h|y
t
,x
t
[
?
??
(E(y
t
, x
t
,h))
]
+E
y,h|,x
[
?
??
(E(y, x,h))
]
(14)
and can be calculated directly (see Larochelle et
al., 2012 , for details) or through a form of Drop-
ping, such as Drop-Out or Drop-Connect (Tom-
czak, 2013). The generative gradients themselves
follow the form
? log p(y
t
, x)
??
= ?E
h|y
t
,x
t
[
?
??
(E(y
t
, x
t
,h))
]
+E
y,x,h
[
?
??
(E(y, x,h))
]
(15)
and, despite being intractable for any sample
(x
t
, y
t
), may be approximated via the contrastive
divergence procedure (Hinton, 2002). The in-
tractable second expectation is replaced with a
point estimate using a single Gibbs sampling step.
To calculate the generative gradient for an unla-
beled sample u, a pseudo-label must be obtained
by using a layer-wise HRBM’s current estimate of
p(y|u), which can be viewed as a form of self-
training or Entropy Regularization (Lee, 2013).
The online procedure for computing the genera-
tive gradient (either labeled or unlabeled example)
for a single HRBM can be found in Ororbia et al.,
(2015).
Setting the coefficients that control learning ob-
jective influences can lead to different model con-
figurations (especially with respect to ?) as well as
impact the gradient-based training of each model
layer (i.e., ? and ?). In this paper, we shall ex-
plore two particular configurations, namely 1) by
setting ? = 0 and ? = 1, which leads to con-
structing a purely generative model of D
train
and
474
Algorithm 1 Top-down fine-tuning of an N-SBEN (ensemble back-propagation). Note that “·” indicates
a Hadamard product, ? is an error signal vector, the prime superscript indicates a derivative (i.e., ?
?
means
derivative function of the sigmoid), and
?
z is the symbol for linear pre-activation values.
Input: (x
t
, y
t
) ? D, learning rate ? and model parameters ? = {?
1
,?
2
, ...,?
N
}
function FINETUNEMODEL((x
t
, y
t
), ?, ?)
?? ?, x
n
? x
t
, y
n
? ? . Initialize list of layer-wise model statistics & variables
// Conduct feed-forward pass to collect layer-wise statistics
for ?
n
? ? do
(h
n
,
?
z
n
, y
h
n
,x
n
)? COMPUTELAYERWISESTATISTICS(x
n
,?
n
)
?
n
? (h
n
,
?
z
n
, y
h
n
, x
n
), x
n
? h
n
, y
n
? y
h
n
// Conduct error back-propagation pass to adjust layer-wise parameters
?
l
? ?
for l? N, l??, while l ? 1 do
(h
l
,
?
z
l
, y
h
l
,x
l
)? ?[l] . Grab relevant statistics for layer l of model
if i = N then
(5
disc
, ?
l
)? COMPUTEDISCIMINATIVEGRADIENT(y
t
,x
l
, ?,h
n
,
?
z,?
l
)
else
?
l
? ?
l
· ?
?
(
?
z
l
)
(5
disc
, ?
l
)? COMPUTEDISCIMINATIVEGRADIENT(y
t
,x
l
, ?
l
,h
n
,
?
z,?
l
)
?
n
? ?
n
? ?(5
disc
)
function COMPUTELAYERWISESTATISTICS(x
t
,?
n
)
y
h
t
? p(y
t
|x
t
,?
n
) . Equation 7 under the layerwise model
?
z? c +Wx
t
+ Ue
y
t
. Can re-use
?
z to perform next step
h
t
? p(h|y
h
t
, x
t
,?
n
) . Equation 4 under the layerwise model
return (h
t
,
?
z, y
h
t
, x
t
)
function COMPUTEDISCIMINATIVEGRADIENT(y
t
,x
l
, ?
l
,h
n
,
?
z,?
l
)
o? p(y|h
n
,?
l
), ? ? softmax
?
(o) · ?(y
t
/o)
5
U
? ?h
T
n
,5
d
? ?, ? ? U?, ? ? ? · ?
?
(
?
z)
if ?
l
6= ? then
? ? ? · ?
l
5
W
? ?x
T
l
,5
c
? ?,5
b
? 0,5
U
?5
U
+ (?e
T
y
t
), ? ?W
T
?
return (5? (5
W
,5
U
,5
b
,5
c
,5
d
), ?)
D
unsup
, and 2) by setting ? = 1 with ? freely
varying (which recovers the model of Ororbia et
al., 2015). In both scenarios, ? is allowed to vary
as a user-defined hyper-parameter. The second set-
ting of ? allows for training the SBEN directly
with only the bottom-up phase defined in this sec-
tion. However, if the first setting is used, a sec-
ond phase may be used to incorporate a top-down
fine-tuning phase. A bottom-up pass simply en-
tails computing this compound gradient for each
layer of the model for 1 or 2 samples per training
iteration. Notice that the first scenario reduces the
number of hyper-parameters to explore in model
selection, requiring only an appropriate value for
? to be found.
3.2.2 Top-Down Fine-tuning (TD)
Although efficient, the bottom-up procedure de-
scribed above is greedy, which means that the gra-
dients are computed for each layer-wise HRBM
independent of gradient information from other
layers of the model. One way we propose to
introduce a degree of joint training of param-
eters is to incorporate a second phase that ad-
justs the SBEN parameters via a modified form
of back-propagation. Such a routine can further
exploit the SBEN’s multiple predictors (or entry
points) where additional error signals may be com-
puted and aggregated while signals are reverse-
propagated down the network. We hypothesize
that holistic fine-tuning ensures that discrimina-
tive information is incorporated into the generative
475
Algorithm 2 The Bottom-Up-Top-Down training procedure for learning an N-SBEN.
Input: (x
t
, y
t
) ? D
train
, (u
t
) ? D
unlab
, rates ? & ?, p¯, & parameters ? = {?
1
,?
2
, ...,?
N
}
function BOTTOMUPTOPDOWN((y
t
, x
t
, u
t
, ?, ?, ?)
APPLYBOTTOMUPPASS(y
t
, x
t
, u
t
, ?, ? = 0, ? = 1, ?, ?) . See (Ororbia II et al., 2015)
// Up to two calls can be made to the top-down tuning routine
FINETUNEMODEL(x
t
, y
t
, ?, ?) . See Algorithm 1 for details
v
t
? p
ensemble
(y|x,?
n
) . Calculate pseudo-label probability using Equation 9
if max[v
t
] > p¯ then
v
t
? TOONEHOT(v
t
) . Convert to 1-hot vector using argmax of model conditionals
FINETUNEMODEL(u
t
,v
t
, ?, ?)
features being constructed in the bottom-up learn-
ing step. Furthermore, errors from experts above
are propagated down to lower layers, which were
initially frozen during the greedy, bottom-up train-
ing phase.
Fine-tuning in the context of training an SBEN
is different from using a pre-trained MLP that
is subsequently fine-tuned with back-propagation.
First, since the SBEN is a more complex architec-
ture than an MLP, pre-initializing an MLP would
be insufficient given that one would be tossing po-
tentially useful information stored in the SBEN’s
class filters (and corresponding class bias vectors)
of each layer-wise expert (i.e., U and d). Second,
merely using the SBEN as an intermediate model
ignores the fact the SBEN can already perform
classification directly. To avoid losing such infor-
mation and to fully exploit the model’s predictive
ability, we adapt the back-propagation algorithm
for training MLP’s to operate on the SBEN, which
we shall call ensemble back-propagation since
the fine-tuning method propagates error deriva-
tives down the network from many points of entry.
Ensemble back-propagation is described in Algo-
rithm 1.
With this second online training step, the
Bottom-Up-Top-Down (BUTD) training algorithm
for fully training an SBEN proceeds with a sin-
gle bottom-up modification step followed by a
single top-down joint fine-tuning step using the
ensemble back-propagation procedure defined in
Algorithm 1 for each training time step. A full
top-down phase can consist of up to two calls to
the ensemble back-propagation procedure. One
is used to jointly modify the SBEN’s parame-
ters with respect to the sample taken from D
train
.
A second one is potentially needed to tune pa-
rameters with respect to the sample drawn from
D
unlab
. For the unlabeled sample, if the high-
est class probability assigned by the SBEN (us-
ing Equation 9) is greater than a pre-set threshold
(i.e., max[p
ensemble
(y|u)] > p¯), a pseudo-label is
created for that sample by converting the model’s
mean vector to a 1-hot encoding. The probability
threshold p¯ for the potential second call to the en-
semble back-propagation routine allows us to in-
corporate a tunable form of pseudo-labeling (Lee,
2013) into the Bottom-Up-Top-Down learning al-
gorithm.
The high-level view of the BUTD learning algo-
rithm is depicted in Algorithm 2.
4 Experimental Results
We investigate the viability of our deep hybrid ar-
chitecture for semi-supervised text categorization.
Model performance was evaluated on the WebKB
data-set
1
and a small-scale version of the 20News-
Group data-set
2
.
The original WebKB collection contains pages
from a variety of universities (Cornell, Texas,
Washington, and Wisconsin as well as miscella-
neous pages from others). The 4-class classifica-
tion problem we defined using this data-set was
to determine if a web-page could be identified as
one belonging to a Student, Faculty, Course, or
a Project, yielding a subset of usable 4,199 sam-
ples. We applied simple pre-processing to the text,
namely stop-word removal and stemming, chose
to leverage only the k most frequently occurring
terms (this varied across the two experiments), and
binarized the document low-level representation
(only 1 page vector was discarded due to pres-
ence of 0 terms). The 20NewsGroup data-set, on
the other hand, contained 16242 total samples and
was already pre-processed, containing 100 terms,
binary-occurrence low-level representation, with
1
The exact data-set we used can be found and downloaded
at http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/
2
The exact data-set we used can be found and downloaded
at http://www.cs.nyu.edu/˜roweis/data.html.
476
tags for the four top-most highest level domains
or meta-topics in the newsgroups array.
For both data-sets, we evaluated model gen-
eralization performance using a stratified 5-fold
cross-validation (CV) scheme. For each possible
train/test split, we automatically partitioned the
training fold into separate labeled, unlabeled, and
validation subsets using stratified random sam-
pling without replacement. Generalization perfor-
mance was evaluated by estimating classification
error, average precision, average recall, and av-
erage F-Measure, where F-Measure was chosen
to be the harmonic mean of precision and recall,
F1 = 2(precision · recall)/(precision + recall).
4.1 Model Designs
We evaluated the BUTD version of our model,
the 3-SBEN,BUTD, as described in Algorithm 2.
For simplicity, the number of latent variables at
each level of the SBEN was held equal to the di-
mensionality of the data (i.e., a complete repre-
sentation). We compared this model trained with
BUTD against a version utilizing only the bottom-
up phase (3-SBEN,BU) as in Ororbia et al. (2015).
Both SBEN models contained 3 layers of latent
variables.
We compared against an array of baseline clas-
sifiers. We used our implementation of an incre-
mental version of Maximum Entropy, or MaxEnt-
ST (which, as explained in Sarikaya et al., 2014,
is equivalent to a softmax classifier). Further-
more, we used our implementation of the Pega-
sos algorithm (SVM-ST) (Shalev-Shwartz et al.,
2011) which was extended to follow a proper
multi-class scheme (Crammer and Singer, 2002).
This is the online formulation of the SVM, trained
via sub-gradient descent on the primal objective
followed by a projection step (for simplicity, we
opted to using a linear-kernel). Additionally, we
implemented a semi-supervised Bernoulli Naive
Bayes classifier (NB-EM) trained via Expectation-
Maximization as in (Nigam et al., 1999). We
also compared our model against the HRBM
(Larochelle and Bengio, 2008) (effectively a sin-
gle layer SBEN), which serves as a powerful, non-
linear shallow classifier in of itself, as well as a
3-layer sparse deep Rectifier Network (Glorot et
al., 2011a), or Rect, composed of leaky rectifier
units.
All shallow classifiers (except NB-EM and the
HRBM) were extended to the semi-supervised set-
Number of Labeled Samples
Mea
n CV
 Erro
r
32 84 168 504 840 1512 2520
0.10
0.15
0.20
0.25
0.30 Model TypesSVM?STSBEN?BUSBEN?BUTD
Figure 2: Mean CV generalization performance as
a function of labeled sample subset size (using 200
features).
ting by leveraging a simple self-training scheme in
order to learn from unlabeled data samples. The
self-training scheme entailed using a classifier’s
estimate of p(y|u) for an unlabeled sample and,
if max[p(y|u)] > p¯, we created a 1-hot proxy
encoding using the argmax of model’s predictor,
where p¯ is a threshold meta-parameter. Since we
found this simple pseudo-labeling approach, sim-
ilar in spirit to (Lee, 2013), to improve the results
for all classifiers, and thus we report all results uti-
lizing this scheme.
3
All classes of models (SBEN,
HRBM, Rect, SVM-ST, MaxEnt-ST, NB-ST) were
subject to the same model selection procedure de-
scribed in the next section.
4.2 Model Selection
Model selection was conducted using a paral-
lelized multi-setting scheme, where a configura-
tion file for each model was specified, describing
a set of hyper-parameter combinations to explore
(this is akin to a course-grained grid search, where
the points of model evaluation are set manually a
priori). For the SBEN’s, we varied the learning
rate ([0.01, 0.25]) and ? coefficient ([0.1, 1.0]) and
3
All model implementations were computationally veri-
fied for correctness when applicable. Since most discrim-
inative objectives followed a gradient descent optimization
scheme and could be realized in an automatic differentiation
framework, we checked gradient validity via finite difference
approximation.
477
Table 1: WEBKB categorization results on 1% of the training data labeled (8 examples per class), rest
unlabeled (i.e., 5-fold means with standard error of the mean, 250 features).
Error Precision Recall F1-Score
NB-EM 0.369± 0.039 0.684± 0.022 0.680± 0.028 0.625± 0.043
MaxEnt-ST 0.402± 0.026 0.623± 0.025 0.593± 0.015 0.583± 0.020
SVM-ST 0.342± 0.020 0.663± 0.010 0.665± 0.014 0.644± 0.015
HRBM 0.252± 0.023 0.740± 0.019 0.765± 0.016 0.741± 0.021
3-Rect 0.328± 0.020 0.673± 0.017 0.680± 0.021 0.654± 0.023
3-SBEN,BU 0.239± 0.015 0.754± 0.014 0.780± 0.016 0.754± 0.015
3-SBEN,BUTD 0.210± 0.011 0.786± 0.009 0.784± 0.014 0.777± 0.012
Table 2: 20NewsGroup data-set categorization results on 1% of the training data labeled (8 examples per
class), rest unlabeled (i.e., 5-fold means with standard error of the mean).
Error Precision Recall F1-Score
NB-EM 0.275± 0.006 0.7176± 0.010 0.6685± 0.010 0.6697± 0.010
MaxEnt-ST 0.335± 0.005 0.643± 0.007 0.643± 0.007 0.639± 0.007
SVM-ST 0.346± 0.008 0.669± 0.016 0.644± 0.012 0.634± 0.011
HRBM 0.284± 0.006 0.706± 0.012 0.699± 0.009 0.696± 0.008
3-Rect 0.318± 0.009 0.661± 0.011 0.661± 0.012 0.657± 0.011
3-SBEN,BU 0.270± 0.006 0.715± 0.009 0.714± 0.009 0.710± 0.007
3-SBEN,BUTD 0.256± 0.007 0.732± 0.005 0.727± 0.006 0.725± 0.006
experimented with stochastic and mean-field ver-
sions of the models
4
(we found that mean-field did
slightly better for this experiment and thus report
the performance of this model in this paper). The
HRBM’s meta-parameters were tuned using a sim-
ilar set-up to (Larochelle et al., 2012) with learn-
ing rate varied in ([0.01, 0.25]), ? in ([0.1, 0.5]),
and ? in ({0.01, 0.1}). For the SVM-ST algo-
rithm, we tuned its slack variable ?, searching in
the interval [0.0001, 0.5], for MaxEnt-ST its learn-
ing rate in [0.0001, 0.1], and for p¯ of all models
(shallow and deep) that used pseudo-labeling we
searched the interval [0.1, 1.0]. All models of all
configurations were trained for a 10,000 iteration
sweep incrementally on the data and the model
state with lowest validation error for that partic-
ular run was used. The SBEN, HRBM, and Rect
models were also set to use a momentum term of
0.9 (linearly increased from 0.1 in the first 1000
training iterations) and the Rect model made use
of a small L1 regularization penalty to encourage
additional hidden sparsity. For a data-set like the
20NewsGroup, which contained a number of unla-
beled samples greater than training iterations, we
view our schema as simulating access to a data-
4
Mean-field simply means no sampling steps were taken
after computing probability vectors, or “means” in any stage
of the computation.
stream, since all models had access to any given
unlabeled example only once during a training run.
4.3 Model Performance
We first conducted an experiment, using the We-
bKB data-set, exploring classification error as a
function of labeled data subset cardinality (Fig-
ure 2). In this setup, we repeated the strati-
fied cross-fold scheme for each possible labeled
data subset size, comparing the performance of
the SVM model against 3-SBEN,BU (blue dot-
ted curve) and 3-SBEN,BUTD (green dash-dotted
curve). We see that as the number of labeled ex-
amples increases (which entails greater human an-
notation effort) all models improve, nearly reach-
ing 90% accuracy. However, while the perfor-
mance difference between models becomes negli-
gible as the training set becomes more supervised,
as expected, it is in the less scarce regions of the
plot we are interested in. We see that for small
proportions, both variants of the SBEN outper-
form the SVM, and furthermore, the SBEN trained
via full BUTD can reach lower error, especially
for the most extreme scenario where only 8 la-
beled examples per class are available. We no-
tice a bump in the performance of BUTD as nearly
the whole training set becomes labeled and posit
that since the BUTD involves additional pseudo-
478
Table 3: Top-most words that the SBEN (BUTD) model associates with the 4 NewsGroup meta-topics.
Meta-Topic Associated Terms
comp.* windows, graphics, card, driver, scsi, dos, files, display
rec.* players, hockey, season, nhl, team, league, baseball, games
sci.* orbit, shuttle, space, earth,mission, nasa,moon, doctor
talk.* jews, christian, religion, jesus, bible, war, israel, president
labeling steps (as in the top-down phase), there is
greater risk of reinforcing incorrect predictions in
the pseudo-joint
5
tuning of layerwise expert pa-
rameters. For text collections where most of the
data is labeled and unlabeled data is minimal, only
a simple bottom-up pass is needed to learn a good
hybrid model of the data.
The next set of experiments was conducted with
only 1% of the training sets labeled. We observe
(Tables 1 and 2) that our deep hybrid architec-
ture trained via BUTD outperforms all other mod-
els with respect to all performance metrics. While
the SBEN trained with simply an online bottom-up
performs significantly better than the SVM model,
we note a further reduction of error using our pro-
posed BUTD training procedure. The additional
top-down phase serves as a mechanism for uni-
fying the layer-wise experts, where error signals
for both labeled and pseudo-labeled examples in-
crease agreement among all model layer experts.
For the 20NewsGroup data-set, we conducted a
simple experiment to uncover some of the knowl-
edge acquired by our model with respect to the tar-
get categorization task. We applied the mechanism
from (Larochelle et al., 2012) to extract the vari-
ables that are most strongly associated with each
of the clamped target variables in the lowest layer
of a BUTD-trained SBEN. The top-scored terms
associated with each class variable are shown in
Table 3, using the 10 hidden nodes most highly
triggered by the clamped class node, in a model
trained on all of the 20NewsGroup data using a
model configuration determined from CV results
for the 20NewsGroup data-set reported in the pa-
per. Since the SBEN is a composition of layer-
wise experts each capable of classification, we
note that this procedure could be applied to each
level to uncover which unobserved variables are
most strongly associated with each class target.
We speculate that this could serve the basis for un-
5
We use the phrase “pseudo-joint” to differentiate a model
that has all its parameters trained jointly from our own, where
only the top-down phase of BUTD introduces any form of
joint parameter modification.
covering the model’s underlying learnt hierarchy
of the data and be potentially used for knowledge
extraction, a subject for future work in analyzing
black box neural models such as our own.
5 Conclusions
We presented the Bottom-Up-Top-Down proce-
dure for training the Stacked Boltzmann Experts
Network, a hybrid architecture that balances both
discriminative and generative learning goals, in
the context of semi-supervised text categorization.
It combines a greedy, layer-wise bottom-up ap-
proach with a top-down fine-tuning method for
pseudo-joint modification of parameters.
Models were evaluated using two text corpora:
WebKB and 20NewsGroup. We compared re-
sults against several baseline models and found
that our hybrid architecture outperformed the oth-
ers in all settings investigated. We found that
the SBEN, especially when trained with the full
Bottom-Up-Top-Down learning procedure could
in some cases improve classification error by as
much 39% over the Pegasos SVM, and nearly 17%
over the HRBM, especially when data is in very
limited supply. While we were able to demon-
strate the viability of our hybrid model when using
only simple surface statistics of text, future work
shall include application of our models to more
semantic-oriented representations, such as those
leveraged in building log-linear language models
(Mikolov et al., 2013).
Acknowledgments
A.G.O. acknowledges support from The Penn-
sylvania State University and the National Sci-
ence Foundation (DGE-1144860). D.R. acknowl-
edges support from the National Science Founda-
tion (SES-1528409).
479
References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo
Larochelle, et al. 2007. Greedy layer-wise training
of deep networks. Advances in neural information
processing systems, 19:153.
Yoshua Bengio. 2012. Deep learning of representa-
tions for unsupervised and transfer learning. Jour-
nal of Machine Learning Research–Workshop and
Conference Proceedings, 27:17–37.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In Proceedings of the Eleventh Annual Conference
on Computational Learning Theory, pages 92–100.
ACM.
Roberto Calandra, Tapani Raiko, Marc Peter Deisen-
roth, and Federico Montesino Pouzols. 2012.
Learning Deep Belief Networks from Non-
stationary Streams. In Artificial Neural Networks
and Machine Learning - ICANN 2012, number
7553 in Lecture Notes in Computer Science, pages
379–386. Springer Berlin Heidelberg.
Cornelia Caragea, Jian Wu, Kyle Williams, Sujatha
Das, Madian Khabsa, Pradeep Teregowda, and
C. Lee Giles. 2014. Automatic identification of
research articles from crawled documents. In Pro-
ceedings of the Workshop: Web-Scale Classifica-
tion: Classifying Big Data from the Web, New York,
NY.
Noam Chomsky. 1980. Rules and representations. Be-
havioral and brain sciences, 3(01):1–15.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. 20(3):273–297.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265–292.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011a. Deep sparse rectifier networks. In Proc.
14th International Conference on Artificial Intelli-
gence and Statistics, volume 15, pages 315–323.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011b. Domain Adaptation for Large-scale Senti-
ment Classification: A Deep Learning Approach. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 513–520.
Sujatha Das Gollapalli, Cornelia Caragea, Prasenjit
Mitra, and C. Lee Giles. 2013. Researcher Home-
page Classification Using Unlabeled Data. In Pro-
ceedings of the 22Nd International Conference on
World Wide Web, WWW ’13, pages 471–482. In-
ternational World Wide Web Conferences Steering
Committee.
Johan Hastad. 1987. Computational limitations of
small-depth circuits. MIT press.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A Fast Learning Algorithm for Deep
Belief Nets. Neural computation, 18(7):1527–1554.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Geoffrey E. Hinton. 2002. Training Products of Ex-
perts by Minimizing Contrastive Divergence. Neu-
ral Computation, 14(8):1771–1800.
Hugo Larochelle and Yoshua Bengio. 2008. Classi-
fication using Discriminative Restricted Boltzmann
Machines. In Proceedings of the 25th International
Conference on Machine Learning, pages 536–543,
Helsinki, Finland.
Hugo Larochelle, Michael Mandel, Razvan Pascanu,
and Yoshua Bengio. 2012. Learning Algorithms
for the Classification Restricted Boltzmann Ma-
chine. The Journal of Machine Learning Research,
13:643–669.
Chen-Yu Lee, Saining Xie, Patrick Gallagher,
Zhengyou Zhang, and Zhuowen Tu. 2014. Deeply-
Supervised Nets. arXiv:1409.5185 [cs, stat].
Dong-Hyun Lee. 2013. Pseudo-label: The Simple
and Efficient Semi-supervised Learning Method for
Deep Neural Networks. In Workshop on Challenges
in Representation Learning, ICML, Atlanta, GA.
Tao Liu. 2010. A Novel Text Classification Approach
Based on Deep Belief Network. In Proceedings of
the 17th International Conference on Neural Infor-
mation Processing: Theory and Algorithms - Volume
Part I, ICONIP’10, pages 314–321. Springer-Verlag.
Zhengdong Lu and Hang Li. 2013. A Deep Architec-
ture for Matching Short Texts. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 26, pages 1367–1375. Cur-
ran Associates, Inc.
Shixiang Lu, Zhenbiao Chen, and Bo Xu. 2014.
Learning new semi-supervised deep auto-encoder
features for statistical machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, volume 1, pages
122–132, Baltimore, MD.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc., Lake
Tahoe, NV.
480
Kamal Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classifica-
tion. In IJCAI-99 workshop on Machine Learning
for Information Filtering, volume 1, pages 61–67.
Alexander G. Ororbia II, David Reitter, Jian Wu, and
C. Lee Giles. 2015. Online learning of deep hybrid
architectures for semi-supervised categorization. In
ECML PKDD, Porto, Portugal. Springer.
Geoffrey K Pullum and Barbara C Scholz. 2002. Em-
pirical assessment of stimulus poverty arguments.
The linguistic review, 18(1-2):9–50.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969–978, July.
R. Sarikaya, G.E. Hinton, and A. Deoras. 2014.
Application of Deep Belief Networks for Natu-
ral Language Understanding. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing,
22(4):778–784.
Robert E. Schapire. 1990. The Strength of Weak
Learnability. Machine learning, 5(2):197–227.
Tanya Schmah, Geoffrey E. Hinton, Steven L. Small,
Stephen Strother, and Richard S. Zemel. 2008.
Generative versus Discriminative Training of RBMs
for classification of fMRI images. In Advances in
neural information processing systems, pages 1409–
1416.
J¨urgen Schmidhuber. 2015. Deep learning in neural
networks: An overview. Neural Networks, 61:85–
117.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal Esti-
mated Sub-gradient Solver for SVM. Mathematical
programming, 127(1):3–30.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Amarnag Subramanya and Jeff Bilmes. 2008. Soft-
supervised learning for text classification. In Em-
pirical Methods in Natural Language Processing,
pages 1090–1099.
Michael Tomasello. 2001. Perceiving intentions
and learning words in the second year of life. In
Melissa Bowerman and Stephen Levinson, editors,
Language acquisition and conceptual development,
pages 132–158.
Jakub M. Tomczak. 2013. Prediction of Breast
Cancer Recurrence using Classification Restricted
Boltzmann Machine with Dropping. arXiv preprint
arXiv:1308.6324.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked Denoising Autoencoders: Learning
Useful Representations in a Deep Network with a
Local Denoising Criterion. The Journal of Machine
Learning Research, 11:3371–3408.
Junbo Zhang, Guangjian Tian, Yadong Mu, and Wei
Fan. 2014. Supervised Deep Learning with Aux-
iliary Networks. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 353–
361, New York City, New York. ACM.
Guanyu Zhou, Kihyuk Sohn, and Honglak Lee. 2012.
Online Incremental Feature Learning with Denois-
ing Autoencoders. In International Conference on
Artificial Intelligence and Statistics, pages 1453–
1461.
481
