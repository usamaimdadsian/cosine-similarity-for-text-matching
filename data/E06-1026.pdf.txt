Latent Variable Models for Semantic Orientations of Phrases
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Japan Society of the Promotion of Science
tinui@lr.pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose models for semantic orienta-
tions of phrases as well as classification
methods based on the models. Although
each phrase consists of multiple words, the
semantic orientation of the phrase is not a
mere sum of the orientations of the com-
ponent words. Some words can invert the
orientation. In order to capture the prop-
erty of such phrases, we introduce latent
variables into the models. Through exper-
iments, we show that the proposed latent
variable models work well in the classifi-
cation of semantic orientations of phrases
and achieved nearly 82% classification ac-
curacy.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial
areas. It can be applied to, for example, a survey
of new products or a questionnaire analysis. Au-
tomatic sentiment analysis enables a fast and com-
prehensive investigation.
The most fundamental step for sentiment anal-
ysis is to acquire the semantic orientations of
words: desirable or undesirable (positive or neg-
ative). For example, the word “beautiful” is pos-
itive, while the word “dirty” is negative. Many
researchers have developed several methods for
this purpose and obtained good results (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al., 2004; Takamura
et al., 2005; Kobayashi et al., 2001). One of
the next problems to be solved is to acquire se-
mantic orientations of phrases, or multi-term ex-
pressions. No computational model for semanti-
cally oriented phrases has been proposed so far al-
though some researchers have used techniques de-
veloped for single words. The purpose of this pa-
per is to propose computational models for phrases
with semantic orientations as well as classification
methods based on the models. Indeed the seman-
tic orientations of phrases depend on context just
as the semantic orientations of words do, but we
would like to obtain the most basic orientations of
phrases. We believe that we can use the obtained
basic orientations of phrases for affect analysis of
higher linguistic units such as sentences and doc-
uments.
The semantic orientation of a phrase is not a
mere sum of its component words. Semantic
orientations can emerge out of combinations of
non-oriented words. For example, “light laptop-
computer” is positively oriented although neither
“light” nor “laptop-computer” has a positive ori-
entation. Besides, some words can invert the ori-
entation of a neighboring word, such as “low”
in “low risk”, where the negative orientation of
“risk” is inverted to a “positive” by the adjective
“low”. This kind of non-compositional operation
has to be incorporated into the model. We focus
on “noun+adjective” in this paper, since this type
of phrase contains most of interesting properties
of phrases, such as emergence or inversion of se-
mantic orientations.
In order to capture the properties of semantic
orientations of phrases, we introduce latent vari-
ables into the models, where one random variable
corresponds to nouns and another random vari-
able corresponds to adjectives. The words that
are similar in terms of semantic orientations, such
as “risk” and “mortality” (i.e., the positive ori-
entation emerges when they are “low”), make a
cluster in these models. Our method is language-
201
independent in the sense that it uses only cooccur-
rence data of words and semantic orientations.
2 Related Work
We briefly explain related work from two view-
points: the classification of word pairs and the
identification of semantic orientation.
2.1 Classification of Word Pairs
Torisawa (2001) used a probabilistic model to
identify the appropriate case for a pair of words
constituting a noun and a verb with the case of
the noun-verb pair unknown. Their model is the
same as Probabilistic Latent Semantic Indexing
(PLSI) (Hofmann, 2001), which is a generative
probability model of two random variables. Tori-
sawa’s method is similar to ours in that a latent
variable model is used for word pairs. How-
ever, Torisawa’s objective is different from ours.
In addition, we used not the original PLSI, but
its expanded version, which is more suitable for
this task of semantic orientation classification of
phrases.
Fujita et al. (2004) addressed the task of the de-
tection of incorrect case assignment in automat-
ically paraphrased sentences. They reduced the
task to a problem of classifying pairs of a verb
and a noun with a case into correct or incorrect.
They first obtained a latent semantic space with
PLSI and adopted the nearest-neighbors method,
in which they used latent variables as features. Fu-
jita et al.’s method is different from ours, and also
from Torisawa’s, in that a probabilistic model is
used for feature extraction.
2.2 Identification of Semantic Orientations
The semantic orientation classification of words
has been pursued by several researchers (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al., 2004; Takamura et
al., 2005). However, no computational model for
semantically oriented phrases has been proposed
to date although research for a similar purpose has
been proposed.
Some researchers used sequences of words as
features in document classification according to
semantic orientation. Pang et al. (2002) used bi-
grams. Matsumoto et al. (2005) used sequential
patterns and tree patterns. Although such patterns
were proved to be effective in document classi-
fication, the semantic orientations of the patterns
themselves are not considered.
Suzuki et al. (2006) used the Expectation-
Maximization algorithm and the naive bayes clas-
sifier to incorporate the unlabeled data in the clas-
sification of 3-term evaluative expressions. They
focused on the utilization of context information
such as neighboring words and emoticons. Tur-
ney (2002) applied an internet-based technique to
the semantic orientation classification of phrases,
which had originally been developed for word sen-
timent classification. In their method, the num-
ber of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
“phrase NEAR good”) is used to determine the
orientation. Baron and Hirst (2004) extracted col-
locations with Xtract (Smadja, 1993) and classi-
fied the collocations using the orientations of the
words in the neighboring sentences. Their method
is similar to Turney’s in the sense that cooccur-
rence with seed words is used. The three methods
above are based on context information. In con-
trast, our method exploits the internal structure of
the semantic orientations of phrases.
Inui (2004) introduced an attribute plus/minus
for each word and proposed several rules that
determine the semantic orientations of phrases
on the basis of the plus/minus attribute val-
ues and the positive/negative attribute values of
the component words. For example, a rule
[negative+minus=positive] determines “low (mi-
nus) risk (negative)” to be positive. Wilson et
al. (2005) worked on phrase-level semantic orien-
tations. They introduced a polarity shifter, which
is almost equivalent to the plus/minus attribute
above. They manually created the list of polarity
shifters. The method that we propose in this paper
is an automatic version of Inui’s or Wilson et al.’s
idea, in the sense that the method automatically
creates word clusters and their polarity shifters.
3 Latent Variable Models for Semantic
Orientations of Phrases
As mentioned in the Introduction, the semantic
orientation of a phrase is not a mere sum of its
component words. If we know that “low risk” is
positive, and that “risk” and “mortality”, in some
sense, belong to the same semantic cluster, we can
infer that “low mortality” is also positive. There-
fore, we propose to use latent variable models to
extract such latent semantic clusters and to real-
ize an accurate classification of phrases (we focus
202
N Z
A
N
A C
N Z
A C
N Z
A C
N Z
A C
(a) (b) (c) (d) (e)
Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped;
Each node indicates a random variable. Arrows indicate statistical dependency between variables. N , A,
Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations.
on two-term phrases in this paper). The models
adopted in this paper are also used for collabora-
tive filtering by Hofmann (2004).
With these models, the nouns (e.g., “risk” and
“mortality”) that become positive by reducing
their degree or amount would make a cluster. On
the other hand, the adjectives or verbs (e.g., “re-
duce” and “decrease”) that are related to reduction
would also make a cluster.
Figure 1 shows graphical representations of sta-
tistical dependencies of models with a latent vari-
able. N , A, Z and C respectively correspond to
nouns, adjectives, latent clusters and semantic ori-
entations. Figure 1-(a) is the PLSI model, which
cannot be used in this task due to the absence of
a variable for semantic orientations. Figure 1-(b)
is the naive bayes model, in which nouns and ad-
jectives are statistically independent of each other
given the semantic orientation. Figure 1-(c) is,
what we call, the 3-PLSI model, which is the 3-
observable variable version of the PLSI. We call
Figure 1-(d) the triangle model, since three of its
four variables make a triangle. We call Figure 1-
(e) the U-shaped model. In the triangle model and
the U-shaped model, adjectives directly influence
semantic orientations (rating categories) through
the probability P (c|az). While nouns and adjec-
tives are associated with the same set of clusters Z
in the 3-PLSI and the triangle models, only nouns
are clustered in the U-shaped model.
In the following, we construct a probability
model for the semantic orientations of phrases us-
ing each model of (b) to (e) in Figure 1. We ex-
plain in detail the triangle model and the U-shaped
model, which we will propose to use for this task.
3.1 Triangle Model
Suppose that a set D of tuples of noun n, adjective
a (predicate, generally) and the rating c is given :
D = {(n1, a1, c1), · · · , (n|D|, a|D|, c|D|)}, (1)
where c ? {?1, 0, 1}, for example. This can be
easily expanded to the case of c ? {1, · · · , 5}. Our
purpose is to predict the rating c for unknown pairs
of n and a.
According to Figure 1-(d), the generative prob-
ability of n, a, c, z is the following :
P (nacz) = P (z|n)P (a|z)P (c|az)P (n). (2)
Remember that for the original PLSI model,
P (naz) = P (z|n)P (a|z)P (n).
We use the Expectation-Maximization (EM) al-
gorithm (Dempster et al., 1977) to estimate the pa-
rameters of the model. According to the theory of
the EM algorithm, we can increase the likelihood
of the model with latent variables by iteratively in-
creasing the Q-function. The Q-function (i.e., the
expected log-likelihood of the joint probability of
complete data with respect to the conditional pos-
terior of the latent variable) is expressed as :
Q(?) =
?
nac
fnac
?
z
P¯ (z|nac) log P (nazc|?), (3)
where ? denotes the set of the new parameters.
fnac denotes the frequency of a tuple n, a, c in the
data. P¯ represents the posterior computed using
the current parameters.
The E-step (expectation step) corresponds to
simple posterior computation :
P¯ (z|nac) = P (z|n)P (a|z)P (c|az)?
z P (z|n)P (a|z)P (c|az)
. (4)
For derivation of update rules in the M-step (max-
imization step), we use a simple Lagrange method
for this optimization problem with constraints :
?z, ?n P (n|z) = 1, ?z,
?
a P (a|z) = 1, and
?a, z, ?c P (c|az) = 1. We obtain the following
update rules :
P (z|n) =
?
ac fnacP¯ (z|nac)
?
ac fnac
, (5)
203
P (y|z) =
?
nc fnacP¯ (z|nac)
?
nac fnacP¯ (z|nac)
, (6)
P (c|az) =
?
n fnacP¯ (z|nac)
?
nc fnacP¯ (z|nac)
. (7)
These steps are iteratively computed until conver-
gence. If the difference of the values of Q-function
before and after an iteration becomes smaller than
a threshold, we regard it as converged.
For classification of an unknown pair n, a, we
compare the values of
P (c|na) =
?
z P (z|n)P (a|z)P (c|az)
?
cz P (z|n)P (a|z)P (c|az)
. (8)
Then the rating category c that maximize P (c|na)
is selected.
3.2 U-shaped Model
We suppose that the conditional probability of c
and z given n and a is expressed as :
P (cz|na) = P (c|az)P (z|n). (9)
We compute parameters above using the EM al-
gorithm with the Q-function :
Q(?) =
?
nac
fnac
?
z
P¯ (z|nac) log P (cz|na, ?).(10)
We obtain the following update rules :
E step
P¯ (z|nac) = P (c|az)P (z|n)?
z P (c|az)P (z|n)
, (11)
M step
P (c|az) =
?
n fnacP¯ (z|nac)
?
nc fnacP¯ (z|nac)
, (12)
P (z|n) =
?
ac fnacP¯ (z|nac)
?
ac fnac
. (13)
For classification, we use the formula :
P (c|na) =
?
z
P (c|az)P (z|n). (14)
3.3 Other Models for Comparison
We will also test the 3-PLSI model corresponding
to Figure 1-(c).
In addition to the latent models, we test a base-
line classifier, which uses the posterior probabil-
ity :
P (c|na) ? P (n|c)P (a|c)P (c). (15)
This baseline model is equivalent to the 2-term
naive bayes classifier (Mitchell, 1997). The graph-
ical representation of the naive bayes model is (b)
in Figure 1. The parameters are estimated as :
P (n|c) = 1 + fnc|N | + fc
, (16)
P (a|c) = 1 + fac|A| + fc
, (17)
where |N | and |A| are the numbers of the words
for n and a, respectively.
Thus, we have four different models : naive
bayes (baseline), 3-PLSI, triangle, and U-shaped.
3.4 Discussions on the EM computation, the
Models and the Task
In the actual EM computation, we use the tem-
pered EM (Hofmann, 2001) instead of the stan-
dard EM explained above, because the tempered
EM can avoid an inaccurate estimation of the
model caused by “over-confidence” in computing
the posterior probabilities. The tempered EM can
be realized by a slight modification to the E-step,
which results in a new E-step :
P¯ (z|nac) =
(
P (c|az)P (z|n)
)?
?
z
(
P (c|az)P (z|n)
)? , (18)
for the U-shaped model, where ? is a positive
hyper-parameter, called the inverse temperature.
The new E-steps for the other models are similarly
expressed.
Now we have two hyper-parameters : inverse
temperature ?, and the number of possible val-
ues M of latent variables. We determine the
values of these hyper-parameters by splitting the
given training dataset into two datasets (the tempo-
rary training dataset 90% and the held-out dataset
10%), and by obtaining the classification accuracy
for the held-out dataset, which is yielded by the
classifier with the temporary training dataset.
We should also note that Z (or any variable)
should not have incoming arrows simultaneously
from N and A, because the model with such ar-
rows has P (z|na), which usually requires an ex-
cessively large memory.
To work with numerical scales of the rating
variable (i.e., the difference between c = ?1 and
c = 1 should be larger than that of c = ?1
and c = 0), Hofmann (2004) used also a Gaus-
sian distribution for P (c|az) in collaborative filter-
ing. However, we do not employ a Gaussian, be-
cause in our dataset, the number of rating classes is
204
only 3, which is so small that a Gaussian distribu-
tion cannot be a good approximation of the actual
probability density function. We conducted pre-
liminary experiments with the model with Gaus-
sians, but failed to obtain good results. For other
datasets with more classes, Gaussians might be a
good model for P (c|az).
The task we address in this paper is somewhat
similar to the trigram prediction task, in the sense
that both are classification tasks given two words.
However, we should note the difference between
these two tasks. In our task, the actual answer
given two specific words are fixed as illustrated
by the fact ‘high+salary’ is always positive, while
the answer for the trigram prediction task is ran-
domly distributed. We are therefore interested in
the semantic orientations of unseen pairs of words,
while the main purpose of the trigram prediction
is accurately estimate the probability of (possibly
seen) word sequences.
In the proposed models, only the words that ap-
peared in the training dataset can be classified. An
attempt to deal with the unseen words is an in-
teresting task. For example, we could extend our
models to semi-supervised models by regarding C
as a partially observable variable. We could also
use distributional similarity of words (e.g., based
on window-size cooccurrence) to find an observed
word that is most similar to the given unseen word.
However, such methods would not work for the
semantic orientation classification, because those
methods are designed for simple cooccurrence and
cannot distinguish “survival-rate” from “infection-
rate”. In fact, the similarity-based method men-
tioned above failed to work efficiently in our pre-
liminary experiments. To solve the problem of un-
seen words, we would have to use other linguistic
resources such as a thesaurus or a dictionary.
4 Experiments
4.1 Experimental Settings
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper ar-
ticles (1995) written in Japanese, and annotated
the pairs with semantic orientation tags : positive,
neutral or negative. We thus obtained the labeled
dataset consisting of 12066 pair instances (7416
different pairs). The dataset contains 4459 neg-
ative instances, 4252 neutral instances, and 3355
positive instances. The number of distinct nouns is
4770 and the number of distinct adjectives is 384.
To check the inter-annotator agreement between
two annotators, we calculated ? statistics, which
was 0.640. This value is allowable, but not quite
high. However, positive-negative disagreement is
observed for only 0.7% of the data. In other words,
this statistics means that the task of extracting neu-
tral examples, which has hardly been explored, is
intrinsically difficult.
We employ 10-fold cross-validation to obtain
the average value of the classification accuracy.
We split the dataset such that there is no overlap-
ping pair (i.e., any pair in the training dataset does
not appear in the test dataset).
If either of the two words in a pair in the test
dataset does not appear in the training dataset, we
excluded the pair from the test dataset since the
problem of unknown words is not in the scope of
this research. Therefore, we evaluate the pairs that
are not in the training dataset, but whose compo-
nent words appear in the training dataset.
In addition to the original dataset, which we call
the standard dataset, we prepared another dataset
in order to examine the power of the latent variable
model. The new dataset, which we call the hard
dataset, consists only of examples with 17 difficult
adjectives such as “high”, “low”, “large”, “small”,
“heavy”, and “light”. 1 The semantic orientations
of pairs including these difficult words often shift
depending on the noun they modify. Thus, the
hard dataset is a subset of the standard dataset. The
size of the hard dataset is 4787. Please note that
the hard dataset is used only as a test dataset. For
training, we always use the standard dataset in our
experiments.
We performed experiments with all the values
of ? in {0.1, 0.2, · · · , 1.0} and with all the values
of M in {10, 30, 50, 70, 100, 200, 300, 500}, and
predicted the best values of the hyper-parameters
with the held-out method in Section 3.4.
4.2 Results
The classification accuracies of the four methods
with ? and M predicted by the held-out method
are shown in Table 1. Please note that the naive
bayes method is irrelevant of ? and M . The table
shows that the triangle model and the U-shaped
1The complete list of the 17 Japanese adjectives with their
English counterparts are : takai (high), hikui (low), ookii
(large), chiisai (small), omoi (heavy), karui (light), tsuyoi
(strong), yowai (weak), ooi (many), sukunai (few/little), nai
(no), sugoi (terrific), hageshii (terrific), hukai (deep), asai
(shallow), nagai (long), mizikai (short).
205
Table 1: Accuracies with predicted ? and M
standard hard
accuracy ? M accuracy ? M
Naive Bayes 73.40 – – 65.93 – –
3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4
Triangle model 81.39 0.60 174.0 77.95 0.60 191.0
U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3
model achieved high accuracies and outperformed
the naive bayes method. This result suggests that
we succeeded in capturing the internal structure
of semantically oriented phrases by way of latent
variables. The more complex structure of the tri-
angle model resulted in the accuracy that is higher
than that of the U-shaped model.
The performance of the 3-PLSI method is even
worse than the baseline method. This result shows
that we should use a model in which adjectives can
directly influence the rating category.
Figures 2, 3, 4 show cross-validated accuracy
values for various values of ?, respectively yielded
by the 3-PLSI model, the triangle model and the
U-shaped model with different numbers M of pos-
sible states for the latent variable. As the figures
show, the classification performance is sensitive to
the value of ?. M = 100 and M = 300 are mostly
better than M = 10. However, this is a tradeoff
between classification performance and training
time, since large values of M demand heavy com-
putation. In that sense, the U-shaped model is use-
ful in many practical situations, since it achieved a
good accuracy even with a relatively small M .
To observe the overall tendency of errors, we
show the contingency table of classification by the
U-shaped model with the predicted values of hy-
perparameters, in Table 2. As this table shows,
most of the errors are caused by the difficulty of
classifying neutral examples. Only 2.26% of the
errors are mix-ups of the positive orientation and
the negative orientation.
We next investigate the causes of errors by ob-
serving those mix-ups of the positive orientation
and the negative orientation.
One type of frequent errors is illustrated by the
pair “food (’s price) is high”, in which the word
“price” is omitted in the actual example 2. As in
this expression, the attribute (price, in this case) of
an example is sometimes omitted or not correctly
2This kind of ellipsis often occurs in Japanese.
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 2: 3-PLSI model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 3: Triangle model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 4: U-shaped model with standard dataset
206
Table 2: Contingency table of classification result by the U-shaped model
U-shaped model
positive neutral negative sum
positive 1856 281 69 2206
Gold standard neutral 202 2021 394 2617
negative 102 321 2335 2758
sum 2160 2623 2798 7581
identified. To tackle these examples, we will need
methods for correctly identifying attributes and
objects. Some researchers are starting to work on
this problem (e.g., Popescu and Etzioni (2005)).
We succeeded in addressing the data-sparseness
problem by introducing a latent variable. How-
ever, this problem still causes some errors. Pre-
cise statistics cannot be obtained for infrequent
words. This problem will be solved by incorporat-
ing other resources such as thesaurus or a dictio-
nary, or combining our method with other methods
using external wider contexts (Suzuki et al., 2006;
Turney, 2002; Baron and Hirst, 2004).
4.3 Examples of Obtained Clusters
Next, we qualitatively evaluate the proposed meth-
ods. For several clusters z, we extract the words
that occur more than twice in the whole dataset
and are in top 50 according to P (z|n). The model
used here as an example is the U-shaped model.
The experimental settings are ? = 0.6 and M =
60. Although some elements of clusters are com-
posed of multiple words in English, the original
Japanese counterparts are single words.
Cluster 1 trouble, objection, disease, complaint, anx-
iety, anamnesis, relapse
Cluster 2 risk, mortality, infection rate, onset rate
Cluster 3 bond, opinion, love, meaning, longing, will
Cluster 4 vote, application, topic, supporter
Cluster 5 abuse, deterioration, shock, impact, burden
Cluster 6 deterioration, discrimination, load, abuse
Cluster 7 relative importance, degree of influence,
number, weight, sense of belonging, wave,
reputation
These obtained clusters match our intuition. For
example, in cluster 2 are the nouns that are neg-
ative when combined with “high”, and positive
when combined with “low”. In fact, the posterior
probabilities of semantic orientations for cluster 2
are as follows :
P (negative|high, cluster 2) = 0.995,
P (positive|low, cluster 2) = 0.973.
With conventional clustering methods based on
the cooccurrence of two words, cluster 2 would
include the words resulting in the opposite orien-
tation, such as “success rate”. We succeeded in
obtaining the clusters that are suitable for our task,
by incorporating the new variable c for semantic
orientation in the EM computation.
5 Conclusion
We proposed models for phrases with semantic
orientations as well as a classification method
based on the models. We introduced a latent vari-
able into the models to capture the properties of
phrases. Through experiments, we showed that
the proposed latent variable models work well
in the classification of semantic orientations of
phrases and achieved nearly 82% classification ac-
curacy. We should also note that our method is
language-independent although evaluation was on
a Japanese dataset.
We plan next to adopt a semi-supervised learn-
ing method in order to correctly classify phrases
with infrequent words, as mentioned in Sec-
tion 4.2. We would also like to extend our method
to 3- or more term phrases. We can also use the
obtained latent variables as features for another
classifier, as Fujita et al. (2004) used latent vari-
ables of PLSI for the k-nearest neighbors method.
One important and promising task would be the
use of semantic orientations of words for phrase
level classification.
References
Faye Baron and Graeme Hirst. 2004. Collocations
as cues to semantic orientation. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society Series B, 39(1):1–38.
207
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in au-
tomatically generated paraphrases of Japanese sen-
tences. In Proceedings of the 1st International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 14–21.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of ad-
jectives. In Proceedings of the Thirty-Fifth Annual
Meeting of the Association for Computational Lin-
guistics and the Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 174–181.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42:177–196.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22:89–115.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute
of Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115–1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelli-
gence, SLUD-33, pages 45–50.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD-
05), pages 301–310.
Tom M. Mitchell. 1997. Machine Learning. McGraw
Hill.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’02), pages 79–86.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of joint conference on Hu-
man Language Technology / Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP’05), pages 339–346.
Frank Z. Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,,
19(1):143–177.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Pro-
ceedings of the 7th International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-06), pages 502–513.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 133–140.
Kentaro Torisawa. 2001. An unsuperveised method
for canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS 2001), pages
211–218.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21(4):315–346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’02), pages 417–424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of joint
conference on Human Language Technology / Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP’05), pages 347–354.
208
