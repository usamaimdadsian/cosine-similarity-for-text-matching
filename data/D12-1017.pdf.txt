Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 183–193, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Local and Global Context
for Supervised and Unsupervised Metonymy Resolution
Vivi Nastase
HITS gGmbH
Heidelberg, Germany
vivi.nastase@h-its.org
Alex Judea
University of Stuttgart
Stuttgart, Germany
alexander.judea@ims.uni-stuttgart.de
Katja Markert
University of Leeds
Leeds, UK
K.Markert@leeds.ac.uk
Michael Strube
HITS gGmbH
Heidelberg, Germany
michael.strube@h-its.org
Abstract
Computational approaches to metonymy res-
olution have focused almost exclusively on
the local context, especially the constraints
placed on a potentially metonymic word by
its grammatical collocates. We expand such
approaches by taking into account the larger
context. Our algorithm is tested on the data
from the metonymy resolution task (Task 8) at
SemEval 2007. The results show that incorpo-
ration of the global context can improve over
the use of the local context alone, depending
on the types of metonymies addressed. As a
second contribution, we move towards unsu-
pervised resolution of metonymies, made fea-
sible by considering ontological relations as
possible readings. We show that such an unsu-
pervised approach delivers promising results:
it beats the supervised most frequent sense
baseline and performs close to a supervised
approach using only standard lexico-syntactic
features.
1 Introduction
With the exception of explicit tasks in metonymy
and metaphor analysis, computational treatment of
language relies on the assumption that the texts to be
processed have a literal interpretation. This contrasts
with the fact that figurative expressions are com-
mon in language, as exemplified by the metonymy
in the excerpt from a Wikipedia article in Exam-
ple 1 and another in Example 2 from the SemEval
2007 metonymy resolution task (Markert and Nis-
sim, 2009).
(1) In the gold medal game, Canada defeated the
American team 2-0 to win their third consecu-
tive gold.
(2) This keyword is only required when your rela-
tional database is Oracle.
The defeating in Example 1 will not be done
by the country as such, but by a team represent-
ing the country in a sporting event. Hence, in a
metonymy a potentially metonymic expression or
word (here Canada) stands for a conceptually re-
lated entity (here, people of Canada). In the sec-
ond Example, a company name (Oracle) stands for
a product (database) developed by the company.
Metonymy resolution can be important for a
variety of tasks. Textual entailment may need
metonymy resolution (Bentivogli et al., 2007): for
example, we would like to be able to induce from
Example 1 the hypothesis
The Canadian team won . . . .
Leveling and Hartrumpf (2008) show that
metonymy recognition on location proper names
helps geographical information retrieval by ex-
cluding metonymically used place names from
consideration (such as Example 1 or the use of
Vietnam for the Vietnam war). Metonymies also fre-
quently interact with anaphora resolution (Nunberg,
1995; Markert and Hahn, 2002), as in Example 1
where the metonymic use of Canada is referred to
by a plural pronoun afterward (their).
Metonymies can be quite regular: company
names can be used for their management or their
products, country names can be used for associated
sports teams. Following from this, the currently
183
prevalent set-up for metonymy resolution — as in
the SemEval 2007 task — provides a manually com-
piled list of frequent readings or metonymic patterns
such as organization-for-product for pre-
specified semantic classes (such as organizations) as
well as annotated examples for these patterns so that
systems can then treat metonymy resolution as a (su-
pervised) word sense disambiguation task. How-
ever, this approach needs novel, manual provision
of readings as well as annotated examples for each
new semantic class.
In contrast, we will see readings as relations be-
tween the potentially metonymic word (PMW) and
other concepts in a large concept network, a priori
allowing all possible relations as readings. We base
this approach on the observation that metonymic
words stand in for concepts that they are related
with – e.g. the part for the whole, the company
for the product. These readings are obtained on
the fly and are therefore independent of manually
provided, preclassified interpretations or semantic
classes, leading eventually to the possibility of un-
supervised metonymy resolution. We achieve this
by first linking a PMW to an article in Wikipedia.
Then we extract from a large concept network de-
rived from Wikipedia the relations surrounding the
PMW.
As there will be (many) more than one such rela-
tion, these need to be ranked or scored. We achieve
this in a probabilistic framework where we condi-
tion the probability of a relation on the context of
the PMW. This ranking showcases our second major
innovation in that the flexibility of our framework al-
lows us to incorporate a wider context than in most
prior approaches. Let us consider the indications for
metonymic readings and its interpretation in Exam-
ple 1, on the one hand, and Example 2, on the other
hand. In Example 1, the grammatical relation to the
verb defeat and the verb’s selectional preferences in-
dicate the metonymy. We will call all such grammat-
ically related words and the grammatical relations
the local context of the PMW. Such types of local
context have been used by most prior approaches
(Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991;
Nastase and Strube, 2009, among others). However,
Example 2 shows that the local context can be am-
biguous or often weak, such as the verb to be. In
these examples, the wider context (database, key-
word) is a better indication for a metonymy but has
not been satisfactorily integrated in prior approaches
(see Section 2). We here call all words surround-
ing the PMW but not grammatically related to it the
global context.
In our approach we integrate both the local and
the global context in our probabilistic framework.
For the local context, we compute the selectional
preferences for the words related to the PMW from a
corpus of English Wikipedia articles and generalize
them in the Wikipedia concept network, thus (auto-
matically) providing a set of abstractions – general
concepts in the network that capture the semantic
classes required by the local context. In the next
step we compute probabilities of the global con-
text surrounding the PMWs under each (locally re-
quired) abstraction, and combine this with the se-
lectional preferences of the grammatically related
words. That we can integrate local and global con-
text in one probabilistic but also knowledge-based
framework is possible because we combine two de-
scriptions of meaning – ontological and distribu-
tional – by exploiting different sources of informa-
tion in Wikipedia (category-article hierarchy and ar-
ticle texts).
We compute the probabilities of the relations (=
readings) between the concept corresponding to the
PMW and its directly related concepts. These can
be used either (i) as additional features in a super-
vised approach or (ii) directly for unsupervised res-
olution. We do both in this paper and show that (i)
the supervised approach using both local and global
context can outperform one using just local con-
text, dependent on the semantic class studied and
(ii) that an unsupervised approach — although lower
than the supervised one — outperforms the super-
vised most frequent reading baseline and performs
close to a standard supervised model with the basic
set of lexico-syntactic features (Nissim and Markert,
2005).
2 Related Work
The word sense disambiguation setting for
metonymy resolution as developed by Nissim
and Markert (2005) and used for the SemEval 2007
task (Markert and Nissim, 2009) uses a small, pre-
specified number of frequently occurring readings.
184
The approaches building on this work (Farkas et
al., 2007; Nicolae et al., 2007, among others) are
supervised, mostly using shallow surface features
as well as grammatical relations.1 Most effective
in the SemEval task as summarized in Markert
and Nissim (2009) has been the local, grammatical
context, with the two systems relying on the global
context or the local/global context in a BOW model
(Leveling, 2007; Poibeau, 2007) not outperforming
the most frequent reading baseline. We believe
that might be due to the lack of a link between the
local and global context in these approaches — in
our work, we condition the global context on the
abstractions and selectional preferences yielded by
the local context and achieve better results.
Lapata (2003), Shutova (2009) as well as Roberts
and Harabagiu (2011) deal with the issue of logical
metonymy, where the participant stands in for the
full event: e.g. Mary enjoyed the book., where book
stands in for reading the book, and this missing event
(reading) can be inferred from a corpus. Utiyama
et al. (2000), Lapata (2003) propose a probabilis-
tic model for finding the correct interpretation of
such metonymies in an unsupervised manner. How-
ever, these event type metonymies differ from the
problem dealt with in our paper and the SemEval
2007 task in that their recognition (i.e. their distinc-
tion from literal occurrences) is achieved simply by
grammatical patterns (a noun instead of a gerund or
to-infinitive following the verb) and the problem is
limited to interpretation.
Our view of relations in a concept network being
the interpretations of metonymies is strongly remi-
niscent of older work in metonymy resolution such
as Hobbs et al. (1993), Fass (1991), Markert and
Hahn (2002) or the use of a generative lexicon and
its relations in Pustejovsky (1991), which also are
unsupervised. However, these approaches lacked
scalability due to the use of small hand-modeled
knowledge bases which our use of a very large
Wikipedia-derived ontology overcomes. In addition,
most of these approaches (Fass, 1991; Hobbs et al.,
1993; Pustejovsky, 1991; Harabagiu, 1998) rely on
the view that metonymies violate selectional restric-
tions in their immediate, local context, usually those
1Brun et al. (2007) is semi-supervised but again relies on the
local grammatical context.
imposed by the verbs on their arguments. As can
be seen in the Example 2, this misses metonymies
which do not violate selectional restrictions. Nas-
tase and Strube (2009) use more flexible proba-
bilistic selectional preferences instead of strict con-
straint violations as well as WordNet as a larger tax-
onomy but are also restricted to the local context.
Markert and Hahn (2002) do propose a treatment of
metonymies that takes into account the larger dis-
course in the form of anaphoric relations between
a metonymy and the prior context. However, they
constrain discourse integration to potential PMWs
that are definite NPs and the context to few previous
noun phrases. In addition, their framework uses a
strict rule-based ranking of competing readings that
cannot be easily extended.
The work presented here also relies on a con-
cept network, built automatically from Wikipedia.
This resource provides us with links between enti-
ties in the text, and also a variety of ontological re-
lations for the PMW, that will allow us to identify a
wide variety of metonymic interpretations. Our ap-
proach combines information from the concept net-
work with automatically acquired selectional prefer-
ences as well as a possibility to combine in a prob-
abilistic framework the influence of the local and
global context on the interpretation of a potentially
metonymic word.
3 The Approach
The approach we present takes into account both
the local, grammatical, context and the larger textual
context of a potentially metonymic word. Figure 1
presents a graphical representation of our approach.
On the one hand, the word/term to be interpreted
(the potentially metonymic word/term – PMW) is
mapped onto a concept in the concept network (Sec-
tion 3.3), which gives us access to the conceptual
relations (Ri) between the PMW and other concepts
(cx ? CRi). On the other hand, any word w gram-
matically related to the PMW via a grammatical re-
lation r provides us with semantic restrictions on the
interpretation of the PMW, namely preferred seman-
tic classes Aj (we call them abstractions) and a se-
lectional preference score.2 These are automatically
2We restrict the grammatical context that provides selec-
tional preferences to verbs or adjectives grammatically related
185
A 1 A 2
1R kR
A n
12c
14c
11c
13c 1n?1c
1nc
k1c k2
c
k4ck3c
kmc
km?1c w1w2w3
wl
w
r
...
...
PMW
p(Ri|Aj) p(Aj|Cont,w,r)
Global context
...
... ...
...
... ...
Figure 1: Metonymy resolution using selectional preferencesAj derived from local contextw and r, semantic relations
Ri to the PMW from a concept network, and the global context surrounding a term to be interpreted
acquired by using a corpus of Wikipedia articles and
a repository of encyclopedic knowledge (presented
in Section 3.1), as described in detail in 3.2. Because
the abstractions Aj and the PMW’s related concepts
(cx) come from the same structured resource, we
can compute the probabilities for each Ri given the
grammatically related word w and the grammatical
relation r. The global context can also easily be
added to the computation, as the probability of each
word in the context relative to an abstraction Aj can
be computed through the resource’s is a hierarchy
and its link to Wikipedia articles. This is detailed in
Section 3.4.
3.1 A concept network obtained from
Wikipedia
We use a Wikipedia article dump (January 2011)
which provided over 3.5 million English articles,
interconnected through a hierarchy of categories
and hyperlinks. This partly structured repository
is transformed into a large-scale multilingual con-
cept network, whose nodes are concepts correspond-
ing to articles or categories in Wikipedia (Nastase
et al., 2010). Concepts in this network are con-
nected through a variety of semantic relations (e.g.
is a, member of, nationality) derived from category
names and infoboxes. The version of WikiNet used
to the PMW.
had 3,707,718 nodes and 49,931,266 relation in-
stances of 494 types, and is freely available3.
WikiNet is used here as a concept inventory,
and its links and structure to generalize more spe-
cific concepts identified in texts to general concepts.
The fact that nodes in WikiNet correspond to arti-
cles/categories in Wikipedia is used to link article
texts in Wikipedia to general concepts, for the pur-
pose of computing various probability scores (de-
tailed in Section 3.4).
3.2 Selectional preferences and abstractions
To compute selectional preferences we use the set of
English Wikipedia articles, which describe specific
concepts. Wikipedia contributors are encouraged to
insert hyperlinks, which link important terms in an
article to the corresponding articles. A hyperlink
consists of two parts, the actual link (i.e. a URL)
and a phrase to appear in the text. Hyperlinks then
constitute a bridge from the textual level to the con-
ceptual level without the need for word sense dis-
ambiguation. We exploit these links to gather con-
cept arguments for verbs and adjectives, and gen-
eralize these using the concept network built from
Wikipedia.
The corpus of Wikipedia articles was first en-
riched with hyperlinks, making the “one sense per
3http://www.h-its.org/english/research/
nlp/download/wikinet.php
186
Algorithm 1 computeSelPrefs(G,WkN)
Input: G – grammatical relation triples
WkN – WikiNet
M – maximum number of generalization steps
Output: ?
? = {}
for all (w, r) such that (c, r, w) ? G do
S = {(c, f)|f is the frequency of (c, r, w) in G}
?w,r = S
mdl = MDL(?w,r,S)
for all i = 1,M do
?? = abstract(S,WkN)
mdl?? = MDL(??,S)
if mdl?? < mdl then
?w,r = ??
? = {?w,r} ? ?
return ?
Algorithm 2 MDL(?,S)
Input: ? = {(c, f)} – a scored list of concepts
S – the set of observations (concept collocates)
Output: MDL(?,S)
?? =< f1, ..., fn >; (ci, fi) ? ?
remove {(c, f) ? ?|f = 1} // parameter description
length :
L(??|?) = |?|?12 ? log(|S|) // data description length :
for all (c, f) ? ? do
L(S|?, ??) = L(S|?, ??) + f ? log( fhyponyms(c)?|?| )
return L(??|?)? L(S|?, ??)
Algorithm 3 abstract(S,WkN)
Input: S = {(c, f)|(w,R, c) ? G}
WkN – WikiNet
Output: S ?
S ? = {}
for c|(c, ) ? S do
while c has only one is a link do
c = c?, (c, is a, c?) ?WkN
C = {(c?, c)|(c, is a, c?) ?WkN}
for (c?, c) ? C do
if (c?, f ?) ? S ? then
replace (c?, f ?) with (c?, f ? + f|C| ), (c, f) ? S
in S ?
else
S ?? = {(c?, f)}, (c, f) ? S
// Remove hyponyms.
for all {(c, c?) ? S ?|(c?, is a, c) ?WkN} do
// update frequency f of c
fc = fc + fc? , f ? S
delete c?
return S ?
discourse” assumption – a phrase that appears as-
sociated with a hyperlink once in the article body
will be associated with the same hyperlink through-
out the article (this applies to the article title as well,
which is not hyperlinked in the article itself). This
new version of the corpus was then split into sen-
tences, and those without hyperlinks were removed.
The remaining 18 million sentences were parsed
with a parallelized version of Ensemble4 (Surdeanu
and Manning, 2010), and we extracted G, the set of
all grammatical relations of the type (verb, depen-
dency, hyperlink) and (adjective, dependency, hy-
perlink), with the hyperlinks resolved to their cor-
responding node (concept) in the network ( |G| =
1,578,413 triples). For each verb and adjective in the
extracted collocations, and for each of their depen-
dency relations, their collocates were generalized in
the network defined by the hypernym/hyponym re-
lations in WikiNet following a method similar to the
Minimum Description Length principle (Li and Abe,
1998).
Essentially, we aimed to determine a small set of
(more general) concepts that describe the set of col-
locates for a word w and grammatical relation r.
Starting from the concept collocates gathered, we
go upwards following WikiNet’s is a links, and for
each node found that covers at least N concept col-
locates (N is a parameter, N=2 in the experiments
presented here), the MDL score of the node is com-
puted (Algorithm 2). We place a limit M on the
number of upward steps in the hierarchy (M=3 in
our experiments). The disjoint set of nodes that has
the lowest overall MDL score is chosen (?), and for
each node in this cut (which we call abstraction),
we compute the selectional preference score, based
on the number of concepts it dominates.
As an example, for the verb defeat, the corpus
leads to collocations such as5:
defeat
nsubj
Earle Page (10357) – 8, Manuela Maleeva
(1092361) – 7, New York Yankees
(10128601) – 5, Tommy Haas (1118005)
– 5, . . .
obj
4http://www.surdeanu.name/mihai/
ensemble/
5The format is:
Article name (Article Id) – frequency.
187
New York Yankees (10128601) – 9, Oak-
land Athletics (11641124) – 6, Phoenix
Suns (11309373) – 4, Jason Suttie
(10080653) – 3, Ravana (100234) – 3, . . .
Determining abstractions and selectional prefer-
ences leads to the following information6:
defeat
nsubj
Martial artists (118977183) – 0.5, Person
(219599) – 0.3518, Interest (146738) –
0.037, . . .
obj
Video games (9570081) – 0.25, British
games (24489088) – 0.25, Person (219599)
– 0.1445, Interest (146738) – 0.1341, . . .
3.3 Linking the PMW to the concept network
In our environment, linking the PMW to the con-
cept network is equivalent to finding its correspond-
ing concept in our ontology, WikiNet. We see this
corresponding concept as the literal reading of the
PMW. Doing so is a non-trivial task (see the Cross-
Lingual Link Discovery task at NTCIR-9 (Tang et
al., 2011) and the Cross-Lingual Entity Linking task
– part of the Knowledge Base Population track – at
TAC 20117). In our particular setting, where we use
the metonymy data from SemEval 2007, the domain
of the PMW is well defined: locations and compa-
nies, respectively. Using these constraints, finding
the corresponding Wikipedia articles is much sim-
plified, by using the category hierarchy and con-
straining the concepts to fall under the Geography
and Companies categories respectively. When mul-
tiple options are present, we find instead a matching
disambiguation page. In this case we pick the article
that is listed first on this disambiguation page. On
a manually checked random sample, the accuracy of
the approach was 100% (on a sample of 100 PMWs).
3.4 Scoring conceptual relations with local and
global context
We work under the assumption that the concept cor-
responding to the PMW is related to the possible in-
terpretations through a semantic relation, in particu-
lar one that is captured in the concept network. After
6The format is:
Concept name (Concept Id) – selectional preference score.
7http://nlp.cs.qc.cuny.edu/kbp/2011/
countries : Administrator of, Architect of,
Based in, Built in, Continent, ...
companies : Association, Brand, Company, Dis-
tributed by, Executive of, ...
Table 1: Example conceptual relations
establishing the connection to the resource by link-
ing the PMW to the concept cPMW corresponding to
its literal interpretation (see Section 3.3), we extract
the relations in which it is involved (Ri, i = 1, k),
and the concepts it is connected to through these re-
lations (CRi = {cx|(cPMWRicx)}). Table 1 shows
examples of conceptual relations extracted for com-
panies and countries.
We are interested in computing the likelihood of
a conceptual relation being the correct interpreta-
tion of a PMW, given its local and global context
p(Ri|Cont, w, r).
3.4.1 The local context
The local context considered in this work are all
grammatically related verbs and adjectives w and
their associated grammatical relation r. The gram-
matical analysis (see Section 3.2) provides the set of
abstractions corresponding to the grammatically re-
lated word w and grammatical relation r: Aj , j =
1, n. Remember that these are local context con-
straints on the interpretation of the PMW.
Through the knowledge resource used we can es-
tablish and quantify connections between each cx
and Aj , and thus between eachRi and Aj :
p(Ri|Aj) =
?
x?CRi
p(cx|Aj)(3)
where p(cx|Aj) is the probability of concept cx un-
der abstraction Aj , which is computed based on the
semantic relations in WikiNet:
p(cx|Aj) =
?
H
?
hi?H
p(hi|hi+1)
whereH is in turn each path from cx toAj following
is a links in WikiNet, starting with cx (i.e. h0 = cx)
and ending in Aj . p(hi|hi+1) is the probability of
the child node hi given its ancestor hi+1. Within this
work we assume a uniform probability distribution
in each node:
188
p(hi|hi+1) =
1
|descendants(hi+1)|
Through this, it is straightforward that
?
cx p(cx|Aj) = 1 when cx ranges over all
concepts subsumed by Aj , and is thus a valid
probability distribution.
3.4.2 The global context
The abstractions obtained before are concepts.
We extract all nodes in the network subsumed
by these concepts, and their corresponding articles
in Wikipedia (if they have one). This produces
“abstraction-specific” article sets, based on which
we compute the probability of the global context of
a PMW for each abstraction. We are interested in
the probability of an abstraction, given the context
and the word w and grammatical relation r, which
we compute as:
p(Aj |Cont, w, r) =
p(Cont|Aj , w, r) ? p(Aj , w, r)
p(Cont, w, r)
which, considering that p(Cont, w, r) is the same
for a given context, we approximate as
p(Aj |Cont) ? p(Cont|Aj) ? p(Aj , w, r)
p(Aj , w, r) = p(Aj |w, r)?p(w, r), and we approxi-
mate it through the computed selectional preference
p(Aj |w, r), since p(w, r) is constant for a given ex-
ample to analyze.
p(Cont|Aj , w, r) =
n?
j=1
p(Cont|Aj)p(Aj |w, r)
=
n?
j=1
(
m?
l=1
p(wl|Aj))p(Aj |w, r)
where Cont is the global context consisting of m
words wl, l = 1,m.8
8The global context therefore could be all words in a text
or all words in a sentence or any other token-based definition
in our framework. As the SemEval 2007 data gives metonymic
examples in a three-sentence context we use all the words in the
3 sentences as our global context.
p(wl|Aj) =
count(wl,Aj)
|Aj |
where Aj is the set of articles subsumed by abstrac-
tion Aj , and count(wl,Aj) is the number of times
word wl appears in the article collection Aj .
3.4.3 Putting it all together
This enables us now to compute p(Ri|Cont, w, r)
based on the formulas 3, 4:
p(Ri|Cont, w, r) =
n?
j=1
(p(Ri|Aj)?p(Aj |Cont, w, r))
4 Experiments
The computed probabilities for each conceptual re-
lation (= potential readings) of the PMW in the con-
cept network can be used as features in a supervised
framework or directly as an unsupervised prediction,
returning the most likely conceptual relation given
the context as the required reading.
Although the latter is our ultimate goal, to allow
comparison with related work from the metonymy
resolution task (Task 8) at SemEval 2007, we first
investigate the supervised set-up. We then simulate
the unsupervised setting in Section 4.3.
4.1 Data
We use the data from the metonymy resolution task
(Task 8) at SemEval 2007. It consists of training and
test data for country and company names which are
potentially metonymic. Table 2 shows the statistics
of the data, and the possible interpretations for the
PMWs. The training-test division was achieved ran-
domly so that the test data can have metonymic read-
ings for which no training data exists, showing again
the limitations of a supervised approach of prespec-
ified readings.
Grammatical features The features used by Nis-
sim and Markert (2005), and commonly used for
the supervised classification of metonymy readings
(Markert and Nissim, 2009):
• grammatical role of PMW (subj, obj, ...);
• lemmatized head/modifier of PMW (announce,
say, ...);
189
reading train test
locations 925 908
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
organizations 1090 842
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
Table 2: Statistics for the Task 8 data
• determiner of PMW (def, indef, bare, demonst,
other, ...);
• grammatical number of PMW (sg, pl);
• number of grammatical roles in which the
PMW appears in its current context;
• number of words in PMW.
All these features can be extracted from the gram-
matically annotated and POS tagged data provided
by the organizers.
The annotations provided are dependency rela-
tions, many of which contain a preposition as an ar-
gument (e.g. (to, pp, UK) from the example ... the
visit to the UK of ...). Such relations are not infor-
mative, but together with the head that dominates the
prepositional complement (e.g. visit to) they may be.
Because of this, we process the provided annotations
and add wherever possible to the simple prepositions
the head of their subsuming constituent. This would
change the above mentioned dependency to (visit,
prep-to, UK).
Semantic relations as features To evaluate the
proposed approach we use the PMW’s conceptual
relations as features. The feature values are the
p(Ri|Cont, w, r) scores.
For the “countries” portion of the data this adds
109 semantic relation features, and for companies
29 features. Table 1 showed examples of these new
features.
4.2 Supervised learning
We use the SMO classifier in the WEKA machine
learning toolkit (Witten and Frank, 2000) with its
standard settings, training on the SemEval 2007
(Task 8) training set.
Table 3 shows the results of various configura-
tions on the test data, in comparison with a most
frequent reading baseline (assigning literal to all
PMWs) as well as a system M&N that shows the re-
sults computed using only the features proposed by
Nissim and Markert (2005). In addition, we com-
pare to the best results9 at SemEval 2007 (SEmax)
and Nastase and Strube (2009) (N09). Nastase and
Strube (2009) added WordNet supersenses as fea-
tures, and their values are selectional preferences
computed with reference to WordNet. These are
similar to our abstractions, which in our approach
serve to link the local and the global context to the
ontological relations, but do not appear as features.
Our system SP shows the results obtained us-
ing the M&N features plus the conceptual relation
features conditioned on both local and global con-
text whereas SPlocal and SPglobal use conceptual
relations conditioned on local (p(Aj |Cont, w, r) ?
p(Aj |w, r)) or global context (p(Aj |Cont, w, r) ?
p(Aj |Cont) =
?n
j=1(
?m
l=1 p(wl|Aj))) only.
While the differences in overall accuracies are
small, there are significant differences in classifying
individual classes, as shown in Tables 4 – 510, where
the distrib. column shows the class distribution in
the test data. It is interesting to note that, in our set-
ting, the global context is more useful than the local
9We show the best result for each category, not necessarily
from the overall best performing system. This holds for Tables
4 and 5 as well.
10The detailed results for previous approaches are reproduced
from (Nastase and Strube, 2009). We include only the classes
that have a non-zero F-score for at least one of the presented
approaches.
190
task ? method? baseline SEmax N09 M&N SP SPlocal SPglobal SPunsup
LOCATION-COARSE 79.4 85.2 86.1 83.4 85.8 83.0 85.0 81.6
LOCATION-MEDIUM 79.4 84.8 85.9 82.3 85.7 82.7 84.6 81.5
LOCATION-FINE 79.4 84.4 85.0 81.3 84.7 82.1 83.8 81.0
ORGANIZATION-COARSE 61.8 76.7 74.9 74.0 77.0 76.4 76.8 67.8
ORGANIZATION-MEDIUM 61.8 73.3 72.4 69.4 74.6 74.0 74.4 66.3
ORGANIZATION-FINE 61.8 72.8 71.0 68.5 72.8 71.9 72.7 65.3
Table 3: Accuracy scores
task ? method? distrib. SEmax N09 SP
LOCATION-COARSE
literal 79.4 91.2 91.6 91.4
non-literal 20.6 57.6 59.1 58.5
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.4
metonymic 18.4 58.0 61.5 61.6
mixed 2.2 8.3 16 9.1
LOCATION-FINE
literal 79.4 91.2 91.6 91.4
place-for-people 15.5 58.9 61.7 61.1
place-for-event 1.1 16.7 0 0
obj-for-name 0.4 66.7 0 0
mixed 2.2 8.3 16 9.1
Table 4: Fine-grained results for each classification task
for countries (F-scores)
one for resolving metonymies. Combining local and
global evidence improves over both, indicating that
the information they provide is not redundant.
For companies the difference is small in terms of
accuracy, but in classification of individual classes
the difference in performance is higher, but because
of the small data size not statistically significant.
Countries in WikiNet have a high number of sur-
rounding relations, because they are used as cat-
egorization criteria for professionals, for example,
which generates fine-grained relations such as Ad-
ministrator of, Ambassador of, Chemist of .... Such
a fine grained distinction between different profes-
sions for people in a country is not necessary, or in-
deed, desirable, for the metonymy resolution task.
The results show that despite this shortcoming, the
results are on par with the state-of-the-art, but in fu-
ture work we plan to explore the task of relation gen-
eralization and its impact on the current task.
task ? method? distrib. SEmax N09 SP
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 82.7
non-literal 38.2 65.2 61.6 65.5
ORGANIZATION-MEDIUM
literal 61.8 82.5 81.4 82.7
metonymic 31.0 60.4 58.7 63.1
mixed 7.2 30.8 26.8 27.4
ORGANIZATION-FINE
literal 61.8 82.6 81.4 82.7
org-for-members 19.1 63.0 59.7 66.5
org-for-product 8.0 50.0 44.4 35.0
org-for-facility 2.0 22.2 36.3 45.5
org-for-name 0.7 80.0 58.8 44.4
mixed 7.2 34.3 27.1 27.4
Table 5: Fine-grained results for each classification task
for companies (F-scores)
4.3 Simulating unsupervised metonymy
resolution
In an unsupervised metonymy resolution approach,
we would assign as interpretation the conceptual re-
lation whose probability given the PMW, global and
local contexts is highest. To simulate then the un-
supervised metonymy resolution task, we make the
relation features (used in the supervised approach)
binary, where for each instance the relation that has
highest probability has the value 1, the others 0.
Using only the relation features simulates an un-
supervised approach – this set-up learns a map-
ping between the relations used as features and
the metonymy classes in the data used. Column
SPUnsup in Table 3 shows the results obtained in
this configuration. As expected the results are lower,
but still close to the supervised method when using
only grammatical features (M&N) for the location
191
setting. The results also significantly beat the base-
line (apart from the Location-Fine setting). One fea-
ture that contributes greatly to the results, especially
for the company semantic class, is the grammatical
role of the PMW, but we could not incorporate this
in the unsupervised setting.
The results in the simulated unsupervised set-
ting indicate that relations are a viable substitute
for manually provided classes in an unsupervised
framework, while leaving space for improvement.
5 Conclusion
We have explored the usage of local and global con-
text for the task of metonymy resolution in a prob-
abilistic framework. The global context has been
rarely used for the task of determining the intended
reading of a potentially metonymic word (PMW)
in context. We rely on automatically computed se-
lectional preferences, extracted from a corpus of
Wikipedia articles, and generalized based on a con-
cept network also extracted from Wikipedia. De-
spite relying on automatically derived resources, the
presented approach produces results on-a-par with
current state-of-the-art systems. The method de-
scribed here is also a step towards the unsupervised
resolution of metonymic words in context, by tak-
ing into account knowledge about the concept cor-
responding to the literal interpretation of the PMW,
and its relations to other concepts. This frame-
work would also allow for exploring the metonymy
resolution phenomena in various languages (since
Wikipedia and WikiNet are multilingual), and inves-
tigate whether the same relations apply or different
languages have different metonymic patterns.
Acknowledgments
Katja Markert is the recipient of an Alexander-von-
Humboldt Fellowship for Experienced Researchers.
This work was financially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853) and the
Klaus Tschirra Foundation. We thank the review-
ers for the helpful comments, and Helga Kra¨mer-
Houska for additional support for conference partic-
ipation.
References
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2007. Building textual entailment specialized data
sets: A methodology for isolating linguistic phenom-
ena relevant to inference. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, La Valetta, Malta, 17–23 May 2010.
Caroline Brun, Maud Ehrmann, and Guillaume Jacquet.
2007. XRCE-M: A hybrid system for named en-
tity metonymy resolution. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23–24 June
2007, pages 488–491.
Richa´rd Farkas, Eszter Simon, Gyo¨rgy Szarvas, and
Da´niel Varga. 2007. GYDER: Maxent metonymy res-
olution. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23–24 June 2007, pages 161–164.
Dan C. Fass. 1991. met?: A method for discriminating
metonomy and metaphor by computer. Computational
Linguistics, 17(1):49–90.
Sanda M. Harabagiu. 1998. Deriving metonymic co-
ercions from WordNet. In Proceedings of the Work-
shop on the Usage of WordNet in Natural Language
Systems, Montral, Quebec, Canada, 16 August, 1998,
pages 142–148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69–142.
Maria Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, 7–12 July 2003,
pages 545–552.
Johannes Leveling and Sven Hartrumpf. 2008. On
metonymy recognition for geographic information re-
trieval. International Journal of Geographical Infor-
mation Science, 22(3):289–299.
Johannes Leveling. 2007. FUH (FernUniversita¨t in Ha-
gen): Metonymy recognition using different kinds of
context for a memory-based learner. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23–24
June 2007, pages 153–156.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217–244.
Katja Markert and Udo Hahn. 2002. Metonymies in dis-
course. Artificial Intelligence, 135(1/2):145–198.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation, 43(2):123–138.
192
Vivi Nastase and Michael Strube. 2009. Combining
collocations, lexical and encyclopedic knowledge for
metonymy resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, Singapore, 6-7 August 2009, pages
910–918.
Vivi Nastase, Michael Strube, Benjamin Bo¨rschinger,
Ca¨cilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, La Valetta,
Malta, 17–23 May 2010.
Cristina Nicolae, Gabriel Nicolae, and Sanda Harabagiu.
2007. UTD-HLT-CG: Semantic architecture for
metonymy resolution and classification of nominal re-
lations. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23–24 June 2007, pages 454–459.
Malvina Nissim and Katja Markert. 2005. Learning to
buy a Renault and talk to BMW: A supervised ap-
proach to conventional metonymy. In Proceedings of
the 6th International Workshop on Computational Se-
mantics, Tilburg, Netherlands, January 12-14, 2005.
Geoffrey Nunberg. 1995. Transfers of meaning. Journal
of Semantics, 12(1):109–132.
Thierry Poibeau. 2007. Up13: Knowledge-poor meth-
ods (sometimes) perform poorly. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-1), Prague, Czech Republic, 23–24
June 2007, pages 418–421.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguistics, 17(4):209–241.
Kirk Roberts and Sanda M. Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detection
of argument coercions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK, 27-29 July 2011,
pages 980–990.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference on
Natural Language Processing, Singapore, 2–7 August
2009, pages 1–9.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Proceedings of Human Language Tech-
nologies 2010: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, Cal., 2–4 June 2010, pages 649–
652.
Ling-Xiang Tang, Shlomo Geva, Andrew Trotman, Yue
Xu, and Kelly Y. Itakura. 2011. Overview of the
NTCIR-9 crosslink task: Cross-lingual link discovery.
In Proceedings of the 9th NII Test Collection for IR
Systems Workshop meeting – NTCIR-9 Tokyo, Japan,
6–9 December 2011.
Masao Utiyama, Masaki Murata, and Hitoshi Isahara.
2000. A statistical approach to the processing
of metonymy. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
Saarbru¨cken, Germany, 31 July – 4 August 2000,
pages 885–891.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Diego, CA.
193
