Keeping the initiative: an empirically-motivated approach to predicting
user-initiated dialogue contributions in HCI
Kerstin Fischer and John A. Bateman
Faculty of Linguistics and Literary Sciences and SFB/TR8
University of Bremen
Bremen, Germany
{kerstinf,bateman}@uni-bremen.de
Abstract
In this paper, we address the problem
of reducing the unpredictability of user-
initiated dialogue contributions in human-
computer interaction without explicitly re-
stricting the user’s interactive possibili-
ties. We demonstrate that it is possible to
identify conditions under which particular
classes of user-initiated contributions will
occur and discuss consequences for dia-
logue system design.
1 Introduction
It is increasingly recognised that human-computer
dialogue situations can benefit considerably from
mixed-initiative interaction (Allen, 1999). Interac-
tion where there is, or appears to be, little restric-
tion on just when and how the user may make a di-
alogue contribution increases the perceived natu-
ralness of an interaction, itself a valuable goal, and
also opens up the application of human-computer
interaction (HCI) to tasks where both system and
user are contributing more equally to the task be-
ing addressed.
Problematic with the acceptance of mixed-
initiative dialogue, however, is the radically in-
creased interpretation load placed on the dialogue
system. This flexibility impacts negatively on
performance at all levels of system design, from
speech recognition to intention interpretation. In
particular, clarification questions initiated by the
user are difficult to process because they may ap-
pear off-topic and can occur at any point. But pre-
venting users from posing such questions leads to
stilted interaction and a reduced sense of control
over how things are proceeding.
In this paper we pursue a partial solution to the
problem of user-initiated contributions that takes
its lead from detailed empirical studies of how
such situations are handled in human-human inter-
action. Most proposed computational treatments
of this situation up until now rely on formalised
notions of relevance: a system attempts to inter-
pret a user contribution by relating it to shared
goals of the system and user. When a connection
can be found, then even an apparently off-topic
clarification can be accomodated. In our approach,
we show how the search space for relevant connec-
tions can be constrained considerably by incorpo-
rating the generic conversation analytic principle
of recipient design (Sacks et al., 1974, p727). This
treats user utterances as explicit instructions for
how they are to be incorporated into the unfold-
ing discourse—an approach that can itself be ac-
comodated within much current discourse seman-
tic work whereby potential discourse interpreta-
tion is facilitated by drawing tighter structural and
semantic constraints from each discourse contri-
bution (Webber et al., 1999; Asher and Lascarides,
2003). We extend this here to include constraints
and conditions for the use of clarification subdia-
logues.
Our approach is empirically driven through-
out. In Section 2, we establish to what extent
the principles of recipient design uncovered for
natural human interaction can be adopted for the
still artificial situation of human-computer inter-
action. Although it is commonly assumed that re-
sults concerning human-human interaction can be
applied to human-computer interaction (Horvitz,
1999), there are also revealing differences (Amal-
berti et al., 1993). We report on a targetted com-
parison of adopted dialogic strategies in natural
human interaction (termed below HHC: human-
human communication) and human-computer in-
teraction (HCI). The study shows significant and
reliable differences in how dialogue is being man-
aged. In Section 3, we interpret these results with
respect to their implications for recipient design.
The results demonstrate not only that recipient de-
sign is relevant for HCI, but also that it leads to
specific and predictable kinds of clarification dia-
logues being taken up by users confronted with an
artificial dialogue system. Finally, in Section 4, we
discuss the implications of the results for dialogic
185
system design in general and briefly indicate how
the required mechanisms are being incorporated in
our own dialogue system.
2 A targetted comparison of HHC and
HCI dialogues
In order to ascertain the extent to which tech-
niques of recipient design established on the ba-
sis of human-human natural interaction can be
transferred to HCI, we investigated comparable
task-oriented dialogues that varied according to
whether the users believed that that they were in-
teracting with another human or with an artificial
agent. The data for our investigation were taken
from three German corpora collected in the mid-
1990s within a toy plane building scenario used
for a range of experiments in the German Collab-
orative Research Centre Situated Artificial Com-
municators (SFB 360) at the University of Biele-
feld (Sagerer et al., 1994). In these experiments,
one participant is the ‘constructor’ who actually
builds the model plane, the other participant is the
‘instructor’, who provides instructions for the con-
structor.
The corpora differ in that the constructor in the
HHC setting was another human interlocutor; in
the other scenario, the participants were seated in
front of a computer but were informed that they
were actually talking to an automatic speech pro-
cessing system (HCI).1 In all cases, there was no
visual contact between constructor and instructor.
Previous work on human-human task-
oriented dialogues going back to, for example,
Grosz (1982), has shown that dialogue structure
commonly follows task structure. Moreover,
it is well known that human-human interaction
employs a variety of dialogue structuring mech-
anisms, ranging from meta-talk to discourse
markers, and that some of these can usefully be
employed for automatic analysis (Marcu, 2000).
If dialogue with artificial agents were then to be
structured as it is with human interlocutors, there
would be many useful linguistic surface cues
available for guiding interpretation. And, indeed,
a common way of designing dialogue structure in
HCI is to have it follow the structure of the task,
since this defines the types of actions necessary
and their sequencing.
1In fact, the interlocutors were always humans, as the ar-
tificial agent in the HCI conditions was simulated employing
standard Wizard-of-Oz methods allowing tighter control of
the linguistic responses received by the user.
Figure 1: Contrasting dialogue structures for HHC
and HCI conditions
Previous studies have not, however, addressed
the issue of dialogue structure in HCI system-
atically, although a decrease in framing signals
has been noted by Hitzenberger and Womser-
Hacker (1995)—indicating either that the dis-
course structure is marked less often or that there
is less structure to be marked. A more precise
characterisation of how task-structure is used or
expressed in HCI situations is then critical for fur-
ther design. For our analysis here, we focused
on properties of the overall dialogue structure and
how this is signalled via linguistic cues. Our re-
sults show that there are in fact significant differ-
ences in HCI and HHC and that it is not possi-
ble simply to take the human-human interaction
results and transpose results for one situation to
the other.
The structuring devices of the human-to-human
construction dialogues can be described as fol-
lows. The instructors first inform their communi-
cation partners about the general goal of the con-
struction. Subsequently, and as would be expected
for a task-oriented dialogue from previous stud-
ies, the discourse structure is hierarchical. At the
top level, there is discussion of the assembly of
the whole toy airplane, which is divided into in-
dividual functional parts, such as the wings or
the wheels. The individual constructional steps
then usually comprise a request to identify one or
more parts and a request to combine them. Each
step is generally acknowledged by the communi-
cation partner, and the successful combination of
the parts as a larger structure is signalled as well.
All the human-to-human dialogues were similar in
these respects. This discourse structure is shown
graphically in the outer box of Figure 1.
Instructors mark changes between phases with
signals of attention, often the constructor’s first
name, and discourse particles or speech routines
that mark the beginning of a new phase such as
186
goal discourse marker explicit marking
usage HHC HCI HHC HCI HHC HCI
none 27.3 100 0 52.5 13.6 52.5
single 40.9 0 9.1 25.0 54.5 27.5
frequent 31.8 0 90.9 22.5 31.8 20.0
Percentage of speakers making no,
single or frequent use of a particular
structuring strategy.
HCI: N=40; HHC: N=22. All differ-
ences are highly significant (ANOVA
p<0.005).
Table 1: Distribution of dialogue structuring devices across experimental conditions
also [so] or jetzt geht’s los [now]. This structur-
ing function of discourse markers has been shown
in several studies and so can be assumed to be
quite usual for human-human interaction (Swerts,
1998). Furthermore, individual constructional
steps are explicitly marked by means of als er-
stes, dann [first of all, then] or der erste Schritt
[the first step]. In addition to the marking of the
construction phases, we also find marking of the
different activities, such as description of the main
goal versus description of the main architecture,
or different phases that arise through the address-
ing of different addressees, such as asides to the
experimenters.
Speakers in dialogues directed at human inter-
locutors are therefore attending to the following
three aspects of discourse structure:
• marking the beginning of the task-oriented
phase of the dialogue;
• marking the individual constructional steps;
• providing orientations for the hearer as to the
goals and subgoals of the communication.
When we turn to the HCI condition, however,
we find a very different picture—indicating that a
straightforward tuning of dialogue structure for an
artificial agent on the basis of the HHC condition
will not produce an effective system.
These dialogues generally start as the HHC di-
alogues do, i.e., with a signal for getting the com-
munication partner’s attention, but then diverge by
giving very low-level instructions, such as to find
a particular kind of component, often even before
the system has itself given any feedback. Since
this behaviour is divorced from any possible feed-
back or input produced by the artificial system, it
can only be adopted because of the speaker’s ini-
tial assumptions about the computer. When this
strategy is successful, the speaker continues to use
it in following turns. Instructors in the HCI condi-
tion do not then attempt to give a general orienta-
tion to their hearer. This is true of all the human-
computer dialogues in the corpus. Moreover, the
dialogue phases of the HCI dialogues do not cor-
respond to the assembly of an identifiable part of
the airplane, such as a wing, the wheels, or the
propeller, but to much smaller units that consist
of successfully identifying and combining some
parts. The divergent dialogue structure of the HCI
condition is shown graphically in the inner dashed
box of Figure 1.
These differences between the experimental
conditions are quantified in Table 1, which shows
for each condition the frequencies of occurrence
for the use of general orienting goal instructions,
describing what task the constructor/instructor is
about to address, the use of discourse markers,
and the use of explicit signals of changes in task
phase. These differences prove (a) that users are
engaging in recipient design with respect to their
partner in these comparable situations and (b) that
the linguistic cues available for structuring an in-
terpretation of the dialogue in the HCI case are
considerably impoverished. This can itself obvi-
ously lead to problems given the difficulty of the
interpretation task.
3 Interpretation of the observed
differences in terms of recipient design
Examining the results of the previous section more
closely, we find signs that the concept of the com-
munication partner to which participants were ori-
enting was not the same for all participants. Some
speakers believed structural marking also to be
useful in the HCI situation, for example. In this
section, we turn to a more exact consideration of
the reasons for these differences and show that di-
rectly employing the mechanisms of recipient de-
sign developed by Schegloff (1972) is a beneficial
strategy. The full range of variation observed, in-
cluding intra-corpus variation that space precluded
us describing in detail above, is seen to arise from
a single common mechanism. Furthermore, we
show that precisely the same mechanism leads to
a predictive account of user-initiated clarificatory
dialogues.
187
The starting point for the discussion is the
conversation analytic notion of the insertion se-
quence. An insertion sequence is a subdialogue
inserted between the first and second parts of an
adjacency pair. They are problematic for artificial
agents precisely because they are places where the
user takes the initiative and demands information
from the system. Clarificatory subdialogues are
regularly of this kind. Schegloff (1972) analyses
the kinds of discourse contents that may constitute
insertion sequences in human-to-human conversa-
tions involving spatial reference. His results im-
ply a strong connection between recipient design
and discourse structure. This means that we can
describe the kind of local sequential organisation
problematic for mixed-initiative dialogue interpre-
tation on the basis of more general principles.
Insertion sequences have been found to address
the following kinds of dialogue work:
Location Analysis: Speakers check upon spa-
tial information regarding the communica-
tion partners, such as where they are when on
a mobile phone, which may lead to an inser-
tion sequence and is also responsible for one
of the most common types of utterances when
beginning a conversation by mobile phone:
i.e., “I’m just on the bus/train/tram”.
Membership Analysis: Speakers check upon
information about the recipient because the
communication partner’s knowledge may
render some formulations more relevant than
others. As a ‘member’ of a particular class of
people, such as the class of locals, or of the
class of those who have visited the place be-
fore, the addressee may be expected to know
some landmarks that the speaker may use for
spatial description. Membership groups may
also include differentiation according to ca-
pabilities (e.g., perceptual) of the interlocu-
tors.
Topic or Activity Analysis: Speakers attend to
which aspects of the location addressed are
relevant for the given topic and activity. They
have a number of choices at their disposal
among which they can select: geographical
descriptions, e.g. 2903 Main Street, descrip-
tions with relation to members, e.g. John’s
place, descriptions by means of landmarks,
or place names.
These three kinds of interactional activity each
give rise to potential insertion sequences; that is,
they serve as the functional motivation for par-
ticular clarificatory subdialogues being explored
rather than others. In the HCI situation, however,
one of them stands out. The task of membership
analysis is extremely challenging for a user faced
with an unknown artificial agent. There is little ba-
sis for assigning group membership; indeed, there
are not even grounds for knowing which kind of
groups would be applicable, due to lack of experi-
ence with artificial communication partners.
Since membership analysis constitutes a pre-
requisite for the formulation of instructions, recip-
ient design can be expected to be an essential force
both for the discourse structure and for the motiva-
tion of particular types of clarification questions in
HCI. We tested this prediction by means of a fur-
ther empirical study involving a scenario in which
the users’ task was to instruct a robot to measure
the distance between two objects out of a set of
seven. These objects differed only in their spatial
position. The users had an overview of the robot
and the objects to be referred to and typed their in-
structions into a notebook. The relevant objects
were pointed at by the instructor of the experi-
ments. The users were not given any information
about the system and so were explicitly faced with
a considerable problem of membership analysis,
making the need for clarification dialogues partic-
ularly obvious. The results of the study confirmed
the predicted effect and, moreover, provide a clas-
sification of clarification question types. Thus, the
particular kinds of analysis found to initiate inser-
tion sequences in HHC situations are clearly active
in HCI clarification questions as well.
21 subjects from varied professions and with
different experience with artificial systems partic-
ipated in the study. The robot’s output was gener-
ated by a simple script that displayed answers in
a fixed order after a particular ‘processing’ time.
The dialogues were all, therefore, absolutely com-
parable regarding the robot’s linguistic material;
moreover, the users’ instructions had no impact on
the robot’s linguistic behaviour. The robot, a Pio-
neer 2, did not move, but the participants were told
that it could measure distances and that they were
connected to the robot’s dialogue processing sys-
tem by means of a wireless LAN connection. The
robot’s output was either “error” (or later in the
dialogues a natural language variant) or a distance
188
usr11-1 hallo# [hello#]
sys ERROR
usr11-2 siehst du was [do you see anything?]
sys ERROR
usr11-3 was siehst du [what do you see?]
sys ERROR 652-a: input is invalid.
usr11-4 miß den abstand zwischen der vordersten tasse und
der linken tasse [measure the distance between
the frontmost cup and the left cup]
Figure 2: Example dialogue extract showing
membership analysis clarification questions
in centimeters. This forced users to reformulate
their dialogue contributions—an effective method-
ology for obtaining users’ hypotheses about the
functioning and capabilities of a system (Fischer,
2003). In our terms, this leads directly to an ex-
plicit exploration of a user’s membership analysis.
As expected in a joint attention scenario, very
limited location analysis occurred. Topic analysis
is also restricted; spatial formulations were chosen
on the basis of what users believed to be ‘most un-
derstandable’ for the robot, which also leads back
to the task of membership analysis.
In contrast, there were many cases of member-
ship analysis. There was clearly great uncertainty
about the robot’s prerequisites for carrying out the
spatial task and this was explicitly specified in the
users’ varied formulations. A simple example is
given in Figure 2.
The complete list of types of questions related
to membership analysis and which digress from
the task instructions in our corpus is given in Ta-
ble 2. Each of these instances of membership anal-
ysis constitutes a clarification question that would
initiate an off-topic subdialogue if the robot had
reacted to it.
4 Consequences for system design
So far our empirical studies have shown that there
are particular kinds of interactional problems that
will regularly trigger user-initiated clarification
subdialogues. These might appear off-topic or
out of place but when understood in terms of
the membership and topic/activity analysis, it be-
comes clear that all such contributions are, in a
very strong sense, ‘predictable’. These results can,
and arguably should,2 be exploited in the follow-
ing ways. One is to extend dialogue system de-
sign to be able to meet these contingently rele-
2Doran et al. (2001) demonstrate a negative relationship
between number of initiative attempts and their success rate.
vant contributions whenever they occur. That is,
we adapt dialogue manager, lexical database etc.
so that precisely these apparently out-of-domain
topics are covered. A second strategy is to de-
termine discourse conditions that can be used to
alert the dialogue system to the likely occurrence
or absence of these kinds of clarificatory subdia-
logues (see below). Third, we can design explicit
strategies for interaction that will reduce the like-
lihood that a user will employ them: for example,
by providing information about the agent’s capa-
bilities, etc. as listed in Table 2 in advance by
means of system-initiated assertions. That is, we
can guide, or shape, to use the terminology intro-
duced by Zoltan-Ford (1991), the users’ linguistic
behaviour. A combination of these three capabil-
ities promises to improve the overall quality of a
dialogue system and forms the basis for a signifi-
cant part of our current research.
We have already ascertained empirically dis-
course conditions that support the second strat-
egy above, and these follow again directly from
the basic notions of recipient design and mem-
bership analysis. If a user already has a strong
membership analysis in place—for example, due
to preconceptions concerning the abilities (or,
more commonly, lack of abilities) of the artifi-
cial agent—then this influences the design of that
user’s utterances throughout the dialogue. As a
consequence, we have been able to define distinc-
tive linguistic profiles that lead to the identifica-
tion of distinct user groups that differ reliably in
their dialogue strategies, particularly in their ini-
tiation of subdialogues. In the human-robot dia-
logues just considered, for example, we found that
eight out of 21 users did not employ any clarifica-
tion questions at all and an additional four users
asked only a single clarification question. Provid-
ing these users with additional information about
the robot’s capabilities is of limited utility because
these users found ways to deal with the situation
without asking clarification questions. The sec-
ond group of participants consisted of nine users;
this group used many questions that would have
led into potentially problematic clarification dia-
logues if the system had been real. For these users,
the presentation of additional information on the
robot’s capabilities would be very useful.
It proved possible to distinguish the members
of these two groups reliably simply by attend-
ing to their initial dialogue contributions. This is
189
domain example (translation)
perception VP7-3 [do you see the cups?]
readiness VP4-25 [Are you ready for another task?]
functional capabilities VP19-11 [what can you do?]
linguistic capabilities VP18-7 [Or do you only know mugs?]
cognitive capabilities VP20-15 [do you know where is left and right of you?]
Table 2: Membership analysis related clarification questions
use of task-oriented greetings
clarification beginnings
none 58.3 11.1
single 25.0 11.1
frequent 16.7 77.8
N = 21; average number of clarification questions
for task-oriented group: 1.17 clarification ques-
tions per dialogue; average number for ‘greeting’-
group 3.2; significance by t-test p<0.01
Table 3: Percentage of speakers using no, a sin-
gle, or frequent clarification questions depending
on first utterance
where their pre-interaction membership analysis
was most clearly expressed. In the human-robot
dialogues investigated, there is no initial utterance
from the robot, the user has to initiate the inter-
action. Two principally different types of first ut-
terance were apparent: whereas one group of users
begins the interaction with task-instructions, a sec-
ond group begins the dialogue by means of a greet-
ing, an appeal for help, or a question with regard
to the capabilities of the system. These two dif-
ferent ways of approaching the system had sys-
tematic consequences for the dialogue structure.
The dependent variable investigated is the num-
ber of utterances that initiate clarification subdia-
logues. The results of the analysis show that those
who greet the robot or interact with it other than
by issuing commands initiate clarificatory subdi-
alogues significantly more often than those who
start with an instruction (cf. Table 3). Thus,
user modelling on the basis of the first utterance
in these dialogues can be used to predict much
of users’ linguistic behaviour with respect to the
initiation of clarification dialogues. Note that for
this type of user modelling no previous informa-
tion about the user is necessary and group assign-
ment can be carried out unobtrusively by means of
simple key word spotting on the first utterance.
Whereas the avoidance of clarificatory user-
initiated subdialogues is clearly a benefit, we can
also use the results of our empirical investigations
to motivate improvements in the other areas of in-
teractive work undertaken by speakers. In particu-
lar topic and activity analysis can become prob-
lematic when the decompositions adopted by a
user are either insufficient to structure dialogue ap-
propriately for interpretation or, worse, are incom-
patible with the domain models maintained by the
artificial agent. In the latter case, communication
will either fail or invoke rechecking of member-
ship categories to find a basis for understanding
(e.g., ‘do you know what cups are?’). Thus, what
can be seen on the part of a user as reducing the
complexity of a task can in fact be removing in-
formation vital for the artificial agent to effect suc-
cessful interpretation.
The results of a user’s topic and activity analy-
sis make themselves felt in the divergent dialogue
structures observed. As shown above in Figure 1,
the structure of the dialogues is thus much flatter
than the one found in the corresponding HHC dia-
logues, such that goal description and marking of
subtasks is missing, and the only structure results
from the division into selection and combination
of parts. In our second study, precisely the same
effects are observed. The task of measuring dis-
tances between objects is often decomposed into
‘simpler’ subtasks; for example, the complexity of
the task is reduced by achieving reference to each
of the objects first before the robot is requested to
measure the distance between them.
This potential mismatch between user and sys-
tem can also be identified on the basis of the inter-
action. Proceeding directly to issuing low-level in-
structions rather than providing background gen-
eral goal information is a clear linguistically
recognisable cue that a nonaligned topic/activity
analysis has been adopted. A successful dialogue
system can therefore rely on this dialogue tran-
sition as providing an indication of problems to
come, which can again be avoided in advance by
explicit system-initiated assertions of information.
190
Our main focus in this paper has been on setting
out and motivating some generic principles for di-
alogue system design. These principles could find
diverse computational instantiations and it has not
been our aim to argue for any one instantation
rather than another. However, to conclude, we
summarise briefly the approach that we are adopt-
ing to incorporating these mechanisms within our
own dialogue system (Ross et al., 2005).
Our system augments an information-state
based approach with a distinguished vocabulary
of discourse transitions between states. We attach
‘conceptualisation-conditions’ to these transitions
which serve to post discourse goals whose partic-
ular function is to head off user-initiated clarifi-
cation. The presence of a greeting is one such
condition; the immediate transition to basic-level
instructions is another. Recognition and produc-
tion of instructions is aided by treating the seman-
tic types that occur (‘cups’, ‘measure’, ‘move’,
etc.) as elements of a domain ontology. The di-
verse topic/activity analyses then correspond to
the specification of the granularity and decom-
position of activated domain ontologies. Sim-
ilarly, location analyses correspond to common
sense geographies, which we model in terms simi-
lar to those of ontologies now being developed for
Geographic Information Systems (Fonseca et al.,
2002).
The specification of conceptualisation-
conditions triggered by discourse transitions
and classifications of the topic/activity analysis
given by the semantic types provided in user ut-
terances represents a direct transfer of the implicit
strategies found in conversation analyses to the
design of our dialogue system. For example, in
our case many simple clarifications like ‘do you
see the cups?,’ ‘how many cups do you see?’ as
well as ‘what can you do?’ are prevented by pro-
viding information in advance on what the robot
can perceive to those users that use greetings.
Similarly, during a scene description where the
system has the initiative, the opportunity is taken
to introduce terms for the objects it perceives as
well as appropriate ways of describing the scene,
e.g., by means of ‘There are two groups of cups.
What do you want me to do?’ a range of otherwise
necessary clarificatory questions is avoided. Even
in the case of failure, users will not doubt those
capabilities of the system that it has displayed it-
self, due to alignment processes also observable in
human-to-human dialogical interaction (Pickering
and Garrod, 2004). After a successful interaction,
users expect the system to be able to process
parallel instructions because they reliably expect
the system to behave consistently (Fischer and
Batliner, 2000).
5 Conclusions
In this paper, the discourse structure initiated by
users in HCI situations has been investigated and
the results have been three-fold. The structures
initiated in HCI are much flatter than in HHC; no
general orientation with respect to the aims of a
sub-task are presented to the artificial communica-
tion partner, and marking is usually reduced. This
needs to be accounted for in the mapping of the
task-structure onto the discourse model, irrespec-
tive of the kind of representation chosen. Sec-
ondly, the contents of clarification subdialogues
have also been identified as particularly depen-
dent on recipient design. That is, they concern
the preconditions for formulating utterances par-
ticularly for the respective hearer. Here, the less
that is known about the communication partner,
the more needs to be elicited in clarification dia-
logues: however, crucially, we can now state pre-
cisely which kinds of elicitations will be found
(cf. Table 2). Thirdly, users have been shown to
differ in the strategies that they take to solve the
uncertainty about the speech situation and we can
predict which strategies they in fact will follow in
their employment of clarification dialogues on the
basis of their initial interaction with the system (cf.
Table 3).
Since the likelihood for users to initiate such
clarificatory subdialogues has been found to be
predictable, we have a basis for a range of implicit
strategies for addressing the users’ subsequent lin-
guistic behaviour. Recipient design has therefore
been shown to be a powerful mechanism that, with
the appropriate methods, can be incorporated in
user-adapted dialogue management design.
Information of the kind that we have uncovered
empirically in the work reported in this paper can
be used to react appropriately to the different types
of users in two ways: either one can adapt the
system or one can try to adapt the user (Ogden
and Bernick, 1996). Although techniques for both
strategies are supported by our results, in general
we favour attempting to influence the user’s be-
haviour without restricting it a priori by means
191
of computer-initiated dialogue structure. Since the
reasons for the users’ behaviour have been shown
to be located on the level of their conceptualisation
of the communication partner, explicit instruction
may in any case not be useful—explicit guidance
of users is not only often impractical but also is
not received well by users. The preferred choice is
then to influence the users’ concepts of their com-
munication partner and thus their linguistic be-
haviour by shaping (Zoltan-Ford, 1991). In par-
ticular, Schegloff’s analysis shows in detail the
human interlocutors’ preference for those location
terms that express group membership. Therefore,
in natural dialogues the speakers constantly signal
to each other who they are, what the other per-
son can expect them to know. Effective system
design should therefore provide users with pre-
cisely those kinds of information that constitute
their most frequent clarification questions initially
and in the manner that we have discussed.
Acknowledgement
The authors gratefully acknowledge the support of
the Deutsche Forschungsgemeinschaft (DFG) for
the work reported in this paper.
References
Christine Doran, John Aberdeen, Laurie Damianos and
Lynette Hirschman. 2001. Comparing Several As-
pects of Human-Computer and Human-Huamn Di-
alogues. Proceedings of the 2nd SIGdial Workshop
on Discourse and Dialogue, Aalborg, Denmark.
James Allen. 1999. Mixed-initiative interaction. IEEE
Intelligent Systems, Sept./Oct.:14–16.
R. Amalberti, N. Carbonell, and P. Falzon. 1993.
User representations of computer systems in human-
computer speech interaction. International Journal
of Man-Machine Studies, 38:547–566.
Nicholas Asher and Alex Lascarides. 2003. Logics
of conversation. Cambridge University Press, Cam-
bridge.
Kerstin Fischer and Anton Batliner. 2000. What
makes speakers angry in human-computer conver-
sation. In Proceedings of the Third Workshop on
Human-Computer Conversation, Bellagio, Italy.
Kerstin Fischer. 2003. Linguistic methods for in-
vestigating concepts in use. In Thomas Stolz and
Katja Kolbe, editors, Methodologie in der Linguis-
tik. Frankfurt a.M.: Peter Lang.
Frederico T. Fonseca, Max J. Egenhofer, Peggy
Agouris, and Gilberto Caˆmara. 2002. Using ontolo-
gies for integrated geographic information systems.
Transactions in GIS, 6(3).
Barbara J. Grosz. 1982. Discourse analysis. In
Richard Kittredge and John Lehrberger, editors,
Sublanguage. Studies of Language in Restricted Se-
mantic Domains, pages 138–174. Berlin, New York:
De Gruyter.
L. Hitzenberger and C. Womser-Hacker. 1995.
Experimentelle Untersuchungen zu multimodalen
natu¨rlichsprachigen Dialogen in der Mensch-
Computer-Interaktion. Sprache und Datenverar-
beitung, 19(1):51–61.
Eric Horvitz. 1999. Uncertainty, action, and interac-
tion: In pursuit of mixed-initiative computing. IEEE
Intelligent Systems, Sept./Oct.:17–20.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: a surface-based approach. Computa-
tional Linguistics, 26(3):395–448, Sep.
W.C. Ogden and P. Bernick. 1996. Using natural lan-
guage interfaces. In M. Helander, editor, Handbook
of Human-Computer Interaction. Elsevier Science
Publishers, North Holland.
Martin J. Pickering and Simon Garrod. 2004. Towards
a mechanistic psychology of dialogue. Behavioural
and Brain Sciences, 27(2):169–190.
R.J. Ross, J. Bateman, and H. Shi. 2005. Applying
Generic Dialogue Models to the Information State
Approach. In Proceedings of Symposium on Dia-
logue Modelling and Generation. Amsterdam.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organisation of turn-taking
for conversation. Language, 50:696–735.
Gerhard Sagerer, Hans-Ju¨rgen Eikmeyer, and Gert
Rickheit. 1994. ”Wir bauen jetzt ein Flugzeug”:
Konstruieren im Dialog. Arbeitsmaterialien. Report
SFB 360, University of Bielefeld.
E. A. Schegloff. 1972. Notes on a conversational prac-
tice: formulating place. In D. Sudnow, editor, Stud-
ies in Social Interaction, pages 75–119. The Free
Press, New York.
Marc Swerts. 1998. Filled pauses as markers of dis-
course structure. Journal of Pragmatics, 30:485–
496.
Bonnie Webber, Alistair Knott, Matthew Stone, and
Aravind Joshi. 1999. Discourse relations: a struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th. Annual Meeting
of the American Association for Computational Lin-
guistics (ACL’99), pages 41–48, University of Mary-
land. American Association for Computational Lin-
guistics.
Elizabeth Zoltan-Ford. 1991. How to get people to
say and type what computers can understand. Inter-
national journal of Man-Machine Studies, 34:527–
647.
192
