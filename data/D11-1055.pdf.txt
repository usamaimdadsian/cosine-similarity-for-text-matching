Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Predicting a Scientific Community’s Response to an Article
Dani Yogatama Michael Heilman Brendan O’Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al., 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al., 2009; Joshi et al., 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper’s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (§3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper’s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999–2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al., 2009a;
Radev et al., 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a “ridge” model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a “maximum entropy” model
(Berger et al., 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?ˆ = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? yˆi)2,
where the prediction rule for new documents is:
yˆ =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (“lasso”) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: yˆ =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al., 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in §3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT×d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, §6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT×T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al., 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall’s ? Kendall’s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
• Authors’ last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
• NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
• Authors’ last names as binary features.
• Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
• Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in §5).
• Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a “null” tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in §2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper’s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the “forecast gap”. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(§3.2), trained on all past years (“all years”) and on
the single most recent past year (“one year”), respec-
tively. The last model (“time series”) is a GLM with
time series regularization (§3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999–2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999–2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999–2007 and 1999–2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)v
training
modeling test (unused)
gap
toa ta taltaddd
oae
lae
training
modeling test (unused)
gap
toa tal tatanddd
oae
lae
training
modeling test
toa taddd
oae
lae
gap
tagtan
test
tai
test
tag
test
tan
gap
trr a
s(u)p)v.(u
u)p)v
Figure 3: An illustration of how the datasets were seg-
mented for the experiments. Portions of data for which
we report results are shaded. Time spans are not to scale.
2008, respectively) as a forecast gap, since we would
not have observed complete responses of papers in
these years when forecasting. For the “one year”
models, we trained ridge regressions only on the
most recent past year, using papers in 2007 and
2008, respectively, as training data.10 To test the
additive benefit of text features, we trained models
with just metadata features (NBER programs and
authors, denoted “Meta”) and with both metadata
10Papers from the most recent past year in a training set have
incomplete responses, so the models were trained on extrapo-
lated responses for that year. For the NBER development set
from 2005, a ridge regression on just 2004 papers (for which
extrapolation is needed) outperformed a regression on just 2003
(for which extrapolation is not needed), 278 to 367 mean abso-
lute error. For the ACL development set from 2001, a regression
on just 2000 (for which extrapolation is needed) led to slightly
lower performance (59% versus 61%) than a regression on just
1998 (for which extrapolation is not needed), probably due to
the relatively small number of conferences and papers in 2000.
For consistency with the other models and with the NBER ex-
periments, we evaluated regressions on the most recent (extrap-
olated) year in our ACL experiments.
599
Features Model Modeling Forecasting1999–07 2008 2009
– median 333 371 397
Meta one year 279 354 375
Meta all years 303 334 378
Meta time series 279 353 375
Full one year 271 346 351
Full all years 265 †300 339
Full time series ?†245 ?321 ?332
Table 3: Mean absolute errors for the NBER download
predictions. “?” indicates statistical significance between
time series models using metadata features and the full
feature set. “†” indicates statistical significance between
the time series and ridge regression models using the full
feature set (Wilcoxon signed-rank test, p < 0.01).
and text features (denoted “Full”).
To evaluate the modeling capabilities, we trained
the ridge regression and time series regularization
models on papers from 1999–2008 and predicted the
numbers of downloads of held-out papers in 1999–
2007. For comparison, we also trained ridge regres-
sion models on each individual year (“one year”)
and predicted the numbers of downloads of the held-
out papers in the corresponding year.
Table 3 shows mean absolute errors for each
method on both forecasting test splits, and mean ab-
solute errors averaged across papers over nine mod-
eling test splits. For interpretability, we report pre-
dictions in terms of download counts, though the
models were trained with log counts (§2.1). The re-
sults show that even a simple n-gram representation
of text contains a valuable, learnable signal that is
predictive of future downloads. While the time se-
ries model did not significantly outperform ridge re-
gression at predicting future downloads, it did result
in significantly better performance for modeling pa-
pers in the past.
5.3 Forecasting ACL Citations
We now turn to the problem of predicting citation
levels. Recall that here we aim to predict whether
an ACL paper will be cited within our dataset within
three years. Our experimental setup (Figure 3) is
similar to the setup for the NBER dataset, except
that we use logistic regression to model the discrete
cited-or-not response variable. We also make the
simplifying assumption that all citations occur at the
end of each year. Therefore, the forecast gap is only
Feat. Model Modeling Forecasting1980–03 2004 2005 2006
– majority 55 56 60 50
Meta one year 61 56 54 62
Meta all years 65 58 53 60
Meta time series 66 56 53 56
Full one year 69 70 64 67
Full all years 67 69 70 70
Full time series 70 ?69 ?70 ?72
Table 4: Classification accuracy (%) for predicting
whether ACL papers will be cited within three years. “?”
indicates statistical significance between time series mod-
els using metadata features and the full feature set (bi-
nomial sign test, p < 0.01). With the full feature set,
differences between the time series and ridge (all years)
models are not statistically significant at the 0.01 level,
but for the modeling task p is estimated at 0.026, and for
the 2006 forecasting task, p is estimated at 0.050.
two years (we have observed complete citations in
the test year).
After feature pruning, there were 30,760 total fea-
tures, of which 1,694 are metadata features. We
considered ? ? 5{2,1,··· ,?8,?9} (“Full”) and ? ?
5{2,1,··· ,?11,?12} (“Meta”); and ? ? 5{6,5,··· ,0,?1}
(both “Full” and “Meta”), selecting the best values
using the development data.
Again, we compare four methods: a baseline of
always predicting the most frequent class in the
training data, “all years” and “one year” logistic re-
gression models, and a logistic regression with the
time series regularizer.
For the forecasting task, we used papers in 2004,
2005, and 2006 as test sets. As the training sets for
the “all years” and time series models, we used pa-
pers from 1980 up to the last year before each test
set, with the last two years extrapolated. As the
training sets for the “one year” models, we used pa-
pers from the year immediately before the test set,
with extrapolated responses.
To evaluate modeling capabilities, we predicted
citation levels of held-out papers in 1980–2003. We
used the “all years” and time series models trained
on 1980–2005. We trained “one year” models sepa-
rately for each year and predicted downloads for the
held-out papers in that year.
Table 4 shows classification accuracy for each
model on the test data for both the forecasting and
modeling tasks. It is again clear that adding text sig-
600
nificantly improved the performance of the model.
Also, the time series regression model shows a
small, though not statistically significant, gain for
modeling whether past papers will be cited—as well
as similarly small gains on two of the three forecast-
ing test years.
5.4 Ranking
We can also use the models for ranking to help de-
cide which papers are expected to have the greatest
impact. With rankings, we can use the same metric
both for download and citation predictions. For the
NBER data, we ranked test-set papers based on the
predicted numbers of downloads and computed the
correlation to the actual numbers of downloads. For
the ACL data, we ranked papers based on the prob-
ability of being cited (within the next three years)
and computed the correlation to the actual numbers
of citations.11
To measure ranking models’ ranking quality, we
used Kendall’s ? , a nonparametric statistic that mea-
sures the similarity of two different orderings over
the same set of items. Here, the items are scien-
tific papers and the two metrics are the gold stan-
dard numbers of downloads (or citations) and model
predictions for the numbers of downloads, or cita-
tion probabilities. If q is the chance that a randomly
drawn pair of items will be ranked in the same way
by the two metrics, then ? = 2(q ? 0.5).
Table 5 shows Kendall’s ? for each model for the
forecasting tasks (i.e., prediction of future citations
or downloads) in both datasets. As in the previous
experiments, we see small benefits for the time se-
ries regression model on most held-out data splits—
and larger benefits for including text features along
with metadata features.
6 Analysis
An advantage of the time series regularized regres-
sion model is its interpretability. Inspecting feature
coefficients in the model allows us to identify trends
and changes of interests over time within a scientific
community.
11Here, we use models of responses to individual papers for
ranking (i.e., in a pointwise ranking scheme). Time series reg-
ularization could also be applied to ranking models that model
pairwise preferences to optimize metrics like Kendall’s ? di-
rectly, as discussed by Joachims (2002).
Feat. Model NBER ACL’08 ’09 ’04 ’05 ’06
Meta one year .29 .22 .17 .08 .16
Meta all years .31 .22 .15 .12 .21
Meta time series .29 .22 .14 .10 .17
Full one year .35 .31 .44 .39 .33
Full all years .43 .37 .42 .43 .40
Full time series .43 .38 .47 .44 .43
Table 5: Kendall’s ? rank correlation for future prediction
models on both datasets.
2000 2005 2010
?
0.
00
4
0.
00
4
unemployment_rate
2000 2005 2010
inflation_rate
time series
all years
one year
Figure 4: Coefficients for two NBER bigram features.
First, we illustrate the difference between the time
series and the other models in Figure 4, for NBER
models’ weights for unemployment rate and infla-
tion rate appearing in a paper’s abstract. The year-
to-year weights of “one year” models fluctuate sub-
stantially, and the “all years” model is necessar-
ily constant, but the time series regularizer gives a
smooth trajectory.
6.1 Trends
Previous work has examined the flow of ideas
as trends in word and phrase frequencies, as in
the Google Books Ngram Viewer (Michel et al.,
2011).12 Topic models have been used extensively to
explore trends in low-dimensional spaces (Blei and
Lafferty, 2006; Wang et al., 2008; Wang and McCal-
lum, 2006; Ahmed and Xing, 2010). By contrast,
our approach allows us to examine trends in the im-
pact of text related to specific observation variables:
the coefficient trendline for a feature illustrates its
association with measurements of scholarly impact
(citation and download frequency).
Text frequencies can be quite different from the
discriminative weights our model assigns to fea-
tures. Figure 5 illustrates the ?t,j trends in the ACL
time series model for some selected terms that oc-
12http://ngrams.googlelabs.com
601
1980 1990 2000
?
0.
01
0
0.
00
0
0.
01
0
discourse
generation
grammars
machine_translation
parsing
semantics
Time series coef.
1980 1990 2000
0.
00
00
0.
00
10
Term frequencies
Figure 5: Feature trends: model coefficients vs. term fre-
quencies over time in the ACL corpus. Term freq. is the
fraction of tokens (or bigrams for m.t.) that year, that are
the term, averaged over a centered five-year window.
cur frequently in conference session titles. On the
right are term frequencies (with smoothing, since
year-to-year frequencies are bumpy). Most terms
decline over time. On the left, by contrast, are the
weights learned by our time series model. They
tell a very different story: for example, parsing has
shown a definite increase in interest, while interest
in grammars (e.g., formalisms) has declined some-
what. These trends have face validity, giving cre-
dence to our analysis; they also broadly agree with
Hall et al. (2008).
6.2 Authors
The regression method also allows analysis of author
influence, since we fit a coefficient for each of the
authors in the ACL dataset. Figure 6(a) addresses
the following question: do prolific authors get cited
more often, even after accounting for the content of
their papers?13 The effect is present but relatively
small according to our model: the total number of
papers co-authored by an author has a weak corre-
lation to the author’s citation prediction coefficient
(? = 0.16).
Next, does the model provide more information
than the simple citation probability of an author?
Figure 6(b) compares coefficients to an author’s pa-
pers’ probability of being cited. Since we did not
prune author features, there are many authors with
13More precisely: if a prolific author and a non-prolific au-
thor write a paper, does the prolific author’s paper have a higher
probability of being cited than the non-prolific author’s, all
other things being equal?
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l l
l
l
l l
l
l l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l l
l l
l
l
l
ll l
l
l l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l l
l
l
l
l
l
l
ll
l
llll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
0 10 20 30 40
?
0.
00
2
0.
00
1
0.
00
4
Doc. frequency
?
(a) Doc. freq. vs. coef.
0.0 0.4 0.8
?
0.
00
2
0.
00
1
0.
00
4
l
l
l
l
l
l
l l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
lll l
l
l
l
l l
l
l
ll
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
ll
l
l
ll
l
l
l
l l
ll
l llll
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
Citation proportion
?
(b) Citation prop. vs. coef.
Figure 6: Analysis of author citation coefficients. Every
point is one ACL author, and the vertical axis shows the
citation coefficient, compared to (a) the number of docu-
ments co-authored by the author; and (b) the proportion
of an author’s papers that are cited within three years.
The vertical bar is the macro-averaged citation propor-
tion across authors, 41%.
only a few papers, resulting in unsmoothed proba-
bilities of 0, 0.5, 1, etc. (these correspond to the ver-
tical “bands” in the plot). By contrast, the `2-penalty
of the model naturally assigned coefficients close to
zero for such authors if it is justified.
In general, the simple probability agrees with the
coefficient, but there are differences. The semantics
of the regression imply we are measuring the rela-
tive citation probability of an author, controlling for
text and venue effects. If an author has a high cita-
tion prediction coefficient but a low citation proba-
bility, that implies the author has better-cited work
than would be expected according to the n-grams in
his or her papers. We have omitted names of au-
thors from the figure for clarity and confidentiality,
but high outlier authors tend to be well-known re-
searchers in the ACL community. Obviously, since
the prediction model is not perfect, it is not possible
to completely verify this hypothesis, but we feel this
analysis is reasonably suggestive.
7 Related Work
Previous work on modeling scientific literature
mostly focused on citation graphs (Borner et al.,
2003; Qazvinian and Radev, 2008). Some re-
searchers, e.g., Erosheva et al. (2004), have used
text content. Most of these are based on topic mod-
els: Gerrish and Blei (2010) measure scholarly im-
pact, Hall et al. (2008) study the “history of ideas”,
602
and Ramage et al. (2010) rank universities based on
scholarly output using topic models.
Download rates and citation prediction were two
of the main tasks in the KDD Cup 2003 (McGovern
et al., 2003; Brank and Leskovec, 2003). Bethard
and Jurafsky (2010) considered the problem slightly
differently and proposed an information retrieval ap-
proach to citation prediction. Our approach is novel
in that we formulate the problem as a forecasting
task and we seek to predict future impact of articles.
Linear regression with text features has been used
to predict financial risk (Kogan et al., 2009) and
movie revenues (Joshi et al., 2010). While the fore-
casts in those papers are similar to ours, those au-
thors did not consider a forecast gap or allowing the
parameters of the model to vary over time.
Our time series regularization is closely related
to the fused lasso (Tibshirani et al., 2005). It pe-
nalizes a loss function by the `1-norm of the co-
efficients and their differences. The `1-penalty for
differences between coefficients encourages sparsity
in the differences. We use the `2-norm to induce
smooth changes across time steps.
8 Conclusions
We presented a statistical approach to predicting a
scientific community’s response to an article, based
on its textual content. To improve the interpretability
of the linear model, we developed a novel time series
regularizer that encourages gradual changes across
time steps. Our experiments showed that text fea-
tures significantly improve accuracy of predictions
over baseline models, and we found that the feature
weights learned with the time series regularizer re-
flect important trends in the literature.
Acknowledgements
We thank the National Bureau of Economic Re-
search for providing the NBER dataset for this
research, Fallaw Sowell for helpful discussions,
and three anonymous reviewers for comments on
an earlier draft of this paper. This research was
supported by the Intelligence Advanced Research
Projects Activity under grant number N10PC20222
and TeraGrid resources provided by the Pittsburgh
Supercomputing Center under grant number TG-
DBS110003.
References
A. Ahmed and E. P. Xing. 2010. Timeline: A dy-
namic hierarchical Dirichlet process model for recov-
ering birth/death and evolution of topics in text stream.
In Proc. of UAI.
A. L. Berger, V. J. Della Pietra, and S. A. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
S. Bethard and D. Jurafsky. 2010. Who should I cite?
Learning literature search models from citation behav-
ior. In Proc. of CIKM.
D. Blei and J. Lafferty. 2006. Dynamic topic models. In
Proc. of ICML.
K. Borner, C. Chen, and K. Boyack. 2003. Visualiz-
ing knowledge domains. In B. Cronin, editor, Annual
Review of Information Science and Technology, vol-
ume 37, pages 179–255. Information Today, Inc.
G. Box, G. M. Jenkins, and G. Reinsel. 2008. Time Se-
ries Analysis: Forecasting and Control. Wiley Series
in Probability and Statistics.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge University Press.
J. Brank and J. Leskovec. 2003. The download estima-
tion task on KDD Cup 2003. SIGKDD Explorations,
5(2):160–162.
A. Cameron and P. Trivedi. 1998. Regression Analysis of
Count Data. Cambridge University Press.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
membership models of scientific publications. In
Proc. of PNAS.
S. Gerrish and D. M. Blei. 2010. A language-based
approach to measuring scholarly impact. In Proc. of
ICML.
D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying
the history of ideas using topic models. In Proc. of
EMNLP.
J. D. Hamilton. 1994. Time Series Analysis. Princeton
University Press.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The Ele-
ments of Statistical Learning: Data Mining, Inference,
and Prediction. Springer.
A. E. Hoerl and R. W. Kennard. 1970. Ridge regression:
Biased estimation for nonorthogonal problems. Tech-
nometrics, 12(1):55–67.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of KDD.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proc. of HLT-NAACL.
S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of HLT-NAACL.
603
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGSmethod for large scale optimization. Mathemat-
ical Programming B, 45(3):503–528.
P. Mccullagh and A. J. Nelder. 1989. Generalized Linear
Models. London: Chapman & Hall.
P. McCullagh. 1980. Regression models for ordinal data.
Journal of the Royal Statistical Society B, 42(2):109–
142.
A. McGovern, L. Friedland, M. Hay, B. Gallagher,
A. Fast, J. Neville, and D. Jensen. 2003. Exploit-
ing relational structure to understand publication pat-
terns in high-energy physics. SIGKDD Explorations,
5(2):165–172.
J. Michel, Y. Shen, A. Aiden, A. Veres, M. Gray, The
Google Books Team, J. Pickett, D. Hoiberg, D. Clancy,
P. Norvig, J. Orwant, S. Pinker, M. Nowak, and
E. Aiden. 2011. Quantitative analysis of culture using
millions of digitized books. Science, 331(6014):176–
182.
V. Qazvinian and D. R. Radev. 2008. Scientific paper
summarization using citation summary networks. In
Proc. of COLING.
D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009a. A bibliometric and network analysis of
the field of computational linguistics. Journal of the
American Society for Information Science and Tech-
nology.
D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b.
The ACL anthology network corpus. In Proc. of ACL
Workshop on Natural Language Processing and Infor-
mation Retrieval for Digital Libraries.
D. Ramage, C. D. Manning, and D. A. McFarland. 2010.
Which universities lead and lag? Toward university
rankings based on scholarly output. In Proc. of NIPS
Workshop on Computational Social Science and the
Wisdom of the Crowds.
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and
K. Knight. 2005. Sparsity and smoothness via the
fused lasso. Journal of the Royal Statistical Society B,
67(1):91–108.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In Proc. of KDD.
C. Wang, D. Blei, and D. Heckerman. 2008. Continuous
time dynamic topic models. In Proc. of UAI.
604
