Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377–386,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics
Inferring Selectional Preferences from Part-Of-Speech N-grams 
 
 
Hyeju Jang and Jack Mostow 
Project LISTEN (www.cs.cmu.edu/~listen), School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
hyejuj@cs.cmu.edu, mostow@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
We present the PONG method to compute 
selectional preferences using part-of-speech 
(POS) N-grams.  From a corpus labeled with 
grammatical dependencies, PONG learns the 
distribution of word relations for each POS 
N-gram.  From the much larger but unlabeled 
Google N-grams corpus, PONG learns the 
distribution of POS N-grams for a given pair 
of words.  We derive the probability that one 
word has a given grammatical relation to the 
other. PONG estimates this probability by 
combining both distributions, whether or not 
either word occurs in the labeled corpus.  
PONG achieves higher average precision on 
16 relations than a state-of-the-art baseline in 
a pseudo-disambiguation task, but lower 
coverage and recall. 
1 Introduction 
Selectional preferences specify plausible fillers 
for the arguments of a predicate, e.g., celebrate.  
Can you celebrate a birthday?  Sure.  Can you 
celebrate a pencil?  Arguably yes:  Today the 
Acme Pencil Factory celebrated its one-billionth 
pencil.  However, such a contrived example is 
unnatural because unlike birthday, pencil lacks a 
strong association with celebrate.  How can we 
compute the degree to which birthday or pencil 
is a plausible and typical object of celebrate? 
Formally, we are interested in computing the 
probability Pr(r | t, R), where (as Table 1 
specifies), t is a target word such as celebrate, r 
is a word possibly related to it, such as birthday 
or pencil, and R is a possible relation between 
them, whether a semantic role such as the agent 
of an action, or a grammatical dependency such 
as the object of a verb.  We call t the “target” 
because originally it referred to a vocabulary 
word targeted for instruction, and r its “relative.” 
 
Notation Description 
R a relation between words 
t a target word 
r, r' possible relatives of t 
g a word N-gram 
gi and gj i
th and jth words of g 
p the POS N-gram of g 
 
Table 1:  Notation used throughout this paper 
 
Previous work on selectional preferences has 
used them primarily for natural language analytic 
tasks such as word sense disambiguation (Resnik, 
1997),  dependency parsing (Zhou et al., 2011), 
and semantic role labeling (Gildea and Jurafsky, 
2002).  However, selectional preferences can 
also apply to natural language generation tasks 
such as sentence generation and question 
generation.  For generation tasks, choosing the 
right word to express a specified argument of a 
relation requires knowing its connotations – that 
is, its selectional preferences.  Therefore, it is 
useful to know selectional preferences for many 
different relations.  Such knowledge could have 
many uses.  In education, they could help teach 
word connotations.  In machine learning they 
could help computers learn languages.  In 
machine translation, they could help generate 
more natural wording. 
This paper introduces a method named PONG 
(for Part-Of-Speech N-Grams) to compute 
selectional preferences for many different 
relations by combining part-of-speech 
information and Google N-grams.  PONG 
achieves higher precision on a pseudo-
377
disambiguation task than the best previous model 
(Erk et al., 2010), but lower coverage. 
The paper is organized as follows.  Section 2 
describes the relations for which we compute 
selectional preferences.  Section 3 describes 
PONG.  Section 4 evaluates PONG.  Section 5 
relates PONG to prior work.  Section 6 concludes.   
2 Relations Used 
Selectional preferences characterize constraints 
on the arguments of predicates.  Selectional 
preferences for semantic roles (such as agent and 
patient) are generally more informative than for 
grammatical dependencies (such as subject and 
object).  For example, consider these 
semantically equivalent but grammatically 
distinct sentences: 
Pat opened the door. 
The door was opened by Pat.   
In both sentences the agent of opened, namely 
Pat, must be capable of opening something – an 
informative constraint on Pat.  In contrast, 
knowing that the grammatical subject of opened 
is Pat in the first sentence and the door in the 
second sentence tells us only that they are nouns. 
Despite this limitation, selectional preferences 
for grammatical dependencies are still useful, for 
a number of reasons.  First, in practice they 
approximate semantic role labels.  For instance, 
typically the grammatical subject of opened is its 
agent.  Second, grammatical dependencies can be 
extracted by parsers, which tend to be more 
accurate than current semantic role labelers.  
Third, the number of different grammatical 
dependencies is large enough to capture diverse 
relations, but not so large as to have sparse data 
for individual relations.  Thus in this paper, we 
use grammatical dependencies as relations. 
A parse tree determines the basic grammatical 
dependencies between the words in a sentence.  
For instance, in the parse of Pat opened the door, 
the verb opened has Pat as its subject and door 
as its object, and door has the as its determiner.  
Besides these basic dependencies, we use two 
additional types of dependencies. 
Composing two basic dependencies yields a 
collapsed dependency (de Marneffe and Manning, 
2008).  For example, consider this sentence: 
The airplane flies in the sky. 
Here sky is the prepositional object of in, which 
is the head of a prepositional phrase attached to 
flies.  Composing these two dependencies yields 
the collapsed dependency prep_in between flies 
and sky, which captures an important semantic 
relation between these two content words:  sky is 
the location where flies occurs.  Other function 
words yield different collapsed dependencies.  
For example, consider these two sentences: 
The airplane flies over the ocean. 
The airplane flies and lands. 
Collapsed dependencies for the first sentence 
include prep_over between flies and ocean, 
which characterizes their relative vertical 
position, and conj_and between flies and lands, 
which links two actions that an airplane can 
perform.  As these examples illustrate, collapsing 
dependencies involving prepositions and 
conjunctions can yield informative dependencies 
between content words. 
Besides collapsed dependencies, PONG infers 
inverse dependencies.  Inverse selectional 
preferences are selectional preferences of 
arguments for their predicates, such as a 
preference of a subject or object for its verb.  
They capture semantic regularities such as the set 
of verbs that an agent can perform, which tend to 
outnumber the possible agents for a verb (Erk et 
al., 2010). 
3 Method 
To compute selectional preferences, PONG 
combines information from a limited corpus 
labeled with the grammatical dependencies 
described in Section 2, and a much larger 
unlabeled corpus.  The key idea is to abstract 
word sequences labeled with grammatical 
relations into POS N-grams, in order to learn a 
mapping from POS N-grams to those relations.  
For instance, PONG abstracts the parsed 
sentence Pat opened the door as NN VB DT NN, 
with the first and last NN as the subject and 
object of the VB.  To estimate the distribution of 
POS N-grams containing particular target and 
relative words, PONG POS-tags Google N-
grams (Franz and Brants, 2006). 
Section 3.1 derives PONG’s probabilistic 
model for combining information from labeled 
and unlabeled corpora.  Section 3.2 and Section 
3.3 describe how PONG estimates probabilities 
from each corpus.  Section 3.4 discusses a 
sparseness problem revealed during probability 
estimation, and how we address it in PONG. 
3.1 Probabilistic model 
We quantify the selectional preference for a 
relative r to instantiate a relation R of a target t as 
the probability Pr(r | t, R), estimated as follows.  
By the definition of conditional probability: 
378
Pr( , , )Pr( | , ) Pr( , )
r t Rr t R t R
 
We care only about the relative probability of 
different r for fixed t and R, so we rewrite it as:   
Pr( , , )r t R  
We use the chain rule: 
Pr( | , ) Pr( | ) Pr( )R r t r t t 
and notice that t is held constant: 
Pr( | , ) Pr( | )R r t r t 
We estimate the second factor as follows:
 
Pr( , ) freq( , )Pr( | ) Pr( ) freq( )
t r t rr t t t
 
We calculate the denominator freq(t) as the 
number of  N-grams in the Google N-gram 
corpus that contain t, and the numerator freq(t, r) 
as the number of N-grams containing both t and r. 
To estimate the factor Pr(R | r, t) directly from 
a corpus of text labeled with grammatical 
relations, it would be trivial to count how often a 
word r bears relation R to target word t.  
However, the results would be limited to the 
words in the corpus, and many relation 
frequencies would be estimated sparsely or 
missing altogether; t or r might not even occur. 
Instead, we abstract each word in the corpus as 
its part-of-speech (POS) label.  Thus we abstract 
The big boy ate meat as DT JJ NN VB NN.  We 
call this sequence of POS tags a POS N-gram.  
We use POS N-grams to predict word relations.  
For instance, we predict that in any word 
sequence with this POS N-gram, the JJ will 
modify (amod) the first NN, and the second NN 
will be the direct object (dobj) of the VB.   
This prediction is not 100% reliable.  For 
example, the initial 5-gram of The big boy ate 
meat pie has the same POS 5-gram as before.  
However, the dobj of its VB (ate) is not the 
second NN (meat), but the subsequent NN (pie).  
Thus POS N-grams predict word relations only 
in a probabilistic sense. 
To transform Pr(R | r, t) into a form we can 
estimate, we first apply the definition of 
conditional probability: 
 Pr( , , )Pr( | , ) Pr( , )
R t rR t r t r 
To estimate the numerator Pr(R, t, r), we first 
marginalize over the POS N-gram p: 
 Pr( , , , )  Pr( , )p
R t r p
t r
 
We expand the numerator using the chain rule: 
 Pr( | , , ) Pr( | , ) Pr( , )
Pr( , )p
R t r p p t r t r
t r
 
Cancelling the common factor yields: 
 
Pr( | , , ) Pr( | , )
p
R p t r p t r
 
We approximate the first term Pr(R | p, t, r) as 
Pr(R | p), based on the simplifying assumption 
that R is conditionally independent of t and r, 
given p.  In other words, we assume that given a 
POS N-gram, the target and relative words t and 
r give no additional information about the 
probability of a relation.  However, their 
respective positions i and j in the POS N-gram p 
matter, so we condition the probability on them: 
 Pr( | , , ) Pr( | , , )R p t r R p i j 
Summing over their possible positions, we get 
Pr( | , )
Pr( | , , ) Pr( | , )i j
p i j
R r t
R p i j p t g r g
 
As Figure 1 shows, we estimate Pr(R | p, i, j) by 
abstracting the labeled corpus into POS N-grams. 
We estimate Pr(p | t = gi, r = gj) based on the 
frequency of partially lexicalized POS N-grams 
like DT JJ:red NN:hat VB NN among Google N-
grams with t and r in the specified positions. 
Sections 3.2 and 3.3 describe how we estimate 
Pr(R | p, i, j) and Pr(p | t = gi, r = gj), respectively.  
Note that PONG estimates relative rather than 
absolute probabilities.  Therefore it cannot (and 
does not) compare them against a fixed threshold 
to make decisions about selectional preferences.  
3.2 Mapping POS N-grams to relations 
To estimate Pr(R | p, i, j), we use the Penn 
Treebank Wall Street Journal (WSJ) corpus, 
which is labeled with grammatical relations 
using the Stanford dependency parser (Klein and 
Manning, 2003).   
To estimate the probability Pr(R | p, i, j) of a 
relation R between a target at position i and a 
relative at position j in a POS N-gram p, we 
compute what fraction of the word N-grams g 
with POS N-gram p have relation R between 
some target t and relative r at positions i and j: 
Pr( | , , )
freq( . .POS( ) relation( , ) )
freq( . .POS( ) relation( , ))
i j
i j
R p i j
g s t g p g g R
g s t g p g g 
3.3 Estimating POS N-gram distributions 
Given a target and relative, we need to estimate 
their distribution of POS N-grams and positions. 
379
 
Figure 1:  Overview of PONG.   
From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations. 
From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative. 
 
A labeled corpus is too sparse for this purpose, 
so we use the much larger unlabeled Google N-
grams corpus (Franz and Brants, 2006). 
The probability that an N-gram with target t at 
position i and relative r at position j will have the 
POS N-gram p is: 
Pr( | , )
freq( . .POS( ) , , ))
freq( . . )
i j
i j
i j
p t g r g
g s t g p g t g r
g s t g t g r
  
To compute this ratio, we first use a well-
indexed table to efficiently retrieve all N-grams 
with words t and r at the specified positions.  We 
then obtain their POS N-grams from the Stanford 
POS tagger (Toutanova et al., 2003), and count 
how many of them have the POS N-gram p. 
3.4 Reducing POS N-gram sparseness 
We abstract word N-grams into POS N-grams to 
address the sparseness of the labeled corpus, but 
even the POS N-grams can be sparse.  For n=5, 
the rarer ones occur too sparsely (if at all) in our 
labeled corpus to estimate their frequency. 
To address this issue, we use a coarser POS 
tag set than the Penn Treebank POS tag set.  As 
Table 2 shows, we merge tags for adjectives, 
nouns, adverbs, and verbs into four coarser tags.   
Coarse Original  
ADJ JJ, JJR, JJS 
ADVERB RB, RBR, RBS 
NOUN NN, NNS, NNP, NNPS 
VERB VB, VBD, VBG, VBN, VBP, VBZ 
Table 2:  Coarser POS tag set used in PONG 
To gauge the impact of the coarser POS tags, 
we calculated Pr(r | t, R) for 76 test instances 
used in an earlier unpublished study by Liu Liu, 
a former Project LISTEN graduate student.  Each 
instance consists of two randomly chosen words 
in the WSJ corpus labeled with a grammatical 
relation.  Coarse POS tags increased coverage of 
this pilot set – that is, the fraction of instances for 
which PONG computes a probability – from 69% 
to 92%. 
Using the universal tag set (Petrov et al., 2011) 
as an even coarser tag set is an interesting future 
direction, especially for other languages.  Its 
smaller size (12 tags vs. our 23) should reduce 
data sparseness, but increase the risk of over-
generalization. 
4 Evaluation 
To evaluate PONG, we use a standard pseudo-
disambiguation task, detailed in Section 4.1.  
Section 4.2 describes our test set.  Section 4.3 
lists the metrics we evaluate on this test set.  
Section 4.4 describes the baselines we compare 
PONG against on these metrics, and Section 4.5 
describes the relations we compare them on.  
Section 4.6 reports our results.  Section 4.7 
analyzes sources of error. 
4.1 Evaluation task 
The pseudo-disambiguation task (Gale et al., 
1992; Schutze, 1992) is as follows:  given a 
target word t, a relation R, a relative r, and a 
random distracter r', prefer either r or r', 
whichever is likelier to have relation R to word t. 
This evaluation does not use a threshold:  just 
prefer whichever word is likelier according to the 
model being evaluated.  If the model assigns only 
one of the words a probability, prefer it, based on 
the assumption that the unknown probability of 
the other word is lower.  If the model assigns the 
same probability to both words, or no probability 
to either word, do not prefer either word. 
380
4.2 Test set 
As a source of evaluation data, we used the 
British National Corpus (BNC).  As a common 
test corpus for all the methods we evaluated, we 
selected one half of BNC by sorting filenames 
alphabetically and using the odd-numbered files.  
We used the other half of BNC as a training 
corpus for the baseline methods we compared 
PONG to. 
A test set for the pseudo-disambiguation task 
task consists of tuples of the form (R, t, r, r').  To 
construct a test set, we adapted the process used 
by Rooth et al. (1999) and Erk et al. (2010). 
First, we chose 100 (R, t) pairs for each 
relation R at random from the test corpus. Rooth 
et al. (1999) and Erk et al. (2010) chose such 
pairs from a training corpus to ensure that it 
contained the target t.  In contrast, choosing pairs 
from an unseen test corpus includes target words 
whether or not they occur in the training corpus. 
To obtain a sample stratified by frequency, 
rather than skewed heavily toward high-
frequency pairs, Erk et al. (2010) drew (R, t) 
pairs from each of five frequency bands in the 
entire British National Corpus (BNC):  50-100 
occurrences; 101-200; 201-500; 500-1000; and 
more than 1000.  However, we use only half of 
BNC as our test corpus, so to obtain a 
comparable test set, we drew 20 (R, t) pairs from 
each of the corresponding frequency bands in 
that half:  26-50 occurrences; 51-100; 101-250; 
251-500; and more than 500. 
For each chosen (R, t) pair, we drew a separate 
(R, t, r) triple from each of six frequency bands:  
1-25 occurrences; 26-50; 51-100; 101-250; 251-
500; and more than 500.  We necessarily omitted 
frequency bands that contained no such triples.  
We filtered out triples where r did not have the 
most frequent part of speech for the relation R.  
For example, this filter would exclude the triple 
(dobj, celebrate, the) because a direct object is 
most frequently a noun, but the is a determiner. 
Then, like Erk et al. (2010), we paired the 
relative r in each (R, t, r) triple with a distracter r' 
with the same (most frequent) part of speech as 
the relative r, yielding the test tuple (R, t, r, r'). 
Rooth et al. (1999) restricted distracter 
candidates to words with between 30 and 3,000 
occurrences in BNC; accordingly, we chose only 
distracters with between 15 and 1,500 
occurrences in our test corpus.  We selected r' 
from these candidates randomly, with probability 
proportional to their frequency in the test corpus.  
Like Rooth et al. (1999), we excluded as 
distracters any actual relatives, i.e. candidates r' 
where the test corpus contained the triple (R, t, r').  
Table 3 shows the resulting number of (R, t, r, r') 
test tuples for each relation. 
 
Relation R # tuples for R # tuples for RT 
advmod 121 131 
amod 162 128 
conj_and 155 151 
dobj 145 167 
nn 173 158 
nsubj  97 124 
prep_of 144 153 
xcomp 139 140 
Table 3:  Test set size for each relation 
4.3 Metrics 
We report four evaluation metrics:  precision, 
coverage, recall, and F-score.  Precision (called 
“accuracy” in some papers on selectional 
preferences) is the percentage of all covered 
tuples where the original relative r is preferred.  
Coverage is the percentage of tuples for which 
the model prefers r to r' or vice versa.  Recall is 
the percentage of all tuples where the original 
relative is preferred, i.e., precision times 
coverage.  F-score is the harmonic mean of 
precision and recall. 
4.4 Baselines 
We compare PONG to two baseline methods.   
EPP is a state-of-the-art model for which Erk 
et al. (2010) reported better performance than 
both Resnik’s (1996) WordNet model and 
Rooth’s (1999) EM clustering model.  EPP 
computes selectional preferences using 
distributional similarity, based on the assumption 
that relatives are likely to appear in the same 
contexts as relatives seen in the training corpus.  
EPP computes the similarity of a potential 
relative’s vector space representation to relatives 
in the training corpus. 
EPP has various options for its vector space 
representation, similarity measure, weighting  
scheme, generalization space, and whether to use 
PCA.  In re-implementing EPP, we chose the 
options that performed best according to Erk et al. 
(2010), with one exception.  To save work, we 
chose not to use PCA, which Erk et al. (2010) 
described as performing only slightly better in 
the dependency-based space. 
381
Relation Target Relative Description 
advmod verb adverb Adverbial modifier 
amod noun adjective Adjective modifier 
conj_and noun noun Conjunction with “and” 
dobj verb noun Direct object 
nn noun noun Noun compound modifier 
nsubj verb noun Nominal subject 
prep_of noun noun Prepositional modifier 
xcomp verb verb Open clausal complement 
 
Table 4: Relations tested in the pseudo-disambiguation experiment.   
Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.   
Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus. 
 
Relation 
Precision (%) Coverage (%) Recall (%) F-score (%) 
PONG EPP DEP PONG EPP DEP PONG EPP DEP PONG EPP DEP 
advmod 78.7 - 98.6 72.1 - 69.2 56.7 - 68.3 65.9 - 80.7 
advmodT 89.0 71.0 97.4 69.5 100 59.5 61.8 71.0 58.0 73.0 71.0 72.7 
amod 78.8 - 99.0 90.1 - 61.1 71.0 - 60.5 74.7 - 75.1 
amodT 84.1 74.0 97.3 83.6 99.2 57.0 70.3 73.4 55.5 76.6 73.7 70.6 
conj_and 77.2 74.2 100 73.6 100 52.3 56.8 74.2 52.3 65.4 74.2 68.6 
conj_andT 80.5 70.2 97.3 74.8 100 49.7 60.3 70.2 48.3 68.9 70.2 64.6 
dobj 87.2 80.0 97.7 80.7 100 60.0 70.3 80.0 58.6 77.9 80.0 73.3 
dobjT 89.6 80.2 98.1 92.2 100 64.1 82.6 80.2 62.9 86.0 80.2 76.6 
nn 86.7 73.8 97.2 95.3 99.4 63.0 82.7 73.4 61.3 84.6 73.6 75.2 
nnT 83.8 79.7 99.0 93.7 100 60.8 78.5 79.7 60.1 81.0 79.7 74.8 
nsubj 76.1 77.3 100 69.1 100 42.3 52.6 77.3 42.3 62.2 77.3 59.4 
nsubjT 78.5 66.9 95.0 86.3 100 48.4 67.7 66.9 46.0 72.7 66.9 62.0 
prep_of 88.4 77.8 98.4 84.0 100 44.4 74.3 77.8 43.8 80.3 77.8 60.6 
prep_ofT 79.2 76.5 97.4 81.7 100 50.3 64.7 76.5 49.0 71.2 76.5 65.2 
xcomp 84.0 61.9 95.3 85.6 100 61.2 71.9 61.9 58.3 77.5 61.9 72.3 
xcompT 86.4 78.6 98.9 89.3 100 63.6 77.1 78.6 62.9 81.5 78.6 76.9 
average 83.0 74.4 97.9 82.6 99.9 56.7 68.7 74.4 55.5 75.0 74.4 70.5 
 
Table 5:  Coverage, Precision, Recall, and F-score for various relations; RT is the inverse of relation R. 
PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses. 
 
To score a potential relative r0, EPP uses this 
formula:
,
, 0 0
arg ( , ) ,
( )( ) ( , )R tR t
r Seen s R t R t
wt rSelpref r sim r rZ 
Here sim(r0, r) is the nGCM similarity defined 
below between vector space representations of r0 
and a relative r seen in the training data: 
 
2
1
2
1
'
( , ') exp( ( ) )
'
i i
i
n
b b
nGCM
i
n
b
i
a a
sim a a
a a
where a a
 
The weight function wtr,t(a) is analogous to 
inverse document frequency in Information 
Retrieval. 
DEP, our second baseline method, runs the 
Stanford dependency parser to label the training 
corpus with grammatical relations, and uses their 
frequencies to predict selectional preferences.  
To do the pseudo-disambiguation task, DEP 
compares the frequencies of (R, t, r) and (R, t, r'). 
4.5 Relations tested 
To test PONG, EPP, and DEP, we chose the 
most frequent eight relations between content 
words in the WSJ corpus, which occur over 
10,000 times and are described in Table 4.  We 
also tested their inverse relations.  However, EPP 
does not compute selectional preferences for 
adjective and adverb as relatives.  For this reason, 
we did not test EPP on advmod and amod 
relations with adverbs and adjectives as relatives. 
382
4.6 Experimental results 
Table 5 displays results for all 16 relations.  To 
compute statistical significance conservatively in 
comparing methods, we used paired t-tests with 
N = 16 relations. 
PONG’s precision was significantly better 
than EPP (p<0.001) but worse than DEP 
(p<0.0001).  Still, PONG’s high precision 
validates its underlying assumption that POS N-
grams strongly predict grammatical 
dependencies. 
On coverage and recall, EPP beat PONG, 
which beat DEP (p<0.0001).  PONG’s F-score 
was higher, but not significantly, than EPP’s 
(p>0.5) or DEP’s (p>0.02). 
4.7 Error analysis 
In the pseudo-disambiguation task of choosing 
which of two words is related to a target, PONG 
makes errors of coverage (preferring neither 
word) and precision (preferring the wrong word). 
Coverage errors, which occurred 17.4% of the 
time on average, arose only when PONG failed 
to estimate a probability for either word.  PONG 
fails to score a potential relative r of a target t 
with a specified relation R if the labeled corpus 
has no POS N-grams that (a) map to R, (b) 
contain the POS of t and r, and (c) match Google 
word N-grams with t and r at those positions.  
Every relation has at least one POS N-gram that 
maps to it, so condition (a) never fails.  PONG 
uses the most frequent POS of t and r, and we 
believe that condition (b) never fails.  However, 
condition (c) can and does fail when t and r do 
not co-occur in any Google N-grams, at least that 
match a POS N-gram that can map to relation R.  
For example, oversee and diet do not co-occur in 
any Google N-grams, so PONG cannot score diet 
as a potential dobj of oversee. 
Precision errors, which occur 17% of the time 
on average, arose when (a) PONG scored the 
distracter but failed to score the true relative, or 
(b) scored them both but preferred the distracter.  
Case (a) accounted for 44.62% of the errors on 
the covered test tuples. 
One likely cause of errors in case (b) is over-
generalization when PONG abstracts a word N-
gram labeled with a relation by mapping its POS 
N-gram to that relation.  In particular, the coarse 
POS tag set may discard too much information.  
Another likely cause of errors is probabilities 
estimated poorly due to sparse data.   The 
probability of a relation for a POS N-gram rare in 
the training corpus is likely to be inaccurate.  So 
is the probability of a POS N-gram for rare co-
occurrences of a target and relative in Google 
word N-grams.  Using a smaller tag set may 
reduce the sparse data problem but increase the 
risk of over-generalization. 
5 Relation to Prior Work 
In predicting selectional preferences, a key 
issue is generalization.  Our DEP baseline simply 
counts co-occurrences of target and relative 
words in a corpus to predict selectional 
preferences, but only for words seen in the 
corpus.  Prior work, summarized in  
Table 6, has therefore tried to infer the similarity 
of unseen relatives to seen relatives. To illustrate, 
consider the problem of inducing that the direct 
objects of celebrate tend to be days or events. 
Resnik (1996) combined WordNet with a 
labeled corpus to model the probability that 
relatives of a predicate belong to a particular 
conceptual class.  This method could notice, for 
example, that the direct objects of celebrate tend 
to belong to the conceptual class event.  Thus it 
could prefer anniversary or occasion as the 
object of celebrate even if unseen in its training 
corpus.  However, this method depends strongly 
on the WordNet taxonomy. 
Rather than use linguistic resources such as 
WordNet, Rooth et al. (1999) and Wald et al. 
(2008) induced semantically annotated 
subcategorization frames from unlabeled corpora. 
They modeled semantic classes as hidden 
variables, which they estimated using EM-based 
clustering.  Ritter (2010) computed selectional 
preferences by using unsupervised topic models 
such as LinkLDA, which infers semantic classes 
of words automatically instead of requiring a pre-
defined set of classes as input. 
The contexts in which a linguistic unit occurs 
provide information about its meaning.  Erk 
(2007) and Erk et al. (2010) modeled the 
contexts of a word as the distribution of words  
that co-occur with it.  They calculated the 
semantic similarity of two words as the similarity 
of their context distributions according to various 
measures.  Erk et al. (2010) reported the state-of-
the-art method we used as our EPP baseline. 
In contrast to prior work that explored various 
solutions to the generalization problem, we don’t 
so much solve this problem as circumvent it.  
Instead of generalizing from a training corpus 
directly to unseen words, PONG abstracts a word 
N-gram to a POS N-gram and maps it to the 
relations that the word N-gram is labeled with. 
383
 
Table 6:  Comparison with prior methods to compute selectional preferences 
 
To compute selectional preferences, whether the 
words are in the training corpus or not, PONG 
applies these abstract mappings to word N-grams 
in the much larger Google N-grams corpus. 
Some prior work on selectional preferences 
has used POS N-grams and a large unlabeled 
corpus.  The most closely related work we found 
was by Gormley et al. (2011).  They used 
patterns in POS N-grams to generate test data for 
their selectional preferences model, but not to 
infer preferences.  Zhou et al. (2011) identified 
selectional preferences of one word for another 
Reference Relation to 
target 
Lexical 
resource 
Primary  corpus 
(labeled) & 
information 
used 
Generalization  
corpus 
(unlabeled) & 
information used 
Method 
Resnik, 
1996 
Verb-object 
Verb-subject 
Adjective-noun 
Modifier-head 
Head-modifier 
Senses in 
WordNet 
noun 
taxonomy 
Target, relative, 
and relation in a 
parsed, partially 
sense-tagged 
corpus (Brown 
corpus) 
none Information 
theoretic 
model 
Rooth et 
al., 1999 
Verb-object 
Verb-subject 
none Target, relative, 
and relation in a 
parsed corpus 
(parsed BNC) 
none EM-based 
clustering 
Ritter, 
2010 
Verb-subject 
Verb-object 
Subject-verb-
object 
none Subject-verb-
object tuples 
from 500 million 
web-pages 
none LDA model 
Erk, 2007 Predicate and 
Semantic roles 
none Target, relative, 
and relation in a 
semantic role 
labeled corpus 
(FrameNet) 
Words and their 
relations in a 
parsed corpus 
(BNC) 
Similarity 
model based 
on word co-
occurrence  
Erk et al., 
2010 
SYN option:  
Verb-subject 
Verb-object, and 
their inverse 
relations 
SEM option:  
verb and 
semantic roles 
that have nouns 
as their headword 
in a primary 
corpus, and their 
inverse relations 
none Target, relative, 
and relation in 
SYN   option:  a  
parsed corpus 
(parsed BNC) 
SEM   option:  a 
semantic role 
labeled corpus 
(FrameNet) 
Two options: 
 
WORDSPACE:  
an unlabeled 
corpus (BNC) 
 
DEPSPACE:  
Words and their 
subject and object 
relations in a 
parsed corpus 
(parsed BNC) 
Similarity 
model using 
vector space 
representation 
of words 
Zhou et 
al., 2011 
Any (relations 
not distinguished) 
none Counts of words 
in Web or 
Google N-gram 
none PMI 
(Pointwise 
Mutual 
Information) 
This paper All grammatical 
dependencies in a 
parsed corpus, 
and their inverse 
relations 
none POS N-gram 
distribution for 
relations in 
parsed WSJ 
corpus 
POS N-gram 
distribution for 
target and relative 
in Google N-gram 
Combine both 
POS N-gram 
distributions 
384
by using Pointwise Mutual Information (PMI) 
(Fano, 1961) to check whether they co-occur 
more frequently in a large corpus than predicted 
by their unigram frequencies.  However, their 
method did not distinguish among different 
relations. 
6 Conclusion 
This paper describes, derives, and evaluates 
PONG, a novel probabilistic model of selectional 
preferences.  PONG uses a labeled corpus to map 
POS N-grams to grammatical relations.  It 
combines this mapping with probabilities 
estimated from a much larger POS-tagged but 
unlabeled Google N-grams corpus. 
We tested PONG on the eight most common 
relations in the WSJ corpus, and their inverses – 
more relations than evaluated in prior work.  
Compared to the state-of-the-art EPP baseline 
(Erk et al., 2010), PONG averaged higher 
precision but lower coverage and recall.  
Compared to the DEP baseline, PONG averaged 
lower precision but higher coverage and recall.  
All these differences were substantial (p < 0.001). 
Compared to both baselines, PONG’s average F-
score was higher, though not significantly. 
Some directions for future work include:  First, 
improve PONG by incorporating models of 
lexical similarity explored in prior work.  Second, 
use the universal tag set to extend PONG to other 
languages, or to perform better in English.  Third, 
in place of grammatical relations, use rich, 
diverse semantic roles, while avoiding sparsity.  
Finally, use selectional preferences to teach word 
connotations by using various relations to 
generate example sentences or useful questions. 
Acknowledgments 
The research reported here was supported by the 
Institute of Education Sciences, U.S. Department 
of Education, through Grant R305A080157.  The 
opinions expressed are those of the authors and 
do not necessarily represent the views of the 
Institute or the U.S. Department of Education.  
We thank the helpful reviewers and Katrin Erk 
for her generous assistance. 
References 
de Marneffe, M.-C. and Manning, C.D. 2008. 
Stanford Typed Dependencies Manual. 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf, Stanford University, Stanford, CA. 
Erk, K. 2007. A Simple, Similarity-Based Model for 
Selectional Preferences. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, Prague, Czech 
Republic, June, 2007, 216-223. 
Erk, K., Padó, S. and Padó, U. 2010. A Flexible, 
Corpus-Driven Model of Regular and Inverse 
Selectional Preferences. Computational Linguistics 
36(4), 723-763. 
Fano, R. 1961. Transmission  O F   Information:  A  
Statistical  Theory  of  Communications. MIT 
Press, Cambridge, MA. 
Franz, A. and Brants, T. 2006. All Our N-Gram Are 
Belong to You. 
Gale, W.A., Church, K.W. and Yarowsky, D. 1992. 
Work on Statistical Methods for Word Sense 
Disambiguation. In Proceedings of the AAAI Fall 
Symposium on Probabilistic Approaches to Natural 
Language, Cambridge, MA, October 23–25, 1992, 
54-60. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics 
28(3), 245-288. 
Gormley, M.R., Dredze, M., Durme, B.V. and Eisner, 
J. 2011. Shared Components Topic Models with 
Application to Selectional Preference, NIPS 
Workshop on Learning Semantics Sierra Nevada, 
Spain. 
im Walde, S.S., Hying, C., Scheible, C. and Schmid, 
H. 2008. Combining Em Training and the Mdl 
Principle for an Automatic Verb Classification 
Incorporating Selectional Preferences. In 
Proceedings of the 46th Annual Meeting of the 
Association for Computational Linguistics, 
Columbus, OH,  2008, 496-504. 
Klein, D. and Manning, C.D. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of the 41st 
Annual Meeting of the Association for 
Computational Linguistics, Sapporo, Japan, July 7-
12, 2003, E.W. HINRICHS and D. ROTH, Eds. 
Petrov, S., Das, D. and McDonald, R.T. 2011. A 
Universal Part-of-Speech Tagset. ArXiv 
1104.2086. 
Resnik, P. 1996. Selectional Constraints: An 
Information-Theoretic Model and Its 
Computational Realization. Cognition 61, 127-159. 
Resnik, P. 1997. Selectional Preference and Sense 
Disambiguation. In ACL SIGLEX Workshop on 
385
Tagging Text with Lexical Semantics: Why, What, 
and How, Washington, DC, April 4-5, 1997, 52-57. 
Ritter, A., Mausam and Etzioni, O. 2010. A Latent 
Dirichlet Allocation Method for Selectional 
Preferences. In Proceedings of the 48th Annual 
Meeting of the Association for Computational 
Linguistics, Uppsala, Sweden,  2010, 424-434. 
Rooth, M., Riezler, S., Prescher, D., Carroll, G. and 
Beil, F. 1999. Inducing a Semantically Annotated 
Lexicon Via Em-Based Clustering. In Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics on Computational 
Linguistics, College Park, MD,  1999, Association 
for Computational Linguistics, 104-111. 
Schutze, H. 1992. Context Space. In Proceedings of 
the AAAI Fall Symposium on Intelligent 
Probabilistic Approaches to Natural Language, 
Cambridge, MA,  1992, 113-120. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. 
2003. Feature-Rich Part-of-Speech Tagging with a 
Cyclic Dependency Network. In Proceedings of the 
Human Language Technology Conference and 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(HLT-NAACL), Edmonton, Canada,  2003, 252–
259. 
Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011. 
Exploiting Web-Derived Selectional Preference to 
Improve Statistical Dependency Parsing. In 
Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, 
Portland, OR,  2011, 1556–1565. 
 
 
386
