Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1302–1312, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Multi-Domain Learning: When Do Domains Matter?
Mahesh Joshi
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
maheshj@cs.cmu.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, Maryland 21211
mdredze@cs.jhu.edu
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
wcohen@cs.cmu.edu
Carolyn P. Rose´
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cprose@cs.cmu.edu
Abstract
We present a systematic analysis of exist-
ing multi-domain learning approaches with re-
spect to two questions. First, many multi-
domain learning algorithms resemble ensem-
ble learning algorithms. (1) Are multi-domain
learning improvements the result of ensemble
learning effects? Second, these algorithms are
traditionally evaluated in a balanced class la-
bel setting, although in practice many multi-
domain settings have domain-specific class
label biases. When multi-domain learning
is applied to these settings, (2) are multi-
domain methods improving because they cap-
ture domain-specific class biases? An under-
standing of these two issues presents a clearer
idea about where the field has had success in
multi-domain learning, and it suggests some
important open questions for improving be-
yond the current state of the art.
1 Introduction
Research efforts in recent years have demonstrated
the importance of domains in statistical natural lan-
guage processing. A mismatch between training and
test domains can negatively impact system accuracy
as it violates a core assumption in many machine
learning algorithms: that data points are indepen-
dent and identically distributed (i.i.d.). As a result,
numerous domain adaptation methods (Chelba and
Acero, 2004; Daume´ III and Marcu, 2006; Blitzer et
al., 2007) target settings with a training set from one
domain and a test set from another.
Often times the training set itself violates the i.i.d.
assumption and contains multiple domains. In this
case, training a single model obscures domain dis-
tinctions, and separating the dataset by domains re-
duces training data. Instead, multi-domain learn-
ing (MDL) can take advantage of these domain la-
bels to improve learning (Daume´ III, 2007; Dredze
and Crammer, 2008; Arnold et al., 2008; Finkel and
Manning, 2009; Zhang and Yeung, 2010; Saha et al.,
2011). One such example is sentiment classification
of product reviews. Training data is available from
many product categories and while all data should
be used to learn a model, there are important differ-
ences between the categories (Blitzer et al., 2007)1.
While much prior research has shown improve-
ments using MDL, this paper explores what prop-
erties of an MDL setting matter. Are previous im-
provements from MDL algorithms discovering im-
portant distinctions between features in different do-
mains, as we would hope, or are other factors con-
tributing to learning success? The key question of
this paper is: when do domains matter?
Towards this goal we explore two issues. First,
we explore the question of whether domain distinc-
tions are used by existing MDL algorithms in mean-
ingful ways. While differences in feature behaviors
between domains will hurt performance (Blitzer et
al., 2008; Ben-David et al., 2009), it is not clear
if the improvements in MDL algorithms can be at-
tributed to correcting these errors, or whether they
are benefiting from something else. In particular,
there are many similarities between MDL and en-
semble methods, with connections to instance bag-
1Blitzer et al. (2007) do not consider the MDL setup, they
consider a single source domain, and a single target domain,
with little or no labeled data available for the target domain.
1302
ging, feature bagging and classifier combination. It
may be that gains in MDL are the usual ensemble
learning improvements.
Second, one simple way in which domains can
change is the distribution of the prior over the la-
bels. For example, reviews of some products may be
more positive on average than reviews of other prod-
uct types. Simply capturing this bias may account
for significant gains in accuracy, even though noth-
ing is learned about the behavior of domain-specific
features. Most prior work considers datasets with
balanced labels. However, in real world applica-
tions, where labels may be biased toward some val-
ues, gains from MDL could be attributed to simply
modeling domain-specific bias. A practical advan-
tage of such a result is ease of implementation and
the ability to scale to many domains.
Overall, irrespective of the answers to these ques-
tions, a better understanding of the performance of
existing MDL algorithms in different settings will
provide intuitions for improving the state of the art.
2 Multi-Domain Learning
In the multi-domain learning (MDL) setting, exam-
ples are accompanied by both a class label and a do-
main indicator. Examples are of the form (xi, y,di),
where xi ? RN , di is a domain indicator, xi is
drawn according to a fixed domain-specific distri-
bution Ddi , and yi is the label (e.g. yi ? {?1,+1}
for binary labels). Standard learning ignores di, but
MDL uses these to improve learning accuracy.
Why should we care about the domain label? Do-
main differences can introduce errors in a number
of ways (Ben-David et al., 2007; Ben-David et al.,
2009). First, the domain-specific distributions Ddi
can differ such that they favor different features, i.e.
p(x) changes between domains. As a result, some
features may only appear in one domain. This aspect
of domain difference is typically the focus of un-
supervised domain adaptation (Blitzer et al., 2006;
Blitzer et al., 2007). Second, the features may be-
have differently with respect to the label in each do-
main, i.e. p(y|x) changes between domains. As a
result, a learning algorithm cannot generalize the be-
havior of features from one domain to another. The
key idea behind many MDL algorithms is to target
one or both of these properties of domain difference
to improve performance.
Prior approaches to MDL can be broadly catego-
rized into two classes. The first set of approaches
(Daume´ III, 2007; Dredze et al., 2008) introduce pa-
rameters to capture domain-specific behaviors while
preserving features that learn domain-general be-
haviors. A key of these methods is that they do not
explicitly model any relationship between the do-
mains. Daume´ III (2007) proposes a very simple
“easy adapt” approach, which was originally pro-
posed in the context of adapting to a specific target
domain, but easily generalizes to MDL. Dredze et al.
(2008) consider the problem of learning how to com-
bine different domain-specific classifiers such that
behaviors common to several domains can be cap-
tured by a shared classifier, while domain-specific
behavior is still captured by the individual classi-
fiers. We describe both of these approaches in § 3.2.
The second set of approaches to MDL introduce
an explicit notion of relationship between domains.
For example, Cavallanti et al. (2008) assume a fixed
task relationship matrix in the context of online
multi-task learning. The key assumption is that in-
stances from two different domains are half as much
related to each other as two instances from the same
domain. Saha et al. (2011) improve upon the idea
of simply using a fixed task relationship matrix by
instead learning it adaptively. They derive an online
algorithm for updating the task interaction matrix.
Zhang and Yeung (2010) derive a convex formu-
lation for adaptively learning domain relationships.
We describe their approach in § 3.2. Finally, Daume´
III (2009) proposes a joint task clustering and multi-
task/multi-domain learning setup, where instead of
just learning pairwise domain relationships, a hier-
archical structure among them is inferred. Hierar-
chical clustering of tasks is performed in a Bayesian
framework, by imposing a hierarchical prior on the
structure of the task relationships.
In all of these settings, the key idea is to learn
both domain-specific behaviors and behaviors that
generalize between (possibly related) domains.
3 Data
To support our analysis we develop several empir-
ical experiments. We first summarize the datasets
and methods that we use in our experiments, then
1303
proceed to our exploration of MDL.
3.1 Datasets
A variety of multi-domain datasets have been used
for demonstrating MDL improvements. In this pa-
per, we focus on two datasets representative of many
of the properties of MDL.
Amazon (AMAZON) Our first dataset is the Multi-
Domain Amazon data (version 2.0), first introduced
by Blitzer et al. (2007). The task is binary sentiment
classification, in which Amazon product reviews are
labeled as positive or negative. Domains are defined
by product categories. We select the four domains
used in most studies: books, dvd, electronics
and kitchen appliances.
The original dataset contained 2,000 reviews for
each of the four domains, with 1,000 positive and
1,000 negative reviews per domain. Feature extrac-
tion follows Blitzer et al. (2007): we use case insen-
sitive unigrams and bigrams, although we remove
rare features (those that appear less than five times
in the training set). The reduced feature set was se-
lected given the sensitivity to feature size of some of
the MDL methods.
ConVote (CONVOTE) Our second dataset is taken
from segments of speech from United States
Congress floor debates, first introduced by Thomas
et al. (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discus-
sion in the floor debate. We select this dataset be-
cause, unlike the AMAZON data, CONVOTE can be
divided into domains in several ways based on dif-
ferent metadata attributes available with the dataset.
We consider two types of domain divisions: the bill
identifier and the political party of the speaker. Di-
vision based on the bill creates domain differences
in that each bill has its own topic. Division based on
political party implies preference for different issues
and concerns, which manifest as different language.
We refer to these datasets as BILL and PARTY.
We use Version 1.1 of the CONVOTE dataset,
available at http://www.cs.cornell.edu/
home/llee/data/convote.html. More
specifically, we combine the training, development
and test folds from the data stage three/ ver-
sion, and sub-sample to generate different versions
of the dataset required for our experiments. For
BILL we randomly sample speech segments from
three different bills. The three bills and the number
of instances for each were chosen such that we have
sufficient data in each fold for every experiment.
For PARTY we randomly sample speech segments
from the two major political parties (Democrats and
Republicans). Feature processing was identical to
AMAZON, except that the threshold for feature re-
moval was two.
3.2 Learning Methods and Features
We consider three MDL algorithms, two are repre-
sentative of the first approach and one of the second
approach (learning domain similarities) (§2). We fa-
vored algorithms with available code or that were
straightforward to implement, so as to ensure repro-
ducibility of our results.
FEDA Frustratingly easy domain adaptation
(FEDA) (Daume´ III, 2007; Daume´ III et al., 2010b;
Daume´ III et al., 2010a) is an example of a classifier
combination approach to MDL. The feature space
is a cross-product of the domain and input features,
augmented with the original input features (shared
features). Prediction is effectively a linear combina-
tion of a set of domain-specific weights and shared
weights. We combine FEDA with both the SVM
and logistic regression algorithms described below
to obtain FEDA-SVM and FEDA-LR.
MDR Multi-domain regularization (MDR) (Dredze
and Crammer, 2008; Dredze et al., 2009) extends the
idea behind classifier combination by explicitly for-
mulating a classifier combination scheme based on
Confidence-Weighted learning (Dredze et al., 2008).
Additionally, classifier updates (which happen in
an online framework) contain an explicit constraint
that the combined classifier should perform well on
the example. Dredze et al. (2009) consider several
variants of MDR. We select the two best perform-
ing methods: MDR-L2, which uses the underlying
algorithm of Crammer et al. (2008), and MDR-KL,
which uses the underlying algorithm of Dredze et al.
(2008). We follow their approach to classifier train-
ing and parameter optimization.
MTRL The multi-task relationship learning
(MTRL) approach proposed by Zhang and Yeung
1304
(2010) achieves states of the art performance on
many MDL tasks. This method is representative
of methods that learn similarities between domains
and in turn regularize domain-specific parameters
accordingly. The key idea in their work is the use
of a matrix-normal distribution p(X|M ,?,?) as
a prior on the matrix W created by column-wise
stacking of the domain-specific classifier weight
vectors. ? represents the covariance matrix for the
variables along the columns of X . When used as
a prior over W it models the covariance between
the domain-specific classifiers (and therefore the
tasks). ? is learned jointly with the domain-specific
classifiers. This method has similar benefits to
FEDA in terms of classifier combination, but also
attempts to model domain relationships. We use
the implementation of MTRL made available by the
authors2. For parameter tuning, we perform a grid
search over the parameters ?1 and ?2, using the fol-
lowing values for each (a total of 36 combinations):
{0.00001, 0.0001, 0.001, 0.01, 0.1, 1}.
In addition to these multi-task learning methods,
we consider a common baseline: ignoring the do-
main distinctions and learning a single classifier
over all the data. This reflects single-domain learn-
ing, in which no domain knowledge is used and will
indicate baseline performance for all experiments.
While some earlier research has included a sepa-
rate one classifier per domain baseline, it almost al-
ways performs worse, since splitting the domains
provides much less data to each classifier (Dredze
et al., 2009). So we omit this baseline for simplicity.
To obtain a single classifier we use two classifica-
tion algorithms: SVMs and logistic regression.
Support Vector Machines A single SVM run
over all the training data, ignoring domain labels.
We use the SVM implementation available in the LI-
BLINEAR package (Fan et al., 2008). In particular,
we use the L2-regularized L2-loss SVM (option -s
1 in version 1.8 of LIBLINEAR, and also option -B
1 for including a standard bias feature). We tune
the SVM using five-fold stratified cross-validation
on the training set, using the following values for
the trade-off parameterC: {0.0001, 0.001, 0.01, 0.1,
0.2, 0.3, 0.5, 1}.
2http://www.cse.ust.hk/˜zhangyu/codes/
MTRL.zip
Logistic Regression (LR) A single logistic re-
gression model run over all the training data, ignor-
ing domain labels. Again, we use the L2-regularized
LR implementation available in the LIBLINEAR
package (option -s 0, and also option -B 1). We
tune the LR model using the same strategy as the
one used for SVM above, including the values of the
trade-off parameter C.
For all experiments, we measure average accu-
racy overK-fold cross-validation, using 10 folds for
AMAZON, and 5 folds for both BILL and PARTY.
4 When Do Domains Matter?
We now empirically explore two questions regarding
the behavior of MDL.
4.1 Ensemble Learning
Question: Are MDL improvements the result of
ensemble learning effects?
Many of the MDL approaches bear a striking
resemblance to ensemble learning. Traditionally,
ensemble learning combines the output from sev-
eral different classifiers to obtain a single improved
model (Maclin and Opitz, 1999). It is well estab-
lished that ensemble learning, applied on top of a
diverse array of quality classifiers, can improve re-
sults for a variety of tasks. The key idea behind
ensemble learning, that of combining a diverse ar-
ray of models, has been applied to settings in which
data preprocessing is used to create many different
classifiers. Examples include instance bagging and
feature bagging (Dietterich, 2000).
The core idea of using diverse inputs in making
classification decisions is common in the MDL liter-
ature. In fact, the top performing and only success-
ful entry to the 2007 CoNLL shared task on domain
adaptation for dependency parsing was a straightfor-
ward implementation of ensemble learning by cre-
ating variants of parsers (Sagae and Tsujii, 2007).
Many MDL algorithms, among them Dredze and
Crammer (2008), Daume´ III (2009), Zhang and Ye-
ung (2010) and Saha et al. (2011), all include some
notion of learning domain-specific classifiers on the
training data, and combining them in the best way
possible. To be clear, we do not claim that these
approaches can be reduced to an existing ensem-
ble learning algorithm. There are crucial elements
1305
in each of these algorithms that separate them from
existing ensemble learning algorithms. One exam-
ple of such a distinction is the learning of domain
relationships by both Zhang and Yeung (2010) and
Saha et al. (2011). However, we argue that their
core approach, that of combining parameters that are
trained on variants of the data (all data or individual
domains), is an ensemble learning idea.
Consider instance bagging, in which multiple
classifiers are each trained on random subsets of the
data. The resulting classifiers are then combined
to form a final model. In MDL, we can consider
each domain a subset of the data, albeit non-random
and non-overlapping. The final model combines the
domain-specific parameters and parameters trained
on other instances, which in the case of FEDA are the
shared parameters. In this light, these methods are a
complex form of instance bagging, and their devel-
opment could be justified from this perspective.
However, given this justification, are improve-
ments from MDL simply the result of standard en-
semble learning effects, or are these methods re-
ally learning something about domain behavior? If
knowledge of domain was withheld from the algo-
rithm, could we expect similar improvements? As
we will do in each empirical experiment, we propose
a contrarian hypothesis:
Hypothesis: Knowledge of domains is irrelevant
for MDL.
Empirical Evaluation We evaluate this hypothe-
sis as follows. We begin by constructing a true MDL
setting, in which we attempt to improve accuracy
through knowledge of the domains. We will apply
three MDL algorithms (FEDA, MDR, and MTRL) to
our three multi-domain datasets (AMAZON, BILL,
and PARTY) and compare them against a single clas-
sifier baseline. We will then withhold knowledge
of the true domains from these algorithms and in-
stead provide them with random “pseudo-domains,”
and then evaluate the change in their behavior. The
question is whether we can obtain similar benefits
by ignoring domain labels and relying strictly on an
ensemble learning motivation (instance bagging).
For the “True Domain” setting, we apply the
MDL algorithms as normal. For the “Random Do-
main” setting, we randomly shuffle the domain la-
bels within a given class label within each fold, thus
maintaining the same number of examples for each
domain label, and also retaining the same class dis-
tribution within each randomized domain. The re-
sulting “pseudo-domains” are then similar to ran-
dom subsets of the data used in ensemble learning.
Following the standard practice in previous work,
for this experiment we use a balanced number of
examples from each domain and a balanced num-
ber of positive and negative labels (no class bias).
For AMAZON (4 domains), we have 10 folds of 400
examples per fold, for BILL (3 domains) 5 folds of
60 examples per fold, and for PARTY (2 domains) 5
folds of 80 examples per fold. In the “Random Do-
main” setting, since we are randomizing the domain
labels, we increase the number of trials. We repeat
each cross-validation experiment 5 times with differ-
ent randomization of the domain labels each time.
Results Results are shown in Table 1. The first
row shows absolute (average) accuracy for a single
classifier trained on all data, ignoring domain dis-
tinctions. The remaining cells indicate absolute im-
provements against the baseline.
First, we note for the well-studied AMAZON
dataset that our results with true domains are con-
sistent with the previous literature. FEDA is known
to not improve upon a single classifier baseline for
that dataset (Dredze et al., 2009). Both MDR-L2 and
MDR-KL improve upon the single classifier baseline,
again as per Dredze et al. (2009). And finally, MTRL
also improves upon the single classifier baseline. Al-
though the MTRL improvement is not as dramatic as
in the original paper3, the average accuracy that we
achieve for MTRL (84.2%) is better than the best av-
erage accuracy in the original paper (83.65%).
The main comparison to make in Table 1 is be-
tween having knowledge of true domains or not.
“Random Domain” in the table is the case where do-
main identifiers are randomly shuffled within a given
fold. Ignoring the significance test results for now,
overall the results indicate that knowing the true do-
mains is useful for MDL algorithms. Randomiz-
ing the domains does not work better than knowing
true domains in any case. However, in all except
one case, the improvements of MDL algorithms are
3This might be due to a different version of the dataset being
used in a cross-validation setup, rather than their train/test setup,
and also because of differences in baseline approaches.
1306
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
83.93% 83.78% 66.67% 68.00% 62.75% 64.00%
FEDA
True Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25
Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10
MDR-L2
True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00
Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05
MDR-KL
True Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75
Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 H
MTRL
True Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25
Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80
Table 1: A comparison between MDL methods with access to the “True Domain” labels and methods that
use “Random Domain” information, essentially ensemble learning. The first row has raw accuracy numbers,
whereas the remaining entries are absolute improvements over the baseline. N: Significantly better than the
corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly worse than
corresponding baseline, with p < 0.05, using a paired t-test.
significantly better only for the AMAZON dataset4.
And interestingly, exactly in the same case, ran-
domly shuffling the domains also gives significant
improvements compared to the baseline, showing
that there is an ensemble learning effect in operation
for MDR-L2 and MDR-KL on the AMAZON dataset.
For FEDA, randomizing the domains significantly
hurts its performance on the AMAZON data, as is
the case for MDR-KL on the PARTY data. Therefore,
while our contrarian hypothesis about irrelevance of
domains is not completely true, it is indeed the case
that some MDL methods benefit from the ensemble
learning effect.
A second observation to be made from these re-
sults is that, while all of empirical research on MDL
assumes the definition of domains as a given, the
question of how to split a dataset into domains given
various metadata attributes is still open. For exam-
ple, in our experiments, in general, using the po-
litical party as a domain distinction gives us more
improvements over the corresponding baseline ap-
proach5.
We provide a detailed comparison of using true
4Some numbers in Table 1 might appear to be significant,
but are not. That is because of high variance in the performance
of the methods across the different folds.
5The BILL and the PARTY datasets are not directly compa-
rable to each other, although the prediction task is the same.
vs. randomized domains in Table 6, after presenting
the second set of experimental results.
4.2 Domain-specific Class Bias
Question: Are MDL methods improving because
they capture domain-specific class biases?
In previous work, and the above section, experi-
ments have assumed a balanced dataset in terms of
class labels. It has been in these settings that MDL
methods improve. However, this is an unrealistic as-
sumption. Even in our datasets, the original versions
demonstrated class bias: Amazon product reviews
are generally positive, votes on bills are rarely tied,
and political parties vote in blocs. While it is com-
mon to evaluate learning methods on balanced data,
and then adjust for imbalanced real world datasets, it
is unclear what effect domain-specific class bias will
have on MDL methods. Domains can differ in their
proportion of examples of different classes. For ex-
ample, it is quite likely that less controversial bills in
the United States Congress will have more yes votes
than controversial bills. Similarly, if instead of the
category of a product, its brand is considered as a do-
main, it is likely that some brands receive a higher
proportion of positive reviews than others.
Improvements from MDL in such settings may
simply be capturing domain-specific class biases.
1307
domain class cb1 cb2 cb3 cb4
AMAZON
b
- 20 80 60 40
+ 80 20 40 60
d
- 40 20 80 60
+ 60 80 20 40
e
- 60 40 20 80
+ 40 60 80 20
k
- 80 60 40 20
+ 20 40 60 80
BILL
031
N 16 4 8 12
Y 4 16 12 8
088
N 12 16 4 8
Y 8 4 16 12
132
N 8 12 16 4
Y 12 8 4 16
PARTY
D
N 10 30 15 25
Y 30 10 25 15
R
N 30 10 25 15
Y 10 30 15 25
Table 2: The table shows the distribution of in-
stances across domains and class labels within one
fold of each of the datasets, for four different class
bias trials. These datasets with varying class bias
across domains were used for the experiments de-
scribed in §4.2
Consider two domains, where each domain is biased
towards the opposite label. In this case, domain-
specific parameters may simply be capturing the bias
towards the class label, increasing the weight uni-
formly of features predictive of the dominant class.
Similarly, methods that learn domain similarity may
be learning class bias similarity.
Why does the effectiveness of these domain-
specific bias parameters matter? First, if capturing
domain-specific class bias is the source of improve-
ment, there are much simpler methods for learning
that can be just as effective. This would be espe-
cially important in settings where we have many do-
mains, and learning domain-specific parameters for
each feature becomes infeasible. Second, if class
bias accounted for most of the improvement in learn-
ing, it suggests that such settings could be amenable
to unsupervised adaptation of the bias parameters.
Hypothesis: MDL largely capitalizes on
domain-specific class bias.
Empirical Evaluation To evaluate our hypothe-
sis, for each of our three datasets we create 4 random
versions, each with some domain-specific class-bias.
A summary of the dataset partitions is shown in
Table 2. For example, for the AMAZON dataset,
we create 4 versions (cb1 . . . cb4), where each do-
main has 100 examples per fold and each domain
has a different balance between positive and nega-
tive classes. For each of these settings, we conduct
a 10-fold cross validation experiment, then average
the CV results for each of the 4 settings. The re-
sulting accuracy numbers therefore reflect an aver-
age across many types of bias, each evaluated many
times. We do a similar experiment for the BILL and
PARTY datasets, except we use 5-fold CV.
In addition to the multi-domain and baseline
methods, we add a new baseline: DOM-ID. In this
setting, we augment the baseline classifier (which
ignores domain labels) with a new feature that in-
dicates the domain label. While we already include
a general bias feature, as is common in classifica-
tion tasks, these new features will capture domain-
specific bias. This is the only change to the base-
line classifier, so improvements over the baseline are
indicative of the change in domain-bias that can be
captured using these simple features.
Results Results are shown in Table 3. The table
follows the same structure as Table 1, with the ad-
dition of the results for the DOM-ID approach. We
first examine the efficacy of MDL in this setting. An
observation that is hard to miss is that MDL results
in these experiments show significant improvements
in almost all cases, as compared to only a few cases
in Table 1, despite the fact that even the baseline ap-
proaches have a higher accuracy. This shows that
MDL results can be highly influenced by systematic
differences in class bias across domains. Note that
there is also a significant negative influence of class
bias on MTRL for the AMAZON data.
A comparison of the MDL results on true domains
to the DOM-ID baseline gives us an idea of how
much MDL benefits purely from class bias differ-
ences across domains. We see that in most cases,
about half of the improvement seen in MDL is ac-
counted for by a simple baseline of using the do-
main identifier as a feature, and all but one of the
improvements from DOM-ID are significant. This
1308
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
85.52% 85.46% 70.50% 70.67% 65.44% 65.81%
FEDA
True Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 N
Random Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73
MDR-L2
True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94
Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28
MDR-KL
True Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 N
Random Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44
MTRL
True Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 N
Random Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 N
DOM-ID
True Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 N
Random Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 N
Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we
evaluate the ensemble learning effect, we have a setting of using randomized domains. N: Significantly
better than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly
worse than corresponding baseline, with p < 0.05, using a paired t-test.
suggests that in a real-world scenario where differ-
ence in class bias across domains is quite likely, it is
useful to consider DOM-ID as a simple baseline that
gives good empirical performance. To our knowl-
edge, using this approach as a baseline is not stan-
dard practice in MDL literature.
Finally, we also include the “Random Domain”
evaluation in the our class biased version of exper-
iments. Each “Random Domain” result in Table 3
is an average over 20 cross-validation runs (5 ran-
domized trials for each of the four class biased tri-
als cb1 . . . cb4). This setup combines the effects
of ensemble learning and bias difference across do-
mains. As seen in the table, for MDL algorithms the
results are consistently better as compared to know-
ing the true domains for the AMAZON dataset. For
the other datasets, the performance after randomiz-
ing the domains is still significantly better than the
baseline. This evaluation on randomized domains
further strengthens the conclusion that differences in
bias across domains play an important role, even in
the case of noisy domains. Looking at the perfor-
mance of DOM-ID with randomized domains, we
see that in all cases the DOM-ID baseline performs
better with randomized domains. While the dif-
ference is significant mostly only on the AMAZON
domain class cb5 cb6 cb7 cb8
AMAZON
b
- 20 40 60 80
+ 80 60 40 20
d
- 20 40 60 80
+ 80 60 40 20
e
- 20 40 60 80
+ 80 60 40 20
k
- 20 40 60 80
+ 20 40 60 80
Table 4: The table shows the distribution of in-
stances across domains and class labels within one
fold of the AMAZON dataset, for four different class
bias trials. For the BILL and PARTY datasets, similar
folds with consistent bias were created (number of
examples used was different). These datasets with
consistent class bias across domains were used for
the experiments described in §4.2.1
dataset (details in Table 6, columns under “Varying
Class Bias,”) this trend is still counter-intuitive. We
suspect this might be because randomization creates
a noisy version of the domain labels, which helps
learners to avoid over-fitting that single feature.
1309
4.2.1 Consistent Class Bias
We also performed a set of experiments that ap-
ply MDL algorithms to a setting where the datasets
have different class biases (unlike the experiments
reported in Table 1, where the classes are balanced),
but, unlike the experiments reported in Table 3, the
class bias is the same within each of the domains.
We refer to this as the case of consistent class bias
across domains. The distribution of classes within
each domain within each fold is shown in Table 4.
The results for this set of experiments are reported
in Table 5. The structure of Table 5 is identical to
that of Table 1. Comparing these results to those
in Table 1, we can see that in most cases the im-
provements seen using MDL algorithms are lower
than those seen in Table 1. This is likely due to
the higher baseline performance in the consistent
class bias case. A notable difference is in the per-
formance of MTRL — it is significantly worse for
the AMAZON dataset, and significantly better for the
PARTY dataset. For the AMAZON dataset, we be-
lieve that the domain distinctions are less meaning-
ful, and hence forcing MTRL to learn the relation-
ships results in lower performance. For the PARTY
dataset, in the case of a class-biased setup, know-
ing the party is highly predictive of the vote (in the
original CONVOTE dataset, Democrats mostly vote
“no” and Republicans mostly vote “yes”), and this
is rightly exploited by MTRL.
4.2.2 True vs. Randomized Domains
In Table 6 we analyze the difference in perfor-
mance of MDL methods when using true vs. ran-
domized domain information. For the three sets of
results reported earlier, we evaluated whether using
true domains as compared to randomized domains
gives significantly better, significantly worse or
equal performance. Significance testing was done
using a paired t-test with ? = 0.05 as before. As the
table shows, for the first set of results where the class
labels were balanced (overall, as well as within each
domain), using true domains was significantly better
mostly only for the AMAZON dataset. FEDA-SVM
was the only approach that was consistently better
with true domains across all datasets. Note, how-
ever, that it was significantly better than the baseline
approach only for PARTY.
For the second set of results (Table 3) where the
class bias varied across the different domains, us-
ing true domains was either no different from using
randomized domains, or it was significantly worse.
In particular, it was consistently significantly worse
to use true domains on the AMAZON dataset. This
questions the utility of domains on the AMAZON
dataset in the context of MDL in a domain-specific
class bias scenario. Since randomizing the domains
works better for all of the MDL methods on AMA-
ZON, it suggests that an ensemble learning effect
is primarily responsible for the significant improve-
ments seen on the AMAZON data, when evaluated in
a domain-specific class bias setting.
Finally, for the case of consistent class bias across
domains, the trend is similar to the case of no class
bias — using true domains is useful. This table
further supports the conclusion that domain-specific
class bias highly influences multi-domain learning.
5 Discussion and Open Questions
Our analysis of MDL algorithms revealed new
trends that suggest further avenues of exploration.
We suggest three open questions in response.
Question: When are MDL methods most effective?
Our empirical results suggest that MDL can be more
effective in settings with domain-specific class bi-
ases. However, we also saw differences in im-
provements for each method, and for different do-
mains. Differences emerge between the AMAZON
and CONVOTE datasets in terms of the ensemble
learning hypothesis. While there has been some the-
oretical analyses on the topic of MDL (Ben-David
et al., 2007; Ben-David et al., 2009; Mansour et
al., 2009; Daume´ III et al., 2010a), our results sug-
gest performing new analyses that relate ensemble
learning results with the MDL setting. These anal-
yses could provide insights into new algorithms that
can take advantage of the specific properties of each
multi-domain setting.
Question: What makes a good domain for MDL?
To the best of our knowledge, previous work has
assumed that domain identities are provided to the
learning algorithm. However, in reality, there may
be many ways to split a dataset into domains. For
example, consider the CONVOTE dataset, which we
split both by BILL and PARTY. The choice of splits
1310
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
86.06% 86.22% 76.42% 75.58% 69.31% 68.38%
FEDA
True Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25
Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04
MDR-L2
True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19
Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 H
MDR-KL
True Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 N
Random Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34
MTRL
True Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 N
Random Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 N
Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.
Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized
domains. N: Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a paired
t-test. H: Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.
MDL Method No Class Bias (Tab. 1) Varying Class Bias (Tab. 3) Consistent Class Bias (Tab. 5)
better worse equal better worse equal better worse equal
FEDA-SVM AM, BI, PA AM BI, PA AM, PA BI
FEDA-LR AM BI, PA AM BI, PA AM, BI PA
MDR-L2 AM BI, PA AM BI, PA AM, BI PA
MDR-KL PA AM, BI AM BI, PA AM, PA BI
MTRL AM BI, PA AM BI, PA AM, PA BI
DOM-ID-SVM – – – AM BI, PA – – –
DOM-ID-LR – – – AM, BI PA – – –
Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL method
using true domain information was significantly better, significantly worse, or not significantly different
(equal) as compared to using randomized domain information with the same MDL method.
impacted MDL. This poses new questions: what
makes a good domain? How should we choose to di-
vide data along possible metadata properties? If we
can gain improvements simply by randomly creat-
ing new domains (“Random Domain” setting in our
experiments) then there may be better ways to take
advantage of the provided metadata for MDL.
Question: Can we learn class-bias for
unsupervised domain adaptation?
Experiments with domain-specific class biases re-
vealed that a significant part of the improvements
could be achieved by adding domain-specific bias
features. Limiting the multi-domain improvements
to a small set of parameters raises an interesting
question: can these parameters be adapted to a new
domain without labeled data? Traditionally, domain
adaptation without target domain labeled data has
focused on learning the behavior of new features;
beliefs about existing feature behaviors could not be
corrected without new training data. However, by
collapsing the adaptation into a single bias parame-
ter, we may be able to learn how to adjust this pa-
rameter in a fully unsupervised way. This would
open the door to improvements in this challenging
setting for real world problems where class bias was
a significant factor.
Acknowledgments
Research presented here is supported by the Office
of Naval Research grant number N000141110221.
1311
References
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Transfer
Learning in Named Entity Recognition. In Proceed-
ings of ACL-08: HLT, pages 245–253.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In Proceedings of NIPS 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120–128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440–447.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
Bounds for Domain Adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS 2007).
Giovanni Cavallanti, Nicolo` Cesa-Bianchi, and Claudio
Gentile. 2008. Linear Algorithms for Online Multi-
task Classification. In Proceedings of COLT.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
Maximum Entropy Capitalizer: Little Data Can Help
a Lot. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 285–292.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2008. Exact convex confidence-weighted learning. In
Advances in Neural Information Processing Systems
(NIPS).
Hal Daume´ III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101–126.
Hal Daume´ III, Abhishek Kumar, and Avishek Saha.
2010a. A Co-regularization Based Semi-supervised
Domain Adaptation. In Neural Information Process-
ing Systems.
Hal Daume´ III, Abhishek Kumar, and Avishek Saha.
2010b. Frustratingly Easy Semi-Supervised Domain
Adaptation. In Proceedings of the ACL 2010 Work-
shop on Domain Adaptation for Natural Language
Processing, pages 53–59.
Hal Daume´ III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.
Hal Daume´ III. 2009. Bayesian multitask learning with
latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence.
Thomas G. Dietterich. 2000. An experimental compar-
ison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139–157.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ’08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ’08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1-2).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical Bayesian Domain Adaptation. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602–610.
Richard Maclin and David Opitz. 1999. Popular Ensem-
ble Methods: An Empirical Study. Journal of Artifi-
cial Intelligence Research, 11:169–198.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain Adaptation with Multiple
Sources. In Proceedings of NIPS 2008, pages 1041–
1048.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Conference on Natural Language
Learning (Shared Task).
Avishek Saha, Piyush Rai, Hal Daume´ III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327–335.
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
1312
