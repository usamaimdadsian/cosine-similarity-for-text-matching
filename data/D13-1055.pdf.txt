Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 578–589,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Automatically Classifying Edit Categories in Wikipedia Revisions
Johannes Daxenberger† and Iryna Gurevych†‡
† Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technische Universita¨t Darmstadt
‡ Information Center for Education
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we analyze a novel set of fea-
tures for the task of automatic edit category
classification. Edit category classification as-
signs categories such as spelling error correc-
tion, paraphrase or vandalism to edits in a doc-
ument. Our features are based on differences
between two versions of a document includ-
ing meta data, textual and language properties
and markup. In a supervised machine learning
experiment, we achieve a micro-averaged F1
score of .62 on a corpus of edits from the En-
glish Wikipedia. In this corpus, each edit has
been multi-labeled according to a 21-category
taxonomy. A model trained on the same
data achieves state-of-the-art performance on
the related task of fluency edit classification.
We apply pattern mining to automatically la-
beled edits in the revision histories of different
Wikipedia articles. Our results suggest that
high-quality articles show a higher degree of
homogeneity with respect to their collabora-
tion patterns as compared to random articles.
1 Introduction
Due to its ever-evolving and collaboratively built
content, Wikipedia has been the subject of many
NLP studies. While the number of newly created
articles in the online encyclopedia declined in the
last few years (Suh et al., 2009), the number of edits
in existing articles is rather stable.1 It is reasonable
to assume that the latter will not change in the near
1http://stats.wikimedia.org/EN/
TablesDatabaseEdits.htm
future. One of the major reasons for the popular-
ity of Wikipedia is its up-to-dateness (Keegan et al.,
2013), which in turn requires constant editing activ-
ity. Wikipedia’s revision history stores all changes
made to any page in the encyclopedia in separate
revisions. Previous studies have exploited revision
history data in tasks such as preposition error cor-
rection (Cahill et al., 2013), spelling error correc-
tion (Zesch, 2012) or paraphrasing (Max and Wis-
niewski, 2010). However, they all use different ap-
proaches to extract the information needed for their
task. Ferschke et al. (2013) outline several appli-
cations benefiting from revision history data. They
argue for a unified approach to extract and classify
edits from revision histories based on a predefined
edit category taxonomy.
In this work, we show how the extraction and
automatic multi-label classification of any edit in
Wikipedia can be handled with a single approach.
Therefore, we use the 21-category edit classification
taxonomy developed in previous work (Daxenberger
and Gurevych, 2012). This taxonomy enables a fine-
grained analysis of edit activity in revision histories.
We present the results from an automatic classifica-
tion experiment, based on an annotated corpus of ed-
its in the English Wikipedia. Additional information
necessary to reproduce our results, including word
lists and training, development and test data, is re-
leased online.2 To the best of our knowledge, this
is the first approach allowing to classify each single
edit in Wikipedia into one or more of 21 different
edit categories using a supervised machine learning
2http://www.ukp.tu-darmstadt.de/data/
edit-classification
578
approach.
We define our task as edit category classification.
An edit is a coherent, local change which modifies a
document and which can be related to certain meta
data (e.g. its author, time stamp etc.). In edit ca-
tegory classification, we aim to detect all n edits
ekv?1,v with 0 ? k < n in adjacent versions rv?1, rv
of a document (we refer to the older revision as rv?1
and to the newer as rv) and assign each of them
to one or more edit categories. There exist at least
two main applications of edit category classification:
First, a fine-grained classification of edits in collab-
oratively created documents such as Wikipedia ar-
ticles, scientific papers or research proposals, would
help us to better understand the collaborative writing
process. This includes answers to questions about
the kind of contribution of individual authors (Who
has added substantial contents?, Who has improved
stylistic issues?) and about the kind of collabora-
tion which characterizes different articles (Liu and
Ram, 2011). Second, automatic classification of ed-
its generates huge amounts of training data for the
above mentioned NLP systems.
Edit category classification is related to the bet-
ter known task of document pair classification.
In document pair classification, a pair of docu-
ments has to be assigned to one or more categories
(e.g. paraphrase/non-paraphrase, plagiarism/non-
plagiarism). Here, the document may be a very short
text, such as a sentence or a single word. Appli-
cations of document pair classification include pla-
giarism detection (Potthast et al., 2012), paraphrase
detection (Madnani et al., 2012) or text similarity
detection (Ba¨r et al., 2012). In edit category clas-
sification, we also have two documents. However,
these documents are different versions of the same
text. This scenario implies certain characteristics for
a well-designed feature set as we will demonstrate in
this study.
The main contributions of this paper are: First,
we introduce a novel feature set for edit category
classification. Second, we evaluate the performance
of this feature set on different tasks within a cor-
pus of Wikipedia edits. We propose the new task of
edit category classification and show that our model
is able to classify edits from a 21-category taxon-
omy. Furthermore, our model achieves state-of-the-
art performance in a fluency edit classification task
(Bronner and Monz, 2012). Third, we analyze col-
laboration patterns based on edit categories on two
subsets of Wikipedia articles, namely featured and
non-featured articles. We detect correlations be-
tween collaboration patterns and high-quality arti-
cles. This is demonstrated by the fact that featured
articles have a higher degree of homogeneity with
respect to their collaboration patterns as compared
to random articles.
The rest of this paper is structured as follows. In
Section 2, we motivate our experiments based on
previous work. Section 3 explains our training data
and the features we use for the machine learning ex-
periments. In Section 4, we present and discuss the
results of our experiments. We also demonstrate an
application of our classifier model in Section 5 by
mining frequent collaboration patterns in the revi-
sion histories of different articles. Finally, we draw
a conclusion in Section 6.
2 Related Work
Wikipedia is a huge data source for generating train-
ing data for edit category classification, as all pre-
vious versions of each page in the encyclopedia
are stored in its revision history. Unsurprisingly,
the number of studies extracting certain kinds of
Wikipedia edits keeps growing. Most of these use
manually defined rules or filters find the right kind
of edits. Among the latter, there are NLP applica-
tions such as the detection of lexical errors (Nelken
and Yamangil, 2008), spelling error correction (Max
and Wisniewski, 2010; Zesch, 2012), preposition er-
ror correction (Cahill et al., 2013), sentence com-
pression (Nelken and Yamangil, 2008; Yamangil
and Nelken, 2008), summarization (Nelken and Ya-
mangil, 2008), simplification (Yatskar et al., 2010;
Woodsend and Lapata, 2011), paraphrasing (Max
and Wisniewski, 2010; Dutrey et al., 2011), tex-
tual entailment (Zanzotto and Pennacchiotti, 2010;
Cabrio et al., 2012), information retrieval (Aji et al.,
2010; Nunes et al., 2011) and bias detection (Re-
casens et al., 2013).
Bronner and Monz (2012) define features for the
supervised classification of factual and fluency edits.
Their features are calculated both on character- and
word-level. Furthermore, they use features based
on POS tags, named entities, acronyms, and a lan-
579
Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool.
guage model (word n-grams). In their experiments,
character-level features and named entity features
show the highest improvement over the baseline.
Vandalism detection in Wikipedia has mostly
been defined as a binary machine learning task,
where the goal is to classify a pair of adjacent re-
visions as vandalized or not-vandalized based on
edit category features. In Adler et al. (2011), the
authors group these features into meta data (au-
thor, comment and time stamp of a revision), rep-
utation (author and article reputation), textual (lan-
guage independent, i.e. token- and character-based)
and language features (language dependent, mostly
dictionary-based). They carry out cross-validation
experiments on the PAN-WVC-10 corpus (Potthast
and Holfeld, 2011). Classifiers based on reputa-
tion and text performed best. Adler et al. (2011)
use Random Forests as classifier (Breiman, 2001)
in their experiments. This classifier was also used
in the vandalism detection study of Javanmardi et al.
(2011) where it outperformed the classifiers based
on Logistic Regression and Naive Bayes.
Different to the approach of Bronner and Monz
(2012) and previous vandalism classification stud-
ies, we built a model which accounts for multi-
labeling and a fine-grained edit category system.
Our feature set builds upon existing work while
adding a substantial number of new features.
3 Experiments
3.1 Wikipedia Edit Category Corpus
For our experiments, we used the freely avail-
able Wikipedia Edit Category Corpus (WPEC) com-
piled in previous work (Daxenberger and Gurevych,
2012). In this corpus, each pair of adjacent revisions
is segmented into one or more edits. This enables
an accurate picture of the editing process, as an au-
thor may perform several independent edits in the
same revision. Furthermore, edits are multi-labeled,
i.e. each edit is assigned one or more categories.
This is important for a precise description of major
edits, e.g. when an entire new paragraph including
text, references and markup is added. There are four
basic types of edits, namely Insertions, Deletions,
Modifications and Relocations. These are calculated
via a line-based diff comparison on the source text
(including wiki markup). As previously suggested
(Daxenberger and Gurevych, 2012), inside modified
lines, only the span of text which has actually been
changed is marked as edit (either Insertion, Dele-
tion or Modification), not the entire line. We ex-
tracted the data which is not contained in WPEC
(meta data and plain text of rv?1 and rv) using the
Java Wikipedia Library (JWPL) with the Revision
Toolkit (Ferschke et al., 2011).
In Daxenberger and Gurevych (2012), we divide
the 21-category taxonomy into text-base (meaning-
changing edits), surface (non meaning-changing ed-
its) and Wikipedia policy (VANDALISM and RE-
VERT) edits. Among the text-base edits, we include
categories for templates, references (internal and ex-
ternal links), files and information, each of which
is further divided into an insertion (I), deletion (D)
and modification (M) category. Surface edits con-
sist of paraphrases, spelling and grammar correc-
tions, relocations and markup edits. The latter cate-
gory contains all edits which affect markup elements
that are not covered by any of the other categories
and is divided into insertions, deletions and modifi-
cations. This includes, for example, apostrophes in
'''bold text'''. We also suggested an OTHER category,
which is intended for edits which cannot be labeled
due to segmentation errors. Figure 1 shows an exam-
ple edit from WPEC, labeled with the REFERENCE-
580
Feature Value Explanation
M
et
a
D
at
a
Author group user Wikimedia user group of the author
Author is registered* true Author is registered (otherwise: IP user)
Same author* false Authors of rv and rv?1 are the same
Comment length* 0 Number of characters in the comment
Vulgarism in comment false Comment contains a word from in the vulgarism word list
Comment is auto-generated false Entire comment has been auto-generated
Auto-generated comment ratio 0 Auto-generated part of comment divided by length of the comment
Incorrect comment ratio 0 Out-of-dictionary word count divided by word count in the comment
Comment n-grams1 — Presence or absence of token n-grams in the comment
Is revert* false Comment contains a word from in the revert word list
Is minor false Revision has been marked as minor change
Time difference* 505 Time difference between rv?1 and rv (in minutes)
Number of edits 1 Absolute number of edits in the (rv?1, rv)-pair
Te
xt
ua
l
Diff capitals* 0 Difference in the number of capitals
Diff digits* 0 Difference in the number of digits
Diff special characters* 2 Difference in the number of non-alphanumeric characters
Diff whitespace characters 1 Difference in the number of whitespace characters
Diff characters* 9 Difference in the number of characters
Diff tokens* 1 Difference in the number of whitespace-separated tokens
Diff repeated characters 0 Difference in the number of repeated characters
Diff repeated tokens 0 Difference in the number of repeated white-space separated tokens
Cosine similarity 0 Cosine similarity
Levenshtein distance* 9 Levenshtein distance
Optimal string alignment distance 9 Optimal string alignment distance (Damerau-Levenshtein distance)
Ratio diff to paragraph characters 0.02 Diff characters divided by the length of the edited paragraph
Ratio diff to revision characters 0.0005 Diff characters divided by the length of rv?1
Ratio diff to paragraph tokens 0.04 Diff tokens divided by the length of the edited paragraph
Ratio diff to revision tokens 0.0003 Diff tokens divided by the length of rv?1
Ratio old to new paragraph 0 Difference in the number of characters in the edited paragraph
Character n-grams1 p,o,e,t,r,y2 Presence or absence of n-grams of edited characters
Token n-grams1 poetry2 Presence or absence of n-grams of edited tokens
Simple edit type Insertion Modification, Insertion, Deletion or Relocation
M
ar
ku
p
Diff number m 0 Difference in the number of m
Diff type m false Different types of m
Diff type context m true3 Different types of m within the immediate context of the edit
Is covered by m true3 Edit is covered by m in rv?1
Covers m false Edit covers m in rv?1
L
an
gu
ag
e Diff spelling errors* 0 Difference in the number of out-of-dictionary words
Diff vulgar words* 0 Difference in the number of tokens contained in vandalism word list
Semantic similarity -1 Explicit Semantic Analysis with vector indexes from Wiktionary
Diff POS tags* false POS tag sets are symmetrically different
Diff type POS tags* 0 Number of distinct POS tags
1 N-gram features are represented as boolean features.
2 In this example, n = 1 (unigrams).
3 True if m corresponds to internal link, false otherwise.
Table 1: List of edit category classification features with explanations. The values correspond to the the example edit
from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with
* have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012).
581
M category. WPEC was created in a manual anno-
tation study with three annotators. The overall inter-
annotator agreement measured as Krippendorf’s ? is
.67 (Daxenberger and Gurevych, 2012). The exper-
iments in this study are based on the gold standard
annotations in WPEC, which have been derived by
means of a majority vote for each edit.
WPEC consists of 981 revision pairs, segmented
into 1,995 edits. We define edit category classifica-
tion as a multi-label classification task. For the sake
of readability, in the following we will refer to an
edit ekv?1,v as ei, with ei ? E, where 0 ? i < 1995
and E is the set of all edits. An edit ei is the basic
classification unit in our task. Each ei has to be la-
beled with a set of categories y ? C, where C is the
set of all edit categories, |C| = 21.
3.2 Features for Edit Category Classification
We grouped our features into meta data, textual,
markup and language features. An overview and
explanation of all features can be found in Table 1.
The scheme we apply to group edit category clas-
sification features is similar to the system used by
Adler et al. (2011). We re-use some of the features
suggested by Adler et al. (2011), Javanmardi et al.
(2011) and Bronner and Monz (2012), as marked in
Table 1. Features are calculated on edited text spans.
We label the edited text span corresponding to ei in
rv?1 as tv?1 and the edited text span in rv as tv. In
edits which are insertions, we consider tv?1 to be
empty, while tv is considered empty for deletions.
For Relocations, tv?1 = tv.
Table 1 includes the value of each feature for the
example edit from Figure 1. This edit modifies the
link [[Dactyl|Dactylic]] by adding a speci-
fication to the target of that link. For spell-checking,
we use British and US-American English Jazzy dic-
tionaries.3 Markup elements are detected by the
Sweble Wikitext parser (Dohrn and Riehle, 2011).
Meta data features We consider the comment,
author, time stamp or any other flag (“minor
change”) of rv as meta data. The Wikimedia user
group4 of an author specifies the edit permissions
3http://sourceforge.net/projects/
jazzydicts
4http://meta.wikimedia.org/wiki/User_
classes
of this user (e.g. bot, administrator, blocked user).
We indicate whether the revision comments or parts
of it have been auto-generated. This happens when
a page is blanked, i.e. all of its content has been
deleted or replaced or when a new page or redirect is
created (denoted by the Comment is auto-generated
feature). Furthermore, edits within a specific sec-
tion of an article are automatically marked by adding
a prefix with the name of this section to the com-
ment of the revision (denoted by the Auto-generated
comment ratio feature). Meta data features have the
same value for all edits in a (rv?1, rv)-pair.
Textual features Textual features are calculated
based on a certain property of the changed text. In
a preprocessing step, any wiki markup inside tv?1
and tv is deleted. As for the example edit from Fig-
ure 1, tv?1 would correspond to an empty string and
tv would be represented as “ (poetry)”. The n-gram
feature spaces are composed of n-grams that are
present either in tv?1 but not tv, or vice verse. Char-
acter n-grams only contain English alphabet charac-
ters, token n-grams consist of words excluding spe-
cial characters.
Markup features As opposed to textual features,
wiki markup features account for the Wikimedia
specific markup elements. Markup features are cal-
culated based on the number and type of a markup
element m and the surrounding context of an edit.
Here, m can be a template, an external or internal
link, an image or any other element used to describe
markup including HTML tags. The type of m is
defined by the link target for internal and external
links and images, by the name of the template for
templates and by the wiki markup element name for
markup elements. Markup features are calculated on
text spans tv?1 and tv. Naturally, wiki markup is not
deleted beforehand. The edited text spans tv?1 and
tv may be located inside a markup elementm (e.g. a
link or a template). In such cases, our diff algorithm
will not label the entire element m, but rather the
actually modified text. However, such an edit may
change the name of a template or the target of a link
(as in the example edit from Figure 1). We there-
fore include the immediate context sv?1 and sv of
each edit and compare the type of potential markup
elements m in sv?1 and sv. Here, sv (sv?1) is de-
fined as tv (tv?1) including all preceding and follow-
582
Revisions Edits Cardinality
Train 713 1,597 1.20
Test 89 229 1.24
Dev 89 169 1.21
Table 2: Statistics of the training, test and development
set. Cardinality is the average number of edit categories
assigned to an edit.
ing characters in rv (rv?1) which are not separated
from tv (tv?1) by a boundary character (whitespace
or line break). The above described features model
what is actually edited in the text. A number of fea-
tures are calculated on tv?1 only. These features are
more likely to inform about where an edit is con-
ducted. They specify whether tv?1 covers (i.e. con-
tains) a certain wiki markup element and vice versa,
i.e. whether tv?1 is located inside a text span that
belongs to a markup element.
Language Language features are calculated on the
context sv?1 and sv of edits, any wiki markup is
deleted. For the Explicit Semantic Analysis, we use
Wiktionary (Zesch et al., 2008) and not Wikipedia
assuming that the former has a better coverage with
respect to different lexical classes. POS tagging was
carried out using the OpenNLP POS tagger.5 The
vandalism word list contains a hand-crafted set of
around 100 vandalism and spam words from various
places in the web.
3.3 Experimental Setup
We extract features with the help of ClearTK (Ogren
et al., 2008). For the machine learning part, we use
Weka (Hall et al., 2009) with the Meka6 and Mu-
lan (Tsoumakas et al., 2010) extensions for multi-
label classification. We use DKPro Lab (Eckart de
Castilho et al., 2011) to test different parameter com-
binations. We randomly split the gold standard data
from WPEC into 80% training, 10% test and 10%
development set, as shown in Table 2.
Multi-label Classification We report the perfor-
mance of various machine learning algorithms. A
comprehensive overview of multi-label classifica-
tion algorithms and evaluation measures can be
5Maxent model for English, http://opennlp.
apache.org
6http://meka.sourceforge.net
R
an
do
m
M
aj
or
ity
B
R
H
O
M
E
R
R
A
K
E
L
Threshold – – .10 .25 .33
Example
Accuracy .09 .13 .50 .44 .53
Exact Match .06 .13 .35 .36 .44
F1 .09 .13 .55 .47 .56
Precision .10 .13 .54 .46 .56
Recall .10 .13 .61 .50 .60
Label
Macro-F1 .10 .06 .49 .35 .51
Micro-F1 .10 .12 .59 .49 .62
Ranking One Error .90 .87 .42 .48 .34
Table 3: Overall classification results with 3 multi-label
classifiers and a C4.5 decision tree base classifier, as com-
pared to random and majority category baselines.
found in Madjarov et al. (2012). Multi-label classi-
fication problems are solved by either transforming
the multi-label classification task into one or more
single-label classification tasks (problem transfor-
mation method) or by adapting single-label clas-
sification algorithms (algorithm adaption method).
Several algorithms have been developed on top of
the former methods and use ensembles of such clas-
sifiers (ensemble methods). We applied the Bi-
nary Relevance approach (BR), a simple transfor-
mation method which converts the multi-label prob-
lem into |C| binary single-label problems, where |C|
is the number of categories. Hence, this method
trains a classifier for each category in the corpus
(one-against-all). It is the most straightforward ap-
proach when dealing with multi-labeled data. How-
ever, it does not consider possible relationships or
dependencies between categories. Therefore, we
tested two more sophisticated methods. Hierar-
chy of multi-label classifiers HOMER (Tsoumakas
et al., 2008) is a problem transformation method.
It accounts for possibly hierarchical relationships
among categories by dividing the overall category
set into a tree-like structure with nodes of small ca-
tegory sets of size k and leaves of single categories.
Subsequently, a multi-label classifier is applied to
each node in the tree. Random k-labelsets RAKEL
(Tsoumakas et al., 2011) is an ensemble method,
which randomly chooses l typically small subsets
with k categories from the overall set of catego-
ries. Subsequently, all k-labelsets which are found
in the multi-labeled data set are converted into new
categories in a single-labeled data set using the la-
583
bel powerset transformation (Trohidis et al., 2008).
HOMER and BR are among the multi-label clas-
sifiers, which Madjarov et al. (2012) recommend
as benchmark methods. As underlying single-label
classification algorithm, we used a C4.5 decision
tree classifier (Quinlan, 1993), as decision tree clas-
sifiers yield state-of-the-art performance in the re-
lated work.
Multi-label Evaluation We denote the set of rel-
evant categories for each edit ei ? E as yi ?
C and the set of predicted categories as h(ei).
Evaluation measures for multi-label classification
systems are based on either bipartitions or rank-
ings. Among the former, we report example-
based (weighting each edit equally) and label-based
(weighting each edit category equally) measures.
The accuracy of a multi-label classifier is defined
as 1|E|
?|E|
i=1
|h(ei)?yi|
|h(ei)?yi|
, which corresponds to the Jac-
card similarity of h(ei) and yi averaged over all ed-
its. We report subset accuracy (exact match), cal-
culated as 1|E|
?|E|
i=1 I , with I = 1 if h(ei) =
yi and I = 0 otherwise. Example-based pre-
cision is defined as 1|E|
?|E|
i=1
|h(ei)?yi|
|h(ei)|
, recall as
1
|E|
?|E|
i=1
|h(ei)?yi|
|yi|
, and F1 as 1|E|
?|E|
i=1
2×|h(ei)?yi|
|h(ei)|+|yi|
.
For the label-based measures, we report macro-
and micro-averaged F1 scores. As a ranking-based
measure, we report one error, which is defined as
1
|E|
?|E|
i=1J[arg max
c?C
f(ei, c)] /? yiK, JexprK = 1 if
expr is true and JexprK = 0 otherwise. f(ei, c) de-
notes the rank of category c ? C as predicted by
the classifier. The one error measure evaluates the
number of edits where the highest ranked category
in the predictions is not in the set of relevant cate-
gories. It becomes smaller when the performance of
the classifier increases.
Table 3 shows the overall classification scores.
We calculated a random baseline, which multi-labels
edits at random considering the label powerset fre-
quencies it has learned from the training data. Fur-
thermore, we calculated a majority category base-
line, which labels all edits with the most frequent
edit category in the training data. In Figure 2, we
list the results for each category, together with the
average pair-wise inter-rater agreement (F1 scores).
The F1 scores are calculated based on the study we
carried out in Daxenberger and Gurevych (2012).
Parameters and Feature selection All parame-
ters have been adjusted on the development set us-
ing the RAKEL classifier, aiming to optimize accu-
racy. With respect to the n-gram features, we tested
values for n = 1, 2 and 3. For comment n-grams,
unigrams turned out to yield the best overall per-
formance, and bigrams for character and token n-
grams. The word and character n-gram spaces are
limited to the 500 most frequent items, the comment
n-gram space is limited to the 1,500 most frequent
items. To transform ranked output into bipartitions,
it is necessary to set a threshold. This threshold is
reported in Table 3 and has been optimized for each
classifier with respect to label cardinality (average
number of labels assigned to edits) on the develop-
ment set. Since most of the traditional feature se-
lection methods cannot be applied directly to multi-
labeled data, we used the label powerset approach to
transform the multi-labeled data into single-labeled
data and subsequently applied ?2. Feature reduc-
tion to the highest-ranked features clearly improved
the classifier performance on the development set.
We therefore limited the feature space to the 150
highest-ranked features in our experiments.
For the RAKEL classifier, we set l = 42 (twice
the size of the category set) and k = 3. In HOMER,
we used BR as transformation method, random dis-
tribution of categories to the children nodes and
k = 3. For all other classifier parameters, we used
the default settings as configured in Meka respective
Mulan.
4 Discussion
The classifiers significantly outperformed both base-
lines. RAKEL shows best performance for almost
all measures in Table 3. The simpler BR approach,
which assumes no dependencies between categories,
still outperforms HOMER.
We trained and tested the classifier with different
feature groups (see Table 1), to analyze the impor-
tance of single types of features. As shown in Fig-
ure 2, textual features had the highest impact on clas-
sification performance. On the opposite, language
features played a minor role in our experiments.
Among the highest ranked individual features for the
entire set of categories, we find textual (Levenshtein
distance, Simple edit type), markup (Diff number
584
Re
ve
rt
(3
0)
Va
nd
ali
sm
(1
9)
Pa
ra
ph
ra
se
(5
)
Re
loc
at
ion
(2
)
M
ar
ku
p-
I (
27
)
M
ar
ku
p-
M
(2
)
M
ar
ku
p-
D
(1
0)
Sp
ell
in
g/G
r.
(2
0)
Re
fer
en
ce
-I
(3
1)
Re
fer
en
ce
-M
(1
2)
Re
fer
en
ce
-D
(1
2)
In
for
m
at
ion
-I
(3
5)
In
for
m
at
ion
-M
(2
8)
In
for
m
at
ion
-D
(1
8)
Te
m
pl
at
e-I
(1
0)
Te
m
pl
at
e-D
(2
)
Fi
le-
I (
2)
Fi
le-
D
(1
)
Ot
he
r (
16
)
0
0.2
0.4
0.6
0.8
1
F
1
sc
or
e
Human Classifier Textual Markup Meta Data Language
Figure 2: F1 scores of RAKEL with C4.5 as base classifier for individual categories. We add human inter-annotator
agreement as average pair-wise F1 scores as well as F1 scores for classifiers trained and tested on single feature
groups, cf. Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M
and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set.
markup elements) and meta data (Number of edits)
features.
Bronner and Monz (2012) report an accuracy
of .88 for their best performing system on the bi-
nary classification task of distinguishing fluency and
factual edits. The best performing classifier in
their study was Random Forests (Breiman, 2001).
To compare our features with their approach, we
mapped the 21 edit categories from Daxenberger and
Gurevych (2012) to the binary category set (factual
vs. fluency) of Bronner and Monz (2012). Edits la-
beled as SPELLING/GRAMMAR, MARKUP, RELO-
CATION and PARAPHRASE are considered fluency
edits, the remaining categories factual edits. We re-
moved all edits labeled as OTHER, REVERT or VAN-
DALISM from WPEC. After applying the category
mapping, we deleted all edits which were labeled
with both the fluency and factual category. The lat-
ter may happen due to multi-labeling. This resulted
in 1,262 edits labeled as either fluency or factual.
On the 80% training split from Table 2, we trained
a Random Forests classifier with the optimized fea-
ture set and feature reduction as described in Sec-
tion 3.3. The number of trees was set to 100, with
unlimited depth. On the remaining data (test and
development split), we achieved an accuracy of .90.
Although we did not use the same data set as Bron-
ner and Monz (2012), this result suggests that our
feature set is suited for related tasks such as fluency
detection.
With respect to vandalism detection in Wikipedia,
state-of-the-art systems have a performance of
around .82 to .85 AUC-PR on the English Wikipedia
(Adler et al., 2011). We suspect that the low perfor-
mance of our system for Vandalism edits is mostly
due to a lower amount of training data, a higher skew
in the training and test data and the fact that we did
not include features which inform about future ac-
tions (e.g. whether a revision is reverted).
Error Analysis Sparseness is a major problem
for some of the 21 categories, as shown in Fig-
ure 2 by categories such as FILE-D, TEMPLATE-
D, MARKUP-M or PARAPHRASE which have only
very few examples in training, development and
test set. Categories with low inter-annotator agree-
ment in WPEC such as MARKUP-M, PARAPHRASE
or OTHER also yielded low classification accuracy.
We analyzed frequent errors of the classifier with
the help of a confusion matrix. PARAPHRASE ed-
its have been confused with INFORMATION-M by
the classifier. Furthermore, the classifier had prob-
lems to distinguish between VANDALISM and RE-
VERT as well as INFORMATION-I. Generally, modi-
fications as compared to insertions or deletions per-
form worse. All of the classifiers we tested, build
585
their predictions by thresholding over a ranking, cf.
Table 3. This generates a source of errors, because
the classifier is not able to make a prediction, if it
does not have enough confidence for any of the cate-
gories. The imbalance of the data, because of the
high skew in the category distribution, is another
reason for classification errors. In ambiguous cases,
the classifier will be biased toward the category with
more examples in the training data.
5 A closer look at edit sequences: Mining
collaboration patterns
An edit category classifier allows us to label en-
tire article revision histories. We applied the best-
performing model from Section 3.3 trained on the
entire WPEC to automatically classify all edits in the
Wikipedia Quality Assessment Corpus (WPQAC)
as presented in previous work (Daxenberger and
Gurevych, 2012). WPQAC consists of 10 fea-
tured and 10 non-featured articles7, with an over-
all number of 21,578 revisions (9,986 revisions
from featured articles and 11,592 from non-featured
articles), extracted from the April 2011 English
Wikipedia dump. The articles in WPQAC are care-
fully chosen to form comparable pairs of featured
and non-featured articles, which should reduce the
noise of external influences on edit activity such
as popularity or visibility. In Daxenberger and
Gurevych (2012), we have shown significant dif-
ferences in the edit category distribution of arti-
cles with featured status before and after the articles
were featured. We concluded that articles become
more stable after being featured, as shown by the
higher number of surface edits and lower number of
meaning-changing edits.
Different to our previous approach which is based
on the mere distribution of edit categories, in the
present study we include the chronological order of
edits and use a 10 times larger amount of data for our
experiments. We segmented all adjacent revisions
in WPQAC into edits, following the approach ex-
plained in Daxenberger and Gurevych (2012). Dur-
ing the classification process, we discarded revisions
where the classifier could not assign any of the 21
edit categories with a confidence higher than the
7http://en.wikipedia.org/wiki/Wikipedia:
FA
threshold, cf. Table 3. This resulted in 17,640 re-
maining revisions. We applied a sequential pattern
mining algorithm with time constraints (Hirate and
Yamana, 2006; Fournier-Viger et al., 2008) to the
data. The latter is based on the PrefixSpan algorithm
(Pei et al., 2004). Calculations have been carried out
within the open-source SPMF Java data mining plat-
form.8
We created one time-extended sequence database
for the 10 featured articles and one for the 10 non-
featured articles. The sequence databases consist
of one row per article. Each row is a chronologi-
cally ordered list of revisions. Each revision is rep-
resented by the itemset of all edit categories for all
edits in that revision (in alphabetical order).
The output of the algorithm are sequential pat-
terns with time constraints. To obtain meaningful
results, we constrained the output with the follow-
ing parameters:
• Minimum support: 1 (the patterns have to be
present in each article)
• Time interval allowed between two successive
itemsets in the patterns: 1 (patterns are ex-
tracted only from adjacent revisions)
• Minimum time interval between the first item-
set and the last itemset in the patterns: 1 (the
length of the patterns is 2 or higher)
As this output reflects recurring sequences of ad-
jacent revisions labeled with edit categories, we re-
fer to it as collaboration patterns. With these pa-
rameters, the algorithm discovered 1,358 sequen-
tial patterns for featured articles and 968 for non-
featured articles. The number of shared patterns in
featured and non-featured articles is 427, this corre-
sponds to the number of frequent patterns in a se-
quence database which contains all 20 featured and
non-featured articles. The maximum length of pat-
terns which were found was 6 for featured articles,
and 5 for non-featured articles. These numbers show
that the defined collaboration patterns seem to have
discriminative power for different kinds of articles.
Featured articles can be characterized by a higher
8http://www.philippe-fournier-viger.
com/spmf
586
Featured
1 INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 INFORMATION-I 5 INFORMATION-I
1 INFORMATION-D, INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 REFERENCE-I
1 TEMPLATE-D 2 REFERENCE-I
Non-
Featured
1 INFORMATION-I 2 INFORMATION-I, REFERENCE-I 3 INFORMATION-I 4 REFERENCE-I 5 MARKUP-I
1 MARKUP-I 2 REFERENCE-D 3 MARKUP-I
1 VANDALISM 2 REVERT
Table 4: Examples of collaboration patterns which have been found in either all featured or all non-featured articles of
WPQAC.
degree of homogeneity with respect to their collab-
orative patterns due to a higher number and length
of frequent sequential patterns in featured articles as
compared to non-featured articles.
In Table 4, we list some examples of collabora-
tion patterns with a minimum support of 1 which
we found in featured, but not non-featured arti-
cles, or vice verse. Unsurprisingly, patterns which
contain combinations of the most frequent catego-
ries (INFORMATION-I, REFERENCE-I), have a high
overall frequency. The diversity inside collaboration
patterns measured by the number of different edit
categories was higher in non-featured articles. For
example, the VANDALISM - REVERT pattern was
only found in non-featured articles. Patterns in fea-
tured articles tended to be more homogeneous, as
shown by the first pattern in Table 4, a repetition
of additions of information. We conclude that dis-
tinguished, high-quality articles, show a higher de-
gree of homogeneity as compared to a subset of non-
featured articles and the overall corpus.
6 Conclusion
In this study, we evaluated a novel feature set
for building a model to automatically classify
Wikipedia edits. Using a freely available cor-
pus (Daxenberger and Gurevych, 2012), our model
achieved a micro-averaged F1 score of .62 classify-
ing edits within a range of 21 categories. Textual
features had the highest impact on classifier perfor-
mance, whereas language features play a minor role.
The same classifier model obtained state-of-the-art
performance on the related task of fluency edit clas-
sification. Applications which potentially benefit
from our work include the analysis of the writing
process in collaboratively created documents, such
as wikis or research papers. We have demonstrated
how our model can be used to detect collaboration
patterns in article revision histories. On a subset
of articles from the English Wikipedia, we found
that high-quality articles show a higher degree of
homogeneity in their collaborative patterns as com-
pared to random articles. Furthermore, automatic
edit category classification allows to generate huge
amounts of category-filtered training data for NLP
tasks, e.g. spelling and grammar correction or van-
dalism detection. With respect to future work, we
plan to include more resources, e.g. the PAN-WVC-
10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max
and Wisniewski, 2010) to increase the size of train-
ing data. A larger amount of labeled data would
certainly help to improve the classifier performance
for weak categories (e.g. VANDALISM and PARA-
PHRASE) and sparse categories (e.g. TEMPLATE-D,
MARKUP-M). Based on our trained classifier, anno-
tating more examples can be alleviated with the help
of active learning.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806, and by the
Hessian research excellence program “Landes-
Offensive zur Entwicklung Wissenschaftlich-
o¨konomischer Exzellenz” (LOEWE) as part of the
research center “Digital Humanities”. We thank the
anonymous reviewers for their valuable feedback.
References
B Thomas Adler, Luca Alfaro, Santiago M Mola-
Velasco, Paolo Rosso, and Andrew G West. 2011.
Wikipedia Vandalism Detection: Combining Natural
Language, Metadata, and Reputation Features. In
Alexander Gelbukh, editor, Computational Linguistics
587
and Intelligent Text Processing, Lecture Notes in Com-
puter Science, pages 277–288. Springer.
Ablimit Aji, Yu Wang, and Eugene Agichtein. 2010. Us-
ing the Past To Score the Present: Extending Term
Weighting Models Through Revision History Analy-
sis. ReCALL, pages 629–638.
Daniel Ba¨r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation, held in conjunction
with the 1st Joint Conference on Lexical and Compu-
tational Semantics, pages 435–40, Montreal, Canada,
USA.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45(1):5–32.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories. In
European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 356–366, Avi-
gnon, France.
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on The People’s Web meets NLP, pages 34–
43, Jeju Island, Republic of Korea.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 507–
517, Atlanta, GA, USA.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In Proceed-
ings of the 24th International Conference on Compu-
tational Linguistics, pages 711–726, Mumbai, India.
Hannes Dohrn and Dirk Riehle. 2011. Design and imple-
mentation of the Sweble Wikitext parser. In Proceed-
ings of the 7th International Symposium on Wikis and
Open Collaboration, pages 72–81, Mountain View,
CA, USA.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure´lien Max. 2011. Local modifications and
paraphrases in Wikipedia’s revision history. Proce-
samiento del Lenguaje Natural, 46:51–58.
Richard Eckart de Castilho, Iryna Gurevych, and
Richard Eckart de Castilho. 2011. A Lightweight
Framework for Reproducible Parameter Sweeping in
Information Retrieval. In Proceedings of the Work-
shop on Data Infrastructures for Supporting Informa-
tion Retrieval Evaluation, pages 7–10, Glasgow, UK.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia’s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations, pages 97–102, Portland, OR,
USA.
Oliver Ferschke, Johannes Daxenberger, and Iryna
Gurevych. 2013. A Survey of NLP Methods and Re-
sources for Analyzing the Collaborative Writing Pro-
cess in Wikipedia. In Iryna Gurevych and Jungi Kim,
editors, The Peoples Web Meets NLP: Collaboratively
Constructed Language Resources, Theory and Appli-
cations of Natural Language Processing, chapter 5.
Springer.
Philippe Fournier-Viger, Roger Nkambou, and Engel-
bert Mephu Nguifo. 2008. A Knowledge Discovery
Framework for Learning Task Models from User Inter-
actions in Intelligent Tutoring Systems. In Alexander
Gelbukh and Eduardo F. Morales, editors, Proceedings
of the 7th Mexican International Conference on Artifi-
cial Intelligence, Lecture Notes in Computer Science,
pages 765–778. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10–18.
Yu Hirate and Hayato Yamana. 2006. Generalized Se-
quential Pattern Mining with Item Intervals. Journal
of Computers, 1(3):51–60.
Sara Javanmardi, David W. McDonald, and Cristina V.
Lopes. 2011. Vandalism Detection in Wikipedia: A
High-Performing, Feature-Rich Model and its Reduc-
tion Through Lasso. In Proceedings of the 7th Interna-
tional Symposium on Wikis and Open Collaboration,
pages 82–90, Mountain View, CA, USA.
Brian Keegan, Darren Gergle, and Noshir Contractor.
2013. Hot Off the Wiki: Structures and Dynamics
of Wikipedia’s Coverage of Breaking News Events.
American Behavioral Scientist, 57(5):595–622, May.
Jun Liu and Sudha Ram. 2011. Who does what: Col-
laboration patterns in the wikipedia and their impact
on article quality. ACM Trans. Management Inf. Syst.,
2(2):11.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Sas?o Dz?eroski. 2012. An extensive experimental
comparison of methods for multi-label learning. Pat-
tern Recognition, 45(9):3084–3104.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
588
Language Technologies, pages 182–190, Montre´al,
Canada.
Aure´lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedias Revision History. In Proceedings of the
7th Conference on International Language Resources
and Evaluation, Valletta, Malta.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia’s Article Revision History for Training
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31–36, Chicago, IL, USA.
Se´rgio Nunes, Cristina Ribeiro, and Gabriel David. 2011.
Term weighting based on document revision history.
Journal of the American Society for Information Sci-
ence and Technology, 62(12):2471–2478.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC), pages 32–38, Marrakech, Morocco.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal,
and Mei-Chun Hsu. 2004. Mining sequential patterns
by pattern-growth: the PrefixSpan approach. IEEE
Transactions on Knowledge and Data Engineering,
16(11):1424–1440.
Martin Potthast and Teresa Holfeld. 2011. Overview of
the 2nd International Competition on Wikipedia Van-
dalism Detection. In Notebook Papers of CLEF 2011
Labs and Workshops, Amsterdam, Netherlands.
Martin Potthast, Tim Gollub, Matthias Hagen, Johannes
Kiesel, Maximilian Michel, Arnd Oberla¨nder, Mar-
tin Tippmann, Alberto Barro´n-Ceden˜o, Parth Gupta,
Paolo Rosso, and Benno Stein. 2012. Overview of
the 4th International Competition on Plagiarism De-
tection. In CLEF 2012 Evaluation Labs and Workshop
Working Notes Papers, Rome, Italy.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic Models for Analyz-
ing and Detecting Biased Language. In Proceedings of
the 51st Annual Meeting on Association for Computa-
tional Linguistics, pages 1650–1659, Sofia, Bulgaria.
Bongwon Suh, Gregorio Convertino, Ed H. Chi, and Pe-
ter Pirolli. 2009. The singularity is not near: slow-
ing growth of Wikipedia. In Proceedings of the 5th
International Symposium on Wikis and Open Collabo-
ration, Orlando, FL, USA.
Konstantinos Trohidis, Grigorios Tsoumakas, George
Kalliris, and Ioannis Vlahavas. 2008. Multi-label
classification of music into emotions. In 9th Inter-
national Conference on Music Information Retrieval,
pages 325–330, Philadelphia, PA, USA.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2008. Effective and Efficient Multilabel Classi-
fication in Domains with Large Number of Labels. In
Proceedings of the ECML/PKDD 2008 Workshop on
Mining Multidimensional Data, Antwerp, Belgium.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining and
Knowledge Discovery Handbook, chapter 34, pages
667–685. Springer.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2011. Random k-Labelsets for Multi-Label
Classification. IEEE Transactions on Knowledge and
Data Engineering, 23(7):1079–1089.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 409–420, Edinburgh, Scot-
land, UK.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Short Papers,
pages 137–140, Columbus, OH, USA.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ’10, pages 365–368, Los Angeles, CA, USA.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the COLING-Workshop on The People’s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 28–36, Beijing, China.
Torsten Zesch, Christof Mu¨ller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic Re-
latedness. In Proceedings of the Twenty-Third AAAI
Conference on Artificial Intelligence, pages 861–866,
Chicago, IL, USA.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529–538, Avignon, France.
589
