Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116–1127,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
A Fast Re-scoring Strategy to Capture Long-Distance Dependencies
Anoop Deoras
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
adeoras@jhu.edu,
Toma´s? Mikolov
Brno University of Technology
Speech@FIT
Czech Republic
imikolov@fit.vutbr.cz,
Kenneth Church
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
kenneth.church@jhu.edu
Abstract
A re-scoring strategy is proposed that makes
it feasible to capture more long-distance de-
pendencies in the natural language. Two pass
strategies have become popular in a num-
ber of recognition tasks such as ASR (au-
tomatic speech recognition), MT (machine
translation) and OCR (optical character recog-
nition). The first pass typically applies a
weak language model (n-grams) to a lattice
and the second pass applies a stronger lan-
guage model to N best lists. The stronger lan-
guage model is intended to capture more long-
distance dependencies. The proposed method
uses RNN-LM (recurrent neural network lan-
guage model), which is a long span LM, to re-
score word lattices in the second pass. A hill
climbing method (iterative decoding) is pro-
posed to search over islands of confusability
in the word lattice. An evaluation based on
Broadcast News shows speedups of 20 over
basic N best re-scoring, and word error rate
reduction of 8% (relative) on a highly compet-
itive setup.
1 Introduction
Statistical Language Models (LMs) have received
considerable attention in the past few decades. They
have proved to be an essential component in many
statistical recognition systems such as ASR (au-
tomatic speech recognition), MT (machine trans-
lation) and OCR (optical character recognition).
The task of a language model is to assign prob-
ability to any word sequence possible in the lan-
guage. The probability of the word sequence W ?
w1, . . . , wm ? wm1 is typically factored using the
chain rule:
P (wm1 ) =
m?
i=1
P (wi|wi?11 ) (1)
In modern statistical recognition systems, an LM
tends to be restricted to simple n-gram models,
where the distribution of the predicted word depends
on the previous (n ? 1) words i.e. P (wi|wi?11 ) ?
P (wi|wi?1i?n+1).
Noam Chomsky argued that n-grams cannot learn
long-distance dependencies that span over more than
n words (Chomsky, 1957, pp.13). While that might
seem obvious in retrospect, there was a lot of ex-
citement at the time over the Shannon-McMillan-
Breiman Theorem (Shannon, 1948) which was inter-
preted to say that, in the limit, under just a couple of
minor caveats and a little bit of not-very-important
fine print, n-gram statistics are sufficient to capture
all the information in a string (such as an English
sentence). Chomsky realized that while that may be
true in the limit, n-grams are far from the most parsi-
monious representation of many linguistic facts. In
a practical system, we will have to truncate n-grams
at some (small) fixed n (such as trigrams or perhaps
5-grams). Truncated n-gram systems can capture
many agreement facts, but not all.1
By long-distance dependencies, we mean facts
like agreement and collocations that can span over
many words. With increasing order of n-gram mod-
els we can, in theory, capture more regularities in the
1The discussion in this paragraph is taken as-is from an arti-
cle (to appear) by Church (2012).
1116
language. In addition, if we can move to more gen-
eral models then we could hope to capture more, as
well. However, due to data sparsity, it is hard to es-
timate a robust n-gram distribution for large values
of n ( say, n > 10) using the conventional Max-
imum Likelihood techniques, unless a more robust
technique is employed for modeling which gener-
alizes well on unseen events. Some of these well
known long span / complex language models which
have shown to perform very well on many speech
tasks include: structured language model (Chelba
and Jelinek, 2000; Roark, 2001; Wang and Harper,
2002; Filimonov and Harper, 2009), latent seman-
tic analysis language model (Bellegarda, 2000),
topic mixture language models (Iyer and Ostendorf,
1999), whole sentence exponential language mod-
els (Rosenfeld, 1997; Rosenfeld et al., 2001), feed-
forward neural networks (Bengio et al., 2001), re-
current neural network language models (Mikolov
et al., 2010), among many others.
Although better modeling techniques can now
capture longer dependencies in a language, their
incorporation in decoders of speech recognition or
machine translation systems becomes computation-
ally challenging. Due to the prohibitive increase in
the search space of sentence hypotheses (or longer
length word sub sequences), it becomes challenging
to use a long span language model in the first pass
decoding. A word graph (word lattices for speech
recognition systems and hypergraphs for machine
translation systems), encoding exponential number
of hypotheses is hence outputted at the first pass out-
put on which a sophisticated and complex language
model is deployed for re-scoring. However, some-
times even re-scoring of this refined search space
can be computationally expensive due to explosion
of state space.
Previously, we showed in (Deoras et al., 2011)
how to tackle the problem of incorporating long span
information during decoding in speech recogni-
tion systems by variationaly approximating (Bishop,
2006, pp. 462) the long span language model by a
tractable substitute such that this substitute model
comes closest to the long span model (closest in
terms of Kullback Leibler Divergence (Cover and
J.A.Thomas, 1991, pp. 20)). The tractable substi-
tute was then used directly in the first pass speech
recognition systems. In this paper we propose an
approach that keeps the model intact but approxi-
mates the search space instead (which can become
intractable to handle especially under a long span
model), thus enabling the use of full blown model
for re-scoring.With this approach, we can achieve
full lattice re-scoring with a complex model, at a
cost more than 20 times less than of a naive brute
force approach that is commonly used today.
The rest of the paper is organized as follows:
We discuss a particular form of long span language
model in Sec. 2. In Sec. 3 we discuss two standard
re-scoring techniques and then describe and demon-
strate our proposed technique in Sec. 4. We present
experimental results in Sec. 5 followed by conclu-
sions and some remarks in Sec. 6.
2 Recurrent Neural Networks (RNN)
There is a long history of using neural networks to
model sequences. Elman (1990) used recurrent neu-
ral network for modeling sentences of words gen-
erated by an artificial grammar. Work on statistical
language modeling of real natural language data, to-
gether with an empirical comparison of performance
to standard techniques was done by Bengio et al.
(2001). His work has been followed by Schwenk
(2007), who has shown that neural network language
models actually work very well in the state-of-the-
art speech recognition systems. Recurrent Neu-
ral Network based Language Models (RNN-LMs)
(Mikolov et al., 2010) improved the ability of the
original model to capture patterns in the language
without using any additional features (such as part
of speech, morphology etc) i.e. other than lexical
ones. The RNN-LM was shown to have superior
performance than the original feedforward neural
network (Mikolov et al., 2011b). Recently, we also
showed that this model outperforms many other ad-
vanced language modeling techniques (Mikolov et
al., 2011a). We hence decided to work with this
model. This model uses whole history to make pre-
dictions, thus it lies outside the family of n-gram
models. Power of the model comes at a considerable
computational cost. Due to the requirement of un-
limited history, many optimization tricks for rescor-
ing with feedforward-based NNLMs as presented by
Schwenk (2007) cannot be applied during rescoring
with RNN LM. Thus, this model is a good candidate
1117
w( t )
s( t )
y( t )
(delayed)
U V
Figure 1: Schematic Representation of Recurrent Neu-
ral Network Language Model. The network has an input
layer w, a hidden layer s and an output layer y. Matrices
U and V represent synapses.
to show effectiveness and importance of our work.
The basic RNNLM is shown in Fig. 1. The model
has an input layer w(t) that encodes previous word
using 1 of N coding (thus, the size of the input layer
is equal to the size of the vocabulary, and only the
neuron that corresponds to the previous word in a
sequence is set to 1). The hidden layer s(t) has addi-
tional recurrent connections that are delayed by one
time step. After the network is trained, the output
layer y(t) represents probability distribution for the
current word, given the previous word and the state
of the hidden layer from the previous time step.
The training is performed by ‘backpropagation-
through-time’ algorithm that is commonly used for
training recurrent neural networks (Rumelhart et al.,
1986). More details about training, setting initial pa-
rameters, choosing size of the hidden layer etc. are
presented in (Mikolov et al., 2010). Additional ex-
tensions that allow this model to be trained on large
corpora are presented in (Mikolov et al., 2011b).
3 Standard Approaches for Rescoring
3.1 Word Lattice Rescoring
A word lattice, L, obtained at the output of the first
pass decoding, encodes exponential number (expo-
nential in the number of states (nodes) present in
the lattice) of hypotheses in a very compact data
structure. It is a directed acyclic graph G =
(V, E , ns, Ne), where V and E denote set of vertices
(nodes / states) and edges (arcs / links), respectively.
ns and Ne denote the unique start state and set of
end states.
A path, pi, in a lattice is an element of E? with
consecutive transitions. We will denote the origin /
previous state of this path by p[pi] and destination /
next state of this path by n[pi]. A path, pi is called
a complete path if p[pi] = ns and n[pi] ? Ne. A
path, pi, is called a partial path if p[pi] = ns but n[pi]
may or may not belong to Ne. A path, pi, is called
a trailing path if p[pi] may or may not be equal to
ns and n[pi] ? Ne. We will also denote the time
stamp at the start of the path by Ts[pi] and the time
stamp at the end of the path by Te[pi]. Since there
are nodes attached to the start and end of any path,
we will denote the time stamp at any node u ? V by
T [u]. Associated with every path, pi, is also a word
sequence W [pi] ? W?, where W is the vocabulary
used during speech recognition. For the sake of sim-
plicity, we will distinguish word sequence of length
1 from the word sequences of length greater than 1
by using lower and upper casing i.e. w[·] and W [·]
respectively.
The acoustic likelihood of the path pi ? E? is then
given as:
A[pi] =
|pi|?
j=1
P (aj |w[pij ])
where ?j ? {1, 2, . . . , |pi|} pij ? E , pi = |pi|j=1pij
and P (aj |w[pij ]) is the acoustic likelihood of the
acoustic substring aj , spanning between Ts[pij ] and
Te[pij ], conditioned on the word w[pij ] associated
with the edge pij .2 Similarly, the language model
score of the path pi is given as:
L[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?m+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?m+1]) is the
m-th order Markov approximation for estimating the
probability of a word given the context upto that
point. The speech recognizer, which uses m-th or-
der Markov LM for first pass recognition, imposes a
constraint on the word lattice such that at each state
there exists an unambiguous context of consecutive
m? 1 words.
A first pass output is then a path pi? having Max-
imum a Posterior (MAP) probability.3 Thus pi? is
2We will use  symbol to denote concatenation of paths or
word strings.
3Note that asterisk symbol here connotes that the path is op-
1118
obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?L[pi],
where ? is the scaling parameter needed to balance
the dynamic variability between the distributions of
acoustic and language model (Ogawa et al., 1998).
Efficient algorithms such as single source shortest
path (Mohri et al., 2000) can be used for finding out
the MAP path.
Under a new n-gram Language Model, rescor-
ing involves replacing the existing language model
scores of all paths pi. If we denote the new language
model by Lnew and correspondingly the score of the
path pi by Lnew[pi], then it is simply obtained as:
Lnew[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?n+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?n+1]) is the n-
th order Markov approximation for estimating the
probability of a word given the unambiguous con-
text of n ? 1 words under the new rescoring LM.
If the Markov rescoring n-gram LM needs a bigger
context for the task of prediction (i.e. n > m, where
m? 1 is the size of the unambiguous context main-
tained at every state of the word lattice), then each
state of the lattice has to be split until an unambigu-
ous context of length as large as that required by the
new re-scoring language model is not maintained.
The best path, pi? is then obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?Lnew[pi],
where ? acts as the new scaling parameter which
may or may not be equal to the old scaling parameter
?.
It should be noted that if the rescoring LM needs a
context of the entire past in order to predict the next
word, then the lattice has to be expanded by splitting
the states many more times. This usually blows up
the search space even for a reasonably small number
timal under some model. This should not be confused with the
Kleene stars appearing as superscripts for E andW , which serve
the purpose of regular expressions implying 0 or many occu-
rances of the element of E and V respectively.
of state splitting iterations. When the task is to do
rescoring under a long span LM, such as RNN-LM,
then exact lattice re-scoring option is not feasible. In
order to tackle this problem, a suboptimal approach
via N best list rescoring is utilized. The details of
this method are presented next.
3.2 N best List Rescoring
N best list re-scoring is a popular way to cap-
ture some long-distance dependencies, though the
method can be slow and it can be biased toward the
weaker language model that was used in the first
pass.
Given a word lattice, L, top N paths
{pi1, . . . , piN} are extracted such that their joint
likelihood under the baseline acoustic and language
models are in descending order i.e. that:
A[pi1]?L[pi1] ? A[pi2]?L[pi2] ? . . . ? A[piN ]?L[piN ]
Efficient algorithms exist for extractingN best paths
from word lattices (Chow and Schwartz, 1989;
Mohri and Riley, 2002). If a new language model,
Lnew, is provided, which now need not be restricted
to finite state machine family, then that can be de-
ployed to get the score of the entire path pi. If we
denote the new LM scores by Lnew[·], then under N
best list paradigm, optimal path p˜i is found out such
that:
p˜i = argmax
pi?{pi1,...,piN}
A[pi]?Lnew[pi], (2)
where ? acts as the new scaling parameter which
may or may not be equal to ?. If N  |L| (where
|L| is the total number of complete paths in word lat-
tice, which are exponentially many), then the path
obtained using (2) is not guaranteed to be optimal
(under the rescoring model). The short list of hy-
potheses so used for re-scoring would yield subop-
timal output if the best path pi? (according to the
new model) is not present among the top N candi-
dates extracted from the lattice. This search space
is thus said to be biased towards a weaker model
mainly because the N best lists are representative of
the model generating them. To illustrate the idea,
we demonstrate below a simple analysis on a rel-
atively easy task of speech transcription on WSJ
data.4 In this setup, the recognizer made use of a bi-
4Full details about the setup can be found in (Deoras et al.,
2010)
1119
gram LM to produce lattices and hence N best lists.
Each hypothesis in this set got a rank with the top
most and highest scoring hypothesis getting a rank
of 1, while the bottom most hypothesis getting a
rank of N . We then re-scored these hypotheses with
a better language model (either with a higher order
Markov LM i.e. a trigram LM (tg) or the log linear
combination of n-gram models and syntactic mod-
els (n-gram+syntactic) and re-ranked the hypothe-
ses to obtain their new ranks. We then used Spear-
man’s rank correlation factor, ?, which takes values
in [?1,+1], with ?1 meaning that the two ranked
lists are negatively correlated (one list is in a reverse
order with respect to the other list) and +1 mean-
ing that the two ranked lists are positively correlated
(the two lists are exactly the same). Spearman’s rank
correlation factor is given as:
? = 1? 6
?N
n=1 d2n
N(N2 ? 1) , (3)
where dn is the difference between the old and new
rank of the nth entry (in our case, difference between
n(? {1, 2, . . . , N}) and the new rank which the nth
hypothesis got under the rescoring model).
Table 1 shows how the correlation factor drops
dramatically when a better and a complementary
LM is used for re-scoring, suggesting that theN best
lists are heavily biased towards the starting models.
Huge re-rankings suggests there is an opportunity to
improve and also a need to explore more hypotheses,
i.e. beyond N best lists.
Model (?) WER (%)
bg 1.00 18.2%
tg 0.41 17.4%
n-gram+syntactic 0.33 15.8%
Table 1: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models including,
trigram LM (tg), and the log linear combination of n-
gram models, and syntactic models (n-gram+syntactic).
With a bigger and a better LM, the WER decreases at
the expense of huge re-rankings of N best lists, only
suggesting the fact that N best lists generated under a
weaker model, are not reflective enough of a relatively
better model.
In the next section, we propose an algorithm
which keeps the representation of search space as
simple as that of N best list, but does not restrict it-
self to topN best paths alone and hence does not get
biased towards the starting weaker model.
4 Proposed Approach for Rescoring
A high level idea of our proposed approach is to
identify islands of confusability in the word lattice
and replace the problem of global search over word
lattice by series of local search problems over these
islands in an iterative manner. The motivation be-
hind this strategy is the observation that the recog-
nizer produces bursts of errors such that they have
a temporal scope. The recognizer output (sentence
hypotheses) when aligned together typically shows
a pattern of confusions both at the word level and
at the phrase level. Regions where there are sin-
gleton words competing with one another (reminis-
cent of a confusion bin of a Confusion Network
(CN) (Mangu, 2000)), choice of 1 word edit dis-
tance works well for the formation of local neigh-
borhood. Regions where there are phrases com-
peting with other phrases, choice of variable length
neighborhood works well. Previously, Richardson
et al. (1995) demonstrated a hill climbing frame-
work by exploring 1 word edit distance neighbor-
hood, while in our own previous work (Deoras and
Jelinek, 2009), we demonstrated working of iterative
decoding algorithm, a hill climbing framework, for
CNs, in which the neighborhood was formed by all
words competing with each other in any given time
slot, as defined by a confusion bin.
In this work, we propose a technique which gen-
eralizes very well on word lattices and overcomes
the limitations posed by a CN or by the limited na-
ture of local neighborhood. The size of the neigh-
borhood in our approach is a variable factor which
depends upon the confusability in any particular re-
gion of the word lattice. Thus the local neighbor-
hood are in some sense a function of the confusabil-
ity present in the lattice rather than some predeter-
mined factor. Below we describe the process, virtue
of which, we can cut the lattice to form many self
contained smaller sized sub lattices. Once these sub
lattices are formed, we follow a similar hill climbing
procedure as proposed in our previous work (Deoras
and Jelinek, 2009).
1120
4.1 Islands of Confusability
We will continue to follow the notation introduced
in section 3.1. Before we define the procedure for
cutting the lattice into many small self contained
lattices, we will define some more terms necessary
for the ease of understandability of the algorithm.5
For any node v ? V , we define forward probability,
?(v), as the probability of any partial path pi ? E?,
s.t. p[pi] = ns, n[pi] = v and it is given as:
?(v) =
?
pi?E?
s.t.p[pi]=ns,n[pi]=v
A[pi]?L[pi] (4)
Similarly, for any node v ? V , we define the
backward probability, ?(v), as the probability of any
trailing path pi ? E?, s.t. p[pi] = v, n[pi] ? Ne and it
is given as:
?(v) =
?
pi?E?
s.t.p[pi]=v,n[pi]?Ne
A[pi]?L[pi] (5)
If we define the sum of joint likelihood under the
baseline acoustic and language models of all paths
in the lattice by Z, then it can simply be obtained as:
Z =
?
u?Ne ?(u) = ?(ns)In order to cut the lattice, we want to identify sets
of nodes, S1, S2, . . . , S|S| such that for any set Si ?
S following conditions are satisfied:
1. For any two nodes u, v ? Si we have that:
T [u] = T [v]. We will define this common time
stamp of the nodes in the set by T [Si].
2. 6 ? pi ? E such that Ts[pi] < T [Si] < Te[pi].
The first property can be easily checked by first
pushing states into a linked list associated with each
time marker (this can be done by iterating over all
the states of the graph) then iterating over the unique
time markers and retrieving back the nodes asso-
ciated with it.The second property can be checked
by first iterating over the unique time markers and
for each of the marker, iterating over the arcs and
terminating the loop as soon as some arc is found
5Goel and Byrne (2000) previously demonstrated the lat-
tice segmentation procedure to solve the intractable problem of
MBR decoding. The cutting procedure in our work is different
from theirs in the sense that we rely on time information for
collating competing phrases, while they do not.
out violating property 2 for the specific time marker.
Thus the time complexity for checking property 1 is
O(|V|) and that for property 2 isO(|T |×|E|), where
|T | is the total number of unique time markers. Usu-
ally |T |  |E| and hence the time complexity for
checking property 2 is almost linear in the number
of edges. Thus effectively, the time complexity for
cutting the lattice is O(|V|+ |E|).
Having formed such sets, we can now cut the
lattice at time stamps associated with these sets
i.e. that: T [S1], . . . , T [S|S|]. It can be easily seen
that the number of sub lattices, C, will be equal
to |S| ? 1.We will identify these sub lattices as
L1,L1, . . . ,LC . At this point, we have not formed
self contained lattices yet by simply cutting the par-
ent lattice at the cut points.
Once we cut the lattice at these cut points, we im-
plicitly introduce many new starting nodes and end-
ing nodes for any sub lattice. We will refer to these
nodes as exposed starting nodes and exposed end-
ing nodes. Thus for some jth sub lattice, Lj , there
will be as many new exposed starting nodes as there
are nodes in the set Sj and as many exposed ending
nodes as there are nodes in the set Sj+1. In order
to make these sub lattices consistent with the defini-
tion of a word lattice (see Sec. 3.1), we unify all the
exposed starting nodes and exposed ending nodes.
To unify the exposed starting nodes, we introduce
as many new edges as there are nodes in the set Sj
such that they have a common starting node, ns[Lj ],
(newly created) and distinct ending nodes present
in Sj . To unify the exposed ending nodes of Lj ,
we introduce as many new edges as there are nodes
in the set Sj+1 such that they have distinct starting
nodes present in Sj+1 and a common ending node
ne[Lj ] (newly created). From the totality of these
new edges and nodes along with the ones already
present in Lj forms an induced directed acyclic sub-
graph G[Lj ] = (V[Lj ], E [Lj ], ns[Lj ], ne[Lj ]).
For any path pi ? E [Lj ] such that p[pi] = ns[Lj ]
and n[pi] ? Sj , we assign the value of ?(n[pi])
to denote the joint likelihood A[pi]?L[pi] and as-
sign epsilon for word associated with these edges
i.e. w[pi]. We assign T [Sj ] ? ?T to denote Ts[pi]
and T [Sj ] to denote Te[pi]. Similarly, for any path
pi ? E [Lj ] such that p[pi] ? Sj+1 and n[pi] = ne[Lj ],
1121
we assign the value of ?(p[pi])6 to denote the joint
likelihood A[pi]?L[pi] and assign epsilon for word
associated with these edges i.e. w[pi]. We assign
T [Sj+1] to denote Ts[pi] and T [Sj+1] + ?T to de-
note Te[pi]. This completes the process and we ob-
tain self contained lattices, which if need be, can be
independently decoded and/or analyzed.
4.2 Iterative Decoding on Word Lattices
Once we have formed the self contained lattices,
L1,L1, . . . ,LC , where C is the total number of sub
lattices formed, then the idea is to divide the re-
scoring problem into many small re-scoring prob-
lems carried over the sub lattices one at a time by
fixing single best paths from all the remaining sub
lattices.
The inputs to the algorithm are the sub lattices
(produced by cutting the parent lattice generated un-
der some Markov n-gram LM) and a new rescor-
ing LM, which now need not be restricted to fi-
nite state machine family. The output of the al-
gorithm is a word string, W?, such that it is the
concatenation of final decoded word strings from
each sub lattice. Thus if we denote the final de-
coded path (under some decoding scheme, which
will become apparent next) in the jth sub lattice
by pi?j and the concatenation symbol by ’·’, then
W? = W [pi?1] ·W [pi?2] · . . . ·W [pi?C ] = Cj=1W [pi?j ].
Algorithm 1 Iterative Decoding on word lattices.
Require: {L1,L1, . . . ,LC}, Lnew
PrevHyp? null
CurrentHyp?Cj=1W [pˆij ]
while PrevHyp 6= CurrentHyp do
for i? 1 . . . C do
pˆii ? argmax
pii?E?i :
p[pii]=ns[Li]
n[pii]=ne[Li]
(
Lnew[ 1ˆ · . . . ·pii · . . . · kˆ]
×A[pii]?
?k
j=1
j 6=i
A[ jˆ ]?
)
end for
PrevHyp? CurrentHyp
CurrentHyp?Cj=1W [pˆij ]
end while
?j ? {1, 2, . . . , C} pi?j ? pˆij
6The values of ?(·) and ?(·) are computed under parent lat-
tice structure.
The algorithm is initialized by setting PrevHypo
to null and CurrHypo to the concatenation of 1-best
output from each sub lattice. During the initializa-
tion step, each sub lattice is analyzed independent of
any other sub lattice and under the baseline acoustic
scores and baseline n-gram LM scores, 1-best path
is found out. Thus if we define the best path under
baseline model in some jth sub-lattice by pˆij , Cur-
rHypo is then initialized to: W [pˆi1] · W [pˆi2] · . . . ·
W [pˆiC ]. The algorithm then runs as long as Cur-
rHypo is not equal to PrevHypo. In each iteration,
the algorithm sequentially re-scores each sub-lattice
by keeping the surrounding context fixed. Once all
the sub lattices are re-scored, that constitutes one it-
eration. At the end of each iteration, CurrHypo is
set to the concatenation of 1 best paths from each
sub lattice while PrevHypo is set to the old value
of CurrHypo. Thus if we are analyzing some ith
sub-lattice in some iteration, then 1-best paths from
all but this sub-lattice is kept fixed and a new 1-best
path under the re-scoring LM is found out. It is not
hard to see that the likelihood of the output under
the new re-scoring model is guaranteed to increase
monotonically after every decoding step.
Since the cutting of parent lattices produce many
small lattices with considerably lesser number of
nodes, in practice, an exhaustive search for the 1-
best hypothesis can be carried out via N best list.
Algorithm 1 outlines the steps for iterative decoding
on word lattices.
4.3 Entropy Pruning
In this section, we will discuss a speed up technique
based on entropy of the lattice. Entropy of a lattice
reflects the confidence of the recognizer in recogniz-
ing the acoustics. Based on the observation that if
the N best list / lattice generated under some model
has a very low entropy, then the Spearman’s rank
correlation factor, ? (Eqn. 3), tends to be higher
even when the N best lists / lattice is re-ranked with
a bigger and a better model. A low entropy under
the baseline model only reflects the confidence of
the recognizer in recognizing the acoustic. Table 2
shows the rank correlation values between two sets
of N best lists. Both sets are produced by a bi-
gram LM (bg). The entropy of N best lists in the
first set is 0.05 nats or less. The N best lists in the
second set have an entropy greater than 0.05 nats.
1122
Both these sets are re-ranked with bigger and bet-
ter models (see Table 1 for model definitions). We
can see from Table 2 that the rank correlation values
tend to be higher (indicating little re-rankings) when
the entropy of the N best list, under the baseline
model, is lower. Similarly, the rank-correlation val-
ues tend to be lower (indicating more re-rankings)
whenever the entropy of the N best list is higher.
Note that these entropy values are computed with re-
spect to the starting model (in this case, bigram LM).
Of course, if the starting LM is much weaker than
the rescoring model, then the entropy values need
not be reflective of the difficulty of the overall task.
This observation then suggests that it is safe to re-
score only those N best lists whose entropy under
the starting model is higher than some threshold.
Rescoring Model ?(H?0.05) ?(H>0.05)
bg 1.00 1.00
tg 0.58 0.38
n-gram+syntactic 0.54 0.31
Table 2: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models (see Table 1
for model definitions). Entropy under the baseline model
correlates well with the rank correlation factor, suggest-
ing that exhaustive search need not be necessary for ut-
terances yielding lower entropy.
While computation of entropy for N best list is
tractable, for a word lattice, the computation of en-
tropy is intractable if one were to enumerate all the
hypotheses. Even if we were able to enumerate all
hypotheses, this method tends to be slower. Using
efficient semiring techniques introduced by Li and
Eisner (2009) or using posterior probabilities on the
edges leading to end states, we can compute the en-
tropy of a lattice in one single forward pass using
dynamic programming. It should, however, be noted
that, for dynamic programming technique to work,
only n-gram LMs can be used. One has to resort to
approximate entropy computation via N best list, if
entropy under long span LM is desired.
4.3.1 Speed Up for Iterative Decoding
Our speed up technique is simple. Once we have
formed self contained sub lattices, we want to prune
all but the top few best complete paths (obtained un-
der baseline / starting model) of those sub lattices
whose entropy is below some threshold. Thus, be-
lieving in the original model’s confidence, we want
to focus only on those sub lattices which the recog-
nizer found difficult to decode in the first pass. All
other part of the parent lattice will be not be ana-
lyzed. The thresholds for pruning is very application
and corpus specific and needs to be tuned on some
held out data.
5 Experiments and Results
We performed recognition on the Broadcast News
(BN) dev04f, rt03 and rt04 task using the state-
of-the-art acoustic models trained on the English
Broadcast News (BN) corpus (430 hours of audio)
provided to us by IBM (Chen et al., 2009). IBM also
provided us its state-of-the-art speech recognizer,
Attila (Soltau et al., 2010) and two Kneser-Ney
smoothed backoff n-gram LMs containing 4.7M n-
grams (n ? 4) and 54M n-grams (n ? 4), both
trained on 400M word tokens. We will refer to them
as KN:BN-Small and KN:BN-Big respectively. We
refer readers to (Chen et al., 2009) for more details
about the recognizer and corpora used for training
the models.
We trained two RNN based language models -
the first one, denoted further as RNN-limited, was
trained on a subset of the training data (58M tokens).
It used 400 neurons in the hidden layer. The second
model, denoted as RNN-all, was trained on all of
the training data (400M tokens), but due to the com-
putational complexity issues, we had to restrict its
hidden layer size to 320 neurons.
We followed IBM’s multi-pass decoding recipe
using KN:BN-Small in the first pass followed by ei-
ther N best list re-scoring or word lattice re-scoring
using bigger and better models.7 For the purpose
of re-scoring, we combined all the relevant statisti-
cal models in one unified log linear framework rem-
iniscent of work by Beyerlein (1998). We, however,
trained the model weights by optimizing expected
WER rather than 1-best loss as described in (De-
oras et al., 2010). Training was done on N best
lists of size 2K. We will refer to the log linear com-
7The choice of the order and size of LM to be used in the
first pass decoding was determined by taking into consideration
the capabilities of the decoder.
1123
100 101 102 103 104 105 10610.8
11
11.2
11.4
11.6
11.8
12
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestIter. Dec. (ID)ID with Ent. PruningViterbi BaselineViterbi Rescoring
Figure 2: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Small is 12% which then drops
to 11% when KN:BN-Big is used for re-scoring. N best
list search method obtains the same reduction in WER
by evaluating as many as 228K sentence hypotheses on
an average. The proposed method obtains the same re-
duction by evaluating 14 times smaller search space. The
search effort reduces further to 40 times if entropy based
pruning is employed during re-scoring.
bination of KN:BN-Big and RNN-limited by KN-
RNN-lim; KN:BN-Big and RNN-all by KN-RNN-
all and KN:BN-Big, RNN-limited and RNN-all by
KN-RNN-lim-all.
We used two sets for decoding: rt03+dev04f set
was used as a development set while rt04 was used
as a blind set for the purpose of evaluating the per-
formance of long span RNN models using the pro-
posed approach. We made use of OpenFst C++ li-
braries (Allauzen et al., 2007) for manipulating lat-
tice graphs and generating N best lists. Due to the
presence of hesitation tokens in reference transcripts
and the need to access the silence/pause tokens for
penalizing short sentences, we treated these tokens
as regular words before extracting sentence hypothe-
ses. This, and poorly segmented nature of the test
corpora, led to huge enumeration of sentence hy-
potheses.
5.1 n-gram LM for re-scoring
In this setup, we used KN:BN-Small as the base-
line starting LM which yielded the WER of 12%
on rt03+dev04f set. Using KN:BN-Big as the re-
scoring LM, the WER dropped to 11%. Since the
re-scoring LM belonged to the n-gram family, it was
possible to compute the optimal word string by re-
scoring the whole lattice (see Sec. 3.1). We now
compare the performance of N best list approach
(Sec. 3.2) with our proposed approach (Sec. 4).
N best list achieved the best possible reduction by
evaluating as many as 228K sentence hypotheses
on an average. As against that, our proposed ap-
proach achieved the same performance by evaluat-
ing 16.6K sentence hypotheses, thus reducing the
search efforts by 13.75 times. By carrying out en-
tropy pruning (see Sec. 4.3 ) on sub lattices, our pro-
posed approach required as little as 5.6K sentence
hypotheses evaluations to obtain the same optimal
performance, reducing the search effort by as much
as 40.46 times. For the purpose of this experiment,
entropy based pruning was carried out when the en-
tropy of the sub lattice was below 5 nats. Table 3
compares the two search methods for this setup and
Fig. 2 shows a plot of WER versus the size of the
search space (in terms of number of sentence hy-
potheses evaluated by an n-gram language model).
On rt04, the KN:BN-Small LM gave a WER of
14.1% which then dropped to 13.1% after re-scoring
with KN:BN-Big. Since the re-scoring model was
an n-gram LM, it was possible to obtain the opti-
mal performance via lattice update technique (see
Sec. 3.1). We then carried out the re-scoring of the
word lattices under KN:BN-Big using our proposed
technique and found it to give the same performance
yielding the WER of 13.1%.
5.2 Long Span LM for re-scoring
In this setup, we used the strongest n-gram LM
as our baseline. We thus used KN:BN-Big as the
baseline LM which yielded the WER of 11% on
rt03+dev04f. We then used KN-RNN-lim-all for re-
scoring. Due to long span nature of the re-scoring
LM, it was not possible to obtain the optimal WER
performance. Hence we have compared the perfor-
mance of our proposed method with N best list ap-
proach. N best list achieved the lowest possible
WER after evaluating as many as 33.8K sentence
hypotheses on an average. As against that, our pro-
posed approach in conjunction with entropy pruning
obtained the same performance by evaluating just
1.6K sentence hypotheses, thus reducing the search
by a factor of 21. Fig 3 shows a plot of WER versus
1124
100 101 102 103 10410.3
10.4
10.5
10.6
10.7
10.8
10.9
11
11.1
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestID with Ent. PruningViterbi Baseline
Figure 3: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Big is 11% which then drops to
10.4% when KN-RNN-lim-all is used for re-scoring. N
best list search method obtains this reduction in WER by
evaluating as many as 33.8K sentence hypotheses on an
average, while the proposed method (with entropy prun-
ing) obtains the same reduction by evaluating 21 times
smaller search space.
the size of the search space (in terms of number of
sentence hypotheses evaluated by a long span lan-
guage model).
In-spite of starting off with a very strong n-gram
LM, theN best lists so extracted were still not repre-
sentative enough of the long span rescoring models.
Had we started off with KN:BN-Small, the N best
list re-scoring method would have had no chance of
finding the optimal hypothesis in reasonable size of
hypotheses search space. Table 4 compares the two
search methods for this setup when many other long
span LMs were also used for re-scoring.
On rt04, the KN:BN-Big LM gave a WER of
13.1% which then dropped to 12.15% after re-
scoring with KN-RNN-lim-all using our proposed
technique.8 Since the re-scoring model was not an
n-gram LM, it was not possible to obtain the optimal
performance but we could enumerate huge N best
list to approximate this value. Our proposed method
is much faster than huge N best lists and no worse
in terms of WER. As far as we know, the result ob-
tained on these sets is the best performance ever
reported on the Broadcast News corpus for speech
8The WER obtained using KN-RNN-lim and KN-RNN-all
were 12.5% and 12.3% respectively.
recognition.
Models WER NBest ID Saving
KN:BN-Small 12.0 - - -
KN:BN-Big 11.0 228K 5.6K 40
Table 3: The starting LM is a weak n-gram LM (KN:BN-
Small) and the re-scoring LM is a much stronger but n-
gram LM (KN:BN-Big). The baseline WER in this case
is 12% and the optimal performance by the re-scoring LM
is 11.0%. The proposed method outperforms N best list
approach, in terms of search efforts, obtaining optimal
WER.
Models WER NBest ID Saving
KN:BN-Big 11.0 - - -
KN-RNN-lim 10.5 42K 1.1K 38
KN-RNN-all 10.5 26K 1.3K 20
KN-RNN-lim-all 10.4 34K 1.6K 21
Table 4: The starting LM is a strong n-gram LM
(KN:BN-Big) and the re-scoring model is a long span
LM (KN-RNN-*). The baseline WER is 11.0%. Due
to long span nature of the LM, optimal WER could not
be estimated. The proposed method outperfoms N best
list approach on every re-scoring task.
6 Conclusion
We proposed and demonstrated a new re-scoring
technique for general word graph structures such as
word lattices. We showed its efficacy by demonstrat-
ing huge reductions in the search effort to obtain a
new state-of-the-art performance on a very compet-
itive speech task of Broadcast news. As part of the
future work, we plan to extend this technique for hy-
pergraphs and lattices in re-scoring MT outputs with
complex and long span language models.
Acknowledgement
This work was partly funded by Human Language
Technology, Center of Excellence and by Tech-
nology Agency of the Czech Republic grant No.
TA01011328, and Grant Agency of Czech Repub-
lic project No. 102/08/0707. We would also like to
acknowledge the contribution of Frederick Jelinek
towards this work. He would be a co-author if he
were available and willing to give his consent.
1125
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A General and Efficient Weighted Finite-State Trans-
ducer Library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
J. R. Bellegarda. 2000. Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings
of IEEE, 88(8):1279–1296.
Yoshua Bengio, Re´jean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Peter Beyerlein. 1998. Discriminative Model Combina-
tion. In Proc. of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured Language Modeling. Computer Speech and Lan-
guage, 14(4):283–332.
S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya,
and A. Sethy. 2009. Scaling shrinkage-based lan-
guage models. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 299–304.
Noam Chomsky. 1957. Syntactic Structures. The
Hague: Mouton.
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best
algorithm: an efficient procedure for finding top N sen-
tence hypotheses. In Proceedings of the workshop on
Speech and Natural Language, HLT ’89, pages 199–
202, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kenneth Church. 2012. A Pendulum Swung Too Far.
Linguistic Issues in Language Technology - LiLT. to
appear.
T.M. Cover and J.A.Thomas. 1991. Elements of Infor-
mation Theory. John Wiley and Sons, Inc. N.Y.
Anoop Deoras and Frederick Jelinek. 2009. Iterative De-
coding: A Novel Re-Scoring Framework for Confu-
sion Networks. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 282 –286.
Anoop Deoras, Denis Filimonov, Mary Harper, and Fred
Jelinek. 2010. Model Combination for Speech Recog-
nition using Empirical Bayes Risk Minimization. In
Proc. of IEEE Workshop on Spoken Language Tech-
nology (SLT).
Anoop Deoras, Toma´s? Mikolov, Stefan Kombrink, Mar-
tin Karafia´t, and Sanjeev Khudanpur. 2011. Varia-
tional Approximation of Long-Span Language Mod-
els for LVCSR. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Jeffery Elman. 1990. Finding Structure in Time. In Cog-
nitive Science, volume 14, pages 179–211.
Denis Filimonov and Mary Harper. 2009. A Joint Lan-
guage Model with Fine-grain Syntactic Tags. In Proc.
of 2009 Conference on Empirical Methods in Natural
Language Processing.
V. Goel and W. Byrne. 2000. Minimum Bayes Risk Au-
tomatic Speech Recognition. Computer, Speech and
Language.
Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long
Distance Dependence in Language: Topic Mixtures
Versus Dynamic Cache Models. IEEE Transactions
on Speech and Audio Processing, 7(1):30–39.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40–51, Singapore,
August.
Lidia Luminita Mangu. 2000. Finding consensus in
speech recognition. Ph.D. thesis, The Johns Hopkins
University. Adviser-Brill, Eric.
Toma´s? Mikolov, Martin Karafia´t, Luka´s? Burget,
Jan “Honza” C?ernocky´, and Sanjeev Khudanpur.
2010. Recurrent Neural Network Based Language
Model. In Proc. of the ICSLP-Interspeech.
Toma´s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka´s?
Burget, and Jan “Honza” C?ernocky´. 2011a. Empirical
Evaluation and Combination of Advanced Language
Modeling Techniques. In Proc. of Interspeech.
Toma´s? Mikolov, Stefan Kombrink, Luka´s? Burget,
Jan “Honza” C?ernocky´, and Sanjeev Khudanpur.
2011b. Extensions of Recurrent Neural Network Lan-
guage Model. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Mehryar Mohri and Michael Riley. 2002. An Efficient
Algorithm for the N-Best-Strings Problem. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The de-
sign principles of a weighted finite-state transducer li-
brary. Theoretical Computer Science, 231:17-32.
A. Ogawa, K. Takeda, and F. Itakura. 1998. Balanc-
ing Acoustic and Linguistic Probabilities. In Proc. of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
1126
F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995.
Lattice-based search strategies for large vocabulary
speech recognition. In Proc. of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-Sentence Exponential Language Models: a Ve-
hicle for Linguistic-Statistical Integration. Computer
Speech and Language, 15(1).
Roni Rosenfeld. 1997. A Whole Sentence Maximum
Entropy Language Model. In Proc. of IEEE workshop
on Automatic Speech Recognition and Understanding
(ASRU), Santa Barbara, California, December.
D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating errors.
Nature, 323:533–536.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492–
518.
C. E. Shannon. 1948. A Mathematical Theory of
Communication. The Bell System Technical Journal,
27:379–423, 623–656.
H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. of IEEE
Workshop on Spoken Language Technology (SLT).
Wen Wang and Mary Harper. 2002. The SuperARV lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
1127
