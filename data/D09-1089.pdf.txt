Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 852–861,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
Enhancement of Lexical Concepts Using Cross-lingual Web Mining
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental in linguistics and NLP. Manual
concept compilation is labor intensive, er-
ror prone and subjective. We present a
web-based concept extension algorithm.
Given a set of terms specifying a concept
in some language, we translate them to
a wide range of intermediate languages,
disambiguate the translations using web
counts, and discover additional concept
terms using symmetric patterns. We then
translate the discovered terms back into
the original language, score them, and ex-
tend the original concept by adding back-
translations having high scores. We eval-
uate our method in 3 source languages and
45 intermediate languages, using both hu-
man judgments and WordNet. In all cases,
our cross-lingual algorithm significantly
improves high quality concept extension.
1 Introduction
A concept (or lexical category) is a set of lex-
ical items sharing a significant aspect of their
meanings (e.g., types of food, tool names, etc).
Concepts are fundamental in linguistics and NLP,
in thesauri, dictionaries, and various applications
such as textual entailment and question answering.
Great efforts have been invested in manual
preparation of concept resources such as WordNet
(WN). However, manual preparation is labor in-
tensive, which means it is both costly and slow
to update. Applications needing data on some
very specific domain or on a recent news-related
event may find such resources lacking. In addition,
manual preparation is error-prone and susceptible
to subjective concept membership decisions, fre-
quently resulting in concepts whose terms do not
belong to the same level of granularity
1
. As a re-
sult, there is a need to find methods for automatic
improvement of concept coverage and quality.
The web is a huge up-to-date corpus covering
many domains, so using it for concept extension
has the potential to address the above problems.
The majority of web pages are written in a few
salient languages, hence most of the web-based in-
formation retrieval studies are done on these lan-
guages. However, due to the substantial growth of
the multilingual web
2
, languages in which concept
terms are expressed in the most precise manner
frequently do not match the language where in-
formation is needed. Moreover, representations of
the same concept in different languages may com-
plement each other.
In order to benefit from such cross-lingual in-
formation, concept acquisition systems should be
able to gather concept terms from many available
languages and convert them to the desired lan-
guage. In this paper we present such an algorithm.
Given a set of words specifying a concept in some
source language, we translate them to a range
of intermediate languages and disambiguate the
translations using web counts. Then we discover
additional concept terms using symmetric patterns
and translate the discovered terms back into the
original language. Finally we score the back-
translations using their intermediate languages’
properties, and extend the original concept by
adding back-translations having high scores. The
only language-specific resource required by the al-
gorithm are multilingual dictionaries, and its pro-
cessing times are very modest.
We performed thorough evaluation for 24 con-
cepts in 3 source languages (Hebrew, English and
Russian) and 45 intermediate languages. Concept
definitions were taken from existing WordNet sub-
trees, and the obtained new terms were manually
1
See Section 5.1.1.
2
http://www.internetworldstats.com/stats7.htm
852
scored by human judges. In all cases we have sig-
nificantly extended the original concept set with
high precision. We have also performed a fully
automatic evaluation with 150 concepts, showing
that the algorithm can re-discover WN concepts
with high precision and recall when given only
partial lists as input.
Section 2 discusses related work, Section 3 de-
tails the algorithm, Section 4 describes the evalua-
tion protocol and Section 5 presents our results.
2 Related work
One of the main goals of this paper is the extension
or automated creation of lexical databases such
as WN. Due to the importance of WN for NLP
tasks, substantial research was done on direct or
indirect automated extension of the English WN
(e.g., (Snow et al., 2006)) or WN in other lan-
guages (e.g., (Vintar and Fi?ser, 2008)). The major-
ity of this research was done on extending the tree
structure (finding new synsets (Snow et al., 2006)
or enriching WN with new relationships (Cuadros
and Rigau, 2008)) rather than improving the qual-
ity of existing concept/synset nodes. Other re-
lated studies develop concept acquisition frame-
works for on-demand tasks where concepts are de-
fined by user-provided seeds or patterns (Etzioni et
al., 2005; Davidov et al., 2007), or for fully unsu-
pervised database creation where concepts are dis-
covered from scratch (Banko et al., 2007; Davi-
dov and Rappoport, 2006).
Some papers directly target specific applica-
tions, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an in-
stance of the concept acquisition problem where
the desired concepts contain words that are names
of entities of a particular kind, as done in (Fre-
itag, 2004) using co-clustering and in (Etzioni et
al., 2005) using predefined pattern types.
The two main algorithmic approaches to the
problem are pattern-based concept discovery and
clustering of context feature vectors. The latter
approach represents word contexts as vectors in
some space and uses similarity measures and au-
tomatic clustering in that space (Deerwester et al.,
1990). Pereira et al.(1993), Curran and Moens
(2002) and Lin (1998) use syntactic features in the
vector definition. Pantel and Lin (2002) improves
on the latter by clustering by committee. Cara-
ballo (1999) uses conjunction and appositive an-
notations in the vector representation. While great
effort has been made for improving the computa-
tional complexity of these methods (Gorman and
Curran, 2006), they still remain data and compu-
tation intensive.
The second major algorithmic approach is to
use lexico-syntactic patterns. Patterns have been
shown to produce more accurate results than fea-
ture vectors, at a lower computational cost on large
corpora (Pantel et al., 2004). In concept acquisi-
tion, pattern-based methods were shown to out-
perform LSA by a large margin (Widdows and
Dorow, 2002). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns in
order to acquire relationships, numerous pattern-
based methods have been proposed for the discov-
ery of concepts from seeds (Pantel et al., 2004;
Davidov et al., 2007; Pasca et al., 2006). Most of
these studies were done for English, while some
show the applicability of their methods to other
languages, including Greek, Czech, Slovene and
French.
Most of these papers attempt to discover con-
cepts from data available in some specific lan-
guage. Recently several studies have proposed to
utilize a second language or several specified lan-
guages in order to extract or extend concepts (Vin-
tar and Fi?ser, 2008; van der Plas and Tiedemann,
2006) or paraphrases (Bosma and Callison-Burch,
2007). However, these methods usually require
the availability of parallel corpora, which limits
their usefulness. Most of these methods utilize
distributional measures, hence they do not possess
the advantages of the pattern-based framework.
Unlike in the majority of recent studies, where
the framework is designed with specific languages
in mind, in our task, in order to take advantage
of information from diverse languages, the algo-
rithm should be able to deal well with a wide va-
riety of possible intermediate languages without
any manual adaptations. Relying solely on mul-
tilingual dictionaries and the web, our algorithm
should be able to discover language-specific pat-
terns and concept terms. While some of the pro-
posed frameworks could potentially be language-
independent, little research has been done to con-
firm this. There are a few obstacles that may
hinder applying common pattern-based methods
to other languages. Many studies utilize parsing
or POS tagging, which frequently depend on the
availability and quality of language-specific tools.
Some studies specify seed patterns in advance, and
853
it is not clear whether translated patterns can work
well on different languages. Also, the absence of
clear word segmentation in some languages (e.g.,
Chinese) can make many methods inapplicable.
A few recently proposed concept acquisition
methods require only a handful of seed words and
no pattern pre-specification (Davidov et al., 2007;
Pasca and Van Durme, 2008). While these studies
avoid some of the obstacles above, it still remains
open whether such methods are indeed language-
independent. In the translation to intermediate lan-
guages part of our framework, we adapt the algo-
rithms in (Davidov and Rappoport, 2006; Davi-
dov et al., 2007) to suit diverse languages (includ-
ing ones without explicit word segmentation). We
also develop a method for efficient automated dis-
ambiguation and translation of terms to and from
any available intermediate language.
Our study is related to cross-language infor-
mation retrieval (CLIR/CLEF) frameworks. Both
deal with information extracted from a set of lan-
guages. However, the majority of CLIR stud-
ies pursue different targets. One of the main
CLIR goals is the retrieval of documents based
on explicit queries, when the document lan-
guage is not the query language (Volk and Buite-
laar, 2002). These frameworks usually develop
language-specific tools and algorithms including
parsers and taggers in order to integrate multilin-
gual queries and documents (Jagarlamudi and Ku-
maran, 2007). Our goal is to develop a language-
independent method using cross-lingual informa-
tion, for the extension and improvement of con-
cepts rather than the retrieval of documents. Be-
sides, unlike in many CLIR frameworks, interme-
diate languages are not specified in advance and
the language of requested data is the same as the
language of request, while available information
may be found in many different intermediate lan-
guages.
3 The Algorithm
Our algorithm is comprised of the following
stages: (1) given a set of words in a source lan-
guage as a specification for some concept, we au-
tomatically translate them to a diverse set of inter-
mediate languages, using multilingual dictionar-
ies; (2) the translations are disambiguated using
web counts; (3) for each language, we retrieve a
set of web snippets where these translations co-
appear and apply a pattern-based concept exten-
sion algorithm for discovering additional terms;
(4) we translate the discovered terms back to the
source language, and disambiguate them; (5) we
score the back-translated terms using data on their
behavior in the intermediate languages, and merge
the sets obtained from different languages into a
single one, retaining terms whose score passes a
certain threshold. Stages 1-3 of the algorithm have
been described in (Davidov and Rappoport, 2009),
where the goal was to translate a concept given in
one language to other languages. The framework
presented here includes the new stages 4-5, and its
goal and evaluation methods are completely dif-
ferent.
3.1 Concept specification and translation
We start from a set of words denoting a concept in
a given source language. Thus we may use words
like (apple, banana, ...) as the definition of the
concept of fruit or (bear, wolf, fox, ...) as the def-
inition of wild animals. In order to reduce noise,
we limit the length (in words) of multiword ex-
pressions considered as terms. To calculate this
limit for a language, we randomly take 100 terms
from the appropriate dictionary and set a limit
as Lim
mwe
= round(avg(length(w))) where
length(w) is the number of words in term w. For
languages like Chinese without inherent word seg-
mentation, length(w) is the number of characters
in w. While for many languages Lim
mwe
= 1,
some languages like Vietnamese usually require
two or more words to express terms.
3.2 Disambiguation of translated terms
One of the problems in utilization of multilingual
information is ambiguity of translation. First, in
order to apply the concept acquisition algorithm,
at least some of the given concept terms must be
automatically translated to each intermediate lan-
guage. In order to avoid reliance on parallel cor-
pora, which do not exist or are extremely small for
most of our language pairs, we use bilingual dic-
tionaries. Such dictionaries usually provide many
translations, one or more for each sense, so this
translation is inherently fuzzy. Second, once we
acquire translated term lists for each intermedi-
ate language, we need to translate them back to
the source language and such back-translations are
also fuzzy. In both cases, we need to select the ap-
propriate translation for each term.
While our desire would be to work with as many
languages as possible, in practice, some or even
854
most of the concept terms may be absent from the
appropriate dictionary. Such concept terms are ig-
nored.
One way to deal with ambiguity is by applying
distributional methods, usually requiring a large
single-language corpus or, more frequently, paral-
lel corpora. However, such corpora are not readily
available for many languages and domains. Ex-
tracting such statistical information on-demand is
also computationally demanding, limiting its us-
ability. Hence, we take a simple but effective
query-based approach. This approach, while be-
ing powerful as we show in the evaluation, only
relies on a few web queries and does not rely on
any language-specific resources or data.
We use the conjecture that terms of the same
concept tend to co-appear more frequently than
ones belonging to different concepts
3
. Thus, we
select a translation of a term co-appearing most
frequently with some translation of a different
term of the same concept. We estimate how well
translations of different terms are connected to
each other. Let C = {C
i
} be the given seed
words for some concept. Let Tr(C
i
, n) be the
n-th available translation of word C
i
and Cnt(s)
denote the web count of string s obtained by a
search engine. We select a translation Tr(C
i
)
according to:
F (w
1
, w
2
) =
Cnt(“w
1
? w
2
”)× Cnt(“w
2
? w
1
”)
Cnt(w
1
)× Cnt(w
2
)
Tr(C
i
) =
argmax
s
i
(
max
s
j
j 6=i
(F (Tr(C
i
, s
i
), T r(C
j
, s
j
)))
)
We utilize the Y ahoo! “x * y”,“x * * y” wild-
cards that allow to count only co-appearances
where x and y are separated by a single word or
word pair. As a result, we obtain a set of disam-
biguated term translations. This method is used
both in order to translate from the source lan-
guage to each intermediate language and to back-
translate the newly discovered concept terms from
the intermediate to the source language.
The number of queries in this stage depends on
the ambiguity of the concept terms’ translations.
In order to decrease the amount of queries, if there
are more than three possible senses we sort them
by frequency
4
and take three senses with medium
frequency. This allows us to skip the most ambigu-
ous and rare senses without any significant effect
on performance. Also, if the number of combina-
3
Our results here support this conjecture.
4
Frequency is estimated by web count for a given word.
tions is still too high (>30), we randomly sample
at most 30 of the possible combinations.
3.3 Pattern-based extension of concept terms
in intermediate languages
We first mine the web for contexts containing
the translations. Then we extract from the re-
trieved snippets contexts where translated terms
co-appear, and detect patterns where they co-
appear symmetrically. Then we use the detected
patterns to discover additional concept terms. In
order to define word boundaries, for each language
we manually specify boundary characters such as
punctuation/space symbols. This data, along with
dictionaries, is the only language-specific data in
our framework.
Web mining for translation contexts. In order
to get language-specific data, we need to restrict
web mining each time to the processed interme-
diate language. This restriction is straightforward
if the alphabet or term translations are language-
specific or if the search API supports restriction to
this language
5
. In case where there are no such
natural restrictions, we attempt to detect and add
to our queries a few language-specific frequent
words. Using our dictionaries, we find 1–3 of the
15 most frequent words in a desired language that
are unique to that language, and we ‘and’ them
with the queries to ensure proper language selec-
tion. This works well for almost all languages (Es-
peranto being a notable exception).
For each pair A,B of disambiguated term trans-
lations, we construct and execute the following
two queries: {“A * B”, “B * A”}
6
. When we
have 3 or more terms we also add {A B C D}-like
conjunction queries which include 3-5 words. For
languages with Lim
mwe
> 1, we also construct
queries with several “*” wildcards between terms.
For each query we collect snippets containing text
fragments of web pages. Such snippets frequently
include the search terms. Since Y ahoo! Boss al-
lows retrieval of up to the 1000 first results (50 in
each query), we collect several thousands snippets.
For most of the intermediate languages, only a few
dozen queries (40 on the average) are required to
obtain sufficient data, and queries can be paral-
lelized. Thus the relevant data can be downloaded
5
Yahoo! allows restriction for 42 languages.
6
These are Yahoo! queries where enclosing words in “”
means searching for an exact phrase and “*” means a wild-
card for exactly one arbitrary word.
855
in seconds. This makes our approach practical for
on-demand retrieval or concept verification tasks.
Meta-patterns. Following (Davidov et al.,
2007), we seek symmetric patterns to retrieve
concept terms. We use two meta-pattern types.
First, a Two-Slot pattern type constructed as
follows:
[Prefix] C
1
[Infix] C
2
[Postfix]
C
i
are slots for concept terms. We allow up to
Lim
mwe
space-separated
7
words to be in a sin-
gle slot. Infix may contain punctuation, spaces,
and up to Lim
mwe
× 4 words. Prefix and Post-
fix are limited to contain punctuation characters
and/or Lim
mwe
words.
Terms of the same concept frequently co-appear
in lists. To utilize this, we introduce two additional
List pattern types
8
:
[Prefix] C
1
[Infix] (C
i
[Infix])+ (1)
[Infix] (C
i
[Infix])+ C
n
[Postfix] (2)
Following (Widdows and Dorow, 2002), we define
a pattern graph. Nodes correspond to terms and
patterns to edges. If term pair (w
1
, w
2
) appears
in pattern P , we add nodes N
w
1
, N
w
2
to the graph
and a directed edge E
P
(N
w
1
, N
w
2
) between them.
Symmetric patterns. We consider only sym-
metric patterns. We define a symmetric pat-
tern as a pattern where some concept terms
C
i
, C
j
appear both in left-to-right and right-to-
left order. For example, if we consider the
terms {apple, pineapple} we select a List pattern
“(one C
i
, )+ and C
n
.” if we find both “one apple,
one pineapple, one guava and orange.” and “one
watermelon, one pineapple and apple.”. If no such
patterns are found, we turn to a weaker definition,
considering as symmetric those patterns where the
same terms appear in the corpus in at least two dif-
ferent slots. Thus, we select a pattern “for C
1
and
C
2
” if we see both “for apple and guava,” and “for
orange and apple,”.
Retrieving concept terms. We collect terms in
two stages. First, we obtain “high-quality” core
terms and then we retrieve potentially more noisy
ones. At the first stage we collect all terms
9
that
7
As before, for languages without space-based word sep-
aration Lim
mwe
limits the number of characters instead.
8
(E)+ means one or more instances of E.
9
We do not consider as terms the 50 most frequent words.
are bidirectionally connected to at least two differ-
ent original translations, and call them core con-
cept terms C
core
. We also add the original ones as
core terms. Then we detect the rest of the terms
C
rest
that are connected to the core stronger than
to the remaining words, as follows:
G
in
(c)={w?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
G
out
(c)={w/?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
C
rest
={c||G
in
(c)|>|G
out
(c)|}
For the sake of simplicity, we do not attempt to
discover more patterns/instances iteratively by re-
querying the web. If we have enough data, we use
windowing to improve result quality. If we obtain
more than 400 snippets for some concept, we di-
vide the data into equal parts, each containing up
to 400 snippets. We apply our algorithm indepen-
dently to each part and select only the words that
appear in more than one part.
3.4 Back-translation and disambiguation
At the concept acquisition phase of our framework
we obtained sets of terms for each intermediate
language, each set representing a concept. In or-
der to be useful for the enhancement of the origi-
nal concept, these terms are now back-translated to
the source language. We disambiguate each back-
translated term using the process described in Sec-
tion 3.2. Having sets of back-translated terms for
each intermediate language, our goal is to combine
these into a single set.
3.5 Scoring and merging the back
translations
We do this merging using the following scoring
strategy, assigning for each proposed term t
?
in
concept C the score S(t
?
, C), and selecting terms
with S(t
?
, C) > H where H is a predefined
threshold.
Our scoring is based on the two following con-
siderations. First, we assume that terms extracted
from more languages tend to be less noisy and
language-dependent. Second, we would like to fa-
vor languages with less resources for a given con-
cept, since noise empirically appears to be less
prominent in such languages
10
.
For language L and concept C = {t
1
. . . t
k
}
we get a disambiguated set of translations
{Tr(t
1
, L) . . . T r(t
k
, L)}. We define relative lan-
10
Preliminary experimentation, as well as the evaluation
results presented in this paper, support both of these consid-
erations.
856
guage frequency by
LFreq(L,C) =
?
t
i
?C
(Freq(Tr(t
i
, L)))
?
L
?
,t
i
?C
(Freq(Tr(t
i
, L
?
))
where Freq(Tr(t
i
, L)) is a frequency of term’s t
i
translation to language L estimated by the num-
ber of web hits. Thus languages in which trans-
lated concept terms appear more times will get
higher relative frequency, potentially indicating a
greater concept translation ambiguity. Now, for
each new term t
?
discovered through LNum(t
?
)
different languages L
1
. . . L
LNum(t
?
)
we calculate
a term score
11
S(t
?
, C):
S(t
?
, C) = LNum(t
?
)·
(
1?
?
i
LFreq(L
i
, C)
)
For each discovered term t
?
, S(t
?
, C) ?
[0, LNum(t
?
)], while discovery of t
?
in less fre-
quent languages will cause the score to be closer to
LNum(t
?
). So terms appearing in a greater num-
ber of infrequent languages will get higher scores.
After the calculation of score for each proposed
term, we retain terms whose scores are above the
predefined threshold H . In our experiments we
have used H = 3, usually meaning that acquisi-
tion of a term through 3-4 uncommon intermedi-
ate languages should be enough to accept it. The
same score measure can also be used to filter out
“bad” terms in an already existing concept.
4 Experimental Setup
We describe here the languages, concepts and dic-
tionaries we used in our experiments.
4.1 Languages and concepts
One of the main goals in this research is to take
advantage of concept data in every possible lan-
guage. As intermediate languages, we used 45 lan-
guages including major west European languages
like French or German, Slavic languages like Rus-
sian, Semitic languages as Hebrew and Arabic,
and diverse Asian languages such as Chinese and
Persian. To configure parameters we have used a
set of 10 concepts in Russian as a development set.
These concepts were not used in evaluation.
We examined a wide variety of concepts and for
each of them we used all languages with available
translations. Table 1 shows the resulting top 10
most utilized languages in our experiments.
11
In this expression i runs only on languages with term t
?
hence the summation is not 1.
English Russian Hebrew
German(68%) English(70%) English(66%)
French(60%) German(62%) German(65%)
Italian(60%) French(62%) Italian(61%)
Portuguese(57%) Spanish(58%) French(59%)
Spanish(55%) Italian(56%) Spanish(57%)
Turkish(51%) Portuguese(54%) Portuguese(57%)
Russian(50%) Korean(50%) Korean(48%)
Korean(46%) Turkish(49%) Russian(43%)
Chinese(45%) Chinese(47%) Turkish(43%)
Czech(42%) Polish (44%) Czech(40%)
Table 1: The ten most utilized intermediate languages in
our experiments. In parentheses we show the percentage of
new terms that these languages helped discover.
We have used the English, Hebrew (Ordan and
Winter, 2008) and Russian (Gelfenbeynand et al.,
2003) WordNets as sources for concepts and for
the automatic evaluation. Our concept set selec-
tion was based on English WN subtrees. To per-
form comparable experiments with Russian and
Hebrew, we have selected the same subtrees in
the Hebrew and Russian WN. Concept definitions
given to human judges for evaluation were based
on the corresponding WN glosses. For automated
evaluation we selected 150 synsets/subtrees con-
taining at least 10 single word terms (existing in
all three tested languages).
For manual evaluation we used a subset of 24
of these concepts. In this subset we tried to select
generic concepts manually, such that no domain
expert knowledge was required to check their cor-
rectness. Ten of these concepts were identical to
ones used in (Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006), which allowed us to
compare our results to recent work in case of En-
glish. Table 2 shows these 10 concepts along with
the sample terms. While the number of tested con-
cepts is not very large, it provides a good indica-
tion for the quality of our approach.
Concept Sample terms
Musical instruments guitar, flute, piano
Vehicles/transport train, bus, car
Academic subjects physics, chemistry, psychology
Body parts hand, leg, shoulder
Food egg, butter, bread
Clothes pants, skirt, jacket
Tools hammer, screwdriver, wrench
Places park, castle, garden
Crimes murder, theft, fraud
Diseases rubella, measles, jaundice
Table 2: Ten of the selected concepts with sample terms.
857
4.2 Multilingual dictionaries
We developed tools for automatic access to a num-
ber of dictionaries. We used Wikipedia cross-
language links as our main source (> 60%) for
offline translation. These links include translation
of Wikipedia terms into dozens of languages. The
main advantage of using Wikipedia is its wide cov-
erage of concepts and languages. However, one
problem it has is that it frequently encodes too
specific senses and misses common ones (bear is
translated as family Ursidae, missing its common
“wild animal” sense). To overcome these difficul-
ties, we also used Wiktionary and complemented
these offline resources with automated queries to
several (25) online dictionaries. We start with
Wikipedia definitions, then Wiktionary, and then,
if not found, we turn to online dictionaries.
5 Evaluation and Results
Potential applications of our framework include
both the extension of existing lexical databases
and the construction of new databases from a small
set of seeds for each concept. Consequently, in
our evaluation we aim to check both the ability
to extend nearly complete concepts and the abil-
ity to discover most of the concept given a few
seeds. Since in our current framework we extend
a small subset of concepts rather than the whole
database, we could not utilize application-based
evaluation strategies such as performance in WSD
tasks (Cuadros and Rigau, 2008).
5.1 Human judgment evaluation
In order to check how well we can extend existing
concepts, we count and verify the quality of new
concept terms discovered by the algorithm given
complete concepts from WN. Performing an auto-
matic evaluation of such new terms is a challeng-
ing task, since there are no exhaustive term lists
available. Thus, in order to check how well newly
added terms fit the concept definition, we have to
use human judges.
We provided four human subjects with 24 lists
of newly discovered terms, together with original
concept definitions (written as descriptive natural
language sentences) and asked them to rank (1-10,
10 being best) how well each of these terms fits
the given definition. We have instructed judges to
accept common misspellings and reject words that
are too general/narrow for the provided definition.
We mixed the discovered terms with equal
amounts of terms from three control sets: (1) terms
from the original WN concept; (2) randomly se-
lected WN terms; (3) terms obtained by apply-
ing the single-language concept acquisition algo-
rithm described in Section 3.3 in the source lan-
guage. Kappa inter-annotator agreement scores
were above 0.6 for all tests below.
5.1.1 WordNet concept extension
The middle column of Table 3 shows the judge
scores and average amount of added terms for
each source language. In this case the algorithm
was provided with complete term lists as con-
cept definitions, and was requested to extend these
lists. We can see that while the scores for original
WN terms are not perfect (7/10), single-language
and cross-lingual concept extension achieve nearly
the same scores. However, the latter discovers
many more new concept terms without reducing
quality. The difference becomes more substan-
tial for Hebrew, which is a resource-poor source
language, heavily affecting the performance of
single-language concept extension methods.
The low ranks for WN reflect the ambiguity of
definition of some of its classification subtrees.
Thus, for the ‘body part’ concept defined in Word-
Net as “any part of an organism such as an or-
gan or extremity” (which is not supposed to re-
quire domain-specific knowledge to identify) low
scores were given (correctly) by judges to generic
terms such as tissue, system, apparatus and pro-
cess (process defined in WN as “a natural pro-
longation or projection from a part of an organ-
ism”), positioned in WN as direct hyponyms of
body parts. Low scores were also given to very
specific terms like “saddle” (posterior part of the
back of a domestic fowl) or very ambiguous terms
like “small” (the slender part of the back).
5.1.2 Seed-based concept extension
The rightmost column of Table 3 shows similar in-
formation to the middle column, but when only
the three most frequent terms from the original
WN concept were given as concept definitions.
We can see that even given three words as seeds,
the cross-lingual framework allows to discover
many new terms. Surprisingly, terms extracted by
the cross-lingual framework achieve significantly
higher scores not only in comparison to the single-
language algorithm but also in comparison to ex-
isting WN terms. Thus while the “native” WN
concept and single-language concept extension re-
858
sults get a score of 7/10, terms obtained by the
cross-lingual framework obtain an average score
of nearly 9/10.
This suggests that our cross-lingual framework
can lead to better (from a human judgment point
of view) assignment of terms to concepts, even in
comparison to manual annotation.
Input
all terms 3 terms
English
WordNet 7.2 7.2
Random 1.8 1.8
SingleLanguage 7.0(10) 7.8(18)
Crosslingual 6.9(19) 8.8(26)
Russian
WordNet 7.8 7.8
Random 1.9 1.9
SingleLanguage 7.4(10) 8.1(16)
Crosslingual 7.6(21) 9.0(29)
Hebrew
WordNet 7.0 7.0
Random 1.3 1.3
SingleLanguage 6.5(4) 7.5(6)
Crosslingual 6.8(18) 8.9(24)
Table 3: Human judgment scores for concept extension in
three languages (1 . . . 10, 10 is best). The WordNet, Random
and SingleLanguage rows provide corresponding baselines.
Average count of newly added terms are shown in parenthe-
ses. Average original WN concept size in this set was 36 for
English, 32 for Russian and 27 for Hebrew.
5.2 WordNet-based evaluation
While human judgment evaluation provides a
good indication for the quality of our framework,
it has severe limitations. Thus terms in many con-
cepts require domain expertise to be properly la-
beled. We have complemented human judgment
evaluation with automated WN-based evaluation
with a greater (150) number of concepts. For each
of the 150 concepts, we have applied our frame-
work on a subset of the available terms, and esti-
mated precision and recall of the resulting term list
in comparison to the original WN term list. The
evaluation protocol and metrics were very simi-
lar to (Davidov and Rappoport, 2006; Widdows
and Dorow, 2002) which allowed us to do indirect
comparison to previous work.
Table 4 shows precision and recall for this task
comparing single-language concept extension and
the cross-lingual framework. We can see that
in all cases, utilization of the latter greatly im-
proves recall. It also significantly outperforms
the single-language pattern-based method intro-
duced by (Davidov and Rappoport, 2006), which
achieves average precision of 79.3 on a similar set
in English (in comparison to 86.7 in this study).
We can also see a decrease in precision when the
algorithm is provided with 50% of the concept
terms as input and had to discover the remaining
50%. However, careful examination of the results
shows that this decrease is due to discovery of ad-
ditional correct terms not present in WordNet.
Input
50% terms 3 terms
P R F P R F
English
SingleLanguage 89.2 75.9 82.0 80.6 15.2 25.6
CrossLingual 86.5 91.1 88.7 86.7 60.2 71.1
Russian
SingleLanguage 91.3 69.0 78.6 82.1 18.3 29.9
CrossLingual 84.9 86.2 85.5 85.3 62.1 71.9
Hebrew
SingleLanguage 93.8 38.6 54.7 90.2 5.7 10.7
CrossLingual 86.5 82.4 84.4 93.9 55.6 69.8
Table 4: WordNet-based precision (P) and recall (R) for
concept extension.
5.3 Contribution of each language
Each of the 45 languages we used influences the
score of at least 5% of the discovered terms. How-
ever, it is not apparent if all languages are indeed
beneficial or if only a handful of languages can
be used. In order to check this point we have per-
formed partial automated tests as described in Sec-
tion 5.2, removing one language at a time. We also
tried to remove random subsets of 2-3 languages,
comparing them to removal of one of them. We
saw that in each case removal of more languages
caused a consistent (while sometimes minor) de-
crease both in precision and recall metrics. Thus,
each language contributes to the system.
6 Discussion
We proposed a framework which given a set of
terms defining a concept in some language, uti-
lizes multilingual information available on the
web in order to extend this list. This method
allows to take advantage of web data in many
languages, requiring only multilingual dictionar-
ies. Our method was able to discover a substan-
tially greater number of terms than state-of-the-art
single language pattern-based concept extension
methods, while retaining high precision.
We also showed that concepts obtained by this
method tend to be more coherent in compari-
son to corresponding concepts in WN, a man-
ually prepared resource. Due to its relative
language-independence and modest data require-
ments, this framework allows gathering required
859
concept information from the web even if it is scat-
tered among different and relatively uncommon or
resource-poor languages.
References
Mishele Banko, Michael J Cafarella , Stephen Soder-
land, Matt Broadhead, Oren Etzioni, 2007. Open
information extraction from the Web. IJCAI ’07.
Wauter Bosma, Chris Callison-Burch, 2007. Para-
phrase substitution for recognizing textual entail-
ment.. Evaluation of Multilingual and Multimodal
Information Retrieval, Lecture Notes in Computer
Science ’07.
Sharon Caraballo, 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. ACL
’99.
Montse Cuadros, German Rigau, 2008. KnowNet:
Building a large net of knowledge from the Web.
COLING ’08.
James R. Curran, Marc Moens, 2002. Improvements
in automatic thesaurus extraction SIGLEX 02’, 59–
66.
Dmitry Davidov, Ari Rappoport, 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
COLING-ACL ’06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel,
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ’07.
Dmitry Davidov, Ari Rappoport, 2009. Translation
and extension of concepts across languages. EACL
’09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. In-
dexing by latent semantic analysis. J. of the Ameri-
can Society for Info. Science, 41(6):391–407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in graphs for
lexical acquisition and word sense discrimination.
MEANING ’05.
Oren Etzioni, Michael Cafarella, Doug Downey,
S. Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel Weld, Alexander Yates, 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91134.
Dayne Freitag, 2004. Trained named entity recogni-
tion using distributional clusters. EMNLP ’04.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic
translation of WordNet semantic network to Russian
language (in Russian) International Dialog 2003
Workshop.
J. Gorman, J.R. Curran, 2006. Scaling distributional
similarity to large corpora. COLING-ACL ’06.
Marti Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ’92.
Jagadeesh Jagarlamudi, A Kumaran, 2007. Cross-
lingual information retrieval system for Indian lan-
guages. Working Notes for the CLEF 2007 Work-
shop.
Dekang Lin, 1998. Automatic retrieval and clustering
of similar words. COLING ’98.
Noam Ordan, Shuly Wintner, 2007. Hebrew Word-
Net: a test case of aligning lexical databases across
languages. International Journal of Translation
19(1):39-58, 2007.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and similarities on
the web: fact extraction in the fast lane. COLING-
ACL ’06.
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
ACL ’08.
Patrick Pantel, Dekang Lin, 2002. Discovering word
senses from text. SIGKDD ’02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ’04.
John Paolillo, Daniel Pimienta, Daniel Prado, et al.,
2005. Measuring linguistic diversity on the Internet.
UNESCO Institute for Statistics Montreal, Canada.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the global WordNet grid. CIL18.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional clustering of English words. ACL ’93.
Ellen Riloff, Rosie Jones, 1999. Learning dictionaries
for information extraction by multi-level bootstrap-
ping. AAAI ’99.
Rion Snow, Daniel Jurafsky, Andrew Ng, 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. COLING-ACL ’06.
Lonneke van der Plas, Jorg Tiedemann, 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. COLING-ACL
’06.
860
Martin Volk, Paul Buitelaar, 2002. A systematic eval-
uation of concept-based cross-language information
retrieval in the medical domain. In: Proc. of 3rd
Dutch-Belgian Information Retrieval Workshop.
?
Spela Vintar, Darja Fi?ser, 2008. Harvesting multi-
word expressions from parallel corpora. LREC ’08.
Dominic Widdows, Beate Dorow, 2002. A graph
model for unsupervised lexical acquisition. COL-
ING ’02.
861
