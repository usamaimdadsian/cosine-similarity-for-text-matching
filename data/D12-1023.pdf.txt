Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 244–255, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Minimal Dependency Length in Realization Ranking
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
Comprehension and corpus studies have found
that the tendency to minimize dependency
length has a strong influence on constituent or-
dering choices. In this paper, we investigate
dependency length minimization in the con-
text of discriminative realization ranking, fo-
cusing on its potential to eliminate egregious
ordering errors as well as better match the dis-
tributional characteristics of sentence order-
ings in news text. We find that with a state-
of-the-art, comprehensive realization rank-
ing model, dependency length minimization
yields statistically significant improvements
in BLEU scores and significantly reduces
the number of heavy/light ordering errors.
Through distributional analyses, we also show
that with simpler ranking models, dependency
length minimization can go overboard, too of-
ten sacrificing canonical word order to shorten
dependencies, while richer models manage to
better counterbalance the dependency length
minimization preference against (sometimes)
competing canonical word order preferences.
1 Introduction
In this paper, we show that for the constituent or-
dering problem in surface realization, incorporating
insights from the minimal dependency length the-
ory of language production (Temperley, 2007) into a
discriminative realization ranking model yields sig-
nificant improvements upon a state-of-the-art base-
line. We demonstrate empirically using OpenCCG,
our CCG-based (Steedman, 2000) surface realiza-
tion system, the utility of a global feature encoding
the total dependency length of a given derivation.
Although other works in the realization literature
have used phrase length or head-dependent distances
in their models (Filippova and Strube, 2009; Velldal
and Oepen, 2005; White and Rajkumar, 2009, i.a.),
to the best of our knowledge, this paper is the first
to use insights from the minimal dependency length
theory directly and study their effects, both qualita-
tively and quantitatively.
The impetus for this paper was the discovery
that despite incorporating a sophisticated syntac-
tic model borrowed from the parsing literature—
including features with head-dependent distances at
various scales—White & Rajkumar’s (2009) real-
ization ranking model still often performed poorly
on weight-related decisions such as when to em-
ploy heavy-NP shift. Table 1 illustrates this point.
In wsj 0034.9, the full model (incorporating numer-
ous syntactic features) succeeds in reproducing the
reference sentence, which is clearly preferable to
the rather awkward variant selected by the base-
line model (using various n-gram models). How-
ever, in wsj 0013.16, the full model fails to shift the
temporal modifier for now next to the phrasal verb
turned down, leaving it at the end of its very long
verb phrase where it is highly ambiguous (with mul-
tiple intervening attachment sites). Conversely, in
wsj 0044.3, the full model shifts before next to the
verb, despite the NP cheating being very light, yield-
ing a very confusing ordering given that before is
meant to be intransitive.
The syntactic features in White & Rajku-
mar’s (2009) realization ranking model are taken
from Clark & Curran’s (2007) normal form model
244
wsj 0034.9 they fell into oblivion after the 1929 crash .
FULL [same]
BASELINE they fell after the 1929 crash into oblivion .
wsj 0013.16 separately , the Federal Energy Regulatory Commission [V P turned down for now [NP a request
by Northeast [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]]] .
FULL separately , the Federal Energy Regulatory Commission [V P turned down [NP a request by North-
east [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]] for now] .
wsj 0044.3 she had seen cheating before , but these notes were uncanny .
FULL she had seen before cheating , but these notes were uncanny .
Table 1: Examples of OpenCCG output with White & Rajkumar’s (2009) models—the first represents a successful
case, the latter two egregious ordering errors
(Table 3; see Section 3). In this model, head-
dependenct distances are considered in conjunc-
tion with lexicalized and unlexicalized CCG deriva-
tion steps, thereby appearing in numerous features.
As such, the model takes into account the inter-
action of dependency length with derivation steps,
but in essence does not consider the main ef-
fect of dependency length itself. In this light,
our investigation of dependency length minimiza-
tion can be viewed as examining the question of
whether realization ranking models can be made
more accurate—and in particular, avoid egregious
ordering errors—by incorporating a feature to ac-
count for the main effect of dependency length.
It is important to observe at this point that de-
pendency length minimization is more of a prefer-
ence than an optimization objective, which must be
balanced against other order preferences at times.
A closer reading of Temperley’s (2007) study re-
veals that dependency length can sometimes run
counter to many canonical word order choices. A
case in point is the class of examples involving
pre-modifying adjunct sequences that precede both
the subject and the verb. Assuming that their par-
ent head is the main verb of the sentence, a long-
short sequence would minimize overall dependency
length. However, in 613 examples found in the Penn
Treebank, the average length of the first adjunct was
3.15 words while the second adjunct was 3.48 words
long, thus reflecting a short-long pattern, as illus-
trated in the Temperley p.c. example in Table 2.
Apart from these, Hawkins (2001) shows that argu-
ments are generally located closer to the verb than
adjuncts. Gildea and Temperley (2007) also suggest
that adverb placement might involve cases which go
against dependency length minimization. An exam-
ination of 295 legitimate long-short post-verbal con-
stituent orders (counter to dependency length) from
Section 00 of the Penn Treebank revealed that tem-
poral adverb phrases are often involved in long-short
orders, as shown in wsj 0075.13 in Table 2. In our
setup, the preference to minimize dependency length
can be balanced by features capturing preferences
for alternate choices (e.g. the argument-adjunct dis-
tinction in our dependency ordering model, Table 4).
Via distributional analyses, we show that while sim-
pler realization ranking models can go overboard
in minimizing dependency length, richer models
largely succeed in overcoming this issue, while still
taking advantage of dependency length minimiza-
tion to avoid egregious ordering errors.
2 Background
2.1 Minimal Dependency Length
Comprehension and corpus studies (Gibson, 1998;
Gibson, 2000; Temperley, 2007) point to the ten-
dency of production and comprehension systems to
adhere to principles of dependency length minimiza-
tion. The idea of dependency length minimization
is based on Gibson’s (1998) Dependency Locality
Theory (DLT) of comprehension, which predicts
that longer dependencies are more difficult to pro-
cess. DLT predictions have been further validated
using comprehension studies involving eye-tracking
corpora (Demberg and Keller, 2008). DLT metrics
also correlate reasonably well with activation de-
cay over time expressed in computational models of
245
Temperley (p.c.) [In 1976], [as a film student at the Purchase campus of the State University of New York], Mr.
Lane, shot ...
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on
Thursday].
Table 2: Counter-examples to dependency length minimization
comprehension (Lewis et al., 2006; Lewis and Va-
sishth, 2005).
Extending these ideas from comprehension, Tem-
perley (2007) poses the question: Does language
production reflect a preference for shorter dependen-
cies as well so as to facilitate comprehension? By
means of a study of Penn Treebank data, Temperley
shows that English sentences do display a tendency
to minimize the sum of all their head-dependent
distances as illustrated by a variety of construc-
tions. Further, Gildea and Temperley (2007) report
that random linearizations have higher dependency
lengths compared to actual English, while an “opti-
mal” algorithm (from the perspective of dependency
length minimization), which places dependents on
either sides of a head in order of increasing length,
is closer to actual English. Tily (2010) also applies
insights from the above cited papers to show that
dependency length constitutes a significant pressure
towards language change. For head-final languages
(e.g., Japanese), dependency length minimization
results in the “long-short” constituent ordering in
language production (Yamashita and Chang, 2001).
More generally, Hawkins’s (1994; 2000) processing
domains, dependency length minimization and end-
weight effects in constituent ordering (Wasow and
Arnold, 2003) are all very closely related. The de-
pendency length hypothesis goes beyond the predic-
tions made by Hawkins’ Minimize Domains princi-
ple in the case of English clauses with three post-
verbal adjuncts: Gibson’s DLT correctly predicts
that the first constituent tends to be shorter than the
second, while Hawkins’ approach does not make
predictions about the relative orders of the first two
constituents.
However, it would be very reductive to consider
dependency length minimization as the sole factor
in language production. In fact, a large body of
prior work discusses a variety of other factors in-
volved in language production. These other prefer-
ences are either correlated with dependency length
or can override the minimal dependency length pref-
erence. Complexity (Wasow, 2002; Wasow and
Arnold, 2003), animacy (Snider and Zaenen, 2006;
Branigan et al., 2008), information status consid-
erations (Wasow and Arnold, 2003; Arnold et al.,
2000), the argument-adjunct distinction (Hawkins,
2001) and lexical bias (Wasow and Arnold, 2003;
Bresnan et al., 2007) are a few prominent factors.
More recently, Anttila et al. (2010) argued that the
principle of end weight can be revised by calculat-
ing weight in prosodic terms to provide more ex-
planatory power. As Temperley (2007) suggests,
a satisactory model should combine insights from
multiple approaches, a theme which we investigate
in this work by means of a rich feature set adapted
from the parsing and realization literature. Our fea-
ture design has been inspired by the conclusions of
the above-cited works pertaining to the role of de-
pendency length minimization in syntactic choice
in conjuction with other factors influencing con-
stituent order. However, going beyond Temper-
ley’s corpus study, we confirm the utility of incor-
porating a feature for minimizing dependency length
into machine-learned models with hundreds of thou-
sands of features found to be useful in previous pars-
ing and realization work, and investigate the extent
to which these features can counterbalance a de-
pendency length minimization preference in cases
where canonical word order considerations should
prevail.
2.2 Surface Realization with Combinatory
Categorial Grammar (CCG)
We provide here a brief overview of CCG and the
OpenCCG realizer; for further details, see the works
cited below.
CCG (Steedman, 2000) is a unification-based
categorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
246
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcl ? np sdcl\np
Rule + Word sdcl ? np sdcl\np + bought
Rule + POS sdcl ? np sdcl\np + VBD
Word-Word ?company, sdcl ? np sdcl\np, bought?
Word-POS ?company, sdcl ? np sdcl\np, VBD?
POS-Word ?NN, sdcl ? np sdcl\np, bought?
Word + ?w ?bought, sdcl ? np sdcl\np? + dw
POS + ?w ?VBD, sdcl ? np sdcl\np? + dw
Word + ?p ?bought, sdcl ? np sdcl\np? + dp
POS + ?p ?VBD, sdcl ? np sdcl\np? + dp
Word + ?v ?bought, sdcl ? np sdcl\np? + dv
POS + ?v ?VBD, sdcl ? np sdcl\np? + dv
Table 3: Basic and dependency features from Clark &
Curran’s (2007) normal form model; distances are in in-
tervening words, punctuation marks and verbs, and are
capped at 3, 3 and 2, respectively
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006;
White and Rajkumar, 2009). The input to the
OpenCCG realizer is a semantic graph, where each
node has a lexical predication and a set of seman-
tic features; nodes are connected via dependency re-
lations. Internally, such graphs are represented us-
ing Hybrid Logic Dependency Semantics (HLDS),
a dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). Al-
ternative realizations are ranked using integrated n-
gram or averaged perceptron scoring models. In the
experiments reported below, the inputs are derived
from the gold standard derivations in the CCGbank
(Hockenmaier and Steedman, 2007), and the outputs
are the highest-scoring realizations found during the
realizer’s chart-based search.1
3 Feature Design
In the realm of paraphrasing using tree lineariza-
tion, Kempen and Harbusch (2004) explore features
which have later been appropriated into classifica-
tion approaches for surface realization (Filippova
and Strube, 2007). Prominent features include in-
1The realizer can also be run using inputs derived from
OpenCCG’s parser, though informal experiments suggest that
parse errors tend to decrease generation quality.
formation status, animacy and phrase length. In the
case of ranking models for surface realization, by far
the most comprehensive experiments involving lin-
guistically motivated features are reported in work
of Cahill for German realization ranking (Cahill et
al., 2007; Cahill and Riester, 2009). Apart from
language model and Lexical Functional Grammar
(LFG) c-structure and f -structure based features,
Cahill also designed and incorporated features mod-
eling information status considerations.
The feature sets explored in this paper ex-
tend those in previous work on realization ranking
with OpenCCG using averaged perceptron models
(White and Rajkumar, 2009; Rajkumar et al., 2009;
Rajkumar and White, 2010) to include more com-
prehensive ordering features. The feature classes
are listed below, where DEPLEN, HOCKENMAIER
and DEPORD are novel, and the rest are as in ear-
lier OpenCCG models. The inclusion of the DE-
PORD features is intended to yield a model with a
similarly rich set of ordering features as Cahill and
Forster’s (2009) realization ranking model for Ger-
man. Except where otherwise indicated, features are
integer-valued, representing counts of occurrences
in a derivation.
DEPLEN The total of the length between all se-
mantic heads and dependents for a realization,
where length is in intervening words2 exclud-
ing punctuation. For length purposes, collapsed
named entities were counted as a single word in
the experiments reported here.
NGRAMS The log probabilities of the word se-
quence scored using three different n-gram
models: a trigram word model, a trigram
word model with named entity classes replac-
ing words, and a trigram model over POS tags
and supertags.
HOCKENMAIER As an extra component of the
generative baseline, the log probability of the
derivation according to (a reimplementation
2We also experimented with two other definitions of depen-
dency length described in the literature, namely (1) counting
only nouns and verbs to approximate counting by discourse ref-
erents (Gibson, 1998) and (2) omitting function words to ap-
proximate prosodic weight (Anttila et al., 2010); however, re-
alization ranking accuracy was slightly worse than counting all
non-punctuation words.
247
Feature Type Example
HeadBroadPos + Rel + Precedes + HeadWord + DepWord ?VB, Arg0, dep, wants, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, wants, PRP?
. . . + HeadPOS + DepWord ?VB, Arg0, dep, VBZ, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, VBZ, PRP?
HeadBroadPos + Side + DepWord1 + DepWord2 ?NN, left, an, important?
. . . + DepWord1 + DepPOS2 ?NN, left, an, JJ?
. . . + DepPOS1 + DepWord2 ?NN, left, DT, important?
. . . + DepPOS1 + DepPOS2 ?NN, left, DT, JJ?
. . . + Rel1 + Rel2 ?NN, left, Det, Mod?
Table 4: Basic head-dependent and sibling dependent ordering features
of) Hockenmaier’s (2003) generative syntactic
model.
DISCRIMINATIVE NGRAMS Sequences from each
of the n-gram models in the perceptron model.
AGREEMENT Features for subject-verb and ani-
macy agreement as well as balanced punctua-
tion.
C&C NF BASE The features from Clark & Cur-
ran’s (2007) normal form model, listed in Ta-
ble 3, minus the distance features.
C&C NF DISTANCE The distance features from
the C&C normal form model, where the dis-
tance between a head and its dependent is mea-
sured in intervening words, punctuation marks
or verbs; caps of 3, 3 and 2 (resp.) on the
distances have the effect of binning longer dis-
tances.
DEPORD Several classes of features for ordering
heads and dependents as well as sibling depen-
dents on the same side of the head. The ba-
sic features—using words, POS tags and de-
pendency relations, grouped by the broad POS
tag of the head—are shown in Table 4. There
are also similar features using words and a
word class (instead of words and POS tags),
where the class is either the named entity class,
COLOR for color words, PRO for pronouns,
one of 60-odd suffixes culled from the web, or
HYPHEN or CAP for hyphenated or capital-
ized words. Additionally, there are features for
detecting definiteness of an NP or PP (where
the definiteness value is used in place of the
POS tag).
Model # Alph Feats # Model Feats
GLOBAL 4 4
DEPLEN-GLOBAL 5 5
DEPORD-NONF 790,887 269,249
DEPORD-NODIST 1,035,915 365,287
DEPLEN-NODIST 1,035,916 366,094
DEPORD-NF 1,173,815 431,226
DEPLEN 1,173,816 428,775
Table 6: Model sizes—number of features in alphabet for
each model (satisfying count cutoff of 5) along with num-
ber active in model after 5 training epochs
4 Evaluation
4.1 Experimental Conditions
We followed the averaged perceptron training proce-
dure of White and Rajkumar (2009) with a couple of
updates. First, as noted earlier, we used a reimple-
mentation of Hockenmaier’s (2003) generative syn-
tactic model as an extra component of our genera-
tive baseline; and second, only five epochs of train-
ing were used, which was found to work as well as
using additional epochs on the development set. As
in the earlier work, the models were trained on the
standard training sections (02–21) of an enhanced
version of the CCGbank, using a lexico-grammar
extracted from these sections.
The models tested in the experiments reported be-
low are summarized in Table 5. The three groups
of models are designed to test the impact of the
dependency length feature when added to feature
sets of increasing complexity. In more detail,
the GLOBAL and DEPLEN-GLOBAL models contain
dense features on entire derivations; their values
are the log probabilities of the three n-gram mod-
248
Model Dep Ngram Hocken- Discr Agree- C&C NF C&C NF Dep
Len Mods maier Ngrams ment Base Dist Ord
GLOBAL N Y Y N N N N N
DEPLEN-GLOBAL Y Y Y N N N N N
DEPORD-NONF N Y Y Y Y N N Y
DEPORD-NODIST N Y Y Y Y Y N Y
DEPLEN-NODIST Y Y Y Y Y Y N Y
DEPORD-NF N Y Y Y Y Y Y Y
DEPLEN Y Y Y Y Y Y Y Y
Table 5: Legend for experimental conditions
els used in the earlier work along with the Hock-
enmaier model (and the dependency length feature,
in DEPLEN-GLOBAL). The second group is cen-
tered on DEPORD-NODIST, which contains all fea-
tures except the dependency length feature and the
distance features in Clark & Curran’s normal form
model, which may indirectly capture some depen-
dency length minimization preferences. In addition
to DEPLEN-NODIST—where the dependency length
feature is added—this group also contains DEPORD-
NONF, which is designed to test (as a side compari-
son) whether the Clark & Curran normal form base
features are still useful even when used in conjunc-
tion with the new dependency ordering features. In
the final group, DEPORD-NF contains all the features
examined in this paper except the dependency length
feature, while DEPLEN contains all the features in-
cluding the dependency length feature. Note that the
learned weight of the total dependency length fea-
ture was negative in each case, as expected.
Table 6 shows the sizes of the various models. For
each model, the alphabet—whose size increases to
over a million features—is the set of applicable fea-
tures found to have discriminative value in at least 5
training examples; from these, a subset are made ac-
tive (i.e., take on a non-zero weight) through percep-
tron updates when the feature value differs between
the model-best and oracle-best realization.
4.2 BLEU Results
Following the usual practice in the realization rank-
ing, we first evaluate our results quantitatively us-
ing exact matches and BLEU (Papineni et al., 2002),
a corpus similarity metric developed for MT evalu-
ation. Realization results for the development and
Model % Exact BLEU Signif
Sect 00
GLOBAL 33.03 0.8292 -
DEPLEN-GLOBAL 34.73 0.8345 ***
DEPORD-NONF 42.33 0.8534 **
DEPORD-NODIST 43.12 0.8560 -
DEPLEN-NODIST 43.87 0.8587 ***
DEPORD-NF 43.44 0.8590 -
DEPLEN 44.56 0.8610 **
Sect 23
GLOBAL 34.75 0.8302 -
DEPLEN-GLOBAL 34.70 0.8330 ***
DEPORD-NODIST 41.42 0.8561 -
DEPLEN-NODIST 42.95 0.8603 ***
DEPORD-NF 41.32 0.8577 -
DEPLEN 42.05 0.8596 **
Table 7: Development (Section 00) & test (Section 23)
set results—exact match percentage and BLEU scores,
along with statistical significance of BLEU compared to
the unmarked model in each group (* = p < 0.1, ** =
p < 0.05, *** = p < 0.01); significant within-group
winners (at p < 0.05) are shown in bold
test sections appear in Table 7. For all three model
groups, the dependency length feature yields signif-
icant increases in BLEU scores, even in compar-
ison to the model (DEPORD-NF) containing Clark
& Curran’s distance features in addition to the new
dependency ordering features (as well as all other
features but total dependency length). The second
group additionally shows that the Clark & Curran
normal form base features do indeed have a signif-
icant impact on BLEU scores even when used with
249
Model % DL % DL DL Signif
Lower Greater Mean
GOLD n.a. n.a. 41.02 -
GLOBAL 17.23 21.59 42.40 ***
DEPLEN-GLOBAL 24.37 12.81 40.29 ***
DEPORD-NONF 15.76 19.34 42.34 ***
DEPORD-NODIST 14.58 19.06 42.03 ***
DEPLEN-NODIST 17.75 14.82 40.87 n.s.
DEPORD-NF 14.96 17.65 41.58 ***
DEPLEN 16.28 14.78 40.97 n.s.
Table 8: Dependency length compared to corpus—
percentage of realizations with dependency length less
than and greater than gold standard, along with mean
dependency length, whose significance is tested against
gold; 1671 development set (Section 00) complete real-
izations analyzed
the new dependency ordering model, as DEPORD-
NONF is significantly worse than DEPORD-NODIST
(the impact of the distance features is evident in the
increases from the second group to the third group).
As with the dev set, the dependency length feature
yielded a significant increase in BLEU scores for
each comparison on the test set also.
For each group, the statistical significance of the
difference in BLEU scores between a model and the
unmarked model (-) is determined by bootstrap re-
sampling (Koehn, 2004).3 Note that although the
differences in BLEU scores are small, they end
up being statistically significant because the mod-
els frequently yield the same top scoring realiza-
tion, and reliably deliver improvements in the cases
where they differ. In particular, note that DEPLEN
and DEPORD-NF agree on the best realization 81%
of the time, while DEPLEN-NODIST and DEPORD-
NODIST have 78.1% agreement, and DEPLEN-
GLOBAL and GLOBAL show 77.4% agreement; by
comparison, DEPORD-NODIST and GLOBAL only
agree on the best realization 51.1% of the time.
4.3 Detailed Analyses
The effect of the dependency length feature on the
distribution of dependency lengths is illustrated in
Table 8. The table shows the mean of the total de-
pendency length of each realized derivation com-
3Kudos to Kevin Gimpel for making his resampling
scripts available from http://www.ark.cs.cmu.edu/
MT/paired_bootstrap_v13a.tar.gz.
Model % Short % Long % Eq % Single
/ Long / Short Constit
GOLD 25.25 4.87 4.08 65.79
GLOBAL 23.15 7.86 3.94 65.04
DEPLEN-GLOBAL 24.58 5.57 4.09 65.76
DEPORD-NONF 23.13 6.61 4.03 66.23
DEPORD-NODIST 23.38 6.52 3.94 66.15
DEPLEN-NODIST 24.03 5.38 4.01 66.58
DEPORD-NF 23.74 5.92 3.96 66.40
DEPLEN 24.36 5.36 4.07 66.21
Table 9: Distribution of various kinds of post-verbal con-
stituents in the development set (Section 00); 4692 gold
cases considered
pared to the corresponding gold standard derivation,
as well as the number of derivations with greater and
lower dependency length. According to paired t-
tests, the mean dependency lengths for the DEPLEN-
NODIST and DEPLEN models do not differ signifi-
cantly from the gold standard. In contrast, the mean
dependency length of all the models that do not in-
clude the dependency length feature does differ sig-
nificantly (p < 0.001) from the gold standard. Ad-
ditionally, all these models have more realizations
with dependency length greater than the gold stan-
dard, in comparison to the dependency length min-
imizing models; this shows the efficacy of the de-
pendency length feature in approximating the gold
standard. Interestingly, the DEPLEN-GLOBAL model
significantly undershoots the gold standard on mean
dependency length, and has the most skewed dis-
tribution of sentences with greater vs. lesser depen-
dency length than the gold standard.
Apart from studying dependency length directly,
we also looked at one of the attested effects of de-
pendency length minimization, viz. the tendency to
prefer short-long post-verbal constituents in produc-
tion (Temperley, 2007). The relative lengths of ad-
jacent post-verbal constituents were computed and
their distribution is shown in Table 9. While cal-
culating length, punctuation marks were excluded.
Four kinds of constituents were found in the post-
verbal domain. For every verb, apart from single
constituents and equal length constituents, short-
long and long-short sequences were also observed.
Table 9 demonstrates that for both the gold standard
corpus as well as the realizer models, short-long
constituents were more frequent than long-short or
equal length constituents. This follows the trend re-
250
Model % Light % Heavy Signif
/ Heavy / Light
GOLD 8.60 0.36 -
GLOBAL 7.73 2.02 ***
DEPLEN-GLOBAL 8.35 0.75 **
DEPORD-NONF 7.98 1.15 ***
DEPORD-NODIST 8.04 1.12 ***
DEPLEN-NODIST 8.23 0.45 n.s.
DEPORD-NF 8.26 0.71 **
DEPLEN 8.36 0.51 n.s.
Table 10: Distribution of heavy unequal constituents
(length difference > 5) in Section 00; 4692 gold cases
considered and significance tested against the gold stan-
dard using a ?-square test
ported by previous corpus studies of English (Tem-
perley, 2007; Wasow and Arnold, 2003). The figures
reported here show the tendency of the DEPLEN*
models to be closer to the gold standard than the
other models, especially in the case of short-long
constituents.
We also performed an analysis of relative con-
stituent lengths focusing on light-heavy and heavy-
light cases; specifically, we examined unequal
length constituent sequences where the length dif-
ference of the constituents was greater than 5, and
the shorter constituent was under 5 words. Table 10
shows the results. Using a ?-square test, the distri-
bution of heavy unequal length constituent counts in
the DEPLEN-NODIST and DEPLEN models does not
significantly differ from that of the gold standard. In
contrast, for all the other models, the counts do dif-
fer significantly from the gold standard.
4.4 Examples
Table 11 shows examples of how the dependency
length feature (DEPLEN) affects the output even in
comparison to a model (DEPORD) with a rich set
of discriminative syntactic and dependency order-
ing features, but no features directly targeting rel-
ative weight. In wsj 0015.7, the dependency length
model produces an exact match, while the DEPORD
model fails to shift the short temporal adverbial next
year next to the verb, leaving a confusingly repeti-
tive this year next year at the end of the sentence.
In wsj 0020.1, the dependency length model pro-
duces a nearly exact match with just an equally ac-
ceptable inversion of closely watching. By contrast,
the DEPORD model mistakenly shifts the direct ob-
ject South Korea, Taiwan and Saudia Arabia to the
end of the sentence where it is difficult to under-
stand following two very long intervening phrases.
In wsj 0021.8, both models mysteriously put not in
front of the auxiliary and leave out the complemen-
tizer, but DEPORD also mistakenly leaves before at
the end of the verb phrase where it is again apt to
be interpreted as modifying the preceding verb. In
wsj 0075.13, both models put the temporal modi-
fier on Thursday in its canonical VP-final position,
despite this order running counter to dependency
length minimization. Finally, wsj 0014.2 shows a
case where DEPORD is nearly an exact match (except
for a missing comma), but the dependency length
model fronts the PP on the 12-member board, where
it is grammatical but rather marked (and not moti-
vated in the discourse context).
4.5 Interim Discussion
The experiments show a consistent positive effect of
the dependency length feature in improving BLEU
scores and achieving a better match with the corpus
distributions of dependency length and short/long
constituent orders. The results in Table 10 are partic-
ulary encouraging, as they show that minimizing de-
pendency length reduces the number of realizations
in which a heavy constituent precedes a light one
down to essentially the level of the corpus, thereby
eliminating many realizations that can be expected
to have egregious errors like those shown in Ta-
ble 11.
Intriguingly, there is some evidence that a nega-
tively weighted total dependency length feature can
go too far in minimizing dependency length, in the
absence of other informative features to counterbal-
ance it. In particular, the DEPLEN-GLOBAL model in
Table 8 has significantly lower dependency length
than the corpus, but in the richer models with dis-
criminative synactic and dependency ordering fea-
tures, there are no significant differences. It may still
be though that additional features are necessary to
counteract the tendency towards dependency length
minimization, for example to ensure that initial con-
stituents play their intended role in establishing and
continuing topics in discourse, as also observed in
Table 11.
251
wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made until
Dec. 31 of this year .
DEPLEN [same]
DEPORD the exact amount of the refund will be determined based on actual collections made until Dec. 31
of this year next year .
wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is closely watching for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights , claiming some success in its trade diplo-
macy , South Korea , Taiwan and Saudi Arabia .
wsj 0021.8 but he has not said before that the country wants half the debt forgiven .
DEPLEN but he not has said before ? the country wants half the debt forgiven .
DEPORD but he not has said ? the country wants half the debt forgiven before .
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on Thurs-
day].
DEPLEN [same]
DEPORD [same]
wsj 0014.2 they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary , on the 12-member board .
DEPORD they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary ? on the 12-member board .
DEPLEN on the 12-member board they succeed Daniel M. Rexinger , retired Circuit City executive vice
president , and Robert R. Glauber , U.S. Treasury undersecretary .
Table 11: Examples of realized output for full models with and without the dependency length feature
4.6 Targeted Human Evaluation
To determine whether heavy-light ordering differ-
ences often represent ordering errors (including
egregious ones), rather than simply representing ac-
ceptable variation, we conducted a targeted human
evaluation on examples of this kind. Specifically,
for each of the DEPLEN* models and their corre-
sponding models without the dependency length fea-
ture, we chose the 25 sentences from the develop-
ment section whose realizations exhibited the great-
est difference in dependency length between sibling
constituents appearing in opposite orders, and asked
two judges (not the authors) to choose which of the
two realizations best expressed the meaning of the
reference sentence in a grammatical and fluent way,
with the choice forced (2AFC). Table 12 shows the
results. Agreement between the judges was high,
Model % Preferred % Agr Signif
GLOBAL 22 - -
DEPLEN-GLOBAL 78 84 ***
DEPORD-NODIST 24 - -
DEPLEN-NODIST 76 92 ***
DEPORD-NF 26 - -
DEPLEN 74 96 ***
Table 12: Targeted human evaluation—percentage of re-
alizations preferred by two human judges in a 2AFC test
among the 25 development set sentences with the great-
est differences in dependency length, with a binomial test
for significance
252
with only one disagreement on the realizations from
the DEPLEN and DEPORD-NF models (involving an
acceptable paraphrase in our judgment), and only
four disagreements on the DEPLEN-GLOBAL and
GLOBAL realizations. Pooling the judgments, the
preference for the DEPLEN* models was well above
the chance level of 50% according to a binomial test
(p < 0.001 in each case). Inspecting the data our-
selves, we found that many of the items did indeed
involve egregious ordering errors that the DEPLEN*
models managed to avoid.
5 Related Work
As noted in the introduction, to the best of our
knowledge this paper is the first to examine the im-
pact of dependency length minimization on realiza-
tion ranking. While there have been quite a few
papers to date reporting results on Penn Treebank
data, since the various systems make different as-
sumptions regarding the specificity of their inputs,
all but the most broad-brushed comparisons remain
impossible at present, and thus detailed studies such
as the present one can only be made within the con-
text of different models for the same system. Some
progress on this issue has been made in the con-
text of the Generation Challenges Surface Realiza-
tion Shared Task (Belz et al., 2011), but it remains
to be seen to what extent fair cross-system compar-
isons using common inputs can be achieved.
For (very) rough comparison purposes, Table 13
lists our results in the context of those reported for
various other systems on PTB Section 23. As the
table shows, the OpenCCG scores are quite com-
petitive, exceeded only by Callaway’s (2005) ex-
tensively hand-crafted system as well as Bohnet et
al.’s (2011) system on shared task shallow inputs
(-S), which performs much better than their sys-
tem on deep inputs (-D) that more closely resemble
OpenCCG’s.
6 Conclusions
In this paper, we have investigated dependency
length minimization in the context of realization
ranking, focusing on its potential to eliminate egre-
gious ordering errors as well as better match the dis-
tributional characteristics of sentence orderings in
news text. When added to a state-of-the-art, com-
System Coverage BLEU % Exact
Callaway (05) 98.5% 0.9321 57.5
Bohnet et al.-S (11) 100% 0.8911
OpenCCG (12) 97.1% 0.8596 42.1
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al. (04) 100% 0.836 35.7
Bohnet et al.-D (11) 100% 0.7943
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al. (08) 100% 0.7440 19.8
Hogan et al. (07) ?100% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al. (05) 90.8% 0.7733
Table 13: PTB Section 23 BLEU scores and exact match
percentages in the NLG literature (Nakanishi et al.’s re-
sults are for sentences of length 20 or less)
prehensive realization ranking model, we showed
that including a dense, global feature for minimiz-
ing total dependency length yields statistically sig-
nificant improvements in BLEU scores and signif-
icantly reduces the number of heavy-light ordering
errors. Going beyond the BLEU metric, we also
conducted a targeted human evaluation to confirm
the utility of the dependency length feature in mod-
els of varying richness. Interestingly, even with the
richest model, in some cases we found that the de-
pendency length feature still appears to go too far in
minimizing dependency length, suggesting that fur-
ther counter-balancing features—especially ones for
the sentence-initial position (Filippova and Strube,
2009)—warrant investigation in future work.
Acknowledgments
This work was supported in part by NSF grants no.
IIS-1143635 and IIS-0812297. We thank the anony-
mous reviewers for helpful comments and discus-
sion, and Scott Martin and Dennis Mehay for their
participation in the targeted human evaluation.
253
References
Arto Anttila, Matthew Adams, and Mike Speriosu. 2010.
The role of prosody in the English dative alternation.
Language and Cognitive Processes.
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76:28–55.
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the Genera-
tion Challenges Session at the 13th European Work-
shop on Natural Language Generation, pages 217–
226, Nancy, France, September. Association for Com-
putational Linguistics.
Bernd Bohnet, Simon Mille, Beno?ˆt Favre, and Leo Wan-
ner. 2011. <stumaba >: From deep representation to
surface. In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 232–235, Nancy, France,
September. Association for Computational Linguis-
tics.
H Branigan, M Pickering, and M Tanaka. 2008. Con-
tributions of animacy to grammatical function assign-
ment and word order during production. Lingua,
118(2):172–189.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R. Har-
ald Baayen. 2007. Predicting the Dative Alternation.
Cognitive Foundations of Interpretation, pages 69–94.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of, ACL-IJCNLP ’09, pages 817–825, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Designing features for parse disambiguation and real-
isation ranking. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the 12th International
Lexical Functional Grammar Conference, pages 128–
147. CSLI Publications, Stanford.
Charles Callaway. 2005. The types and distributions
of errors in a wide coverage surface realizer evalua-
tion. In Proceedings of the 10th European Workshop
on Natural Language Generation.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493–552.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in German clauses. In ACL 2007,
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, June 23-30,
2007, Prague, Czech Republic. The Association for
Computer Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 225–228, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1–76.
Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity. In
Alec Marantz, Yasushi Miyashita, and Wayne O’Neil,
editors, Image, Language, brain: Papers from the First
Mind Articulation Project Symposium. MIT Press,
Cambridge, MA.
Daniel Gildea and David Temperley. 2007. Optimizing
grammars for minimum dependency length. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 184–191, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for general
purpose sentence realisation. In Proc. COLING-08.
John A. Hawkins. 1994. A Performance Theory of Order
and Constituency. Cambridge University Press, New
York.
John A. Hawkins. 2000. The relative order of
prepositional phrases in English: Going beyond
manner-place-time. Language Variation and Change,
11(03):231–266.
John A. Hawkins. 2001. Why are categories adjacent?
Journal of Linguistics, 37:1–34.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
254
Gerard Kempen and Karin Harbusch. 2004. Generat-
ing natural word orders in a semi-free word order lan-
guage: Treebank-based linearization preferences for
German. In Alexander F. Gelbukh, editor, CICLing,
volume 2945 of Lecture Notes in Computer Science,
pages 350–354. Springer.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory re-
trieval. Cognitive Science, 29:1–45, May.
Richard L. Lewis, Shravan Vasishth, and Julie Van Dyke.
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ences, 10(10):447–454.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL-02.
Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Coling 2010: Posters, pages 1032–1040, Beijing,
China, August. Coling 2010 Organizing Committee.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 161–164, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Eric Ringger, Michael Gamon, Robert C. Moore, David
Rojas, Martine Smets, and Simon Corston-Oliver.
2004. Linguistically informed statistical models of
constituent structure for ordering in sentence realiza-
tion. In Proc. COLING-04.
Neal Snider and Annie Zaenen. 2006. Animacy and syn-
tactic structure: Fronted NPs in English. In M. Butt,
M. Dalrymple, and T.H. King, editors, Intelligent Lin-
guistic Architectures: Variations on Themes by Ronald
M. Kaplan. CSLI Publications, Stanford.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300 –
333.
Harry Tily. 2010. The Role of Processing Complexity
in Word Order Variation and Change. Ph.D. thesis,
Stanford University.
Erik Velldal and Stefan Oepen. 2005. Maximum entropy
models for realization ranking. In Proc. MT-Summit
X.
Thomas Wasow and Jennifer Arnold. 2003. Post-verbal
Constituent Ordering in English. Mouton.
Tom Wasow. 2002. Postverbal Behavior. CSLI Publica-
tions, Stanford.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410–419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language & Computation, 4(1):39–75.
Hiroko Yamashita and Franklin Chang. 2001. “Long
before short” preference in the production of a head-
final language. Cognition, 81.
255
