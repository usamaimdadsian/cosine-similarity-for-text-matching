Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 609–618,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
Matching Reviews to Objects using a Language Model
Nilesh Dalvi Ravi Kumar Bo Pang Andrew Tomkins
Yahoo! Research
701 First Ave
Sunnyvale, CA 94089
{ndalvi,ravikumar,bopang,atomkins}@yahoo-inc.com
Abstract
We develop a general method to match un-
structured text reviews to a structured list
of objects. For this, we propose a lan-
guage model for generating reviews that
incorporates a description of objects and a
generic review language model. This mix-
ture model gives us a principled method to
find, given a review, the object most likely
to be the topic of the review. Extensive
experiments and analysis on reviews from
Yelp show that our language model-based
method vastly outperforms traditional tf-
idf-based methods.
1 Introduction
Consider a user searching for reviews of
“Casablanca Moroccan Restaurant.” The search
engine would like to obtain as many reviews of
this restaurant as possible, both to offer a high-
quality result set for even obscure restaurants, and
to enable advanced applications such as aggrega-
tion/summarization/categorization of reviews and
recommendation of alternate restaurants. To solve
this problem, it faces two high-level challenges:
first, identify the restaurant review pages on the
Web; and second, given a review, identify the
restaurant that is being reviewed. There has been
previous work addressing the first challenge (Sec-
tion 2). We focus in this paper on the second.
The Web is replete with restaurant reviews
available on top restaurant verticals such as Yelp
and CitySearch, as well as newspaper articles,
newsgroup discussions, blog posts, small local re-
view aggregators and so forth. Ideally, the search
engine would like to obtain reviews from all pos-
sible sources. While identifying the subject mat-
ter of a given review on the large sites may be
amenable to structured extraction through wrapper
induction or related techniques, it is typically not
cost-effective to apply such techniques to smaller
“tail” sites, and purely unstructured sources re-
quire alternate approaches altogether. In this pa-
per, we explore the setting of matching reviews to
objects using only their textual content. Note that
matching reviews to objects is a pervasive prob-
lem beyond the restaurant domain. Shopping ver-
ticals like to aggregate camera reviews, entertain-
ment verticals wish to collect movie reviews, and
so on. We use restaurant reviews as a running ex-
ample, but the techniques are general.
More specifically, the problem we consider in
this paper is the following. Given a list of struc-
tured objects (restaurants/cameras/movies) and a
text review, identify the object from the list that
is the topic of the review. Our focus on tex-
tual content allows us to expand the universe of
sources from which we can extract reviews to in-
clude sources that are purely textual, such as fo-
rum posts, blog posts, newsgroup postings, and
the like. In fact, even among collections of “struc-
tured” sources like review aggregators, there are
no highly accurate unsupervised techniques to
match a known review page to an object. Struc-
tured (e.g., HTML) cues provide valuable lever-
age in attacking this problem, but the types of tex-
tual cues we focus on are also a key part of the
puzzle; in such a context, our techniques can still
contribute to the overall matching problem.
It is important to contrast our problem against
two settings of related flavor — entity matching,
whose goal is to find the correspondence between
two structured objects and information retrieval
(IR), whose goal is to match unstructured short
text (query) against unstructured text (document).
Our problem is considerably harder than entity
matching for the following reasons. In matching
two structured objects there is often a natural cor-
respondence between their attributes, whereas no
such correspondence exists between an object and
609
its review. For instance, while trying to match a
review to a restaurant object, it is unclear if a spe-
cific portion of the review refers to the name of the
restaurant, or to its location, or is a statement not
concerning specifics of the restaurant. Moreover,
even if we wish to use entity matching, we must
first recognize the entities from a review. There
are two methods to do this, namely, wrapper in-
duction and information extraction. Wrapper in-
duction methods have serious limitations: they are
applicable only to highly-structured websites and
involve human labeling effort that is expensive and
error-prone and entails constant maintenance to
keep wrappers up-to-date. Information extraction
methods (Cardie, 1997; Sarawagi, 2008), on the
other hand, often have limited accuracy.
Our problem is also not amenable to classical
IR methods such as tf-idf. For example, suppose
we want to find the relevant restaurant for a given
review. The standard tf-idf will treat the review as
the query, the set of restaurant as documents and
compute the tf-idf scores. Now consider a restau-
rant called “Food.”
1
Since the term “food” is rare
as a restaurant name, it will get a very high idf
score and hence will likely be the top match for all
reviews containing the word “food.” In fact, unlike
in traditional IR, a “query” (i.e., review) is long
and a “document” (i.e., restaurant) is short — this
demands adapting established IR concepts such as
inverse document frequency and document length
normalization to our setting. If we take the op-
posite view by considering reviews as documents
and restaurants as queries, we still deviate from the
IR setting, since now we need to rank and find the
best “query” for a given “document.” In Section
3.4, we illustrate the shortcomings of both these
approaches.
In fact, the nature of the object database we con-
sider provides several unique opportunities over
traditional IR. First the “document”, i.e., the ob-
ject to be matched, has more semantics, since
each document is associated with one or more se-
mantic attribute, such as the name/location of the
restaurant. Second, the “query”, i.e., the text we
are matching is known to be a review of the ob-
ject, and hence is rendered in a language that is
“review-like” — this can be modeled by a genera-
tive process that produces reviews from objects.
Third, the set of objects we are interested in is
1
1569 Lexington Ave., New York, NY 10029. (212) 348-
0200.
given a priori, and we only seek to match reviews
with one of these objects; this makes our problem
more tractable than open-ended entity recognition.
Our contributions. We propose a general
method to match reviews to objects. To this end,
we postulate a language model for generating re-
views. The intuition behind our model is simple
and natural: when a review is written about an ob-
ject, each word in the review is drawn either from a
description of the object or from a generic review
language that is independent of the object. This
mixture model leads to a method to find, given a
review, the object most likely to be the topic of the
review.
Our method is light-weight and scalable and
can be viewed as obviating the need for highly-
expensive information extraction. Since the
method is text-based and does not rely on any
HTML structural clues, it is especially applicable
to reviews present in blogs and the so-called tail
web sites — web sites for which it is not feasible
to maintain wrappers to automatically extract the
object of a review.
We then report results on over 11K restaurant
reviews from Yelp. The experiments and our ex-
tensive analysis show that our language model-
based method significantly outperforms traditional
tf-idf based methods, which fail to take full ad-
vantage of the properties that are specific to our
setting.
2 Related work
Opinion topic identification is the work closest
to ours. In a recent paper, Stoyanov and Cardie
(2008) approach this problem by treating it as an
exercise in topic coreference resolution. Though
they have to deal with topic ambiguities and a lack
of explicit topic mentions as in our case, their no-
tion of a topic is not driven by a structured list-
ing. There has been some work on fine-grained
opinion extraction from reviews (Kobayashi et al.,
2004; Yi et al., 2003; Popescu and Etzioni, 2005;
Hu and Liu, 2004); see (Pang and Lee, 2008) for a
comprehensive survey. Most of this body of work
focused on identifying product features of the ob-
ject under review, rather than identifying the prod-
uct itself. Note that while a dictionary of prod-
ucts is often more readily available than a dictio-
nary of product features, identifying objects of re-
views is non-trivial even with the help of the for-
mer. Indeed, it has been reported that lexicon-
610
lookup methods have limited success on general
non-product review texts (Stoyanov and Cardie,
2008). In general, this line of work is more rooted
in the information extraction literature, where text
spans covering the object (or features of the ob-
ject) were extracted as the first step; in contrast,
we do not have an explicit extraction phase. Since
the (very extensive) list of candidate objects are
given as input, our task is to rank all matching ob-
jects, and in this sense is closer in nature to infor-
mation retrieval tasks. There has been some work
on detecting reviews in large-scale collections (Ng
et al., 2006; Barbosa et al., 2009); this is a logical
step that precedes the review matching step, the
topic of our paper.
Language modeling is becoming a powerful
paradigm in the realm of information retrieval ap-
plications (Ponte and Croft, 1998; Hiemstra, 1998;
Song and Croft, 1999; Lafferty and Zhai, 2003;
Zhai, 2008). The basic theme behind language
modeling is to first postulate a model for each doc-
ument and for a given query select the document
that is most likely to have generated the query;
smoothing is an important means to manage data
sparsity in language models (Zhai and Lafferty,
2004). As noted earlier, language models devel-
oped for IR are unsuitable for our setting. Further-
more, there are opportunities, such as the presence
of structure in our data, which we use in this work
(Section 3.2). In fact, in a subsequent paper, we
show how a language model specific to each at-
tribute can further improve the accuracy of review
matching (Dalvi et al., 2009).
Entity matching is a well-studied topic in
databases. There are several approaches to entity
matching: non-relational approaches, which con-
sider pairwise attribute similarities between enti-
ties (Newcombe et al., 1959; Fellegi and Sunter,
1969), relational approaches, which exploit the re-
lationships that exist between entities (Ananthakr-
ishna et al., 2002; Kalashnikov et al., 2005), and
collective approaches, which exploit the relation-
ship between various matching decisions, (Bhat-
tacharya and Getoor, 2007; McCallum and Well-
ner, 2004). The EROCS system (Chakaravarthy et
al., 2006), which uses information extraction and
entity matching, is closest in spirit to our problem;
they, however, employ tf-idf to match, which we
show to be significantly sub-optimal in our set-
ting.
3 Model and method
In this section we present the problem formula-
tion, the basic generative model for reviews, a
method based on this model to associate an object
with a review, and the techniques to estimate the
parameters of this model.
Problem formulation. Let E denote a set of ob-
jects. Each object e ? E has a set of attributes
and let text(e) denote the union of the textual con-
tent of all its attributes. Suppose we have a col-
lection of reviews R, where each review is writ-
ten (mainly) about one of the objects in the listing
E . The problem now is to correctly associate each
r ? R with exactly one of e ? E .
We model each review as a bag of words.
Therefore, notation such as “w ? r” for a word
w and a review r makes sense. For a review r and
an object e, let r
e
= r ? text(e).
As a running example, we use E to denote the
set of all restaurants and R to denote the set of all
restaurant reviews.
3.1 A generative model for reviews
We first state the intuition behind our generative
model: when a review r is written about an object
e, some words in r (e.g., the name and the address
of the restaurant) are drawn from text(e) to refer
to the object under discussion, while some other
words are drawn from a generic review language
independent of e.
Formally, let ? ? (0, 1) be a parameter.
Let P
e
(·) denote a distribution whose support is
text(e); this corresponds to the distribution of
words specific to the object e, taken from the de-
scription text(e). We use P
e
(w) to denote the
probability the word w is chosen according to this
distribution. Let P (·) be an object-independent
distribution whose support is the review language,
i.e., all the words that can be used to write a re-
view; we use P (w) to denote the probability the
word w is chosen according to this distribution.
Now, for a given object e, a review r is gener-
ated as follows. Each word in r is generated in-
dependently: with probability ?, a word w is cho-
sen with probability P
e
(w) and with probability
1 ? ?, a word w is chosen with probability P (w).
Thus, the review generation process is a multino-
mial, where the underlying process is a mixture of
object-specific language and a generic review lan-
guage.
611
Given a review r and an object e, by our inde-
pendence assumption,
Pr[r | e] = Z(r)
?
w?r
Pr[w | e]
= Z(r)
?
w?r
((1 ? ?)P (w) + ?P
e
(w)), (1)
where Z(r) is a normalizing term that only de-
pends on the length of r and the counts of the
words in it. Recalling r
e
= r ? text(e), we note
that P
e
(w) assigns zero probability to w 6? r
e
.
From (1), we get
Pr[r | e] = Z(r)
?
w?r\r
e
(1 ? ?)P (w)·
?
w?r
e
((1 ? ?)P (w) + ?P
e
(w))
= Z(r)
?
w?r
(1 ? ?)P (w) ·
?
w?r
e
(
1 +
?
1 ? ?
P
e
(w)
P (w)
)
. (2)
Note that Eq. (2) appears similar to the formula
obtained in the language model approach for IR
(Hiemstra and Kraaij, 1998); the interpretation of
terms, however, is very different. For instance,
P (w) in our case is computed over the “query”
corpus whereas the analogous term (collection fre-
quency) in (Hiemstra and Kraaij, 1998) is com-
puted over the “document” corpus. As the “Food”
restaurant example in Section 1 suggests, using
the “document” frequency is undesirable. The use
of “query” corpus frequency arises naturally from
our generative story and also guides us to a differ-
ent way to estimate P (w); see Section 3.3.
3.2 Matching a review to an object
Given the above review language model (RLM),
we now state how to match a given review to an
object. According to our model, the most likely
object e
?
to have generated a review r is given by
e
?
= argmax
e
Pr[e | r] = argmax
e
Pr[e]
Pr[r]
·Pr[r | e].
In the absence of any information, we assume
a uniform distribution for Pr[e]. (Additional
information about objects, such as their rat-
ing/popularity, can be used to model Pr[e] more
accurately.) From this, we get
e
?
= argmax
e
Pr[r | e],
or equivalently,
e
?
= argmax
e
log Pr[r | e].
Since Z(r)
?
w?r
((1??)P (w)) is independent of
e, using (2), we have
e
?
= argmax
e
?
w?r
e
log
(
1 +
?
1 ? ?
P
e
(w)
P (w)
)
.
(3)
3.3 Estimating the parameters
We now describe how to estimate the parameters
of the model, namely, P (·), P
e
(·), and ?.
Recall that P (·) is the distribution of generic re-
view language. Ideally, for each review r, if we
know the component r
(e)
that came from the dis-
tribution P
e
(·) and the component r
(g)
that came
from P (·), then we can collect the r
(g)
compo-
nents of all the reviews in R, denoted as R
(g)
, and
estimate P (·) by the fraction of occurrences of w
in R
(g)
. More specifically, let c(w,R
(g)
) denote
the number of times w occurs in R
(g)
. With add-
one smoothing, we estimate
P (w) =
c(w,R
(g)
) + 1
?
w
?
c(w
?
,R
(g)
) + |V |
,
where |V | is the vocabulary size.
In reality, we only have access to r and not to the
components r
(e)
and r
(g)
. If we have an aligned
review corpus R
?
, where for each review r, we
know the true object e that generated it, we can
closely approximate r
(e)
with r
e
.
2
Let no-obj(R
?
)
be the set of processed reviews where for each
review-object pair (r, e), words in text(e) are re-
moved from r. By treating no-obj(R
?
) as an ap-
proximation of R
(g)
, we can compute P (w) in the
aforementioned manner. If we only have access
to a review collection R
?
with no object align-
ment, there are other ways to effectively approx-
imate R
(g)
; see Section 5.3 for more details.
Unlike P (·), we cannot learn an individual lan-
guage model P
e
(·) for each e, since we cannot ex-
pect to have training examples of reviews for each
possible object e in the dataset. Thus, we need
a simpler way to model P
e
(w). The most naive
way would be to assume a uniform distribution,
i.e., P
e
(w) = 1/|text(e)|. However, each word
2
There can be exceptions to this, e.g., review of a restau-
rant called “Tasty Bites” might use the word “tasty” from the
review language, but not to refer to the restaurant. Nonethe-
less, we believe these will be rare exceptions and will not
have significant effect in the estimation of P (·).
612
in text(e) may not be generated with equal prob-
ability. In our running example, consider the case
when text(e) contains the full name of the restau-
rant, i.e., “Casablanca Moroccan Restaurant.” A
review for this restaurant is more likely to choose
the word “Casablanca” than any other word to re-
fer to this restaurant since this is arguably more in-
formative than “Moroccan” or “Restaurant.” This
can be captured by using the frequency f
w
of the
word w in R or in {text(e) | e ? E}. For a suit-
able function g(w) that is inversely growing as f
w
(say, g(w) = log(1/f
w
)), we let
P
e
(w) =
g(w)
?
w
?
?text(e)
g(w
?
)
.
Alternatively, it is possible to construct models
where P
e
(w) is more directly estimated from the
data; in fact, one can also use suitable transla-
tion models to estimate P
e
(w) for w that may not
even occur in text(e) — this will help in cases
where reviews use an abbreviation such as “Casa”
or “CMR” to refer to our running example. Such
models require either fine-grained labeled exam-
ples or, as we show in (Dalvi et al., 2009), more
sophisticated estimation techniques.
It is tempting to assume that common words
such as “Restaurant” may not contribute towards
matching a review to an object and hence one can
conveniently set P
e
(w) = 0 for such words w.
(Such a list of words can easily be compiled using
a domain-specific stopword list.) This may hurt —
in our example, the presence of the word “Restau-
rant” in a review might help to disambiguate the
object of reference, if the listing were also to con-
tain a “Casablanca Moroccan Cafe”.
3.4 Properties of the model
Eq. (3) indicates that our method (denoted as
RLM) gives less importance to common words
with high P (w). This corresponds to the intuition
behind the standard tf-idf scheme. Why, then, do
we expect RLM to be more effective? Here, we
discuss the salient features of our method, con-
trasting it with tf-idf in particular.
First, we take a closer look at different ways to
apply tf-idf techniques to our setting. Since the
task is to find the most relevant object given a re-
view, a naive way to apply the standard tf-idf (de-
noted TFIDF) will treat each review to be the query
and each object to be a document and score docu-
ments using the standard tf-idf scoring. This, how-
ever, leads to severe problems since this computes
the inverse document scores over the object corpus
— recall the “Food” example in Section 1.
A more reasonable way to apply tf-idf is to
instead treat objects as queries and reviews as
documents for computing tf-idf scores (denoted
TFIDF
+
). For a word w, let Q(w) =
df(w)
N
,
where N is the number of reviews in the corpus
and df(w) is the number of reviews containing w.
Given a review r and an object e, the score of the
object is given by
?
w?r
e
log (1/Q(w)), and we
want to pick the object with the maximum score.
As we will discuss later, document-length nor-
malization (i.e., normalizing by object description
length so that a restaurant with a long name does
not get an unfair disadvantage) is still non-trivial
here.
As noted earlier, Eq. (3), used by RLM for
matching reviews with objects, has a striking re-
semblance to the TFIDF
+
scoring function. Both
have the form
e
?
= argmax
e
?
w?r
e
log f(w),
where for RLM,
f(w) = f
R
(w) = 1 +
?
1 ? ?
P
e
(w)
P (w)
,
and for TFIDF
+
,
f(w) = f
B
(w) =
1
Q(w)
.
In both cases, f(w) is monotonically decreas-
ing in the frequency of w in the corpus. How-
ever, there are several differences between the two
cases. We highlight some of them here, with the
aim of illustrating the power of our review lan-
guage model (RLM).
Object length normalization. First note that the
P
e
(w) term in f
R
(w) acts as an object length nor-
malizing term, i.e., it adds up to one for each
e and weighs down P (w) for objects with long
text(e). This also has the effect of penalizing re-
views that are missing critical words in the object
description. In contrast, f
B
(w) is unnormalized
with respect to the object length. The standard
document normalization techniques in IR do not
apply well to our setting since our “documents”
(i.e., object descriptions) are short. E.g., if the ob-
ject description contains only one token, the stan-
dard cosine-normalization technique (Salton et al.,
613
1975) will yield a normalized score of 1 irre-
spective of the token. Thus for a review contain-
ing the words “Food” and “Casablanca”, the stan-
dard normalization will yield the same score for a
restaurant named “Food” and a restaurant named
“Casablanca”, ignoring the fact that “Food” is
much more likely to be an object-independent
term. Note that this only becomes a problem when
the entire “document” is part of the match, which
rarely happens in an IR setting where the docu-
ments are typically much longer than the queries.
Indeed, in our experiments, we observe lower per-
formance when we apply cosine-normalization to
the tf-idf scores. On the other hand, in f
R
(w), the
P (w) term can still distinguish the two aforemen-
tioned objects even when P
e
(w) are equal.
Dampening. With ? < 1, f
R
(w) is effectively
a dampened version of
P
e
(w)
P (w)
. In other words, dif-
ferences between very frequent words and very in-
frequent words are somewhat smoothed out. In-
deed, if we modify TFIDF
+
by introducing a sim-
ilar dampening factor into f
B
(w), we observe im-
provement in its performance (Section 5.4).
Removingmentions of an object. Another differ-
ence is that in RLM, P (w) is estimated on reviews
with object mentions removed, since the model in-
dicate that P (w) accounts for object-independent
review language. In contrast, TFIDF
+
computes
Q(w) on full reviews. We illustrate the differ-
ence on the following example. Consider a review
that reads “. . .Maggiano’s has great Fondue.” If
“Maggiano’s” and “Fondue” both occur the same
number of times in the corpus, then they get the
same idf (i.e., Q(w)) score. In RLM, however,
“Maggiano’s” will get much smaller probability
in the generic review distribution P (·) than “Fon-
due”, since “Maggiano’s” almost always occurs in
reviews as restaurant name mentions, thus is re-
moved from the estimation of its P (·) probabil-
ity. On the other hand, the word “Fondue” is more
likely to retain higher probability in P (·) since it
tends to appear as dish names. As a result, our
model will assign higher weight to “Maggiano’s
Restaurant” than “Fondue Restaurant”. As we can
see, RLM evaluates the ability of a word to identify
the review object rather than rely on the absolute
rarity of the word, which is done by tf-idf.
Using term counts. One last difference is that
f
R
(w) uses term counts of words rather than the
standard document counts used by f
B
(w). Our
evaluation suggests that at least in practice, this
does not have a big impact on the overall accuracy.
In the experiments we show that these factors
together account for the performance difference
between RLM and tf-idf. Our model gives a prin-
cipled way to introduce these factors, however.
4 Data
In this section we describe the dataset constructed
for the task of matching restaurant reviews to the
corresponding restaurant objects. Our goal is to
obtain a large collection of reviews on which to
estimate the generic language model, with a sig-
nificant portion of them aligned with the objects
for which the reviews were written; this portion
will serve as the gold-standard test set.
To this end, we obtained a set of reviews from
the Yelp website, yelp.com. This website con-
tains a collection of reviews about various busi-
nesses and for each business, has a webpage con-
taining the business information and a list of re-
views. We crawled all restaurant pages from Yelp.
For each restaurant, we extracted its name and
city location from the business information sec-
tion via HTML cues, and a list of no more than
40 reviews. We obtained the textual content of
299,762 reviews, each aligned with one of a set
of 12,408 unique restaurants hosted on Yelp. Note
that while our technique is not targeted for head
sites like Yelp (where wrapper induction might
be a more accurate approach), this provides a
large-scale dataset, conveniently labeled with ob-
ject information, and simulates the tail-site sce-
nario where we rely heavily on the textual content
of reviews to identify objects.
Many of the reviews in Yelp do not contain any
identifying information. In fact, some of them are
as short as “Great place. Awesome food!!”. We
processed the dataset to retain only reviews that
mention the name of the restaurant, even if par-
tially, and, when the restaurant name is a common
word, also the city of the restaurant. Each of the
remaining reviews is expected to have enough in-
formation for a human to identify the restaurant
corresponding to the review.
To further increase the difficulty of the match-
ing task, we obtained a much more extensive list
of restaurant objects in the Yahoo! Local database,
which contains 681,320 restaurants. Our task
is to match a given Yelp review, using only its
free-form textual content, with its corresponding
614
restaurant in the Yahoo! Local database. We then
proceeded to generate the gold standard that con-
tains the correct restaurant in the Yahoo! Local
database for each review. We employed geocoding
to match addresses across the two databases along
with approximate name matches. Note that in the
final dataset, only half of the restaurants have the
exact same name listed in both Yelp and Yahoo!
Local; this limits the success of naive dictionary-
based methods.
The final aligned dataset contained 24,910 Yelp
reviews (R), covering 6,010 restaurants. We set
aside half of the reviews (R
?
) to estimate the mod-
els and the other half (R
test
) to evaluate our tech-
nique. We also set aside 1,000 reviews as devel-
opment set, on which we conducted initial exper-
iments. The total size of the test corpus, R
test
was 11,217. The splitting of R into R
?
, R
test
,
and the development set was done in such a way
that there are no overlapping restaurants between
them. Also, the reviews that were filtered out
because of lack of identifying information were
added back to R
?
for learning the review language
model, expandingR
?
to a total of 205,447 reviews.
5 Evaluation
In this section we evaluate our proposed review-
language based matching algorithm RLM.
5.1 Experimental considerations
Baseline system. We use the TFIDF and TFIDF
+
algorithms described in Section 3.4 as baseline
algorithms. Since we are comparing objects
that can have varying lengths, we tried the stan-
dard cosine-normalization techniques for docu-
ment length normalization. For reasons described
in Section 3.4, however, the normalization signif-
icantly lowered the accuracy. All the numbers re-
ported here are using tf-idf scores without normal-
ization.
Efficiency. For both RLM and the baseline algo-
rithms, it is impractical to compute the similar-
ity of a review with each object in the database.
Since all objects that do not intersect with the re-
view have a zero score, we built an inverted in-
dex to retrieve all objects containing a given word.
Even a simple inverted index can be very ineffi-
cient since for each review, words such as “Restau-
rant” or “Cafe” retrieve a substantial fraction of
the whole database. Hence, we further optimized
the index by looking at the document frequencies
of the words and considering word bigrams in ob-
ject descriptions. The index only retrieves ob-
jects that have a non-trivial overlap with the re-
view; e.g., an overlap of “Casablanca” is consid-
ered non-trivial while an overlap of “Restaurant”
is considered trivial. Once these candidates are re-
trieved, our scoring function takes into account all
overlapping tokens.
For the YELP dataset, the index returns an av-
erage of 200 restaurants for each review. This
points to the general difficulty of review match-
ing over a large corpus of objects, since a simple
dictionary-based named-entity recognition will hit
at least 200 objects for many reviews.
Experiment settings. For RLM, we conducted
initial experiments and performed parameter esti-
mation on the development data. The experimen-
tal settings we used for RLM are as follows: we
set g(w) = log(1/f
w
) for P
e
, where f
w
is esti-
mated on the review collection. P (w) is estimated
on all reviews in R
?
, where for each review, all to-
kens of its corresponding text(e), if present, are
removed, in order to approximate the generic re-
view language independent of e, as required by
our generative model. We estimate ? to be 0.002,
tuned on the development set; in our experiments,
we observe that the performance is not very sensi-
tive to ?.
5.2 Main results
In this section we present the main comparisons
between RLM and the baseline in details.
Performance measure. Our task resembles a
standard IR task in that our algorithm ranks can-
didate objects for a given review by their “about-
ness” level. Unlike a standard IR task, however,
we are not interested in retrieving multiple “rel-
evant” objects, as each review in our dataset has
only one single correct match from E . A review
match is correct if the top-1 prediction (i.e., e
?
) is
accurate. In what follows, we report the average
accuracy for various experimental settings. Note
that we can take the average accuracy over all re-
views (reported as micro-average), regardless of
which restaurants they are about; or we can first
compute the average for reviews about the same
restaurant, and report the average over all restau-
rants (macro-average). When not specified, we re-
port the micro-average.
Main comparisons. Table 1(a) summarizes the
main comparison. Our proposed algorithm RLM
615
Method Micro-avg. Macro-avg.
RLM 0.647 0.576
TFIDF
+
0.518 0.481
TFIDF 0.314 0.317
(a) Main comparison.
Method Micro-avg. Macro-avg.
RLM-UNIFORM 0.634 0.562
RLM-UNCUT 0.627 0.546
RLM-DECAP 0.640 0.573
(b) RLM variants.
Method Micro-avg. Macro-avg.
TFIDF
+
-N 0.586 0.523
TFIDF
+
-D 0.593 0.533
TFIDF
+
-O 0.522 0.488
TFIDF
+
-ND 0.628 0.549
TFIDF
+
-NDO 0.647 0.576
(c) TFIDF
+
variants.
Table 1: Average accuracy of the top-1 prediction
for various techniques. Micro-average computed
over 11,217 reviews inR
test
; macro-average com-
puted over 2,810 unique restaurants in R
test
.
clearly outperforms the TFIDF
+
baseline mea-
sured by either micro- or macro-average accuracy.
The standard TFIDF, as predicted, performs the
worst.
Some reviews can be particularly difficult to
match, which can be reflected in a low matching
score. Nonetheless, we predict the most likely ob-
ject. Suppose we impose a threshold and return
the most likely object only when its score is above
threshold, we can then compute precision and re-
call at different thresholds. Figure 1 presents the
precision–recall curve (using micro-average) for
both RLM and TFIDF
+
. Again, RLM clearly out-
performs TFIDF
+
across the board.
We then generalize the definition of accuracy
into accuracy@k: a review is considered as cor-
rectly matched if one of the top-k objects returned
is the correct match. We plot accuracy@k as a
function of k. While the gap between RLM and
TFIDF
+
is smaller as k increases, RLM clearly
outperforms TFIDF
+
for all k ? {1, . . . , 10}.
One final comparison is accuracy@1 as a func-
tion of the review length. Given our current set-
ting, longer reviews might be more difficult to
match since they may include more proper nouns
such as dish names and related restaurants, and
Figure 1: Precision–recall curve (of top one pre-
diction): RLM vs. TFIDF
+
baseline.
Figure 2: Accuracy@k (percentage of reviews
whose correct match is returned in one of its top-k
predictions): RLM vs. TFIDF
+
baseline.
Figure 3: Average accuracy of the top-1 prediction
for reviews with different length (on test set): RLM
vs. TFIDF
+
baseline.
616
yield a longer list of highly competitive candi-
date objects. Interestingly, the gap between RLM
and TFIDF
+
is much smaller for shorter reviews.
As reviews get longer, the performance of RLM
is relatively stable, whereas the performance of
TFIDF
+
drops down significantly.
5.3 Experimental choices for RLM
We now examine the experimental choices we
made for different components of RLM by defin-
ing the following variations of RLM.
RLM-UNIFORM: rather than setting g(w) =
log(1/f
w
) for P
e
, we use the uniform distribution
P
e
(w) = 1/|text(e)|. From the third line of Table
1 (b), there is a slight accuracy drop of ? 1.3%.
RLM-UNCUT: suppose we only have access to
a review corpus with no alignment to text(e), and
thus have to approximate P (w) by estimating it
on the set of original “un-cut” reviews, how much
does that affect our performance? As indicated in
the fourth row of Table 1 (b), this reduces accuracy
by about 2% on our test data.
RLM-DECAP: as an alternative way to deal with
lack of aligned data, we consider a variation of
the above algorithm by removing all the capital-
ized words from un-annotated reviews. Clearly,
this can result in both “over-cutting” and “under-
cutting” of true restaurant name mentions. How-
ever, as indicated in the fourth row of Table 1 (b),
this is very close to the best accuracy achieved.
Thus, an effective model can be learned even with-
out aligned data.
5.4 Revisiting TFIDF
+
: what’s amiss?
In this section we revisit the main differences be-
tween our model and the TFIDF
+
outlined in Sec-
tion 3.4, and investigate their empirical impor-
tance by introducing these features into TFIDF
+
and examine their effectiveness in that framework.
Object length normalization. We con-
sider a modified TFIDF
+
measure f
M
(w) =
P
e
(w)/Q(w), which we call TFIDF
+
-N (normal-
ized). As shown in Table 1 (c), this change alone
can increase the average accuracy by nearly 7%.
Dampening. We consider a modified TFIDF
+
measure f
M
(w) = 1 + ? ·
N
df(w)
, which we call
TFIDF
+
-D. Table 1 (c) reports the performance of
using this measure, with ? = 0.1 (set on develop-
ment data). Again, this measure alone can induce
over 7% increase in accuracy. Indeed, combin-
ing normalization and dampening, (i.e., f
M
(w) =
1+? ·P
e
(w) ·
N
df(w)
), denoted as TFIDF
+
-ND, we
get comparable performance to RLM-UNCUT.
Removing mentions of objects. Again, we can
incorporate this in a heuristic way in TFIDF
+
,
which we denote by TFIDF
+
-O. Interestingly,
while using the original f
B
(w) function with
df(w) computed on the object-removed review
collection does not yield a big improvement, this
does bring the performance of the fully modified
TFIDF
+
to the same level of the standard RLM
(see line marked TFIDF
+
-NDO.)
Using term counts. Our investigation suggests
that at least in practice, using Q(w) vs. P (w) is
not a critical decision, as a fully modified TFIDF
+
can achieve the same performance using df(w) to
quantify frequency of the word. Our experiments
on this dataset show that each of the other model-
ing decisions incorporated in RLM is important.
6 Conclusions
We proposed a generative model for reviews
where reviews are generated from the mixture of
a distribution involving object terms and a generic
review language model. The model provides us
a principled way to match reviews to objects.
Our evaluation on a real-world dataset shows that
our techniques vastly outperforms standard tf-idf
based techniques.
Acknowledgments
We thank Don Metzler for many discussions and
the anonymous reviewers for their comments.
References
R. Ananthakrishna, S. Chaudhuri, and V. Ganti. 2002.
Eliminating fuzzy duplicates in data warehouses. In
Proc. 28th VLDB, pages 586–596.
L. Barbosa, R. Kumar, B. Pang, and A. Tomkins. 2009.
For a few dollars less: Identifying review pages sans
human labels. In Proc. NAACL.
I. Bhattacharya and L. Getoor. 2007. Collective entity
resolution in relational data. ACM TKDD, 1(1).
C. Cardie. 1997. Empirical methods in information
extraction. AI Magazine, 18(4):65–80.
V. T. Chakaravarthy, H. Gupta, P. Roy, and M. Mo-
hania. 2006. Efficiently linking text documents
with relevant structured information. In Proc. 32nd
VLDB, pages 667–678.
617
N. Dalvi, R. Kumar, B. Pang, and A. Tomkins. 2009.
A translation model for matching reviews to objects.
Manuscript.
I. P. Fellegi and A. B. Sunter. 1969. A theory for record
linkage. JASIS, 64:1183–1210.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at
TREC7: Ad-hoc and cross-language track. In Proc.
7th TREC, pages 174–185.
D. Hiemstra. 1998. A linguistically motivated prob-
abilistic model of information retrieval. In Proc.
ECDL, pages 569–584.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proc. AAAI, pages 755–760.
D. V. Kalashnikov, S. Mehrotra, and Z. Chen. 2005.
Exploiting relationships for domain-independent
data cleaning. In Proc. 5th SDM.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proc. 1st IJCNLP,
pages 596–605.
J. Lafferty and C. Zhai. 2003. Probabilistic relevance
models based on document and query generation. In
W. B. Croft and J. Lafferty, editors, Language Mod-
eling and Information Retrieval. Academic Publish-
ers.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. 17th NIPS.
H. B. Newcombe, J. M. Kennedy, S. J. Axford, and
A. P. James. 1959. Automatic linkage of vital
records. Science, 130:954–959.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proc. 21st COLING/44th ACL, pages 611–
618.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.
J. M. Ponte and W. B. Croft. 1998. A language model-
ing approach to information retrieval. In Proc. 21st
SIGIR, pages 275–281.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT/EMNLP.
G. Salton, A. Wong, and C. S. Yang. 1975. A vec-
tor space model for automatic indexing. Commun.
ACM, 18(11):613–620.
S. Sarawagi. 2008. Information extraction. Founda-
tions and Trends in Databases, 1(3):261–377.
F. Song and W. B. Croft. 1999. A general language
model for information retrieval. In Proc. 22nd SI-
GIR, pages 279–280.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proc. COLING.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extrating sentiments about a
given topic. In Proc. 3rd ICDM, pages 427–434.
C. Zhai and J. Lafferty. 2004. A study of smoothing
methods for language models applied to information
retrieval. ACM TOIS, 22(2):179–214.
C. Zhai. 2008. Statistical language models for infor-
mation retrieval a critical review. Foundations and
Trends in Information Retrieval, 2(3):137–213.
618
