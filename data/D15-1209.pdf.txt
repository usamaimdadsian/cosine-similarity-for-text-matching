Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1817–1827,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Leave-one-out Word Alignment without Garbage Collector Effects
Xiaolin Wang Masao Utiyama Andrew Finch
Taro Watanabe? Eiichiro Sumita
Advanced Translation Research and Development Promotion Center
National Institute of Information and Communications Technology, Japan
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
tarow@google.com
Abstract
Expectation-maximization algorithms,
such as those implemented in GIZA++
pervade the field of unsupervised word
alignment. However, these algorithms
have a problem of over-fitting, leading to
“garbage collector effects,” where rare
words tend to be erroneously aligned
to untranslated words. This paper
proposes a leave-one-out expectation-
maximization algorithm for unsupervised
word alignment to address this prob-
lem. The proposed method excludes
information derived from the alignment
of a sentence pair from the alignment
models used to align it. This prevents
erroneous alignments within a sentence
pair from supporting themselves. Ex-
perimental results on Chinese-English
and Japanese-English corpora show that
the F
1
, precision and recall of alignment
were consistently increased by 5.0% –
17.2%, and BLEU scores of end-to-end
translation were raised by 0.03 – 1.30.
The proposed method also outperformed
l
0
-normalized GIZA++ and Kneser-Ney
smoothed GIZA++.
1 Introduction
Unsupervised word alignment (WA) on bilingual
sentence pairs serves as an essential foundation
for building most statistical machine translation
(SMT) systems. A lot of methods have been pro-
posed to raise the accuracy of WA in an effort to
improve end-to-end translation quality. This pa-
per contributes to this effort through refining the
widely used expectation-maximization (EM) algo-
rithm for WA (Dempster et al., 1977; Brown et al.,
1993b; Och and Ney, 2000).
? The author now is affiliated with Google, Japan.
The EM algorithm for WA has a great influ-
ence in SMT. Many well-known toolkits includ-
ing GIZA++ (Och and Ney, 2003), the Berkeley
Aligner (Liang et al., 2006; DeNero and Klein,
2007), Fast Align (Dyer et al., 2013) and SyM-
GIZA++ (Junczys-Dowmunt and Sza, 2012), all
employ this algorithm. GIZA++ in particular is
frequently used in systems participating in many
shared tasks (Goto et al., 2011; Cettolo et al.,
2013; Bojar et al., 2013).
However, the EM algorithm for WA is well-
known for introducing “garbage collector ef-
fects.” Rare words have a tendency to collect
garbage, that is they have a tendency to be erro-
neously aligned to untranslated words (Brown et
al., 1993a; Moore, 2004; Ganchev et al., 2008;
V Grac¸a et al., 2010). Figure 1(a) shows a real
sentence pair, denoted s, from the GALE Chinese-
English Word Alignment and Tagging Training
corpus (GALE WA corpus)1 with it’s human-
annotated word alignment. The Chinese word
“HE ZHANG,” denoted w
r
, which means river
custodian, only occurs once in the whole corpus.
We performed EM training using GIZA++ on this
corpus concatenated with 442,967 training sen-
tence pairs from the NIST Open Machine Trans-
lation (OpenMT) 2006 evaluation2. The resulting
alignment is shown in Figure 1(b). It can be seen
that w
r
is erroneously aligned to multiple English
words.
To find the cause of this, we checked the align-
ments in each iteration i of s, denoted ai
s
. We
found that in a1
s
, w
r
together with the other
source-side words were aligned with uniform
probability to all the target-side words since the
alignment models provided no prior information.
However, in a2
s
, w
r
became erroneously aligned,
1Released by Linguistic Data Consortium, catalog
number LDC2012T16, LDC2012T20, LDC2012T24 and
LDC2013T05.
2http://www.itl.nist.gov/iad/mig/
tests/mt/2006/
1817
because the alignment distribution3 of w
r
was
only learned from a1
s
, thus consisted of non-zero
values only for generating the target-side words in
s. Therefore, the alignment probabilities from the
rare word w
r
to the unaligned words in s were ex-
traordinarily high, since almost all of the proba-
bility mass was distributed among them. In other
words, the story behind these garbage collector ef-
fects is that erroneous alignments are able to pro-
vide support for themselves; the probability distri-
bution learned only from s is re-applied to s. In
this way, these “garbage collector effects” are a
form of over-fitting.
Motivated by this observation, we propose a
leave-one-out EM algorithm for WA in this pa-
per. Recently this technique has been applied
to avoid over-fitting in kernel density estima-
tion (Roux and Bach, 2011); instead of performing
maximum likelihood estimation, maximum leave-
one-out likelihood estimation is performed. Fig-
ure 1(c) shows the effect of using our technique
on the example. The garbage collection has not
occurred, and the alignment of the word “HE
ZHANG” is identical to the human annotation.
2 Related Work
The most related work to this paper is train-
ing phrase translation models with leave-one-out
forced alignment (Wuebker et al., 2010; Wuebker
et al., 2012). The differences are that their work
operates at the phrase level, and their aim is to im-
prove translation models; while our work operates
at the word level, and our aim is to provide better
word alignment. As word alignment is a founda-
tion of most MT systems, our method have a wider
application.
Recently, better estimation methods during the
maximization step of EM have been proposed
to avoid the over-fitting in WA, such as using
Kneser-Ney Smoothing to back-off the expected
counts (Zhang and Chiang, 2014) or integrating
the smoothed l
0
prior to the estimation of prob-
ability (Vaswani et al., 2012). Our work differs
from theirs by addressing the over-fitting directly
in the EM algorithm by adopting a leave-one-out
approach.
Bayesian methods (Gilks et al., 1996; Andrieu
et al., 2003; DeNero et al., 2008; Neubig et al.,
3The probability distribution of generating target lan-
guage words from w
r
. The description here is only based on
IBM model1 for simplicity, and the other alignment models
are similar.
     	

	

 	
 
 

(a)
     	

		
 
 	
	 

(b)
     	

	

 	
 
 

(c)
Figure 1: Examples of supervised word alignment.
(a) gold alignment; (b) standard EM (GIZA++);
(c) Leave-one-out alignment (proposed).
2011), also attempt to address the issue of over-
fitting, however EM algorithms related to the pro-
posed method have been shown to be more effi-
cient (Wang et al., 2014).
3 Methodology
This section first formulates the standard EM for
WA, then presents the leave-one-out EM for WA,
and finally briefly discusses handling singletons
and effecient implementation. The main notation
used in this section is shown in Table 1.
3.1 Standard EM for IBM Models 1, 2 and
HMM Model
To perform WA through EM, the parallel corpus
is taken as observed data, the alignments are taken
as latent data. In order to maximize the likelihood
of the alignment model ? given the data S, the fol-
lowing two steps are conducted iteratively (Brown
et al., 1993b; Och and Ney, 2000; Och and Ney,
2003),
Expectation Step (E step): calculating the con-
ditional probability of alignments for each sen-
tence pair,
P (a|s, ?) =
?
J
j=1
?ali(aj |aj?1, I)?lex(fj |ea
j
),(1)
where ?ali(i|i?, I) is the alignment probability and
?lex(f |e) is the translation probability. Note that
1818
f a foreign sentence (f
1
, . . . , f
J
)
e an English sentence (e
1
, . . . , e
I
)
s a sentence pair (f , e)
a an alignment (a
1
, . . . , a
J
) where f
j
is
aligned to e
a
j
B
i
a list of the indexes of the foreign words
which are aligned to e
i
B
i,k
the index of the k-th foreign word
which is aligned to e
i
B
i
is the average of all elements in B
i
?
i
the largest index of an English word
s.t . ?
i
< i and |B
?
i
| > 0
?
i
the fertility of e
i
E
i
the word class of e
i
?
·
an probabilistic model
?
¯
s
·
a leave-one-out probabilistic model for
s
n
x
(s,a) the number of times that an event x
happens in (s,a)
N
x
(s) the marginal number of times that an
event x happens in s
Table 1: Main Notation. Note that N
x
(s) =
?
a
n
x
(s,a)P (a|s). In practical calculation, for
IBM models 1, 2 and HMM model, this summa-
tion is performed by dynamic programming; for
IBM model 4, it is performed approximately us-
ing the best alignment and its neighbors.
(1) is a general form for IBM model 1, model 2
and the HMM model.
Maximization step (M step): re-estimating the
probability models,
?ali(i|i
?
, I) ?
?
s
N
i|i
?
,I
(s)
?
s
N
i
?
,I
(s)
(2)
?lex(f |e) ?
?
s
N
f |e
(s)
?
s
n
e
(s)
(3)
where N
i
?
,I
(s) is the marginal number of times e
i
?
is aligned to some foreign word if the length of e is
I , or 0 otherwise; N
i|i
?
,I
(s) is the marginal number
of times the next alignment position after i? is i in
a if the length of e is I , or 0 otherwise; n
e
(s) is the
count of e in e; N
f |e
(s,a) is the marginal number
of times e is aligned to f .
3.2 Leave-one-out EM for IBM Models 1, 2
and HMM Model
Leave-one-out EM for WA differs from standard
EM in the way the alignment and translation prob-
abilities are calculated. Each sentence pair will
have its own alignment and translation probability
models calculated by excluding the sentence pair
itself. More formally, leave-one-out EM for WA
are formulated as follows,
Leave-one-out E step: employing leave-one-
out models for each s to calculate the conditional
probability of alignments
P (a|s, ?
¯
s
) =
?
J
j=1
?
¯
s
ali(aj |aj?1, I)?
¯
s
lex(fj |eaj ),(4)
where ?¯sali(i|i?, I) and ?¯slex(fj |eaj ) are the leave-
one-out alignment probability and translation
probability, respectively.
Leave-one-out M step: re-estimating leave-
one-out probability models,
?
¯
s
ali(i|i
?
, I) ?
?
s
?
6=s
N
i|i
?
,I
(s
?
)
?
s
?
6=s
N
i
?
,I
(s
?
)
(5)
?
¯
s
lex(f |e) ?
?
s
?
6=s
N
f |e
(s
?
)
?
s
?
6=s
n
e
(s
?
)
. (6)
3.3 Standard EM for IBM Model 4
The framework of the standard EM for IBM
Model 4 is similar with the one for IBM Models 1,
2 and HMM Model, but the calculation of align-
ment probability is more complicated.
E step: calculating the conditional probabil-
ity through the reverted alignment (Och and Ney,
2003),
P (a|s, ?) = P (B
0
|B
1
, . . . , B
I
)·
I
?
i=1
P (B
i
|B
i?1
, e
i
) ·
I
?
i=1
?
j?B
i
?lex(fj |ei), (7)
where B
0
means the set of foreign words aligned
with the empty word; P (B
0
|B
1
, . . . , B
I
) is as-
sumed to be a binomial distribution for the size
of B
0
(Brown et al., 1993b) or an modified distri-
bution to relieve deficiency (Och and Ney, 2003).
The distribution P (B
i
|B
i?1
, e
i
) is decomposed
as
P (B
i
|B
i?1
, e
i
) = ?fer(?i|ei)·
?hea(Bi,1 ?B?
i
|E
?
i
) ·
?
i
?
k=2
?oth(Bi,k ?Bi,k?1),
(8)
where ?fer is a fertility model; ?hea is a probabil-
ity model for the head (first) aligned foreign word;
?oth is a probability model for the other aligned
foreign words. ?hea is assumed to be conditioned
1819
on the word class E
?
i
, following the paper of
(Och and Ney, 2003) and the implementation of
GIZA++ and CICADA.
M step: re-estimating the probability models,
?fer(?|e) ?
?
s
N
?|e
(s)
?
s
?
?
?
N
?
?
|e
(s)
(9)
?hea(?i|E) ?
?
s
N
hea
?i|E
(s)
?
s
?
?i
?
N
hea
?i
?
|E
(s)
(10)
?oth(?i) ?
?
s
N
oth
?i
(s)
?
s
?
?i
?
N
oth
?i
?
(s)
, (11)
where ?i is a difference of the indexes of two for-
eign words.
3.4 Leave-one-out EM for IBM Model 4
The leave-one-out treatment were applied to the
three component probability models ?fer, ?hea and
?oth of IBM model 4.
Leave-one-out E step: calculating the condi-
tional probability through leave-one-out probabil-
ity models
P (a|s, ?
¯
s
) = P (B
0
|B
1
, . . . , B
I
)·
I
?
i=1
P
¯
s
(B
i
|B
i?1
, e
i
) ·
I
?
i=1
?
j?B
i
?
¯
s
lex(fj |ei), (12)
P
¯
s
(B
i
|B
i?1
, e
i
) = ?
¯
s
fer(?i|ei)·
?
¯
s
hea(Bi,1 ?B?
i
|E
?
i
) ·
?
i
?
k=2
?
¯
s
oth(Bi,k ?Bi,k?1).
(13)
Leave-one-out M step: re-estimating the leave-
one-out probability models,
?
¯
s
fer(?|e) ?
?
s
?
6=s
N
?|e
(s
?
)
?
s
?
6=s
?
?
?
N
?
?
|e
(s
?
)
(14)
?
¯
s
hea(?i|E) ?
?
s
?
6=s
N
hea
?i|E
(s
?
)
?
s
?
6=s
?
?i
?
N
hea
?i
?
|E
(s
?
)
(15)
?
¯
s
oth(?i) ?
?
s
?
6=s
N
oth
?i
(s
?
)
?
s
?
6=s
?
?i
?
N
oth
?i
?
(s
?
)
. (16)
3.5 Handling Singletons
Singletons are the words that occur only once in
corpora. Singletons cause problems when apply-
ing leave-one-out to lexicalized models such as the
translation model ?¯slex and the fertility model ?¯sfer.
When calculating (6) and (14) for singletons, the
denominators become zero, thus the probabilities
are undefined.
For singletons, there is no prior information to
guide their alignment, so we back off to uniform
distributions. In that case, the alignments are pri-
marily determined by the rest of the sentence.
In addition, singletons can be in the target side
of the translation model ?¯slex. In that case, the prob-
abilities become zero. This is handled by setting a
minimum probability value of 1.0× 10?12, which
was decided by pilot experiments.
3.6 Implementation Details
To alleviate memory requirements and increase
speed, our implementation did not build or store
the local alignment models explicitly for each sen-
tence pair. The following formula was used to effi-
ciently calculate (5), (6) and (14–16) to build tem-
porary probability models,
?
s
?
6=s
N
x
(s
?
) = (
?
s
?
N
x
(s
?
))?N
x
(s), (17)
where x is a alignment event. Our implemen-
tation maintained global counts of all alignment
events
?
s
?
N
x
(s
?
), and (considerably smaller) lo-
cal counts N
x
(s) from each sentence pair s.
Take the translation model ?¯slex for example. For
a sentence pair s = (f
1
. . . f
J
, e
1
. . . e
I
), it is cau-
clulated as,
?
¯
s
lex(fj |ei) =
(
?
s
?
N
(f
j
|e
i
)
(s
?
))?N
(f
j
|e
i
)
(s)
(
?
s
?
n
e
i
(s
?
))? n
e
i
(s)
.
(18)
The global counts to be maintained are
?
s
?
N
(f
j
|e
i
)
(s
?
) and n
e
i
(s
?
), and the local counts
are
?
s
N
(f
j
|e
i
)
(s) and n
e
i
(s). Therefore the
memory cost is,
|E| · (|F|+ 1) +
?
s
I
s
(J
s
+ 1), (19)
where |E| is the size of English vocabulary, |F| is
the size of foreign language vocabulary, I
s
is the
length of the English sentence of s, and J
s
is the
length of the foreign sentence of s.
The calculation of the leave-one-out translation
model is performed for each English word and for-
eign word in s. Therefore, the time cost is,
?
s
I
s
(J
s
+ 1). (20)
1820
In addition, because the local counts N
(f
j
|e
i
)
(s)
and n
e
i
(s) are read in order, storing them in a ex-
ternal memory such as a hard disk will not slow
down the running speed much. This will reduce
the memory cost to
|E| · (|F|+ 1). (21)
This cost is independent to the number of sentence
pairs4.
The speed of the proposed method can be
boosted through parallelism. These calculations
on each sentence pair can be performed indepen-
dently. We found empirically that when our im-
plementation of the proposed method is run on a
16-core computer, it finishes the task earlier than
GIZA++5.
4 Experiments
The proposed WA method was tested on two
language pairs: Chinese-English and Japanese-
English (Table 2). Performance was measured
both directly using the agreement with reference
to manual WA annotations, and indirectly using
the BLEU score in end-to-end machine translation
tasks. GIZA++ and our own implementation of
standard EM were used as baselines.
4.1 Experimental Settings
The Chinese-English experimental data consisted
of the GALE WA corpus and the OpenMT cor-
pus. They are from the same domain, both con-
tain newswire texts and web blogs. The OpenMT
evaluation 2005 was used as a development set for
MERT tuning (Och, 2003), and the OpenMT eval-
uation 2006 was used as a test set. The Japanese-
English experimental data was the Kyoto Free
Translation Task (Neubig, 2011)6. The corpus
contains a set of 1,235 sentence pairs that are man-
ually word aligned.
The corpora were processed using a standard
procedure for machine translation. The English
texts were tokenized with the tokenization script
released with Europarl corpus (Koehn, 2005) and
converted to lowercase; the Chinese texts were
segmented into words using the Stanford Word
Segmenter (Xue et al., 2002)7; the Japanese texts
4We found the memory of our server is large enough, so
we did not implement it
5We plan to make our code public available.
6http://www.phontron.com/kftt/
7http://nlp.stanford.edu/software/
segmenter.shtml
were segmented into words using the Kyoto Text
Analysis Toolkit (KyTea8). Sentences longer than
100 words or those with foreign/English word
length ratios between larger than 9 were filtered
out.
GIZA++ was run with the default Moses set-
tings (Koehn et al., 2007). The IBM model 1,
HMM model, IBM model 3 and IBM model 4
were run with 5, 5, 3 and 3 iterations. We imple-
mented the proposed leave-one-out EM and stan-
dard EM in IBM model 1, HMM model and IBM
model 4. In the original work (Och and Ney, 2003)
this combination of models achieved comparable
performance to the default Moses settings. They
were run with 5, 5 and 6 iterations.
The standard EM was re-implemented as a
baseline to provide a solid basis for comparison,
because GIZA++ contains many undocumented
details. Our implementation is based on the toolkit
of CICADA (Watanabe and Sumita, 2011; Watan-
abe, 2012; Tamura et al., 2013)9. We named the
implemented aligner AGRIPPA, to support our in-
house decoders OCTAVIAN and AUGUSTUS.
In all experiments, WA was performed indepen-
dently in two directions: from foreign languages
to English, and from English to foreign languages.
Then the grow-diag-final-and heuristic was used to
combine the two alignments from both directions
to yield the final alignments for evaluation (Och
and Ney, 2000; Och and Ney, 2003).
4.2 Word Alignment Accuracy
Word alignment accuracy of the baseline and the
proposed method is shown in Table 3 in terms of
precision, recall and F
1
(Och and Ney, 2003). The
proposed method gave rise to higher quality align-
ments in all our experiments. The improvement
in F
1
, precision and recall based on IBM Model
4 is in the range 8.3% to 9.1% compared with the
GIZA++ baseline, and in the range 5.0% to 17.2%
compared with our own baseline.
The most meaningful result comes from the
comparison of the models trained using standard
EM log-likelihood training, and the proposed EM
leave-one-out log-likelihood training. These mod-
els are identical except for way in which the model
likelihood is calculated. In all our experiments the
proposed method gave rise to higher quality align-
ments. The standard EM implementation achieved
8http://www.phontron.com/kytea/
9http://www2.nict.go.jp/univ-com/multi trans/cicada/
1821
Corpus # Sent. pairs # Foreign Words # English Words
Chinese-English (GALE WA, OpenMT)
WA 18,057 392,447 518,137
Train 442,967 12,265,072 13,444,927
Eval. 05 1,082† 29,688 138,952
Eval. 06 1,664† 37,827 189,059
Japanese-English (Kyoto Free Translation)
WA 1,235 34,403 30,822
Train 329,882 6,085,131 5,911,486
Develop 1,166 26,856 24,309
Test 1,160 28,501 26,734
Table 2: Experimental Data. † Each consists of one foreign sentence and four English reference sen-
tences.
Models standard EM (GIZA++) standard EM (ours) Leave-one-out(prop.)
F
1
P R F
1
P R F
1
P R
Chinese-English (GALE WA, OpenMT)
Model 1 0.498 0.656 0.401 0.518 0.670 0.423 0.553 0.689 0.461
HMM 0.584 0.720 0.491 0.593 0.722 0.503 0.665 0.774 0.583
Model 4 0.624 0.698 0.565 0.593 0.688 0.522 0.677 0.756 0.612
Japanese-English (Kyoto Free Translation)
Model 1 0.508 0.601 0.439 0.513 0.606 0.444 0.535 0.618 0.471
HMM 0.573 0.667 0.502 0.579 0.665 0.512 0.626 0.687 0.575
Model 4 0.577 0.594 0.561 0.570 0.617 0.530 0.628 0.648 0.609
Table 3: Word alignment accuracy measured by F
1
, precision and recall.
alignment performance approximately compara-
ble to GIZA++, whereas the proposed method ex-
ceeded the performance of both implementations.
4.3 End-to-end Translation Quality
BLEU scores achieved by the phrase-based and
hierachical SMT systems10 which were trained
from different alignment results, are shown in
Table 4. Each experiment was conducted three
times to mitigate the variance in the results due to
MERT. The results show that the proposed align-
ment method achieved the highest BLEU score in
all experiments. The improvement over the base-
line is in range 0.03 to 1.03 for phrase-based sys-
tems, and ranged from 0.43 to 1.30 for hierarchical
systems.
Hierarchical systems benifit more from the pro-
posed method than phrase-based systems. We
think this is because that hierarchical systems are
more sensitive to word alignment quality than
phrase-based systems. Phrase-based systems only
10from the Moses toolkit
0.
40
0.
45
0.
50
0.
55
0.
60
0.
65
0.
70
Size of training corpora (Log)
W
o
rd
 a
lig
nm
en
t F
1
1k 4k 18k 64k 256k 461k
Norm. EM (Giza++)
Norm. EM (our)
Leave?one?out EM (prop.)
Figure 2: Curve of word alignment accuracy (F
1
)
under training corpora of different sizes.
1822
SMT Systems standard EM (GIZA++) standard EM (ours) Leave-one-out (prop.)
Chinese-English (GALE WA, OpenMT)
Phrase-based 31.85 ± 0.26 31.01 ± 0.18 32.04 ± 0.08
Hierarchical 32.27 ± 0.23 31.40 ± 0.26 32.70 ± 0.14
Japanese-English (Kyoto Free Translation)
Phrase-based 18.35 ± 0.27 18.20 ± 0.20 18.38 ± 0.11
Hierarchical 19.48 ± 0.08 19.39 ± 0.02 20.10 ± 0.07
Table 4: End-to-end translation quality measured by BLEU
Corpus size standard EM (GIZA++) standard EM (ours) Leave-one-out(prop.)
F
1
P R F
1
P R F
1
P R
1K 0.429 0.466 0.397 0.419 0.463 0.382 0.470 0.568 0.402
4K 0.499 0.547 0.459 0.492 0.549 0.445 0.568 0.668 0.494
18K† 0.571 0.630 0.521 0.553 0.621 0.499 0.633 0.721 0.565
64K 0.588 0.659 0.531 0.555 0.638 0.492 0.645 0.712 0.590
256K 0.614 0.687 0.554 0.578 0.667 0.511 0.661 0.718 0.612
461K 0.624 0.698 0.565 0.593 0.688 0.522 0.677 0.756 0.612
Table 5: Effect of training corpus size on word alignment accuracy measured by F
1
, precision and recall
(Chinese-English). † the whole manually word aligned corpus
Corpus size stan.(GIZA++) stan.(ours) LOO(prop.) Gold
Phrase-based
1k 7.86 7.66 9.38 10.01
4k 15.27 15.49 17.06 17.57
18K† 22.15 21.72 24.41 24.11
64K 28.10 27.91 29.23 NA
256K 31.05 30.82 31.51 NA
461K 31.85 31.01 32.04 NA
Hierarchical
1k 7.53 7.54 9.19 10.62
4k 14.89 15.51 17.91 18.31
18K† 22.85 22.56 24.66 24.52
64K 28.82 28.22 29.78 NA
256K 31.47 30.21 31.72 NA
461K 32.27 31.04 32.70 NA
Table 6: Effect of training corpus size on end-to-end translation quality measured by BLEU (Chinese-
English). † the whole manually word aligned corpus
take contiguous parallel phrase pairs as translation
rules, while hierarchical systems also use patterns
made by subtracting (inner) short parallel phrases
from (outer) longer parallel phrases. Both the
outer and inner phrases typically need to be noise-
free in order to produce high quality rules. This
puts a high demand on the alignment quality.
4.4 Effect of Training Corpus Size
Training corpora of different sizes were employed
to perform unsupervised WA experiments and MT
experiments (see Tables 5 and 6).
The training corpora were randomly sampled
from the Chinese-English manual WA corpora and
the parallel training corpus. The manual WA cor-
pus has a priority for being sampled so that the
gold WA annotation is available for MT experi-
1823
10
15
20
25
30
Size of training corpora (Log)
BL
EU
 (p
hra
se
?
ba
se
d)
1k 4k 18k 64k 256k 461k
Standard EM (GIZA++)
Standard EM (ours)
Leave?one?out EM (prop.)
Gold
(a)
10
15
20
25
30
Size of training corpora (Log)
BL
EU
 (H
ier
a
rc
hi
ca
l)
1k 4k 18k 64k 256k 461k
Standard EM (GIZA++)
Standard EM (ours)
Leave?one?out EM (prop.)
Gold
(b)
Figure 3: Curves of translation quality (BLEU) under training corpora of different sizes. (a) Phrase-based
MT; (b) Hierarchical MT.
ments.
The settings of the unsupervised WA experi-
ments and the MT experiments are the same with
the previous experiments. In the WA experiments,
GIZA++, our implemented standard EM and the
proposed leave-one-out EM are applied to training
corpora with the same parameter settings as the
previous. In the MT experiments, the WA results
of different methods and the gold WA (if available)
are employed to extract translation rules; the rest
settings including language models, development
and test corpus, and parameters are the same as the
previous.
On word alignment accuracy, the proposed
method achieved improvements of F
1
from 0.041
to 0.090 under the different training corpora (Table
5. The maximum improvement compared with
GIZA++ is 0.069 when the training corpus has
4,000 sentence pairs. The maximum improvement
compared with our own implement is 0.090 when
the training corpus has 64,000 sentence pairs.
Figure 2 shows that the extent of improvements
slightly changes under different training corpora,
but they are all quite stable and obvious.
On translation quality, the proposed method
achieved improvements of BLEU under the dif-
ferent training corpora. The improvements ranged
from 0.19 to 1.72 for phrase-based MT and ranged
from 0.25 to 3.02 (see Table 5). The improve-
ments are larger under smaller training corpora
(see Figure 3).
In addition, the BLEUs achieved by the pro-
posed method is close to the ones achieved by gold
WA annotations. The proposed method slightly
outperforms the gold WA annotations when us-
ing the full manual WA corpus of 18,057 sentence
pairs.
4.5 Comparison to l
0
-Normalization and
Kneser-Ney Smoothing Methods
The proposed leave-one-word word align-
ment method was empirically compared to
l
0
-normalized GIZA++ (Vaswani et al., 2012)11
and Kneser-Ney smoothed GIZA++ (Zhang and
Chiang, 2014)12. l
0
-normalization and Kneser-
Ney smoothing methods are established methods
to overcome the sparse problem. This enables
the probability distributions on rare words to be
estimated more effectively. In this way, these
two GIZA++ variants are related to the proposed
method.
l
0
-normalized GIZA++ and Kneser-Ney
smoothed GIZA++ were run with the same
settings as GIZA++, which came from the
default settings of MOSES. For the settings of
l
0
-normalized GIZA++ that are not in common
with GIZA++ were the default settings. As for
Kneser-Ney smoothed GIZA++, the smooth
switches of IBM models 1 – 4 and HMM model
11http://www.isi.edu/
˜
avaswani/
giza-pp-l0.html
12https://github.com/hznlp/giza-kn
1824
GIZA++ l
0
-Normalization Kneser-Ney Smooth. Leave-one-out(prop.)
Word Alignment Quality
F
1
P R F
1
P R F
1
P R F
1
P R
All Words 0.624 0.698 0.565 0.629 0.700 0.571 0.656 0.726 0.599 0.678 0.755 0.615
S.W.F=1 0.458 0.435 0.483 0.448 0.471 0.427 0.515 0.532 0.499 0.398 0.693 0.279
S.W.F?2 0.466 0.451 0.481 0.461 0.485 0.440 0.522 0.545 0.501 0.450 0.707 0.330
S.W.F?5 0.476 0.480 0.473 0.478 0.509 0.451 0.534 0.572 0.501 0.502 0.722 0.385
S.W.F?10 0.485 0.505 0.466 0.491 0.531 0.456 0.541 0.593 0.498 0.529 0.733 0.414
Translation Quality (BLEU)
Phrase-based 31.85 ± 0.26 31.52 ± 0.06 31.94 ± 0.19 32.04 ± 0.08
Hierarchical 32.27 ± 0.23 32.20 ± 0.04 32.47 ± 0.33 32.70 ± 0.14
Table 7: Empirical Comparision with l
0
-Normalized and Kneser-Ney Smoothed GIZA++’s
were turned on.
The experimental results are presented in Ta-
ble 7. The experiments were run on the Chinese-
English language pair. The word alignment qual-
ity was evaluated separately for all words and for
various levels of rare words. The leave-one-out
method outperformed related methods in terms
of precision, recall and F
1
when evaluated on all
words.
Rare words were categorized based on the num-
ber of occurences in the source-language text of
the training data. The evaluations were carried
out on the subset of alignment links that had a
rare word on the source side. Table 7 presents
the results for thresholds 1, 2, 5 and 10. The
proposed method achieved much higher preci-
sion on rare words than the other methods, but
performed poorly on recall. The Kneser-Ney
Smoothed GIZA++ had higher recall. The ex-
planation might be that the leave-one-out method
punishes rare words more than the Kneser-Ney
smoothing method, by totally removing the de-
rived expected counts of current sentence pair
from the alignment models. This leads to rare
words being passively aligned. In other words, the
leave-one-out method would align rare words un-
less the confidence is high. Therefore, we plan to
seek a method to integrate Kneser-Ney smoothing
into the proposed leave-one-out method in the fu-
ture work.
The BLEU scores achieved by phrase-based
SMT and hierarchical SMT for different align-
ment methods are presented in Table 7. The
proposed method outperforms the other methods.
The Kneser-Ney Smoothed GIZA++ performed
the second best. We tried to further analyze the
relation between word alignment and BLEU, but
found the analysis was obscured by the many
processing stages. These stages include paral-
lel phrase extraction (or translation rule extraction
from hierarchical SMT), log-linear model, MERT
tuning and practical decoding where a lot of prun-
ing happened.
5 Conclusion
This paper proposes a leave-one-out EM algo-
rithm for WA to overcome the over-fitting prob-
lem that occurs when using standard EM for WA.
The experimental results on Chinese-English and
Japanese-English corpora show that both the WA
accuracy and the end-to-end translation are im-
proved.
In addition, we have a interesting finding about
the effect of manual WA annotations on train-
ing MT systems. In a Chinese-English parallel
training corpus of 18,057 sentence pairs, the man-
ual WA annotation outperformed the unsupervised
WA results produced by standard EM algorithms.
However, the unsupervised WA results produced
by proposed leave-one-out EM algorithm outper-
formed the manual WA annotation.
Our future work will focus on increasing the
gains in end-to-end translation quality through the
proposed leave-one-out aligner. It is a interest-
ing question why GIZA++ achieved competitive
BLEU scores though its alignment accuracy mea-
sured by F
1
was substantially lower. The answer
to this question which may reveal essence of good
word alignment for MT and eventually help to im-
prove MT. In addition, we plan to improve the pro-
posed method by integrating Kneser-Ney smooth-
ing.
Acknowledgments
We appreciated the valuable comments from the
reviewers.
1825
References
Christophe Andrieu, Nando De Freitas, Arnaud
Doucet, and Michael I. Jordan. 2003. An intro-
duction to MCMC for machine learning. Machine
learning, 50(1-2):5–43.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1–44.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993a. But
dictionaries are data too. In Proceedings of the
Workshop on Human Language Technology, HLT
’93, pages 202–205, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993b.
The mathematics of statistical machine translation:
parameter estimation. Computational linguistics,
19(2):263–311.
Mauro Cettolo, Jan Niehues, Sebastian Stu¨ker, Luisa
Bentivogli, and Marcello Federico. 2013. Report
on the 10th IWSLT evaluation campaign. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, pages 29–38.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal
Statistical Society. Series B (Methodological), pages
1–38.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 17–24.
John DeNero, Alexandre Bouchard-Coˆte´, and Dan
Klein. 2008. Sampling alignment structure un-
der a bayesian translation model. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’08, pages 314–
323, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 644–648.
Kuzman Ganchev, Joao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations? Pro-
ceedings of the 46th Annual Meeting on Association
for Computational Linguistics, page 42.
Walter R. Gilks, Sylvia Richardson, and David J.
Spiegelhalter. 1996. Markov chain Monte Carlo in
practice, volume 2. CRC press.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559–578.
Marcin Junczys-Dowmunt and Arkadiusz Sza. 2012.
Symgiza++: Symmetrized word alignment mod-
els for machine translation. In Pascal Bouvry,
Mieczyslaw A. Klopotek, Franck Leprvost, Malgo-
rzata Marciniak, Agnieszka Mykowiecka, and Hen-
ryk Rybinski, editors, Security and Intelligent In-
formation Systems (SIIS), volume 7053 of Lecture
Notes in Computer Science, pages 379–390, War-
saw, Poland. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics on Interactive
Poster and Demonstration Sessions, pages 177–180.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit, volume 5, pages 79–86.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association of Computa-
tional Linguistics: Human Language Technologies,
pages 104–111. Association for Computational Lin-
guistics.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 518. Association for Computational
Linguistics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In ACL, pages 632–641.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of the 18th confer-
ence on Computational linguistics-Volume 2, pages
1086–1090. Association for Computational Linguis-
tics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
1826
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Nicolas Le Roux and Francis Bach. 2011. Local com-
ponent analysis. Technical report.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Proceedings of the 51th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 841–851.
Joa˜o V Grac¸a, Kuzman Ganchev, and Ben Taskar.
2010. Learning tractable word alignment models
with complex constraints. Computational Linguis-
tics, 36(3):481–504.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: unsupervised word alignment with the l
0
-
norm. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 311–319. Association
for Computational Linguistics.
Xiaolin Wang, Masao Utiyama, Andrew Finch, and Ei-
ichiro Sumita. 2014. Empirical study of unsuper-
vised chinese word segmentation methods for smt
on large-scale corpora. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
752–758, Baltimore, Maryland, June. Association
for Computational Linguistics.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
translation system combination by confusion forest.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1249–1257. Associ-
ation for Computational Linguistics.
Taro Watanabe. 2012. Optimized online rank learning
for machine translation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 253–262. Association for Computational Lin-
guistics.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475–484. Association for Computa-
tional Linguistics.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012. Leave-one-out phrase model training for
large-scale deployment. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
pages 460–467. Association for Computational Lin-
guistics.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of the 19th International Con-
ference on Computational Linguistics, pages 1–8.
Association for Computational Linguistics.
Hui Zhang and David Chiang. 2014. Kneser-ney
smoothing on expected counts. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 765–774, Baltimore, Maryland, June.
Association for Computational Linguistics.
1827
