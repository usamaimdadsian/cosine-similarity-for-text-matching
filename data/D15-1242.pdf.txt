Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2044–2048,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Specializing Word Embeddings for Similarity or Relatedness
Douwe Kiela, Felix Hill and Stephen Clark
Computer Laboratory
University of Cambridge
douwe.kiela|felix.hill|stephen.clark@cl.cam.ac.uk
Abstract
We demonstrate the advantage of special-
izing semantic word embeddings for either
similarity or relatedness. We compare two
variants of retrofitting and a joint-learning
approach, and find that all three yield spe-
cialized semantic spaces that capture hu-
man intuitions regarding similarity and re-
latedness better than unspecialized spaces.
We also show that using specialized spaces
in NLP tasks and applications leads to clear
improvements, for document classification
and synonym selection, which rely on ei-
ther similarity or relatedness but not both.
1 Introduction
Most current models of semantic word representa-
tion exploit the distributional hypothesis: the idea
that words occurring in similar contexts have sim-
ilar meanings (Harris, 1954; Turney and Pantel,
2010; Clark, 2015). Such representations (or em-
beddings) can reflect human intuitions about simi-
larity and relatedness (Turney, 2006; Agirre et al.,
2009), and have been applied to a wide variety
of NLP tasks, including bilingual lexicon induc-
tion (Mikolov et al., 2013b), sentiment analysis
(Socher et al., 2013) and named entity recognition
(Turian et al., 2010; Guo et al., 2014).
Arguably, one of the reasons behind the popu-
larity of word embeddings is that they are “gen-
eral purpose”: they can be used in a variety of
tasks without modification. Although this behav-
ior is sometimes desirable, it may in other cases be
detrimental to downstream performance. For ex-
ample, when classifying documents by topic, we
are particularly interested in related words rather
than similar ones: knowing that dog is associated
with cat is much more informative of the topic
than knowing that it is a synonym of canine. Con-
versely, if our embeddings indicate that table is
closely related to chair, that does not mean we
should translate table into French as chaise.
This distinction between “genuine” similarity
and associative similarity (i.e., relatedness) is
well-known in cognitive science (Tversky, 1977).
In NLP, however, semantic spaces are generally
evaluated on how well they capture both similar-
ity and relatedness, even though, for many word
combinations (such as car and petrol), these two
objectives are mutually incompatible (Hill et al.,
2014b). In part, this oversight stems from the dis-
tributional hypothesis itself: car and petrol do not
have the same, or even very similar, meanings,
but these two words may well occur in similar
contexts. Corpus-driven approaches based on the
distributional hypothesis therefore generally learn
embeddings that capture both similarity and relat-
edness reasonably well, but neither perfectly.
In this work we demonstrate the advantage of
specializing semantic spaces for either similar-
ity or relatedness. Specializing for similarity is
achieved by learning from both a corpus and a
thesaurus, and for relatedness by learning from
both a corpus and a collection of psychological as-
sociation norms. We also compare the recently-
introduced technique of graph-based retrofitting
(Faruqui et al., 2015) with a skip-gram retrofitting
and a skip-gram joint-learning approach. All three
methods yield specialized semantic spaces that
capture human intuitions regarding similarity and
relatedness significantly better than unspecialized
spaces, in one case yielding state-of-the-art results
for word similarity. More importantly, we show
clear improvements in downstream tasks and ap-
plications: specialized similarity spaces improve
synonym detection, while association spaces work
better than both general-purpose and similarity-
specialized spaces for document classification.
2 Approach
The underlying assumption of our approach is
that, during training, word embeddings can be
“nudged” in a particular direction by includ-
ing information from an additional semantic data
2044
source. For directing embeddings towards genuine
similarity, we use the MyThes thesaurus devel-
oped by the OpenOffice.org project
1
. It contains
synonyms for almost 80,000 words in English. For
directing embeddings towards relatedness, we use
the University of South Florida (USF) free asso-
ciation norms (Nelson et al., 2004). This dataset
contains scores for free association (an experi-
mental measure of cognitive association) of over
10,000 concept words. For raw text data we use a
dump of the English Wikipedia plus newswire text
(8 billion words in total)
2
.
2.1 Evaluations (Intrinsic and Extrinsic)
For instrinsic comparisons with human judge-
ments, we evaluate on SimLex (Hill et al.,
2014b) (999 pairwise comparisons), which explic-
itly measures similarity, and MEN (Bruni et al.,
2014) (3000 comparisons), which explicitly mea-
sures relatedness. We also consider two down-
stream tasks and applications. In the TOEFL
synonym selection task (Landauer and Dumais,
1997), the objective is to select the correct syn-
onym for a target word from a multiple-choice set
of possible answers. For a more extrinsic evalua-
tion, we use a document classification task based
on the Reuters Corpus Volume 1 (RCV1) (Lewis
et al., 2004). This dataset consists of over 800,000
manually categorized news articles.
3
2.2 Joint Learning
The standard skip-gram training objective for a se-
quence of training wordsw
1
, w
2
, ..., w
T
and a con-
text size c is the log-likelihood criterion:
1
T
T
?
t=1
J
?
(w
t
) =
1
T
T
?
t=1
?
?c?j?c
log p(w
t+j
|w
t
)
where p(w
t+j
|w
t
) is obtained via the softmax:
p(w
t+j
|w
t
) =
exp
u
>
w
t+j
v
w
t
?
w
?
exp
u
>
w
?
v
w
t
where u
w
and v
w
are the context and target vec-
tor representations for word w, respectively, and
w
?
ranges over the full vocabulary (Mikolov et al.,
1
https://www.openoffice.org/lingucomponent/thesaurus.html
2
The script for obtaining this corpus is available from
http://word2vec.googlecode.com/svn/trunk/demo-train-big-model-v1.sh
3
We exclude articles with multiple topic labels in order to
avoid multi-class document classification. The dataset con-
tains a total of 78 topic labels and 33,226 news articles.
2013a). For our joint learning approach, we sup-
plement the skip-gram objective with additional
contexts (synonyms or free-associates) from an
external data source. In the sampling condition,
for target word w
t
, we modify the objective to in-
clude an additional context w
a
sampled uniformly
from the set of additional contexts A
w
t
:
1
T
T
?
t=1
(
J
?
(w
t
) + [w
a
? U
A
w
t
] log p(w
a
|w
t
)
)
In the all condition, all additional contexts for a
target word are added at each occurrence:
1
T
T
?
t=1
?
?
J
?
(w
t
) +
?
w
a
?A
w
t
log p(w
a
|w
t
)
?
?
The set of additional contexts A
w
t
contains the
relevant contexts for a word w
t
; e.g., for the word
dog, A
dog
for the thesaurus is the set of all syn-
onyms of dog in the thesaurus.
2.3 Retrofitting
Faruqui et al. (2015) introduced retrofitting as a
post-hoc graph-based learning objective that im-
proves learned word embeddings. We experi-
ment with their method, calling it graph-based
retrofitting. In addition, we introduce a similar ap-
proach that instead uses the same objective func-
tion that was used to learn the original skip-gram
embeddings. In other words, we first train a stan-
dard skip-gram model, and then learn from the ad-
ditional contexts in a second training stage as if
they form a separate corpus:
1
T
T
?
t=1
?
w
a
?A
w
t
log p(w
a
|w
t
)
We call this approach skip-gram retrofitting. In
all cases, our embeddings have 300 dimensions,
which has been found to work well (Mikolov et
al., 2013a; Baroni et al., 2014)
3 Results for Intrinsic Evaluation
We compare standard skip-gram embeddings with
retrofitted and jointly learned specialized embed-
dings, as well as with “fitted” embeddings that
were randomly initialized and learned only from
the additional semantic resource. In each case, the
2045
Method SimLex-999 MEN
Skip-gram 0.31 0.68
Fit-Norms 0.08 0.14
Fit-Thesaurus 0.26 0.14
Joint-Norms-Sampled 0.43 0.72
Joint-Norms-All 0.42 0.67
Joint-Thesaurus-Sampled 0.38 0.69
Joint-Thesaurus-All 0.44 0.60
GB-Retrofit-Norms 0.32 0.71
GB-Retrofit-Thesaurus 0.38 0.68
SG-Retrofit-Norms 0.35 0.71
SG-Retrofit-Thesaurus 0.47 0.69
Table 1: Spearman ? on a genuine similarity
(SimLex-999) and relatedness (MEN) dataset.
training algorithm was run for a single iteration
(results from more iterations are presented later).
As shown in Table 1, embeddings that were
specialized for similarity using a thesaurus per-
form better on SimLex-999, and those special-
ized for relatedness using association data per-
form better on MEN. Fitting, or learning only from
the additional semantic resource without access
to raw text, does not perform well. Skip-gram
retrofitting with the thesaurus performs best on
SimLex-999; joint learning with sampling from
the USF norms performs best on MEN, although
the two retrofitting approaches are close. There
is an interesting difference between the two joint
learning approaches: while sampling a single
free associate as additional context works best
for relatedness, presenting all additional contexts
(synonyms) works best for similarity. In both
cases, skip-gram retrofitting matches or outper-
forms graph-based retrofitting.
More training iterations All the results above
were obtained using a single training iteration.
When retrofitting, however, it is easy to learn from
multiple iterations of the thesaurus or the USF
norms. The results are shown in Figure 1, where
the dashed lines are the joint learning and standard
skip-gram results for comparison with retrofitting
scores. As would be expected, too many iterations
leads to overfitting on the semantic resource, with
performance eventually decreasing after the ini-
tial increase. The results show that retrofitting is
particularly useful for similarity, as indicated by
the large increase in performance on SimLex-999.
The highest performance obtained, at 5 iterations,
is a Spearman ?
s
correlation of 0.53, which, as far
as we know, matches the current state-of-the-art.
4
For relatedness-specific embeddings, the effect
is less clear: joint learning performs compara-
tively much better. Retrofitting does outperform
it, at around 2-10 iterations on the USF norms,
but the improvement is marginal. The highest
retrofitting score is 0.74; the highest joint learn-
ing score is 0.72. Both are highly competitive re-
sults on MEN, and outperform e.g. GloVe at 0.71
(Pennington et al., 2014). Joint learning with a
thesaurus, however, leads to poor performance on
MEN, as expected: the embeddings get dragged
away from relatedness and towards similarity.
3.1 Curriculum learning?
The fact that joint learning works better when sup-
plementing raw text input with free associates, but
skip-gram retrofitting works better with additional
thesaurus information, could be due to curriculum
learning effects (Bengio et al., 2009). Unlike the
USF norms, many of the words from the thesaurus
are unusual and have low frequency. This suggests
that the thesaurus is more ‘advanced’ (from the
perspective of the learning model) than the USF
norms as an information source. Its information
may be detrimental to model optimization when
encountered early in training (in the joint learning
condition) because the model has not acquired the
basic concepts on which it builds. However, with
retrofitting the model first acquires good represen-
tations for frequent words from the raw text, after
which it can better understand, and learn from, the
information in the thesaurus.
4 Downstream Tasks and Applications
4.1 TOEFL Synonym Task
Unsupervised synonym selection has many appli-
cations including the generation of thesauri and
other lexical resources from raw text (Kageura et
al., 2000). In the well-known TOEFL evalua-
tion (Freitag et al., 2005) models are required to
identify true synonyms to question words from a
selection of possible answers. To test our models
on this task, for each question in the dataset, we
rank the multiple-choice answers according to the
cosine similarity between their word embeddings
and that of the target word, and choose the highest-
ranked option.
4
Hill et al. (2014a) obtain a score of 0.52 using neural
translation embeddings.
2046
Figure 1: Varying the number of iterations when retrofitting
Method TOEFL Doc
Skip-gram 77.50 83.96
Joint-Norms-Sampled 78.75 84.46
Joint-Norms-All 66.25 84.82
Joint-Thesaurus-Sampled 81.25 83.90
Joint-Thesaurus-All 80.00 83.56
GB-Retrofit-Norms 80.00 80.58
GB-Retrofit-Thesaurus 83.75 80.24
SG-Retrofit-Norms 80.00 84.56
SG-Retrofit-Thesaurus 88.75 84.55
Table 2: TOEFL synonym selection and docu-
ment classification accuracy (percentage of cor-
rectly answered questions/correctly classified doc-
uments).
As Table 2 shows, similarity-specialized em-
beddings perform much better than standard em-
beddings and relatedness-specialized embeddings.
Retrofitting outperforms joint learning, and skip-
gram retrofitting matches or outperforms graph-
based retrofitting.
4.2 Document Classification
To investigate how well the various semantic
spaces perform on document classification, we
first construct document-level representations by
summing the vector representations for all words
in a given document. After setting aside a small
development set for tuning the hyperparameters of
the supervised algorithm, we train a support vec-
tor machine (SVM) classifier with a linear kernel
and evaluate document topic classification accu-
racy using ten-fold cross-validation.
The results are reported in the rightmost col-
umn of Table 2. Relatedness-specialized embed-
dings perform better on document topic classi-
fication than similarity embeddings, except with
graph-based retrofitting, which in fact performs
below the standard skip-gram model. The joint-
learning model with all relevant free association
norms presented as context for each target word is
the best performing model. The differences in the
table appear small, but the dataset contains more
than 10,000 documents, so every percentage point
is worth more than 100 documents. Joint learning
while presenting all relevant association norms for
each target word performs best on this task.
5 Conclusion
We have demonstrated the advantage of special-
izing embeddings for the tasks of genuine simi-
larity and relatedness. In doing so, we compared
two retrofitting methods and a joint learning ap-
proach. Specialized embeddings outperform stan-
dard embeddings by a large margin on instrinsic
similarity and relatedness evaluations. We showed
that the difference in how embeddings are spe-
cialized carries to downstream NLP tasks, demon-
strating that similarity embeddings are better at
the TOEFL synonym selection task and related-
ness embeddings at a document topic classifica-
tion task. Lastly, we varied the number of itera-
tions that we use for retrofitting, showing that per-
formance could be improved even further by going
over several iterations of the semantic resource.
Acknowledgments
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St Johns College, Cambridge.
SC is supported by ERC Starting Grant DisCoTex
2047
(306920) and EPSRC grant EP/I037512/1. We
thank Yoshua Bengio, Kyunghyun Cho and Ivan
Vuli´c for useful discussions and the anonymous
reviewers for their helpful comments.
References
Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In NAACL,
pages 19–27.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th annual international confer-
ence on machine learning, pages 41–48. ACM.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.
Stephen Clark. 2015. Vector Space Models of Lexical
Meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, chapter 16.
Wiley-Blackwell, Oxford.
Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of NAACL.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distribu-
tional representations of synonymy. In Proceedings
of the Ninth Conference on Computational Natural
Language Learning, pages 25–32.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 110–120.
Zelig Harris. 1954. Distributional Structure. Word,
10(23):146—162.
Felix Hill, Kyunghyun Cho, S´ebastien Jean, Coline
Devin, and Yoshua Bengio. 2014a. Embedding
word similarity with neural machine translation.
CoRR, abs/1412.6448.
Felix Hill, Roi Reichart, and Anna Korhonen.
2014b. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.
Kyo Kageura, Keita Tsuji, and Akiko N Aizawa. 2000.
Automatic thesaurus generation through multiple
filtering. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 397–
403.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361–397.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages
for machine translation. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402–407.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12:1532–1543.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, Seattle, WA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394.
Peter D. Turney and Patrick Pantel. 2010. From
Frequency to Meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188, January.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4).
2048
