Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 501–512,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Anchor Graph:
Global Reordering Contexts for Statistical Machine Translation
Hendra Setiawan ?
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bowen Zhou
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bing Xiang ?
Thomson Reuters
3 Times Square
NY 10036, USA
Abstract
Reordering poses one of the greatest chal-
lenges in Statistical Machine Translation re-
search as the key contextual information may
well be beyond the confine of translation units.
We present the “Anchor Graph” (AG) model
where we use a graph structure to model
global contextual information that is crucial
for reordering. The key ingredient of our AG
model is the edges that capture the relation-
ship between the reordering around a set of
selected translation units, which we refer to as
anchors. As the edges link anchors that may
span multiple translation units at decoding
time, our AG model effectively encodes global
contextual information that is previously ab-
sent. We integrate our proposed model into a
state-of-the-art translation system and demon-
strate the efficacy of our proposal in a large-
scale Chinese-to-English translation task.
1 Introduction
Reordering remains one of the greatest challenges
in Statistical Machine Translation (SMT) research as
the key contextual information may span across mul-
tiple translation units.1 Unfortunately, previous ap-
proaches fall short in capturing such cross-unit con-
textual information that could be critical in reorder-
ing. For example, state-of-the-art translation mod-
els, such as Hiero (Chiang, 2005) or Moses (Koehn
et al., 2007), are good at capturing local reordering
within the confine of a translation unit, but their for-
mulation is approximately a simple unigram model
? This work was done when the authors were with IBM.
1We define translation units as phrases in phrase-based SMT
or as translation rules in syntax-based SMT.
over derivation (a sequence of the application of
translation units) with some aid from target language
models. Moving to a higher order formulation (say
to a bigram model) is highly impractical for several
reasons: 1) it has to deal with a severe sparsity issue
as the size of the unigram model is already huge;
and 2) it has to deal with a spurious ambiguity issue
which allows multiple derivations of a sentence pair
to have radically different model scores.
In this paper, we develop “Anchor Graph” (AG)
where we use a graph structure to capture global
contexts that are crucial for translation. To circum-
vent the sparsity issue, we design our model to rely
only on contexts from a set of selected translation
units, particularly those that appear frequently with
important reordering patterns. We refer to the units
in this special set as anchors where they act as ver-
tices in the graph. To address the spurious ambigu-
ity issue, we insist on computing the model score for
every anchors in the derivation, including those that
appear inside larger translation units, as such our AG
model gives the same score to the derivations that
share the same reordering pattern.
In AG model, the actual reordering is modeled
by the edges, or more specifically, by the edges’ la-
bels where different reordering around the anchors
would correspond to a different label. As detailed
later, we consider two distinct set of labels, namely
dominance and precedence, reflecting the two domi-
nant views about reordering in literature, i.e. the first
one that views reordering as a linear operation over
a sequence and the second one that views reordering
as a recursive operation over nodes in a tree struc-
ture The former is prevalent in phrase-based con-
text, while the latter in hierarchical phrase-based and
501
syntax-based context. More concretely, the domi-
nance looks at the anchors’ relative positions in the
translated sentence, while the precedence looks at
the anchors’ relative positions in a latent structure,
induced via a novel synchronous grammar: Anchor-
centric, Lexicalized Synchronous Grammar.
From these two sets of labels, we develop two
probabilistic models, namely the dominance and the
orientation models. As the edges of AG link pairs
of anchors that may appear in multiple translation
units, our AG models are able to capture high or-
der contextual information that is previously absent.
Furthermore, the parameters of these models are es-
timated in an unsupervised manner without linguis-
tic supervision. More importantly, our experimental
results demonstrate the efficacy of our proposed AG-
based models, which we integrate into a state-of-the-
art syntax-based translation system, in a large scale
Chinese-to-English translation task. We would like
to emphasize that although we use a syntax-based
translation system in our experiments, in principle,
our approach is applicable to other translation mod-
els as it is agnostic to the translation units.
2 Anchor Graph Model
Formally, an AG consists of {A,L} where A is a
set of vertices that correspond to anchors, while L
is a set of labeled edges that link a pair of anchors.
In principle, our AG model is part of a transla-
tion model that focuses on the reordering within the
source sentence F and its translation E. Thus, we
start by first introducing A into a translation model
(either word-based, phrase-based or syntax-based
model) followed by L. Given an F , A is essentially
a subset of non-overlapping (word or phrase) units
that make up F . As the information related to A is
not observed, we introduce A as a latent variable.
Let P (E,? |F ) be a translation model where ?
corresponds to the alignments between units in F
and E. 2 We introduce A into a translation model,
2Alignment (?) represents an existing latent variable. De-
pending on the translation units, it can be defined at different
level, i.e. word, phrase or hierarchical phrase. As during trans-
lation, we are interested in the anchors that appear inside larger
translation units, we set ? at word level, which information can
be induced for (hierarchical) phrase units by either keeping the
word alignment from the training data inside the units or infer-
ring it via lexical translation probability. We use the former.
as follow:
P (E,? |F ) =
?
?A?
P (E,?,A?|F ) (1)
P (E,?,A?|F ) = P (E,? |A?, F )P (A?) (2)
As there can be many possible subsets of F and
summing over all possibleA is intractable, we make
the following approximation for P (A?) such that we
only need to consider one particular A?: P (A?) =
?(A? = A?) which returns 1 only for A?, otherwise
0. The exact definition of the heuristic will be de-
scribed in Section 7, but in short, we equateA? with
units that appear frequently with important reorder-
ing patterns in training data.
Given an A?, we then introduce the edges of AG
(L) into the equation as follow:
P (E,? |A?, F ) = P (E,?,L|A?, F ) (3)
Note that L is also a latent variable but its values are
derived deterministically from (F,E,?) and A?,
thus no extra summation is present in Eq. 3.
Then, we further simplify Eq. 3 by factorizing it
with respect to each individual edges, as follow:
P (E,?,L|A?, F ) ?
?
?am,an?A?
m<n
P (Lm,n|am, an) (4)
where Lm,n ? L corresponds to the label of an edge
that links am and an.
In principle, Lm,n can take any arbitrary value.
For addressing the reordering challenge, it should
ideally correspond to some aspect of the reordering
around am and an, for example, how the reorder-
ing around am affects the reordering around an. As
mentioned earlier, we choose to associate Lm,n with
the dominance and the precedence relations between
am and an, where the former looks at the relative po-
sitions of the two anchors when they are projected
into a latent tree structure, while the latter looks at
their relative positions when they are projected into
the target sentence. We illustrate the two in Fig. 1.
Furthermore, we assume that dominance and
precedence are independent and develop one model
for each, resulting in the dominance and the orien-
tation models, which we describe in Section 3 and 4
respectively. To make the model more compact, we
502
introduce an additional parameterO that restricts the
maximum order of AG as follows:
?
O?
o=1
|A?|+o?1?
i=0
Po(Li?o,i|ai?o, ai) (5)
Thus, we only consider edges that link two anchors
that are at most O? 1 anchors apart. For O = 1, the
AG model only considers relations between neigh-
boring anchors. Following the standard practice in
the n-gram language modeling, we append O num-
ber of pseudo anchors at the beginning and at the end
of F , which represent the sentence delimiter mark-
ers. We do so in a monotone order.
Figure 1: The illustration of the dominance and the prece-
dence relations. The former looks at the anchors’ pro-
jection on a derivation structure. The latter looks at the
anchors’ projection on the translated sentence.
3 Dominance Model
This section describes our dominance model where
we equate Lm,n in Eq. 4 with dom(am, an) that ex-
presses to the dominance relation between am and
an in a latent tree structure. Due to reordering, an-
chors can only appear in specific nodes. We first
describe a novel formalism of Anchor-centric, Lexi-
calized Synchronous Grammar (AL-SG), used to in-
duce the tree structure and then discuss the proba-
bilistic formulation of the model. Just to be clear,
we introduce AL-SG mainly to facilitate the compu-
tation of dom(am, an). The actual translation model
at decoding time remains either phrase-based, hier-
archical phrase-based or syntax-based model.
3.1 Anchor-centric, Lexicalized Synchronous
Grammar
Given (F,E,?) and A, Anchor-centric, Lexical-
ized Synchronous Grammar (AL-SG) produces a
tree structure where the nodes are decorated with
anchors-related information. As the name alludes,
the core of AL-SG is anchor-centric constituents
(ACC), which corresponds to nodes, composed from
merging anchors with by either their left, their right
neighboring constituents or both.
More concretely, first of all, we consider a span
on the source sentence F to be a constituent if it is
consistent with the alignment (?). Second of all, we
can construct a larger constituent by merging smaller
constituents given that the larger constituent is also
consistent with the alignment. These two constraints
are similar to the heuristic applied to extract hierar-
chical phrases (Chiang, 2005).
Then, specific to AL-SG, we consider an anchor a
to lexicalize a constituent c, if: a) we can compose c
from at most three smaller constituents: cL, a and cR
where a is the anchor while cL,cR are the (possibly
empty) constituents immediately to the left and to
the right of a; and b) we can create smaller anchors-
centric constituents from concatenating a with cL
and a with cR. If a can lexicalize c, then the node
associated with c would be marked with a. In com-
puting dom(am, an), we look at the constituents that
cover both anchors and check whether the anchors
can lexicalized any of such constituents.
Now, we will describe AL-SG in a formal way.
For simplicity, we use a simple grammar, called In-
version Transduction Grammar (ITG) (Wu, 1997),
although in practice, we handle a more powerful
synchronous grammar. Hence, we proceed to de-
scribe Anchor-centric, Lexicalized ITG (AL-ITG).
An AL-ITG is a quadruple {?,A,V,R} where:
• ? = {(f/e)} is a set of terminal symbols,
which represents all possible units defined over
(F,E,?) where each pair corresponds to a link
in ?. We define ? at the most fine-grained
level (i.e. word-level), as we insist on comput-
ing model score for each anchors even if they
appear inside larger units.
• A ? ? is a set of anchors, which is a subset of
the terminal symbols.
• V = {{P,X, Y } × {A, ?}} is a set of (possi-
bly lexicalized) nonterminal symbols. P rep-
resents the terminal symbols (?); while X and
Y correspond to the spans that are created from
merging two adjacent constituents. On the tar-
503
Figure 2: An illustration of an aligned Chinese-English sentence pair with one possible AL-ITG derivation obtained
by applying the grammar in a left-to-right fashion. Circles represent alignment points. Black circle represents the
anchor; boxes represent the anchor’s neighbors. In the derivation tree, the anchors are represented by their position
and in bold. For succinctness, we omit the preterminal rules in the tree.
get side, for X , the order of the two children
follows the source order, while for Y , the or-
der follows the inverse. Nonterminal symbols
can be lexicalized with zero or more than one
anchor. We represent a lexicalized constituent
as a nonterminal symbol followed by a bracket
which contains the lexicalizing anchors, e.g.
P (H) where H is the anchors lexicalizing P .
• R is a set of production rules which can be clas-
sified into the following categories:
– Preterminal rules. We propagate the sym-
bol if it corresponds to an anchor.
P (H = f/e)? f/e, if f/e ? A?
P (H = ?)? f/e, otherwise
– Monotone production rules, which reorder
the children in monotone order, denoted
by square brackets (“[”,“]”).
X(H1 ?H2)? [P (H1)P (H2)]
X(H1 ?H2)? [X(H1)P (H2)]
X(H1 ?H2)? [X(H1)X(H2)]
X(H1)? [X(H1)Y (H2)]
X(H2)? [Y (H1)P (H2)]
X(H2)? [Y (H1)X(H2)]
X(?)? [Y (H1)Y (H2)]
– Inverse production rules, which reorder
the children in the inverse order, denoted
by angle brackets (“?”,“?”).
Y (H1 ?H2)? ?P (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)Y (H2)?
Y (H1)? ?Y (H1)X(H2)?
Y (H2)? ?X(H1)P (H2)?
Y (H2)? ?X(H1)Y (H2)?
Y (?)? ?X(H1)X(H2)?
Like ITG, AL-ITG only permits two kind of re-
ordering operations, namely monotone and inverse.
To accommodate the lexicalization, we first assign
a unique nonterminal symbol for each, i.e. X for
monotone reordering and Y for inverse reordering.
Then, we lexicalize Xs and Y s with anchors as long
as they satisfy the constraint that the child shares the
same label as the parent. This constraint guarantees
that the constituents are valid ACCs. It also enables
the anchors to lexicalize long constituents, although
the terminal symbols are defined at word-level.
Fig. 2 illustrates an example Chinese-to-English
translation with a AL-ITG derivation when the
grammar is applied in a left-to-right fashion. Admit-
tedly, AL-ITG (or more generally AL-SG) is suscep-
tible to spurious ambiguity as it produces multiple
derivation trees for a given (F,E,?). Fortunately,
the value of dom(am, an) is identical for all deriva-
tions, since the computation of dom(am, an) relies
504
only on whether am and an can lexicalize at least
one constituent that covers both anchors. Hence,
we only need to look at one derivation to compute
dom(am, an). Generalizing AL-ITG to a more pow-
erful formalism is trivial; we just need to forbid the
propagation for non-binarizeable production rules.
3.2 Probabilistic Model
We read-off the dominance relations dom(am, an)
from D obtained from the application of AL-SG to
(F,E,?). As lexicalization is a bottom-up process,
for reading-off dom(am, an), it is sufficient to look
at the lowest common ancestor (LCA) of both an-
chors; if the anchors cannot lexicalize the LCA, they
won’t be able to lexicalize the constituents larger
than LCA. To be more concrete, let’s consider theD
in Fig. 2. In that D, the LCA of am = yu3/with10
and an = de7/that7 is Y5(7). Then, we check the
anchors that can lexicalize the LCA. Let V (H) be
the LCA, then dom(am, an) ?
(LH) , if am ? H ? an 6? H
(RH) , if am 6? H ? an ? H
(BL) , if am ? H ? an ? H
(BD) , if am 6? H ? an 6? H
The value refers to cases where am and an can
lexicalize V (H) and it is useful to model spans
that share a simple, uniform reordering, i.e. all-
monotone or all-inverse, while the value refers to
the cases where am and an cannot lexicalize V (H)
and it is useful to model spans that involve in a com-
plex reordering. Meanwhile, the and refer to cases
where only one anchor can lexicalize V (H), i.e. am
and an respectively. These values are useful for
modeling cases where the surroundings of the two
anchors exhibit different kind of reordering pattern.
With such definition, the edge labels L in Fig. 2
are indicated in Table 1. Note that in Table 1, we
don’t specify the relations involving pseudo anchors,
although they are crucial.
The final probabilistic formulation of the domi-
nance model is as follows:
?
O?
o=1
|A|+o?1?
i=0
Pdomo(dom(ai?o, ai)|ai?o, ai) (6)
As shown, we allocate a separate model Pdomo for
each separate order (o) where each Pdomo will con-
HHH
HHn
m
1 2 3 4 5
1 = (shi2/is2) - - - - -
2 = (yu3/with10) LH - - - -
3 = (you5/have8) LH BD - - -
4 = (de7/that7) LH RH RH - -
5 = (zhi10/of4) LH RH RH BL -
Table 1: The dominance relations between pairs of an-
chors according to the derivation in Fig. 2.
tribute as one additional feature in the log-linear
model of the translation model. In allocating a sep-
arate model for each o, we conjecture that different
pair of anchors contributes differently depending on
how far the two anchors are.
4 Orientation Model
In this section, we introduce the orientation model
(ori) where we equate Lm,n with the precedence re-
lations between a pair of anchors. Instead of directly
modeling the precedence between the two anchors,
we approximate it by modeling the precedence of
each anchor with its neighboring constituents. For-
mally, we approximate P (Lm,n|am, an) as
PoriR(ori(am,MR(am))|am)×
PoriL(ori(an,ML(an))|an) (7)
where MR(am) is the largest constituent to the right
of the first anchor am, ML(an) the largest con-
stituent to the left of the second anchor an, and ori()
a function that maps the anchor and the neighboring
constituent to a particular orientation.
Plugging Eq. 7 into Eq. 5 results in the following
approximation of P (?|A):
C.
|A|?1?
i=0
{PoriL(ori(ai,ML(ai))|ai)×
PoriR(ori(ai,MR(ai))|ai)}
O (8)
where C is a constant term related to the pseudo an-
chors and O is the maximum order of the AG. In
practice, we can safely ignore both C and O as they
are constant for a given AG. As shown, the orienta-
tion model is simplified into a model that looks at the
reordering of the anchors’ neighboring constituents.
The exact definition of ML and MR will be
discussed in Section 5. Their orientation, i.e.
505
oriL(CL, a) and oriR(CR, a) respectively, may take
one of the following four values: (MA), (RA), (MG)
and (RG). The first clause (monotone, reverse) in-
dicates whether the target order follows the source
order; the second (adjacent, gap) indicates whether
the anchor and its neighboring constituent are adja-
cent or separated by an intervening when projected.
5 Parameter Estimation
For each (F,E,?), the training starts with the iden-
tification of the regions in the source sentences as
anchors (A). For our Chinese-English experiments,
we use a simple heuristic that equates anchors (A?)
with constituents whose corresponding word class
belongs to function words-related classes, bearing
a close resemblance to (Setiawan et al., 2007). In
total, we consider 21 part-of-speech tags; some of
which are as follows: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD (ad-
jectives) and P (prepositions).
5.1 Extracting Events from (F,E,?)
The parameter estimation first involves extracting
two statistics from (F,E,?), namely dom(am, an)
for the dominance model as well as ori(a,ML(a))
and ori(a,MR(a)) for the orientation model. In-
stead of developing a separate algorithm for each,
we describe a unified way to extract these statistics
via the largest neighboring constituents of the an-
chors, i.e. ML(a) and MR(a). This approach en-
ables the dominance model to share the same resid-
ual state information as the orientation model.3
Let am be an anchor and MR(am) be its largest
neighboring constituent to the right. Let an be
an anchor to the left of am and ML(an) be an’s
largest neighboring constituent to the left. Ac-
cording to AL-SG, we say that am dominates an
if ori(am,MR(am)) ? {MA,RA} and an ?
MR(am). By the same token, we say that an dom-
inates am if ori(an,ML(an)) ? {MA,RA} and
am ? ML(an). The constraints on the orientation
reflect the fact that in AL-SG, anchors can only be
propagated through monotone or inverse production
rules, which correspond to the MA and RA respec-
tively. The fact that we are looking at the largest
3The analogy in an n-gram language model is the first n?1
words of the hypothesis that have incomplete history.
neighboring constituents guarantees that if the other
anchor is outside that constituent, then that other an-
chor is never dominated.
More formally, given an aligned sentence pair
? = (F,E,?), let ?(?) be all possible con-
stituents that can be extracted from ?:4
{(f j2j1/e
i2
i1) :?(j, i) ??: ((j1? j? j2) ? (ii? i? i2))
?(¬(j1? j? j2) ? ¬(ii? i? i2))
Then, let the anchors A be a subset of ?(?).
Given A ? ?(?), let a = (f j2j1/e
i2
i1) ? A be a par-
ticular anchor. And, let CL(a) ? ?(?) be a’s left
neighbors and let CR(a) ? ?(?) be a’s right neigh-
bors, iff:
?CL = (f
j4
j3/e
i4
i3) ? CL(a) : j4 + 1 = j1
?CR = (f
j6
j5/e
i6
i5) ? CR(a) : j2 + 1 = j5
Then, let ML(a) ? CL(a) and MR(a) ? CR(a) be
the largest left and right neighbors according to:
ML(a) = arg max
(f
j4
j3
/e
i4
i3
)?CL(a)
(j4 ? j3)
MR(a) = arg max
(f
j6
j5
/e
i6
i5
)?CR(a)
(j6 ? j5)
Let ML = (f
j4
j3/e
i4
i3) and MR = (f
j6
j5/e
i6
i5).
We then proceed to extract oriL(a,ML(a)) and
oriR(a,MR(a)) respectively as follows:
• MA, if (i4 +1) = i1 for oriL or if (i2 +1) = i5
for oriR
• RA, if (i2 + 1) = i3 for oriL or if (i6 + 1) = i1
for oriR
• MG, if (i4 +1) < i1 for oriL or if (i2 +1) < i5
for oriR
• RG, if (i2 + 1) < i3 for oriL or if (i6 + 1) < i1
for oriR.
Then, we proceed to extract dom(am, an). Given
two anchors am, an where m < n, we define the
4We represent a constituent as a source and target phrase
pair (f j2j1/e
i2
i1
) where the subscript and the superscript indicate
the starting and the ending indices as such f j2j1 denotes a source
phrase that spans from j1 to j2.
506
dominance relation between am and an viaMR(am)
and ML(an). Let am = (f
j2
j1/e
i2
i1), MR(am) =
(f j4j3/e
i4
i3), an = (f
j6
j5/e
i6
i5) and ML(an) = (f
j8
j7/e
i8
i7).
Then, ldom(am, an) is true only if (j4 ? j6)
and oriR(am,MR(am)) ? {MA,RA}. Simi-
larly, rdom(am, an) is true only if (j7 ? j1) and
oriL(an,ML(an)) ? {MA,RA}.
Hence, dom(am, an) is as follows:
• LH, if ldom(am, an) ? ¬rdom(am, an)
• RH, if ¬ldom(am, an) ? rdom(am, an)
• BL, if ldom(am, an) ? rdom(am, an)
• BD, if ¬ldom(am, an) ? ¬rdom(am, an)
5.2 Parameterization and Training
After extracting events, we are now ready to train
the models. To estimate them, we train a discrimi-
native classifier for each model and use the normal-
ized posteriors at decoding time as additional feature
scores in SMT’s log-linear framework.
At a high level, we use a rich set of binary fea-
tures ranging from lexical to part-of-speech (POS)
and to syntactic features. Additionally, we augment
the feature set with compound features, e.g. a con-
junction of the source word of the left anchor and the
source word of the right anchor. Although they in-
crease the number of features significantly, we found
that they are empirically beneficial.
Suppose a = (f j2j1 /e
i2
i1), ML(a) = (f
j4
j3 /e
i4
i3) and
MR(a) = (f
j6
j5 /e
i6
i5), then based on the context’s
location, the elementary features employed in our
classifiers can be categorized into:
• anchor-related: (the actual word of f j2j1 ),
(part-of-speech (POS) tag of ), (’s parent in the
parse tree), (ei2i1’s actual target word).
• surrounding: (the previous word / f j1?1j1?1 ), (the
next word / f j2+1j2+1 ), (’s POS tag), (’s POS tag),
(’s parent), (’s parent).
• non-local: (the previous anchor’s source word)
, (the next anchor’s source word), (’s POS tag),
(’s POS tag).
There is a separate set of elementary features for am
and an and we come up with manual combination to
construct compound features.
In training the models, we manually come up with
around 30-50 types of features, which consists of a
combination of elementary and compound features.
Due to space constraints, we will describe the ac-
tual features that we use and the classification per-
formance of our models elsewhere. In total, we
generate around one hundred millions binary fea-
tures from our training data that contains six million
sentence pairs. To reduce the number of features,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al., 2008). After training,
the number of features in our classifiers decreases to
below 1 million features for each classifier.
6 Decoding
As mentioned earlier, we wish to avoid the spuri-
ous ambiguity issue where different derivations have
radically different scores although they lead to the
same reordering. This section describes our decod-
ing algorithm that avoids spurious ambiguity issue
by incrementally constructing MLs and MRs thus
allowing the computation of the models over partial
hypotheses.
In our experiments, we integrate our dominance
model as well as our orientation model into a syntax-
based SMT system that uses SCFG formalism. In-
tegrating the models into syntax-based SMT sys-
tems is non-trivial, especially since the anchors of-
ten reside within translation rules and the model
doesn’t always decompose naturally with the hy-
pothesis structure. To facilitate that, we need to
first induce the necessary alignment for all transla-
tion units in the hypothesis.
To describe the algorithm, let us consider a cheat-
ing exercise where we have to translate the Chinese
sentence in Fig. 2 with the following set of hierar-
chical phrases:
Xa??Aozhou
1shi2X1,Australia
1 is2X1?
Xb??yu
3 Beihan4X1, X1with
3 North4 Korea?
Xc??you
5bangjiao6, have5dipl.6 rels.?
Xd??X1 de
7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
As a case in point, let us consider D = Xa ? Xb
? Xd ? Xc, which will lead to the correct English
507
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have
5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of
10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with
3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia
1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 2: The application of the shift-reduce parsing algorithm, which corresponds to the following derivation D =
Xa ? Xb ? Xd ? Xc. Anchor is in bold. In column Op., S, R and A refer to shift, reduce and accept operation
respectively.
translation as in Fig. 2. Note that the translation
rules contain internal word alignment, which we as-
sume to have been previously inferred.
The algorithm bears a close resemblance to the
shift-reduce algorithm found in phrase-based decod-
ing (Galley and Manning, 2008; Feng et al., 2010;
Cherry et al., 2012). A stack is used to accumulate
(partial) information about a, ML and MR for each
a ? A in the derivation. This algorithm takes an in-
put stream and applies either the shift or the reduce
operations starting from the beginning until the end
of the stream. The shift operation advances the input
stream by one symbol and push the symbol into the
stack; while the reduce operation applies some rule
to the top-most elements of the stack. The algorithm
terminates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the input
stream is the target string of the rule and the symbol
is the corresponding source index of the elements of
the target string. The reduction rule looks at two in-
dices and merge them if they are adjacent (i.e. has
no intervening phrase). We forbid the application
of the reduction rule to anchors. Table 2 shows the
execution trace of the algorithm for the derivation
described earlier. For conciseness, we assume that
there is only one anchor and that is de7/that7.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the corre-
sponding target word and then enumerates the target
string in a left to right fashion. If it finds a target
word with a source index, it applies the shift oper-
ation, pushing the index to the stack. Unless the
symbol corresponds to an anchor, it tries to apply
the reduce operation. Line (4) indicates the special
treatment to the anchor. If the symbol being read
is a nonterminal, then we push the entire stack that
corresponds to that nonterminal. For example, when
the algorithm reads Xd at line (6), it pushes the en-
tire stack from line (5).
As MLs and MRs are being incremen-
tally constructed, we can immediately com-
pute Pdomo(dom(am, an)|am, an) as soon
as a partial derivation covers both am
and an. For example, we can compute
Pdom1(dom(you5/have8, de7/that7) = ),
Pdom1(dom(de7/that7, zhi10/of4) = ) and
Pdom2(dom(you5/have8, zhi10/of4) = ) at
partial hypothesis Xd ? Xc which corresponds to a
constituent spanning from 5-11.
7 Experiments
Our baseline systems is a state-of-the-art string-to-
dependency system (Shen et al., 2008). The sys-
tem is trained on 10 million parallel sentences that
are available to the Phase 1 of the DARPA BOLT
Chinese-English MT task. The training corpora in-
clude a mixed genre of newswire, weblog, broad-
cast news, broadcast conversation, discussion fo-
rums and comes from various sources such as LDC,
HK Law, HK Hansard and UN data.
In total, our baseline model employs more than
50 features, including from our proposed dominance
and orientation models. In addition to the standard
508
Model
newswire weblog newswire+weblog
BLEU TER Comb BLEU TER Comb BLEU TER Comb
(a) (b) (c) (d) (e) (f) (g) (h) (i)
(1) S2D 37.63 53.17 7.77 27.60 57.19 14.77 33.39 54.97 10.79
(2) +dom1 38.12 52.31 7.10 27.56 56.58 14.51 33.64 54.24 10.30
(3) +dom2 38.31 52.28 6.99 27.66 56.57 14.45 33.78 54.20 10.21
(4) +dom3 38.31 52.52 7.10 28.24 56.56 14.16 34.02 54.33 10.15
(5) +dom4 38.54 52.22 6.84 28.38 56.55 14.08 34.20 54.16 9.98
(6) +dom5 38.17 52.57 7.20 28.67 56.27 13.80 34.16 54.27 10.05
(7) +dom6 38.17 52.52 7.18 28.64 56.22 13.79 34.10 54.18 10.04
(8) +ori 38.52 52.43 6.96 28.26 56.54 14.14 34.15 54.27 10.06
(9) +ori+dom1 38.87 52.05 6.59 28.01 56.48 14.23 34.26 54.03 9.89
(10) +ori+dom2 38.96 51.87 6.45 27.98 56.23 14.12 34.29 53.82 9.77
(11) +ori+dom3 39.19 51.77 6.29 28.19 56.15 13.98 34.52 53.73 9.61
(12) +ori+dom4 39.34 51.77 6.21 28.41 56.17 13.88 34.60 53.69 9.54
(13) +ori+dom5 39.31 51.67 6.18 28.62 56.09 13.74 34.76 53.65 9.45
Table 3: The NIST MT08 results on newswire (nw), weblog (wb) and combined genres. S2D is the baseline string-
to-dependency system (line 1). Lines 2-7 shows the results of the dominance model with O = 1 ? 6. Line 8 shows
result on adding ori to the baseline. Lines 9-13 shows the results of the orientation complemented with the dominance
model with varying O. The best BLEU, TER and Comb on each genre of the first set are in italic while those of the
second set are in bold. For BLEU, higher scores are better, while for TER and Comb, lower scores are better.
features such as translation probabilities, we incor-
porate features that are found useful for developing
a state-of-the-art baseline, such as the provenance
features (Chiang et al., 2011). We use a 6-gram
language model, which was trained on 10 billion
English words from multiple corpora, including the
English side of our parallel corpus plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. We also train a class-based language model
(Chen, 2009) on two million English sentences se-
lected from the parallel corpus. As for our string-to-
dependency system, we train 3-gram models for left
and right dependencies and unigram for head using
the target side of the parallel corpus. To train our
models, we select a set of 5 million sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the feature weights
with PRO (Hopkins and May, 2011) to minimize
(TER-BLEU)/2 metric. As for the blind test set,
we report the performance on the NIST MT08 eval-
uation set, which consists of 691 sentences from
newswire and 666 sentences from weblog. We pick
the weights that produce the highest development set
scores to decode the test set.
We perform two sets of experiments. The first set
looks at the contribution of the dominance model
with varying values of o. The second one looks at
the combination of the dominance model and the
orientation model. Table 3 summarizes the experi-
mental results on NIST MT08 sets, categorized by
genres. We report the results on newswire genre in
columns a-c, those on weblog genre in column d-f,
and those on mixed genre in column g-i. The perfor-
mance of our baseline string-to-dependency syntax-
based SMT is shown in the first line.
Lines 2-7 in Table 3 show the results of our first
set of experiments, starting from the result of dom1,
which looks at only at pairs of adjacent anchors, to
the result of dom6, which looks at pairs of anchors
that are at most 5 anchors away. As shown in line
2, our dominance model provides a nice improve-
ment of around 0.5 point over the baseline even if it
only looks at restricted context. Increasing the or-
der of our dominance model provides an additional
gain. However, the gain is more pronounced in the
weblog genre (up to around 1 BLEU point) than in
the newswire genre. We conjecture that this may be
the artifact of our tune set, which comes from the
weblog genre. We stop at dom6 because we observe
509
that the weight of the feature score that corresponds
to the maximum order (o = 6) has a negative sign,
which often indicates a high correlation between the
new features and existing ones.
Lines 8-13 in Table 3 shows the results of our sec-
ond set of experiments. Line 8 shows the result of
adding the orientation model (ori) to the baseline
system. As shown, integrating ori shows a signifi-
cant gain. On top of which, we then integrate dom1
to dom5. We see a very encouraging result as adding
the dominance model increases the performance fur-
ther, consistently over different value of o. This sug-
gests that the dominance model is complementary
to the orientation model. Our best result provides
more than 1 BP improvement and 1 TER reduction
consistently over different genres. We see this result
as confirming our intuition that the global contextual
information provided by our AG model can signifi-
cantly improve the performance of SMT even in a
state-of-the-art system.
8 Related Work
Our work intersects with existing work in many dif-
ferent respects. In this section, we mainly focus on
work related to introducing higher-order contextual
information to reordering model.
In providing global contextual information, our
work is related to a large amount of literature. To
name a few, Zens and Ney (2006) improves the lexi-
calized reordering model of Tillman (2004) by in-
corporating part-of-speech information. Chang et
al. (2009) incorporates contexts from syntactic parse
tree. Bach et al. (2009) exploits the dependency in-
formation and Xiong et al. (2012) uses the predicate-
argument structure.
Vaswani et al. (2011) introduces rule markov
models for a forest-to-string model in which the
number of possible derivations is restricted. More
recently, Durrani et al. (2013) and Zhang et al.
(2013) cast reordering process as a Markov process.
Similar to these models, our proposed model also
provide context dependencies to the application of
translation rules, however, as they focus on mini-
mal translation units (MTU) where we focus on a
selected set of translation units. (Banchs et al., 2005)
introduces a bigram model for monotone phrase-
based system, but their definition of translation units
is suitable only for language pairs with limited re-
ordering, such as translating Spanish to English.
In equating anchors with the function word class,
our work is closely related to the function word-
centered model of Setiawan et al. (2007), especially
the orientation model. Our dominance model is
closely related to the reordering model of Setiawan
et al. (2009), except that they only look at pair of ad-
jacent anchors, forming a chain structure instead of
a graph like in our dominance model. Furthermore,
we provide a discriminative treatment to the model
to include a richer set of features including syntac-
tic features. This work can be seen as modeling the
identity of the neighboring of the anchors, similar to
(Setiawan et al., 2013). However, instead of looking
at the words at the borders, we look at whether the
neighboring constituents contain other anchors.
9 Conclusion
We propose the “Anchor Graph” (AG) model to en-
code global contextual information. A selected set
of translation units, which we call anchors, serves
as the vertices of AG. And as the edges, we model
two types of relations, namely the dominance and
the precedence relations, where the former looks at
the positions of the anchors in the derivation struc-
ture, while the latter looks at the positions of the
anchors in the surface structure, resulting into two
probabilistic models over edge labels. As the mod-
els look at the pairs of anchors that go beyond multi-
ple translation units, our AG model provides global
contextual information.
Our AG model embodies (admittedly crudely)
some basic principles of sentence organization,
namely categorization (in categorizing units into an-
chors and non-anchors), linear order (in modeling
the precedence of anchors) and constituency struc-
ture (in modeling the dominance between anchors).
We are encouraged by the facts that we learn these
principles in an unsupervised way and that we can
achieve a significant improvement over a strong
baseline in a large-scale Chinese-to-English trans-
lation task. In the future, we hope to continue this
line of research, perhaps by learning to identify an-
chors automatically from training data or by using
our models to induce derivations directly from un-
aligned sentence pair.
510
Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-12-C-0015 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be inter-
preted as representing the official views or policies,
either expressed or implied, of the DARPA.
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada, August. International Associa-
tion for Machine Translation.
Rafael E. Banchs, Josep M. Crego, Adria` de Gispert, Pa-
trik Lambert, and Jose´ B. Marin˜o. 2005. Statisti-
cal machine translation of Euparl data by using bilin-
gual n-grams. In Proceedings of the ACL Workshop
on Building and Using Parallel Texts, pages 133–136,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and
Structure in Statistical Translation (SSST-3) at NAACL
HLT 2009, pages 51–59, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 468–476, Boulder, Colorado,
June. Association for Computational Linguistics.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 200–209, Montre´al, Canada, June. Association
for Computational Linguistics.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 455–460, Portland, Oregon, USA,
June. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 263–270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1–11, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Coling 2010: Posters,
pages 285–293, Beijing, China, August. Coling 2010
Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848–856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352–1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation, June.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712–719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function words
in hierarchical phrase-based translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 324–332, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
511
1264–1274, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577–585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL
2004: Short Papers, pages 101–104, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856–864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404, Sep.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 902–911, Jeju Island, Korea, July.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL): Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 55–63, New York City, NY, June. Associa-
tion for Computational Linguistics.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proceedings of the
2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 12–21, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
512
