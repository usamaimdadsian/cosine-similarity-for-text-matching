Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980–990,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Unsupervised Learning of Selectional Restrictions and Detection of
Argument Coercions
Kirk Roberts and Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083, USA
{kirk,sanda}@hlt.utdallas.edu
Abstract
Metonymic language is a pervasive phe-
nomenon. Metonymic type shifting, or ar-
gument type coercion, results in a selectional
restriction violation where the argument’s se-
mantic class differs from the class the predi-
cate expects. In this paper we present an un-
supervised method that learns the selectional
restriction of arguments and enables the de-
tection of argument coercion. This method
also generates an enhanced probabilistic reso-
lution of logical metonymies. The experimen-
tal results indicate substantial improvements
the detection of coercions and the ranking of
metonymic interpretations.
1 Introduction
Metonymic language is pervasive in today’s social
interactions. For example, it is typical to find ques-
tions that require metonymic resolution:
(Q1) Did you enjoy War and Peace?
(Q2) Does anyone have any advice on how to start
a bowling team?1
In order to process such questions and capture the
intention of the person that posed them, coercions
are needed. Question (Q1) is interpreted as whether
you enjoyed reading “War and Peace”, while (Q2)
is interpreted as asking for advice on organizing,
forming, or registering a bowling team. The qual-
ity of the answers therefore depends on the ability
to (1) recognize when metonymic language is used,
and (2) to produce coercions that capture the user’s
intention. One important step in this direction was
1Both questions taken from Yahoo Answers.
taken by SemEval-2010 Task 7, which focused on
the ability to recognize (a) an argument’s selectional
restriction for predicates such as arrive at, cancel,
or hear, and (b) the type of coercion that licensed
a correct interpretation of the metonymy. Details of
the task are reported in (Pustejovsky et al., 2010).
Approaches to metonymy based on this task are lim-
ited, however, because (a) the task is focused only on
semantically non-ambiguous predicates and (b) the
selectional restrictions of the arguments were cho-
sen from a pre-defined set of six semantic classes
(artifact, document, event, location, proposition, and
sound). However, metonymy coercion systems ca-
pable of providing the interpretations of questions
(Q1) and (Q2) clearly cannot operate with the sim-
plifications designed for this task.
Inspired by recent advances in modeling selec-
tional preferences with latent-variable models (Rit-
ter et al., 2010; ´O Se´aghdha, 2010), we propose
an unsupervised model for learning selectional re-
strictions. The model assumes that (1) arguments
have a single selected class exemplified by the se-
lectional restriction, and (2) the selected class can
be inferred from the data, in part by modeling how
coercive each predicate is. The model is capable of
operating with both ambiguous and disambiguated
predicates, producing superior results for predicates
that have been disambiguated. The selectional re-
strictions and coercions detected by the model re-
ported in this paper can be used to enhance the logi-
cal metonymy approach reported in Lapata and Las-
carides (2003). The experimental results show a sig-
nificant improvement in the ranking of interpreta-
tions.
980
The rest of this paper is organized as follows.
Section 2 discusses related work. Section 3 de-
tails unsupervised models that inform detection of
metonymies. Section 4 outlines a method for disam-
biguating ambiguous predicates. Section 5 describes
the enhanced interpretation of logical metonymies
when conventional constraints are known. Section 6
outlines our implementation and experimental de-
sign. Section 7 presents our experimental results in
three broad tasks: (i) semantic class induction, (ii)
coercion detection, and (iii) logical metonymy inter-
pretation. Section 8 summarizes the conclusions.
2 Previous Work
Lapata and Lascarides (2003) propose a probabilis-
tic ranking model for logical metonymies. They es-
timate these probabilities using co-occurrence fre-
quencies of predicate-argument pairs in a corpus.
Shutova (2009) extends this approach to provide
sense-disambiguated interpretations from WordNet
(Fellbaum, 1998) by using the alternative interpre-
tations to disambiguate polysemous words. Shutova
and Teufel (2009) extend this approach further by
clustering these sense-disambiguated interpretations
into distinct groups of meaning (e.g., {read, browse,
look through} and {write, produce, work on} for
“enjoy book”). Not only do these approaches as-
sume logical metonymies have already been iden-
tified, but they are susceptible to providing interpre-
tations that are themselves logical metonymies (e.g.,
finish book). In this paper, we propose an enhance-
ment to resolving logical metonymies by ruling out
event-invoking predicates in order to provide more
semantically valid interpretations.
Recently, the resolution of several linguistic prob-
lems has benefited from Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) models. ´O Se´aghdha
(2010) examines several selectional preference mod-
els based on LDA in predicting human judgements
on predicate-argument plausibility. Both LDA and
an extension, ROOTH-LDA (based on Rooth et al.
(1999)), perform well at predicting plausibility on
unseen predicate-argument pairs. Inspired by these
results, we propose to extend selectional preference
models in order to learn selectional restrictions.
Alternatively, unsupervised algorithms exist that
both induce semantic classes (Rooth et al., 1999;
Lin and Pantel, 2001) and cluster predicates by their
selectional restrictions (Rumshisky et al., 2007) but
none of these provide a sufficient framework for de-
termining if a specific argument violates its predi-
cate’s selectional restriction.
3 Unsupervised Learning of Selectional
Restrictions
In predicate-argument structures, predicates impose
selectional restrictions in the form of semantic ex-
pectations on their arguments. Whenever the seman-
tic class of the argument meets these constraints a
selection occurs. For example, the predicate “hear”
imposes the semantics related to sound on the ar-
gument “voice”. Because the semantic class for
“voice” conforms to these constraints, we call its se-
mantic class the selected class. However, when the
semantic class of the argument violates these con-
straints, we follow Pustejovsky et al. (2010) and re-
fer to this as a coercion. In this case, we call the
argument’s semantic class the coerced class. For ex-
ample, “hear speaker” is a coercion where the ar-
gument class, person, is implicitly coerced into the
voice of the speaker, a sound.
3.1 A Baseline Model
We consider the LDA-based selectional prefer-
ence model reported in ´O Se´aghdha (2010) as a
baseline for modeling selectional restrictions. For-
mally, we define our LDA baseline model as follows.
Let V be the predicate vocabulary size, let A be the
argument vocabulary size, and let K be the number
of argument classes. Let avi be the ith (non-unique)
argument realized by predicate v. Let cvi be the class
for avi . Let ?v be the class distribution for predicate
v and ?k be the argument distribution for class k.
The graphical model for this LDA is shown in Fig-
ure 1(a). The generative process for LDA is:
For each argument class k = 1..K:
1. Choose ?k ? Dirichlet(?)
For each unique predicate v = 1..V :
2. Choose ?v ? Dirichlet(?)
For every argument i = 1..nv:
3. Choose cvi ? Multinomial(?v)
4. Choose avi ? Multinomial(?c
v
i )
Following Griffiths and Steyvers (2004), we col-
lapse ? and ? and estimate the model using Gibbs
981
? ? c a
? ?
V
N
K
(a)
?
?0
?1
?
s
?
c
x
a
? ?
V
N
K
(b)
Figure 1: Graphical models for (a) LDA, and (b) coercion LDA (cLDA).
Sampling. This yields the update equation:
p(cvi = k|av;?, ?) ?
fvk + ?
fv + K?
fak + ?
fk + A?
(1)
Where fak is the frequency of argument a being as-
signed class k; fk is the frequency of class k being
assigned to any argument; fvk is the frequency of
predicate v having an argument of class k; and fv is
the total number of non-unique arguments for pred-
icate v.
3.2 A Coercion Model
We now incorporate our assumptions for selec-
tional restriction modeling. Namely: (1) there is
one selected class per predicate, and (2) the predi-
cate’s selected class can be chosen from the classes
of its arguments. To accomplish this, we must also
account for the coerciveness of each predicate. We
assign a latent variable ?v for each predicate v that
controls how coercive v should be. The additional
hyper-parameters ?0 and ?1 act as priors on ?v. The
generative process for this coercion LDA model,
which we denote cLDA, is:
For each argument class k = 1..K:
1. Choose ?k ? Dirichlet(?)
For each unique predicate v = 1..V :
2. Choose sv ? Uniform(1,K)
3. Choose ?v ? Dirichlet(?)2
4. Choose ?v ? Beta(?0, ?1)
For every argument i = 1..nv:
5. Choose cvi ? Multinomial(?v)
6. Choose xvi ? Bernoulli(?v)
7. If xvi = 1, Choose avi ? Multinomial(?c
v
i )
Else Choose avi ? Multinomial(?s
v)
The model variable sv represents the selected class
for predicate v. The coerced class is represented
2With the exception that the probability of drawing the se-
lected class sv is zero. This can be seen as drawing the multi-
nomial ?v from a Dirichlet distribution with K-1 components.
for each argument i by cvi , where xvi chooses be-
tween the selected and coerced class. The variable
xvi is similar to switching variables in other graph-
ical models such as Chemudugunta et al. (2007)
and Reisinger and Mooney (2010), where switch-
ing variables are used to choose between a back-
ground distribution and a document-specific distri-
bution. In this case, the switching variable chooses
between a specific class and a predicate-specific dis-
tribution. The graphical model for cLDA is shown
in Figure 1(b). Note that cLDA is virtually equiva-
lent to LDA when ?v is 1 and ?1 is small because the
selected class will be ignored. In this way, highly co-
ercive predicates have less of an impact on the argu-
ment clustering because they are more reliant on the
multinomial ?. We use Gibbs sampling to perform
model inference and collapse ?, ?, and ? , integrat-
ing them out using multinomial-Dirichlet conjugacy
(the Beta distribution used by ? is just a special case
of the Dirichlet with only two parameters).
The update formula for the selected class sv is:
p(sv = k|av, cv ,xv;?, ?)
?
nv?
i
P (avi |sv = k;?)
?
?
i?Sv
favi k + ?
fk + A?
(2)
Where nv is the number of argument observations
for predicate v; Sv is the set of arguments of v that
are selections; and favi k is the frequency of word a
v
i
being assigned to class k for any predicate. We then
sample cvi and xvi jointly:
p(cvi = k, xvi = q|sv, cvi¯ ,xvi¯ ,av;?, ?, ?)
? p(cvi =k;?)p(xvi =q; ?)p(avi |sv, cvi¯ ,xvi¯ ,av;?)
? fvk + ?fv + K?
fvq + ?q
fv0 + ?0 + fv1 + ?1
faz + ?
fz + A?
(3)
982
Where fvq, fv0, and fv1 is the frequency of x values
that equal q, 0, and 1, respectively, for predicate v;
faz is the frequency of word a being in class z and
fz is the frequency all words being in class z, where
z is defined as being equal to k when xvi = 1, or sv
when xvi = 0.
Note that Equation (2) results in a sampling of
the selected class for v proportional to the number
of arguments in each class for v, fulfilling our sec-
ond assumption. Also note from Equation (3), the
second term corresponds to the coerciveness of the
predicate. When the predicate is very coercive, the
marginal probability associated with xvi = 0 will be
very low. If all predicates become entirely coercive,
most x values will become 1 and the cLDA will be-
come almost equivalent to an LDA model.
3.3 Coercion Detection
After the latent parameters have been estimated,
we still require a method to determine if a given
predicate-argument pair is a coercion or not. We
assign a score in [0, 1] instead of a binary value.
Higher scores (near 1) indicate high likelihood of
selection, while lower scores (near 0) indicate coer-
cion. The LDA model must rely on a scoring method
using the predicate-class and argument-class mix-
tures:
C1(v, a) =
K?
k
P (k|v)P (a|k)
=
K?
k
?vk?ka (4)
Where ?vk represents the probability of any argument
of v being in the class k and ?ka represents the prob-
ability of the argument a being in class k for any
predicate. C1 is also available as a scoring method
for cLDA by including the proportion of the selected
class sv in ?. Note that since ? and ? are integrated
out for both LDA and cLDA, we instead use their
frequencies smoothed with ? and ?, respectively,
which is their maximum likelihood estimate.
The cLDA model contains two useful parameters
that can identify selections and coercions: the se-
lected class s and the coercion indicator x. This
yields two more coercion scoring metrics:
C2(v, a) = P (a|sv)
= ?sva (5)
C3(v, a) = P (xva = 0|v, a)
= 1.0 ?
?
i?Iva x
v
i
|Iva |
(6)
Where sv is the selected class for predicate v; Iva
is the set of predicate-argument instances for pred-
icate v and argument a; and xvi is 0 for a selection
and 1 for a coercion. Of the three metrics, C3 is the
most direct measure of a coercion as it represents
the average decision the model learned on the same
predicate-argument pair. However, C3 requires a
large sample of instances for a particular predicate
and argument, and so may be quite sparse. In prac-
tice, these different metrics have their own strengths
and weaknesses and the best performing method of-
ten depends on the final task.
4 Predicate Sense Induction
Our assumption of a single selected class per predi-
cate ignores predicate polysemy. However, the same
lexical item may have multiple meanings, each with
a separate selected class. We therefore propose a
method of partitioning a predicate’s arguments by
the induced senses of the predicate. This allows sep-
arate induced predicates to each select a separate ar-
gument class. Consider the verb fire, which has at
least two distinct common senses: (1) to shoot or
propel an object (e.g., to fire a gun), and (2) to lay
someone off (e.g., to fire an employee). The first
sense selects a weapon (e.g., gun, bullet, rocket),
while the second sense selects a person (e.g., em-
ployee, coach, apprentice).
Specifically, we employ tiered clustering
(Reisinger and Mooney, 2010) using the words
in the predicate’s context. Tiered clustering is a
discrete clustering method, as opposed to methods
such as (Brody and Lapata, 2009) that assign a
distribution of word senses to each word instance.
Tiered clustering has several advantages over
other discrete clustering approaches. First, tiered
clustering learns a background word distribution in
addition to the clusters. This reduces the impact that
words common to most senses have on the cluster-
ing process and allow clusters to form around only
the most salient words. Second, tiered clustering
983
Cluster 1 Cluster 2 Cluster 3 Cluster 4
(18,391) (16,651) (18,749) (11,833)
shots ball hire gun
gun puck letter imagination
Israeli hired Yeltsin grill
missiles owner minister laser
rockets shots workforce cells
officers coaches executives engine
soldiers net employee brain
rounds circle managers !
bullets Johnson hired engines
weapons Williams union fire
Table 1: Context word clusters resulting from tiered clus-
tering for the verb fire (includes the number of unique
words belonging to each cluster).
uses a Chinese Restaurant Process (CRP) prior to
control both the formation of new clusters (senses)
and the bias toward larger clusters (more common
senses). This conforms with our intuition of how
word senses are distributed: a few common senses
with a gradual transition to a long tail of rare senses.
When deciding which cluster to use for a given
predicate-argument pair, we use the cluster most
associated with the argument.
We use a 10-token window around the predicate
as features. The result of predicate induction on the
verb fire is shown in Table 4. The first three clusters
can be interpreted to be about (1) firing weapons, (2)
sport-related shots (e.g., “fired the puck”), and (3)
lay-offs. One must be careful in choosing the param-
eters for induction, however, as it is possible to par-
tition a unique word sense such that coercions and
selections are placed in a separate clusters. Section 6
discusses our parameter selection experiments.
5 Logical Metonymy Interpretation
Logical metonymies are a unique class of coercions
due to the fact that their eventive interpretation can
be derived from verbal predicates. For instance, for
the logical metonymy “enjoy book”, we know that
read is a good candidate interpretation because (1)
books are objects whose purpose is to be read and
(2) reading is an event that may be enjoyed. We
therefore expect to see many instances of both “read
book” and “enjoy reading” (Lapata and Lascarides,
2003). Conversely, for coercions with non-eventive
interpretations, such as “arrive at meeting”, the in-
terpretation (location of) is more dependent on the
predicate (arrive) than the function of its argument
(meeting).
In this section, we limit our discussion of logical
metonymy to the verb-object case, its correspond-
ing baseline for ranking interpretations, and our pro-
posed enhancements. However, similar baselines
exist for other types of logical metonymy, such as
adjective-noun and noun-noun. Since our enhance-
ment does not depend on any syntactic information
beyond the predicate-argument instances needed for
Section 3.2, it could easily be applied to those as
well.
Lapata and Lascarides (2003) propose a proba-
bilistic ranking model where the probability of an
interpretation e for a verb-object pair (v, o) is pro-
portional to the probability of all three in a verb-
interpretation-object pattern.3 For example, the
probability that read is the correct interpretation of
“enjoy book” is proportional to the likelihood of see-
ing “enjoy reading book” expressed as a syntactic
dependency in a sufficiently large corpus. Due to
data sparsity, they approximate this likelihood of
seeing the object given the verb and interpretation
to simply the likelihood of seeing the object given
the interpretation. We denote this logical metonymy
ranking method as LMLL, formally defined as:
LMLL(e; v, o) = Pc(v, e, o)
= Pc(e)Pc(v|e)Pc(o|e, v)
? Pc(e)Pc(v|e)Pc(o|e)
? fc(v, e)fc(o, e)Nfc(e)
(7)
Where Pc and fc indicate probability and frequency,
respectively, derived from corpus counts. See Lap-
ata and Lascarides (2003) for a detailed explanation
of how these frequencies are obtained.
This model, which we consider our baseline, is
only partially correct as the corpus will contain co-
ercions that form invalid interpretations. Consider
the phrases “enjoy finishing a book” and “enjoy
discussing a book”. Both “finish book” and “dis-
cuss book” are coercions (and logical metonymies)
themselves, and do not form a valid interpretation.4
3They use two patterns: “v e-ing o” and “v to e o”, where e
is tagged as a verb.
4For evidence of the frequency of these phrases, at the time
of this writing, “enjoy finishing a book” and “enjoy finishing
the book” have a combined 728 Google hits, while “enjoy dis-
cussing a book” and “enjoy discussing the book” have a com-
984
Thus, when discovering interpretations for logical
metonymies, we must be aware of the selectional re-
strictions of candidate interpretations.
We propose to incorporate the coercion probabil-
ity learned by our cLDA model in order to rank only
those interpretations that are considered selections:
LM ?(e; v, o) = P (v, e, o, xeo = 0) (8)
However, due to the approximations made to esti-
mate Pc(v, e, o), this probability cannot be directly
calculated as not all the frequencies reflect verb-
object counts. Instead, we can combine the corpus
probability Pc(v, e, o) with the probability that the
verb-object pair (e, o) is a coercion in our model.
We denote this probability Px(e, o), and it may be
derived from the scoring metrics in Equations (4),
(5), or (6) above. We further propose three methods
for enhancing the LMLL baseline using Px(e, o) to
approximate Equation 8.
A naive method for including information from
our cLDA model is to consider the corpus prob-
ability, Pc(v, e, o) and the coercion probability,
Px(e, o), to be independent:
LMIND(e; v, o) = Pc(v, e, o)Px(e, o) (9)
In other words, the rank of an interpretation is dic-
tated by the unweighted combination of its corpus
probability Pc and its coercion probability Px. How-
ever, these two quantities are not likely to be inde-
pendent. Most instances where e is used with either
v or o are in fact selective.5 We therefore experiment
with two shallow learning methods for combining
these two quantities.
The first method is a filtering approach where a
threshold is learned for Px:
LMTH(e; v, o) =
{
Pc(v, e, o) if Px(e, o) ? ?
0 otherwise (10)
Where the threshold ? is learned from a development
set. We expect this model could suffer from noisy Px
values or to simply choose a threshold of zero due to
the prominence of Pc.
Finally, we include a weighted linear model to
bined 7,040 Google hits.
5For comparison, “enjoy reading a book” and “enjoy read-
ing the book” have a combined 6.5 million Google hits
discover the relative value of Pc and Px:
LMWT (e; v, o) = w1Pc(v, e, o)+w2Px(e, o) (11)
Where w1 and w2 are learned weights. We dis-
cuss how the parameters for LMTH and LMWT are
learned in the experimental setup below.
6 Experimental Setup
We use the NYT subsection of the English Gigaword
Fourth Edition (Parker et al., 2009) for a total of
1.8M newswire articles. The Stanford Dependency
Parser (de Marneffe et al., 2006) is used to extract
verb-object relations (dobj) that form the input to our
model. To reduce noise, we keep only verbs listed in
VerbNet (Kipper et al., 1998) with at least 100 ar-
gument instances, discarding have and say, which
are too semantically flexible to select from clear se-
mantic classes and so common they distort the class
distributions. This results in 4,145 unique verbs with
51M argument instances (388K unique arguments).
Additionally we use the dependency parser to ex-
tract open clausal complements of verbs (e.g., “like
to swim”) for use in logical metonymy interpreta-
tion. We believe this to be a more reliable alter-
native to the phrase chunk extraction patterns used
in Lapata and Lascarides (2003). We keep clausal
complements (xcomp) where the dependent is either
a gerund or infinitive in order to estimate Pc(v|e) in
Equation (7).
For tiered clustering we use the same implemen-
tation as Reisinger and Mooney (2010)6 to partition
the surface form of the verb into one or more in-
duced forms. Instead of using a fixed number of
iterations, the clustering was run for 100 iterations
past the best recorded log-likelihood in order to find
the best possible fit to the data. We tuned the hyper-
parameters by maximizing the log-likelihood on a
small held-out set of 20 predicate-argument pairs
(10 selections, 10 coercions). The resulting parti-
tions were fairly conservative, yielding 12,332 in-
duced verbs or about 3 induced verb forms for every
surface form, with 305 verbs not being partitioned at
all.
We implemented both LDA and cLDA as de-
scribed in Sections 3.1 and 3.2. For the ? and ?
6Available at http://github.com/joeraii/UTML-Latent-
Variable-Modeling-Toolkit
985
hyper-parameters, we used the MALLET (McCal-
lum, 2002) defaults of 1.0 and 0.1, respectively, for
both LDA and cLDA. We used the 20 predicate-
argument pairs mentioned above to tune the ? hyper-
parameters as well as the number of iterations. Both
?0 and ?1 were set to 100. We observed that for
both LDA and cLDA, longer runs (in iterations) re-
sulted in improved model log-likelihood but infe-
rior results in terms of detecting coercions. It is
not uncommon in topic modeling for model likeli-
hood to not be completely correlated with the score
on the task for which the topic model was intended
(see Chang et al. (2009)). Both LDA and cLDA
were found to perform best at 50 iterations on this
data, after which their class distributions were less
“smooth” and became rigidly associated with just a
few classes, thus having a negative impact on coer-
cion detection. While further iterations hurt coer-
cion detection, only minor gains in model likelihood
are seen. We believe the small number of iterations
necessary for the model to converge is therefore a
function of the data. In traditional topic modeling,
documents are generally of similar size (i.e., within
an order of magnitude). But in our data, many pred-
icates have 10,000 times more instances than others.
We have not yet empirically explored the impact of
using a more uniform number of arguments for each
predicate. This issue also makes it difficult to take
multiple samples, which we experimented with un-
successfully.
Our a priori intuition was that as the number
of classes was increased, LDA would improve and
cLDA would degrade due to its assumption of a sin-
gle selected class. However, this did not always bear
out in the results for every task described below.
As such, instead of choosing a specific number of
classes for each model, we describe results for each
model with K = 10, 25, and 50.
For logical metonymy, both LMTH and LMWT
require learned parameters. LMTH needs a learned
threshold while LMWT needs two learned weights.
For both, we split the data set into two partitions,
learn the optimal threshold/weights on one partition,
and use it as the parameters for the other partition.
Both methods are trained on the final scoring metric,
described in Section 7.3. For threshold learning, this
involves finding the optimal cut-off to maximize the
score. For weight learning, we use an exhaustive
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA
NMI .382 .448 .389 .435 .391 .383
Rand .717 .731 .721 .760 .723 .730
F1 .425 .319 .192 .543 .311 .205
B3 (C) .553 .513 .444 .525 .476 .341
B3 (E) .453 .351 .223 .521 .324 .234
MUC .545 .545 .531 .500 .532 .544
cLDA
NMI .446 .403 .360 .510 .430 .366
Rand .736 .719 .716 .788 .734 .711
F1 .448 .291 .183 .567 .329 .184
B3 (C) .575 .484 .312 .593 .495 .313
B3 (E) .473 .321 .205 .556 .346 .205
MUC .500 .521 .507 .595 .541 .571
Table 2: Clustering scores for induced classes.
search over the range {1.0, 0.9, . . . , 0.2, 0.1, 10?2,
10?3, . . . , 10?14} for both w1 and w2.
7 Results and Discussion
7.1 Semantic Class Induction
For the evaluation of the argument classes in-
duced by our method, we use a subset of the Word-
Net lexicographer files, which correspond to coarse-
grained semantic classes. We chose this form of
evaluation because, unlike a named entity corpus,
no sentential context is required and is therefore
more consistent with the information available to
our model. We use six of the larger, more seman-
tically coherent WordNet classes: artifact, person,
plant, animal, location, and food. We consider each
of these a cluster and compare them to clusters com-
posed of the top ten non-polysemous words (accord-
ing to WordNet) in each of the classes generated
by both the baseline (LDA) and our model (cLDA).
Words not in both sets of clusters are removed. The
result of this evaluation, compared with six cluster-
ing metrics, is shown in Table 2. For descriptions of
NMI, Rand, and cluster F-measure, see Manning et
al. (2008); for the B3 metrics (Cluster and Element),
see Bagga and Baldwin (1998); for the MUC met-
ric, see Vilain et al. (1995). Each metric has differ-
ent strengths and biases in regards to the number and
distribution of clusters, so all are provided to give a
general picture of class induction performance.
The best performing model on all metrics is cLDA
with induced predicates using 10 classes. However,
as the number of classes is increased and the gran-
ularity of the induced classes becomes more fine-
grained, LDA (predictably) outperforms cLDA on
most metrics. This is consistent with our intuition
986
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA C1 74.4 78.7 80.5 69.7 70.1 73.4
cLDA
C1 80.6 81.2 80.9 76.2 78.4 77.5
C2 75.4 75.9 78.9 73.5 68.3 80.8
C3 67.8 70.8 67.4 70.9 67.4 74.1
Table 3: Accuracy on SemEval-2010 Task 7 data.
that a single-class assumption degrades as the num-
ber of classes increases.
For this evaluation, predicate induction also im-
proved LDA for smaller numbers of classes, but not
to the degree that it improved cLDA. Without pred-
icate induction, LDA outperforms cLDA on all six
metrics for 25 and 50 classes. With predicate in-
duction, LDA outperforms cLDA on only one metric
for 25 classes and five metrics for 50 classes. Thus
the induced predicates do reduce the negative im-
pact caused by the single selected class assumption
for semantic class induction.
7.2 Coercion Detection
For the evaluation of coercion detection, we use
the SemEval-2010 Task 7 data (Pustejovsky et al.,
2010). This data uses the most common sense for
each of five predicates (arrive, cancel, deny, fin-
ish, and hear) with a total of 2,070 sentences an-
notated with the argument’s source type (the argu-
ment’s semantic class) and target type (the predi-
cate’s selected class for that argument). We ignore
the actual argument classes and evaluate on the coer-
cion type, which is a selection when the source and
target type match, and a coercion otherwise.
In order to evaluate unsupervised systems on this
data, we use the corresponding training set (1,031
examples) to learn a threshold for coercion detec-
tion. At test time, if the model output is below the
threshold, a coercion is inferred. Otherwise it is con-
sidered a selection. Therefore, the better a model
can rank selections over coercions, the more accu-
rate threshold it will learn. The results for this eval-
uation are shown in Table 3. The baseline for this
task (threshold = 0, or all selections) is 67.4.
The best overall model on this data is cLDA us-
ing the C1 coercion scoring method (Equation (4)).
This method consistently outperforms the baseline
LDA, especially for smaller numbers of classes, per-
forming best with K = 25. The second metric, C2,
was not as reliable. The third metric, C3, performed
poorly on the task. As discussed in Section 3.3, C3
is a direct result of the sampling for the predicate-
argument pair in question and can thus be expected
to perform poorly on rare predicate-argument pairs.
Given that many of the arguments in this data are
rare or unseen in the Gigaword data (e.g., “cancel
Renault”), C3’s poor performance is understandable.
The use of predicate sense induction based on
tiered clustering to overcome the single-class as-
sumption caused significant degradation in perfor-
mance on this task. Using automatically induced
predicates instead of the surface form caused an av-
erage degradation of 2.6 points across the twelve
tests. A potential explanation for this is that
the evaluated predicates have a single dominant
sense, meaning the single class assumption may be
valid for these predicates (the task-defined selected
classes are: location for arrive, event for cancel and
finish, proposition for deny, and sound for hear).
Therefore it would be interesting to evaluate it on
a set of highly polysemous predicates with multi-
ple dominant senses. Furthermore, the introduction
of predicate sense induction was designed to help
cLDA, and the performance degradation for these
nine tests was not as large as it was for LDA. For
cLDA, C1 had an average degradation of 3.5 points
compared to LDA’s C1 average degradation of 6.5
points. cLDA’s C2 had an average degradation of
only 2.5 points and C3 was actually improved by 2.1
points. This suggests that there is value in assign-
ing different selected classes via sense induction, but
that the two-step approach is not beneficial for these
common predicates. This could be overcome by a
joint approach of inducing predicate classes while
simultaneously detecting coercions, as the presence
of many coercions would be an indicator that more
induced predicates are necessary.
7.3 Logical Metonymy Interpretation
For the evaluation of logical metonymy, we use
both an existing data set and a newly created data
set. Shutova and Teufel (2009) annotated 10 verb-
object logical metonymies from Lapata and Las-
carides (2003) with sense-disambiguated interpreta-
tions and organized the interpretations into clusters
representing different possible meanings. For evalu-
ation purposes we ignore the sense annotations and
clusters and consider all lexical matchings of one
of the annotated interpretations to be correct. The
987
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.381 0.365
LMIND
LDA C1 0.415 0.406 0.383 0.386 0.412 0.395
cLDA
C1 0.408 0.412 0.412 0.407 0.468 0.439
C2 0.415 0.447 0.419 0.414 0.415 0.434
C3 0.416 0.453 0.455 0.395 0.416 0.402
LMTH
LDA C1 0.599 0.568 0.588 0.479 0.520 0.551
cLDA
C1 0.571 0.644 0.751 0.497 0.620 0.708
C2 0.544 0.496 0.633 0.457 0.635 0.660
C3 0.601 0.677 0.767 0.472 0.622 0.571
LMWT
LDA C1 0.383 0.381 0.379 0.365 0.356 0.361
cLDA
C1 0.380 0.387 0.381 0.386 0.377 0.321
C2 0.317 0.342 0.350 0.338 0.340 0.345
C3 0.378 0.370 0.350 0.387 0.382 0.384
Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the
best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.274 0.248
LMIND
LDA C1 0.291 0.286 0.294 0.263 0.267 0.255
cLDA
C1 0.296 0.298 0.285 0.280 0.274 0.288
C2 0.291 0.287 0.288 0.283 0.271 0.285
C3 0.318 0.317 0.333 0.298 0.285 0.307
LMTH
LDA C1 0.478 0.534 0.534 0.414 0.495 0.479
cLDA
C1 0.449 0.504 0.541 0.391 0.495 0.513
C2 0.505 0.478 0.456 0.398 0.429 0.440
C3 0.449 0.496 0.577 0.382 0.439 0.446
LMWT
LDA C1 0.276 0.270 0.271 0.248 0.251 0.249
cLDA
C1 0.271 0.272 0.270 0.257 0.259 0.265
C2 0.274 0.274 0.266 0.250 0.259 0.261
C3 0.271 0.273 0.274 0.253 0.262 0.259
Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.
The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based
interpretation method.
data contains an average of 11 interpretations per
metonymy and has a reported 70% recall.
In order to create a larger data set, we identified
100 verb-object logical metonymies, including those
used in Lapata and Lascarides (2003). Three anno-
tators were asked to provide up to five interpreta-
tions for each metonymy (they were not provided
with any verbs from which to choose, only the verb-
object pair). The annotators provided an average of
4.6 interpretations per metonymy. Because our goal
was recall, inter-annotator agreement was necessar-
ily low, and each logical metonymy had an average
of 11.7 unique interpretations. All annotators agreed
on at least one interpretation for 40 metonymies,
while for 14 they had no interpretations in common.7
Since logical metonymy interpretation is usually
evaluated as a ranking task, we score our methods
7 Data available at
http://www.hlt.utdallas.edu/?kirk/data/lmet.zip
using mean average precision (MAP):
MAP = 1Q
Q?
q=1
?N
n=1 prec(n)× rel(n)
interps(q) (12)
Where Q is the number of metonymies evaluated;
N is the number of interpretations ranked; prec(n)
is the precision at rank n; rel(n) = 1 if interpreta-
tion n is valid, 0 otherwise; and interps(q) is the
number of valid interpretations for the metonymy q.
We rank all 4,145 verbs as interpretations except for
those removed by the threshold technique, as they
have a score of zero. This can give LMTH artifi-
cially high MAP scores since it may remove some
valid interpretations that are low-ranking. However,
since a smaller, higher precision list may be useful
for many applications we still consider MAP a valid
metric and indicate both the highest scoring method
and the highest scoring non-threshold method. The
results on the Shutova and Teufel (2009) data are
988
shown in Table 4. The results on our own data are
shown in Table 5.
The scores reported in the Shutova and Teufel
(2009) data are noticeably higher than the data we
annotated. Since the metonymies in our data are a
super-set of those in their data, and since for those
metonymies our annotators provided approximately
the same number of interpretations (110 versus 120),
this likely indicates the remaining metonymies in
our data are more difficult.
In all cases the best reported scores use cLDA.
Unlike coercion detection on the SemEval data, C3
performs very well, achieving the highest scores
when no predicate sense induction is used. Also un-
like coercion detection, LDA scores do not increase
as the number of classes increase. We suspect both
these differences have to do with the fact that the ar-
guments in this data are far more common. Since
LDA is a selectional preference model and its co-
ercion scores correspond roughly to the plausibility
of seeing a predicate-argument pair, it is less able to
distinguish coercions in common arguments.
Of the logical metonymy ranking methods,
LMTH consistently produces the highest MAP
scores. However, as stated before, by using a cut-off
and removing low-ranking valid interpretations, the
MAP score is increased, which might not be applica-
ble to some applications. The best non-thresholded
ranking method is LMIND, which naively combines
the LMLL score with the coercion probability. In
almost every case this beats out LMWT . Upon in-
spection, we observed that the range of scale for the
LMLL scores are very inconsistent. This can make
it difficult to learn a linear model using these scores
as features, and as a result the learned weights were
forced to ignore the coercion score and rely entirely
on LMLL. We attempted other scaling methods,
such as a rank-based method, but these had poor re-
sults as well, so we leave the problem of the super-
vised learning these weights to future work.
Using induced senses did not result in the dras-
tic and consistent degradation in performance seen
on the SemEval data, and the highest non-threshold
result for the Shutova and Teufel (2009) data used
predicate induction. Both metonymy data sets were
limited to the verbs found in Lapata and Lascarides
(2003), which are still quite common (attempt, be-
gin, enjoy, expect, finish, prefer, start, survive, try,
want). However, the verbs used in our data set had a
greater number of WordNet senses attested in a cor-
pus than the SemEval data (an average of 4.4 senses
for our data versus 3.0 senses for the SemEval data).
This suggests the potential value of sense induction
for highly polysemous predicates and further moti-
vates the integration of sense induction within a se-
lectional restriction model.
8 Conclusion
We have presented a novel topic model that ex-
tends an unsupervised selectional preference model
(LDA) to an unsupervised selectional restriction
model (cLDA) using two assumptions. For the first
assumption, that each predicate has a single selected
class, we proposed a predicate induction method to
overcome predicate polysemy. This improved re-
sults for semantic class induction but proved harmful
for detecting coercions on common predicates with
a single, dominant sense. For the second assump-
tion, that the selected class can be inferred from the
data, we proposed a sampling method based on the
classes of the predicate’s arguments. Superior per-
formance on coercion detection shows the merit of
this assumption.
Additionally, we proposed methods for improving
an existing task, logical metonymy interpretation,
using the learned parameters of our model, showing
positive results.
It is clear that our model may be improved by
more accurate predicate sense induction. To this
end, we plan to develop a model that simultane-
ously induces predicates and learns coercions, using
knowledge of a predicate’s coerciveness to inform
the induction mechanism.
Acknowledgements
We would like to thank Diarmuid ´O Se´aghdha,
Bryan Rink, and Anna Rumshisky for several help-
ful conversations during the course of this work.
We thank Mirella Lapata and Ekaterina Shutova for
making the data from their experiments available
as well as the organizers of SemEval-2010 Task 7
for the associated data set. Additionally, we thank
Srikanth Gullapalli, Aileen McDermott, and Bryan
Rink for annotating the data set used in our exper-
iment. Finally, we thank the anonymous reviewers
for their suggestions on improving this work.
989
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In In Proceedings of
the First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic Coref-
erence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 103–111.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems, pages 1–9.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Language Re-
sources and Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
1998. Class-based construction of a verb lexicon. In
Proceedings of AAAI/IAAI.
Maria Lapata and Alex Lascarides. 2003. A Probabilistic
Account of Logical Metonymy. Computational Lin-
guistics, 21(2):261–315.
Dekang Lin and Patrick Pantel. 2001. Induction of Se-
mantic Classes from Natural Language Text. In Pro-
ceedings of ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pages 317–322.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu¨tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit.
Diarmuid ´O Se´aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435–444.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. The LDC Corpus Catalog., LDC2009T13.
James Pustejovsky, Anna Rumshisky, Alex Plotnick,
Elisabetta Jezek, Olga Batiukova, and Valeria Quochi.
2010. SemEval-2010 Task 7: Argument Selection
and Coercion. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 27–32.
Joseph Reisinger and Raymond J. Mooney. 2010. A
Mixture Model with Sharing for Lexical Semantics. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1173–1182.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
424–434.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
Anna Rumshisky, Victor A. Grinberg, and James Puste-
jovsky. 2007. Detecting selectional behavior of com-
plex types in text. In Fourth International Workshop
on Generative Approaches to the Lexicon.
Ekaterina Shutova and Simone Teufel. 2009. Logical
Metonymy: Discovering Classes of Meaning. In Pro-
ceedings of the CogSci 2009 Workshop on Semantic
Space Models.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the ACL 2009 Student Workshop.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference, pages
45–52.
990
