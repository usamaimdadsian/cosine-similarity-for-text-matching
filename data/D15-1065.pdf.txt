Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 555–560,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Inferring Binary Relation Schemas for Open Information Extraction
Kangqi Luo1 and Xusheng Luo2 and Kenny Q. Zhu3
Department of Computer Science & Engineering
Shanghai Jiao Tong University, Shanghai, China
1luokangqi@sjtu.edu.cn 2freefish 6174@sjtu.edu.cn 3kzhu@cs.sjtu.edu.cn
Abstract
This paper presents a framework to model
the semantic representation of binary re-
lations produced by open information ex-
traction systems. For each binary relation,
we infer a set of preferred types on the
two arguments simultaneously, and gen-
erate a ranked list of type pairs which
we call schemas. All inferred types are
drawn from the Freebase type taxonomy,
which are human readable. Our system
collects 171,168 binary relations from Re-
Verb, and is able to produce top-ranking
relation schemas with a mean reciprocal
rank of 0.337.
1 Introduction
Open information extraction (or Open IE) is a task
of extracting all sorts of relations between named
entities or concepts from open-domain text cor-
pora, without restraining itself to specific rela-
tions or patterns. State-of-the-art Open IE sys-
tems (Carlson et al., 2010; Fader et al., 2011;
Schmitz et al., 2012; Nakashole et al., 2012) ex-
tract millions of binary relations with high preci-
sion from the web corpus. Each extracted relation
instance is a triple of the form ?arg
1
, rel, arg
2
?,
where the relation rel is a lexical or syntactic pat-
tern, and both arguments are multi-word expres-
sions representing the argument entities or con-
cepts.
Whereas Open IE provides concrete relation in-
stances, we are interested in generalizing these
instances into more abstract semantic representa-
tions. In this paper, we focus on inferring the
schemas of binary relations.
For example, given the binary relation “play in”,
an Open IE system extracts many triples of the
form ?X, play in, Y ?. The following relation
triples are extracted in ReVerb:
?Goel Grey, played in,Cabaret?
?Tom Brady, play in,National Football League?
Informally, the goal of our system is to
automatically infer a set of schemas such as
?t
1
, play in, t
2
?, where t
1
and t
2
are two seman-
tic types drawn from a standard knowledge base
such as WordNet (Miller, 1995), Yago (Suchanek
et al., 2007), Freebase (Bollacker et al., 2008), and
Probase (Wu et al., 2012), and each such schema
can be used to represent a set of “play in” relation
instances. For the above example, two possible
schemas for “play in” are:
?film actor, play in, film?
?athlete, play in, sports league?
The schema of a binary relation is useful in-
formation in NLP tasks, such as context-oriented
entity recognition and open domain question an-
swering. Suppose we are to recognize the enti-
ties in the sentence “Granger played in the NBA”.
“Granger” is a highly ambiguous term, while “the
NBA” is probably a sports league. Then with the
the above relation schemas for “play in”, the entity
recognizer knows that “Granger” is more likely to
be an athlete, which results in the correct linking
to “Danny Granger”, who is an NBA player, even
though the Open IE has never extracted such fact
before.
One relevant technique to achieve our goal is
selectional preference (SP) (Resnik, 1996; Erk,
2007; Ritter et al., 2010), which computes the
most appropriate types for a specific argument of
a predicate. SP is based on the idea of mutual in-
formation (Erk, 2007), which tends to select types
which are unique to the relation. In other words,
common types which can be used for many differ-
ent relations are less preferred. However, in Open
IE, many relations are related or even similar, e.g.,
play in, take part in and be involved in. There’s
no reason for these relations not to share schemas.
Therefore in this paper, our problem is, given a re-
555
lation and its instances, identify the smallest types
that can cover as many instances as possible. Our
approach first attempts to link the arguments in the
relation instances to a set of possible entities in a
knowledge base, hence generate a set of ?e
1
, e
2
?
entity pairs. Then we select a pair of types ?t
1
, t
2
?
that covers maximum number of entity pairs. We
resolve ties by selecting the smaller (more spe-
cific) types according to a type taxonomy inferred
from knowledge base.
This paper makes the following contributions:
i) we defined the schema inference problem for
binary relations from Open IE; ii) we developed
a prototype system based on Freebase and entity
linking (Lin et al., 2012; Ratinov et al., 2011;
Hoffart et al., 2011; Rao et al., 2013; Cai et al.,
2013), which simultaneously models the type dis-
tributions of two arguments for each binary rela-
tion; iii) our experiment on ReVerb triples showed
that the top inferred schemas receive decent mean
reciprocal rank (MRR) of 0.337, with respect to
the human labeled ground truth.
2 Problem Definition
A knowledge base K is a 5-tuple ?E,Alist, T,
P, IsA?, where:
- E is a finite set of entities e ? E,
- Alist(e) = {n
1
, n
2
, ...} is a function which
returns a set of names (or aliases) of an entity,
- T is a finite set of types t ? T ,
- P is a finite set of relation instances p(e
1
, e
2
),
where p is a predicate in K .
- IsA is a finite set entity-type pairs (e, t),
representing the isA relation between entities
and types. An entity belongs to at least one
type.
An Open IE triple set S contains all relation in-
stances extracted by the IE system, of the form
?a
1
, rel, a
2
?, where a
1
and a
2
are the arguments
of extracted relation pattern rel. The set of argu-
ment pairs sharing the same relation pattern rel is
denoted by S
rel
.
The problem is, for each S
rel
, return a set of
type pairs (or schemas) from T , ?t
1
, t
2
?, ordered
by the number of argument pairs covered in S
rel
.
If two schemas cover the same number of argu-
ment pairs from S
rel
, the schema covering small-
est number of entities wins.
3 System
The workflow of our system is shown in Figure
Figure 1. The system takes Open IE relation tu-
ples as the input, then performs entity linking, re-
lation grouping and schema ranking to translate
them into final ranked list of schemas.
(1) Entity Linking: Relation arguments are
linked to entities in the knowledge base by fuzzy
string matching. Each entity in the knowledge
base has a unique identifier.
(2) Relation Grouping: Linked tuples shar-
ing similar relation patterns are grouped together.
Besides, each group has a representative relation
pattern, which is generated from all the patterns
within the group.
(3) Schema Ranking: For each linked tuple
in one relation group, argument entities are trans-
formed into types drawn from the knowledge base.
Then this procedure ranks type pairs (schemas) in
terms of how much Open IE tuples a type pair can
cover and how specific a type concept is.
3.1 Entity Linking
In the entity linking step, by matching arguments
to entities in the knowledge base, each relation
tuple is transformed into linked tuples, ltup =
?e
1
, rel, e
2
?, with linking scores.
We aim to support fuzzy matching between ar-
guments and entity aliases, so we take all the
aliases into consideration, and build an inverted in-
dex from words to aliases. Different words in one
alias cannot be treated equally. Intuitively, a word
is more important if it occurs in fewer aliases (n),
and vice versa. Based on the inverted index, we
use inverted document frequency score to approx-
imately model the weight of a word w:
idf(w) = 1 / log(|{n : w ? n}|) (1)
Besides, stop words are removed from aliases,
treating their idf scores as 0. In order to measure
the probability of fuzzy matching from an argu-
ment (a) to an alias (n), we introduce the weighted
overlap score:
overlap(a, n) =
?
w?a?n
idf(w)
?
w?a?n
idf(w)
(2)
We merge all the aliases of an entity together
to producing a similarity score of fuzzy matching
between an entity and an argument:
sim(e, a) = max
n?Alist(e)
overlap(a, n) (3)
556


	




	
	


























	

Figure 1: System Architecture
In order to control the quality of candidate en-
tities, for an argument having m words (with stop
words removed), we only keep entities that have
at least one alias matching m? 1 words in the ar-
gument, and have a similarity score larger than a
threshold, ? . With similarity score computed, we
generate 10 best entity candidates respectively for
both the subject and the object of rel.
Next, we model the joint similarity score (F )
of the relation tuple ?a
1
, rel, a
2
? with each entity
pair combination ?e
1
, e
2
? in two ways. One is a
naive method which only considers the similarity
between arguments and corresponding entities:
F (a
1
, e
1
,a
2
, e
2
, rel) =
sim(e
1
, a
1
) × sim(e
2
, a
2
).
(4)
The other method takes predicate paths between
e
1
and e
2
into consideration. Let ~w be the word
vector of rel, and ~p be a path of predicates con-
necting e
1
and e
2
in at most 2 hops. Here we say
two entities e
1
and e
2
are connected in 1 hop, if
there exists a predicate p, such that p(e
1
, e
2
) (or
p(e
2
, e
1
)) is in the knowledge base.
Similarly, e
1
and e
2
are connected in 2 hops,
if there exists two predicates p
1
, p
2
and a transi-
tion entity e?, such that p
1
(e
1
, e
?
) (or p
1
(e
?
, e
1
))
and p
2
(e
?
, e
2
) (or p
2
(e
2
, e
?
)) are in the knowledge
base.
We hence define the relatedness between ~p and
~w in the form of a conditional probability accord-
ing to the Naive Bayes model:
P (~p | ~w) ?
?
p
P (p | ~w)
?
?
p
P (p)
?
w
P (w | p),
(5)
and we follow the IBM alignment Model 1 (Yao
and Van Durme, 2014) to calculate the conditional
probability between predicates and relation words
P (~p | ~w). Based on the information above, we de-
fine a richer joint similarity score, considering all
valid paths between e
1
and e
2
:
F (a
1
, e
1
, a
2
,e
2
, rel) = sim(e
1
, a
1
)×
sim(e
2
, a
2
)×
?
~p
P (~p |~w).
(6)
Due to the multiplications, the value of P (~p |~w)
varies a lot among different entity pair candidates.
The large deviation makes P (~p |~w) the most im-
portant term in Eq. (6), especially in the case when
none of predicate paths are similar enough to the
relation words. Therefore, we trust the factor of
P (~p |~w) only when there exists a similar predicate
path. In practice, we use a threshold ? to control
whether to use Eq. (6) or Eq. (4). We call this an
ensemble method. For each case of entity link-
ing, if there exists one candidate entity pair satis-
fying P (~p | ~w) > ?, we use the ensemble method,
otherwise we fall back to the naive method for the
current case.
3.2 Relation Grouping
In the step of relation grouping, linked tuples with
similar relation patterns form a group. Each linked
tuple belongs to one unique group.
The idea is to simplify relation patterns by syn-
tactic transformations. If two patterns share the
same simplified pattern, we treat them as being
equivalent and put them into one group. First,
since adjectives, adverbs and modal verbs can
hardly change the type distribution of arguments
in a relation, we remove these words from a pat-
tern. Second, many relations from Open IE con-
tain verbs, which come in different tenses. We
transform all tenses into present tense. In addi-
tion, passive voice in a pattern, if any, is kept in
the transformed pattern. A simple example below
shows a group of relations:
?X, resign from, Y?
?X, had resigned from, Y?
?X, finally resigned from, Y?
557
All linked tuples with the same simplified pat-
tern form a group. This pattern is selected as
the representative pattern, like the pattern “resign
from” in the above example.
3.3 Schema Ranking
Given a relation group, the step of schema rank-
ing produces a ranked list of relation schemas with
two constraints. Take “play in” as an example, the
ideal schemas will contain the pair ?actor, f ilm?
and ?athlete, sports league?
Each linked tuple ?e
1
, rel, e
2
? supports the
type pair ?t
1
, t
2
? where (e
1
, t
1
), (e
2
, t
2
) ? IsA in
the knowledge base. We treat these pairs equally,
since it’s not trivial to tell which type is more re-
lated to the argument given the relation tuple as
context. Combining all tuples in one group, we de-
fine the support of a type pair tp in a group (using
the representative pattern r to stand for the group):
sup
r
(?t
1
, t
2
?) =
{(e
1
, t
1
) ? IsA, (e
2
, t
2
) ? IsA}
(7)
A simple intuition is to rank schemas by the size
of the support. Since one entity belongs to mul-
tiple types, relation schemas with general types
will be ranked higher. However, two different
schemas may share the same support. For in-
stance, given the relation “X die in Y”, suppose
Open IE extractions and entity linking step returns
correct results, the schema ?person, location?
and ?deceased person, location? have identical
supports. The latter one shows a more concrete
representation of the relation, because deceased
person covers small entities than person in the
knowledge base.
Therefore, the schemas cannot be ranked by us-
ing the support alone. Next, we aim to extract the
subsumption relations between types in the knowl-
edge base, building the taxonomy of types.
We first define all entities in t as
cover(t) = {e | (e, t) ? IsA}. (8)
Intuitively, type t
1
is subsumed in t
2
, if all enti-
ties in t
1
also belong to t
2
, that is, cover(t
1
) ?
cover(t
2
). This uses the idea of strict set inclu-
sion. For example, we can learn that the type per-
son subsumes types such as actor, politician and
deceased person.
However, strict set inclusion doesn’t always
hold in the knowledge base. For example, enti-
ties in type award winner are mostly person, but
there still has some organizations in it. The strict
method fails to find the subsumption relation be-
tween award winner and person, while this sub-
sumption actually holds with a large confidence.
To resolve this problem, we use a relaxed set
inclusion, where the set cover(t
1
) can be a subset
of another set cover(t
2
) to a certain degree. We
define the degree of the subsumption as the ratio
between the number of entities in the two sets:
deg(t
1
? t
2
) =
|cover(t
1
) ? cover(t
2
)|
|cover(t
1
)|
. (9)
If deg(t
1
? t
2
) > ?, then t
1
is subsumed by
t
2
, and ? is a confidence parameter determined by
weight tuning. By scanning all types in the knowl-
edge base, all subsumption relations with enough
confidence are extracted, forming our type taxon-
omy.
With a type hierarchy computed by above re-
laxed set inclusion, we can define a schema
?t
1
, t
2
? subsumes another schema ?t
3
, t
4
? if i) t
1
subsumes t
3
and t
2
subsumes t
4
; ii) t
1
subsumes
t
3
and t
2
= t
4
; or iii) t
2
subsumes t
4
and t
1
= t
3
.
If a schema (type pair) tp
1
subsumes another
schema tp
2
, and their supports (|sup
r
(tp)|) are
approximately equal, we give the more specific
schema tp
2
a higher rank in the output list. Here
two supports are roughly equal if:
|sup
r
(tp
1
)| ? |sup
r
(tp
2
)|
max(|sup
r
(tp
1
)| , |sup
r
(tp
2
)|)
< ? (10)
Where ? is a threshold determined in the experi-
ments.
4 Evaluation
Freebase (Bollacker et al., 2008) is a collabora-
tively generated knowledge base, which contains
more than 40 million entities, and more than 1,700
real types 1. In our experiment, We use the 16 Feb.
2014 dump of Freebase as the knowledge base.
ReVerb (Fader et al., 2011) is an Open IE sys-
tem which aims to extract verb based relation in-
stances from web corpora. The release ReVerb
dataset contains more than 14 millions of relation
tuples with high quality. We observed that in Re-
Verb, some argument is unlikely to be an entity in
Freebase, for example:
?Metro Manila, consists of, 12 cities?,
1Freebase types are identified by type id, for example,
sports.pro athlete stands for “professional athlete”.
558
where the object argument is not an entity but a
type. Since types are usually represented by low-
ercase common words, we remove the tuple if one
argument is lowercase, or if it is made up com-
pletely of common words in WordNet. In addition,
because date/time such as “Jan. 16th, 1981” often
occurs in the object argument while Freebase does
not have any such specific dates as entities, we use
SUTime (Chang and Manning, 2012) to recognize
dates as an virtual entity. After cleaning, the sys-
tem collects 3,234,208 tuples and 171,168 relation
groups.
The following parameters are tuned using a de-
velopment set: ? = 0.667, ? = 0.6, ? = 5% and
? = e
?50
. For relation grouping, we use Stanford
Parser (Klein and Manning, 2003) to perform POS
tagging, lemmatizing and parsing on relations.
We first evaluate the results of entity linking.
We randomly pick 200 relation instances from Re-
Verb, and manually labeled arguments with Free-
base entities. For both naive and ensemble strat-
egy, we evaluate the precision, recall, F1 and MRR
score on the labeled set. An output entity pair is
correct, if and only if both arguments are correctly
linked. Experimental results are listed in Table 1.
Table 1: Entity Linking Result
Strategy P R F1 MRR
Naive 0.371 0.327 0.348 0.377
Ensemble 0.386 0.340 0.361 0.381
For the evaluation of relation schema, we first
randomly pick 50 binary relations with support
larger than 500 from the system. For each relation,
we selected top 100 type pairs with the largest sup-
port, as what we evaluated. We assigned 3 human
annotators to label the fitness score of type pair
for the relation. The labeled score ranges from 0
to 3. Then we merge these 3 label sets, forming 50
gold standard rankings. When evaluating a rela-
tion schema list from our system, we calculate the
MRR score (Liu, 2009) by the top schemas in the
gold rankings.
For comparison, we use Pointwise Mutual In-
formation (Church and Hanks, 1990) as our base-
line model, which is used in other selectional pref-
erence tasks (Resnik, 1996). We define the associ-
ation score between relation and type pair as:
PMI(r, tp) = p(r, tp) log
p(r, tp)
p(r, ?)p(?, tp)
(11)
Where p(r, tp) is the joint probability of relation
and type pair in the whole linked tuple set, and ?
stands for any relations or type pairs.
Table 2 shows the MRR scores by using both
baseline model (PMI) and our approach. As the re-
sult shows, our approach improves the MRR score
by 10.1%.
Table 2: End-to-end Schema Inference Results
Approach MRR Score
PMI Baseline 0.306
Our Approach 0.337
Finally, Table 3 shows some example binary re-
lations, and their schemas inferred by our system.
We can see that with a well-defined type hierarchy,
our system is able to extract both coarse-grained
and fine-grained type information from entities,
resulting in a informative type lists.
Table 3: Sample Relation Schemas
Relation Top Schemas
?location, location?
be find at ?employer, location?
?organization, location?
?person, tv program?
appear on ?person, nominated work?
?person, winning work?
?person, nominated work?
be the writer of ?person, film?
?person, book subject?
5 Conclusion
In summary, our work describes a data driven ap-
proach of relation schema inference. By max-
imizing the support of both arguments simulta-
neously, our system is able to generate human-
readable type pairs for a binary relation from Open
IE systems. Our experiments shows that the top
ranked relation schemas for each relation are ac-
curate according to human judges. The proposed
framework can be integrated with future Open IE
systems.
Acknowledgement
Kenny Q. Zhu is the contact author and was sup-
ported by NSFC grant 61373031 and NSFC-NRF
Joint Research Program No. 61411140247.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
559
human knowledge. In ACM SIGMOD, pages 1247–
1250. ACM.
Zhiyuan Cai, Kaiqi Zhao, Kenny Q. Zhu, and Haixun
Wang. 2013. Wikification via link co-occurrence.
In ACM CIKM’13, pages 1087–1096.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI, volume 5,
page 3.
Angel X Chang and Christopher D Manning. 2012.
Sutime: A library for recognizing and normalizing
time expressions. In LREC, pages 3735–3740.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Katrin Erk. 2007. A simple, similarity-based
model for selectional preferences. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, page 216.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535–1545.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fu¨rstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of EMNLP, pages
782–792.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423–430.
Thomas Lin, Oren Etzioni, et al. 2012. Entity link-
ing at web scale. In Proceedings of the Joint Work-
shop on Automatic Knowledge Base Construction
and Web-scale Knowledge Extraction, pages 84–88.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225–331.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In EMNLP, pages
1135–1145.
Delip Rao, Paul McNamee, and Mark Dredze. 2013.
Entity linking: Finding extracted entities in a knowl-
edge base. In Multi-source, multilingual informa-
tion extraction and summarization, pages 93–115.
Springer.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for
disambiguation to wikipedia. In Proceedings of
ACL:HLT, pages 1375–1384.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127–159.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
ACL, pages 424–434.
Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al. 2012. Open language learning
for information extraction. In EMNLP, pages 523–
534.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Wentao Wu, Hongsong Li, Haixun Wang, and
Kenny Q. Zhu. 2012. Probase: a probabilistic tax-
onomy for text understanding. In SIGMOD Confer-
ence, pages 481–492.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
560
