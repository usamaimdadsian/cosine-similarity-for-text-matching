Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182–192,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Joint Learning of Phonetic Units and Word Pronunciations for ASR
Chia-ying Lee, Yu Zhang, James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,yzhang87,jrg}@csail.mit.edu
Abstract
The creation of a pronunciation lexicon re-
mains the most inefficient process in develop-
ing an Automatic Speech Recognizer (ASR).
In this paper, we propose an unsupervised
alternative – requiring no language-specific
knowledge – to the conventional manual ap-
proach for creating pronunciation dictionar-
ies. We present a hierarchical Bayesian model,
which jointly discovers the phonetic inven-
tory and the Letter-to-Sound (L2S) mapping
rules in a language using only transcribed
data. When tested on a corpus of spontaneous
queries, the results demonstrate the superior-
ity of the proposed joint learning scheme over
its sequential counterpart, in which the la-
tent phonetic inventory and L2S mappings are
learned separately. Furthermore, the recogniz-
ers built with the automatically induced lexi-
con consistently outperform grapheme-based
recognizers and even approach the perfor-
mance of recognition systems trained using
conventional supervised procedures.
1 Introduction
Modern automatic speech recognizers require a few
essential ingredients such as a signal representation
of the speech signal, a search component, and typ-
ically a set of stochastic models that capture 1) the
acoustic realizations of the basic sounds of a lan-
guage, for example, phonemes, 2) the realization of
words in terms of these sounds, and 3) how words
are combined in spoken language. When creating
a speech recognizer for a new language the usual
requirements are: first, a large speech corpus with
word-level annotations; second, a pronunciation dic-
tionary that essentially defines a phonetic inventory
for the language as well as word-level pronuncia-
tions, and third, optional additional text data that
can be used to train the language model. Given
these data and some decision about the signal rep-
resentation, e.g., centi-second Mel-Frequency Cep-
stral Coefficients (MFCCs) (Davis and Mermelstein,
1980) with various derivatives, as well as the nature
of the acoustic and language model such as 3-state
HMMs and n-grams, iterative training methods can
be used to effectively learn the model parameters for
the acoustic and language models. Although the de-
tails of the components have changed through the
years, this basic ASR formulation was well estab-
lished by the late 1980’s, and has not really changed
much since then.
One of the interesting aspects of this formulation
is the inherent dependence on the dictionary, which
defines both the phonetic inventory of a language,
and the pronunciations of all the words in the vo-
cabulary. The dictionary is arguably the cornerstone
of a speech recognizer as it provides the essential
transduction from sounds to words. Unfortunately,
the dependency on this resource is a significant im-
pediment to the creation of speech recognizers for
new languages, since they are typically created by
experts, whereas annotated corpora can be relatively
more easily created by native speakers of a language.
The existence of an expert-derived dictionary in
the midst of stochastic speech recognition models is
somewhat ironic, and it is natural to ask why it con-
tinues to receive special status after all these years.
Why can we not learn the inventory of sounds of a
language and associated word pronunciations auto-
matically, much as we learn our acoustic model pa-
rameters? If successful, we would move one step
forward towards breaking the language barrier that
182
limits us from having speech recognizers for all lan-
guages of the world, instead of the less than 2% that
currently exist.
In this paper, we investigate the problem of infer-
ring a pronunciation lexicon from an annotated cor-
pus without exploiting any language-specific knowl-
edge. We formulate our approach as a hierarchi-
cal Bayesian model, which jointly discovers the
acoustic inventory and the latent encoding scheme
between the letters and the sounds of a language.
We evaluate the quality of the induced lexicon and
acoustic model through a series of speech recogni-
tion experiments on a conversational weather query
corpus (Zue et al., 2000). The results demonstrate
that our model consistently generates close perfor-
mance to recognizers that are trained with expert-
defined phonetic inventory and lexicon. Compared
to grapheme-based recognizers, our model is capa-
ble of improving the Word Error Rates (WERs) by
at least 15.3%. Finally, the joint learning framework
proposed in this paper is proven to be much more
effective than modeling the acoustic units and the
letter-to-sound mappings separately, as shown in a
45% WER deduction our model achieves compared
to a sequential approach.
2 Related Work
Various algorithms for learning sub-word based pro-
nunciations were proposed in (Lee et al., 1988;
Fukada et al., 1996; Bacchiani and Ostendorf, 1999;
Paliwal, 1990). In these previous approaches, spo-
ken samples of a word are gathered, and usually
only one single pronunciation for the word is de-
rived based on the acoustic evidence observed in the
spoken samples. The major difference between our
work and these previous works is that our model
learns word pronunciations in the context of letter
sequences. More specifically, our model learns letter
pronunciations first and then concatenates the pro-
nunciation of each letter in a word to form the word
pronunciation. The advantage of our approach is
that pronunciation knowledge learned for a particu-
lar letter in some arbitrary word can subsequently be
used to help learn the letter’s pronunciation in other
words. This property allows our model to potentially
learn better pronunciations for less frequent words.
The more recent work by Garcia and Gish (2006)
and Siu et al. (2013) has made extensive use
of self-organizing units for keyword spotting and
other tasks for languages with limited linguistic
resources. Others who have more recently ex-
plored the unsupervised space include (Varadarajan
et al., 2008; Jansen and Church, 2011; Lee and
Glass, 2012). The latter work introduced a non-
parametric Bayesian inference procedure for auto-
matically learning acoustic units that is most similar
to our current work except that our model also infers
word pronunciations simultaneously.
The concept of creating a speech recognizer for
a language with only orthographically annotated
speech data has also been explored previously by
means of graphemes. This approach has been shown
to be effective for alphabetic languages with rela-
tively straightforward grapheme to phoneme trans-
formations and does not require any unsupervised
learning of units or pronunciations (Killer et al.,
2003; Stu¨ker and Schultz, 2004). As we explain in
later sections, grapheme-based systems can actually
be regarded as a special case of our model; therefore,
we expect our model to have greater flexibilities for
capturing pronunciation rules of graphemes.
3 Model
The goal of our model is to induce a word pronunci-
ation lexicon from spoken utterances and their cor-
responding word transcriptions. No other language-
specific knowledge is assumed to be available, in-
cluding the phonetic inventory of the language. To
achieve the goal, our model needs to solve the fol-
lowing two tasks:
• Discover the phonetic inventory.
• Reveal the latent mapping between the letters
and the discovered phonetic units.
We propose a hierarchical Bayesian model for
jointly discovering the two latent structures from
an annotated speech corpus. Before presenting our
model, we first describe the key latent and observed
variables of the problem.
Letter (lmi ) We use l
m
i to denote the i
th let-
ter observed in the word transcription of the
mth training sample. To be sure, a train-
ing sample involves a speech utterance and its
183
corresponding text transcription. The letter se-
quence composed of lmi and its context, namely
lmi??, · · · , l
m
i?1, l
m
i , l
m
i+1, · · · , l
m
i+?, is denoted as ~l
m
i,?.
Although lmi is referred to as a letter in this paper,
it can represent any character observed in the text
data, including space and symbols indicating sen-
tence boundaries. The set of unique characters ob-
served in the data set is denoted as G. For notation
simplicity, we use L? to denote the set of letter se-
quences of length 2? + 1 that appear in the dataset
and use ~l? to denote the elements in L?. Finally,
P(~l?) is used to represent the parent of ~l?, which is
a substring of ~l? with the first and the last characters
truncated.
Number of Mapped Acoustic Units (nmi ) Each
letter lmi in the transcriptions is assumed to be
mapped to a certain number of phonetic units. For
example, the letter x in the word fox is mapped to
2 phonetic units /k/ and /s/, while the letter e in the
word lake is mapped to 0 phonetic units. We denote
this number as nmi and limit its value to be 0, 1 or 2
in our model. The value of nmi is always unobserved
and needs to be inferred by the our model.
Identity of the Acoustic Unit (cmi,p) For each pho-
netic unit that lmi maps to, we use c
m
i,p, for 1 ? p ?
nmi , to denote the identity of the phonetic unit. Note
that the phonetic inventory that describes the data
set is unknown to our model, and the identities of
the phonetic units are associated with the acoustic
units discovered automatically by our model.
Speech Feature xmt The observed speech data in
our problem are converted to a series of 25 ms 13-
dimensional MFCCs (Davis and Mermelstein, 1980)
and their first- and second-order time derivatives at
a 10 ms analysis rate. We use xmt ? R
39 to denote
the tth feature frame of the mth utterance.
3.1 Generative Process
We present the generative process for a single train-
ing sample (i.e., a speech utterance and its corre-
sponding text transcription); to keep notation sim-
ple, we discard the index variable m in this section.
For each li in the transcription, the model gener-
ates ni, given ~li,?, from the 3-dimensional categori-
cal distribution ?~li,?(ni). Note that for every unique
~li,? letter sequence, there is an associated ?~li,?(ni)
lj 
  1?  p ? ni 
?0 
ci, p 
?0 
K
?c 
di,p  
? 
1 ? i ? Lm 
ni 
xt 
1 ? m ? M 
?l2,n,p 
? ? 
?l,n,p 
G ×{(n,p) | 0 ? n ? 2, 1 ? p ? n} 
?l1,n,p 
G ×G 
G ×G 
?1 
?2 
i-2 ? j ? i+2 
Figure 1: The graphical representation of the pro-
posed hierarchical Bayesian model. The shaded cir-
cle denotes the observed text and speech data, and
the squares denote the hyperparameters of the priors
in our model. See Sec. 3 for a detailed explanation
of the generative process of our model.
distribution, which captures the fact that the number
of phonetic units a letter maps to may depend on its
context. In our model, we impose a Dirichlet distri-
bution prior Dir(?) on ?~li,?(ni).
If ni = 0, li is not mapped to any acoustic units
and the generative process stops for li; otherwise,
for 1 ? p ? ni, the model generates ci,p from:
ci,p ? pi~li,?,ni,p (1)
where pi~li,?,ni,p is a K-dimensional categorical dis-
tribution, whose outcomes correspond to the pho-
netic units discovered by the model from the given
speech data. Eq. 1 shows that for each combination
of~li,?, ni and p, there is an unique categorical distri-
bution. An important property of these categorical
distributions is that they are coupled together such
that their outcomes point to a consistent set of pho-
netic units. In order to enforce the coupling, we con-
struct pi~li,?,ni,p through a hierarchical process.
? ? Dir(?) (2)
pi~li,?,ni,p ? Dir(???) for ? = 0 (3)
pi~li,?,ni,p ? Dir(??pi~li,??1,ni,p) for ? ? 1 (4)
184
To interpret Eq. 2 to Eq. 4, we envision that
the observed speech data are generated by a K-
component mixture model, of which the components
correspond to the phonetic units in the language. As
a result, ? in Eq. 2 can be viewed as the mixture
weight over the components, which indicates how
likely we are to observe each acoustic unit in the
data overall. By adopting this point of view, we
can also regard the mapping between li and the pho-
netic units as a mixture model, and pili,ni,p
1 repre-
sents how probable li is mapped to each phonetic
unit given ni and p. We apply a Dirichlet distribu-
tion prior parametrized by ?0? to pili,ni,p as shown
in Eq. 3. With this parameterization, the mean of
pili,ni,p is the global mixture weight ?, and ?0 con-
trols how similar pili,ni,p is to the mean. More specif-
ically, for large ?0  K, the Dirichlet distribution
is highly peaked around the mean; on the contrary,
for ?0  K, the mean lies in a valley. The parame-
ters of a Dirichlet distribution can also be viewed as
pseudo-counts for each category. Eq. 4 shows that
the prior for pi~li,?,ni,p is seeded by pseudo-counts
that are proportional to the mapping weights over
the phonetic units of li in a shorter context. In other
words, the mapping distribution of li in a shorter
context can be thought of as a back-off distribution
of li’s mapping weights in a longer context.
Each component of the K-dimensional mixture
model is linked to a 3-state Hidden Markov Model
(HMM). These K HMMs are used to model the
phonetic units in the language (Jelinek, 1976). The
emission probability of each HMM state is modeled
by a diagonal Gaussian Mixture Model (GMM). We
use ?c to represent the set of parameters that define
the cth HMM, which includes the state transition
probability and the GMM parameters of each state
emission distribution. The conjugate prior of ?c is
denoted as H(?0)2.
Finally, to finish the generative process, for each
ci,p we use the corresponding HMM ?ci,p to gen-
erate the observed speech data xt, and the genera-
tive process of the HMM determines the duration,
1An abbreviation of pi~li,0,ni,p
2H(?0) includes a Dirichlet prior for the transition probabil-
ity of each state, and a Dirichlet prior for each mixture weight
of the three GMMs, and a normal-Gamma distribution for the
mean and precision of each Gaussian mixture in the 3-state
HMM.
di,p, of the speech segment. The complete genera-
tive model, with ? set to 2, is depicted in Fig. 1; M
is the total number of transcribed utterances in the
corpus, and Lm is the number of letters in utterance
m. The shaded circles denote the observed data, and
the squares denote the hyperparameters of the priors
used in our model. Lastly, the unshaded circles de-
note the latent variables of our model, for which we
derive inference algorithms in the next section.
4 Inference
We employ Gibbs sampling (Gelman et al., 2004) to
approximate the posterior distribution of the latent
variables in our model. In the following sections, we
first present a message-passing algorithm for block-
sampling ni and ci,p, and then describe how we
leverage acoustic cues to accelerate the computa-
tion of the message-passing algorithm. Note that the
block-sampling algorithm for ni and ci,p can be par-
allelized across utterances. Finally, we briefly dis-
cuss the inference procedures for ?~l? , pi~l?,n,p, ?, ?c.
4.1 Block-sampling ni and ci,p
To understand the message-passing algorithm in this
study, it is helpful to think of our model as a sim-
plified Hidden Semi-Markov Model (HSMM), in
which the letters represent the states and the speech
features are the observations. However, unlike in
a regular HSMM, where the state sequence is hid-
den, in our case, the state sequence is fixed to be the
given letter sequence. With this point of view, we
can modify the message-passing algorithms of Mur-
phy (2002) and Johnson and Willsky (2013) to com-
pute the posterior information required for block-
sampling ni and ci,p.
Let L(xt) be a function that returns the index
of the letter from which xt is generated; also, let
Ft = 1 be a tag indicating that a new phone segment
starts at t+ 1. Given the constraint that 0 ? ni ? 2,
for 0 ? i ? Lm and 0 ? t ? Tm, the backwards
messages Bt(i) and B?t (i) for the m
th training sam-
ple can be defined and computed as in Eq. 5 and
Eq. 7. Note that for clarity we discard the index vari-
able m in the derivation of the algorithm.
185
Bt(i) , p(xt+1:T |L(xt) = i, Ft = 1)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
p(nk = 0|~li,?)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
?~li,?(0) (5)
B?t (i) , p(xt+1:T |L(xt+1) = i, Ft = 1)
=
T?t?
d=1
p(xt+1:t+d|~li,?)Bt+d(i) (6)
=
T?t?
d=1
{
K?
ci,1=1
?~li,?(1)pi~li,?,1,1(ci,1)p(xt+1:t+d|?ci,1)
+
d?1?
v=1
K?
ci,1
K?
ci,2
?~li,?(2)pi~li,?,2,1(ci,1)pi~li,?,2,2(ci,2)
× p(xt+1:t+v|?ci,1)p(xt+v+1:t+d|?ci,2)}Bt+d(i)
(7)
We use xt1:t2 to denote the segment consisting of
xt1 , · · · , xt2 . Our inference algorithm only allows
up to U letters to emit 0 acoustic units in a row. The
value of U is set to 2 for our experiments. Bt(i)
represents the total probability of all possible align-
ments between xt+1:T and li+1:L. B?t (i) contains
the probability of all the alignments between xt+1:T
and li+1:L that map xt+1 to li particularly. This
alignment constraint between xt+1 and li is explic-
itly shown in the first term of Eq. 6, which represents
how likely the speech segment xt+1:t+d is generated
by li given li’s context. This likelihood is simply
the marginal probability of p(xt+1:t+d, ni, ci,p|~li,?)
with ni and ci,p integrated out, which can be ex-
panded and computed as shown in the last three rows
of Eq. 7. The index v specifies where the phone
boundary is between the two acoustic units that li
is aligned with when ni = 2. Eq. 8 to Eq. 10 are
the boundary conditions of the message passing al-
gorithm. B0(0) carries the total probably of all pos-
sible alignments between l1:L and x1:T . Eq. 9 spec-
ifies that at most U letters at the end of an sentence
can be left unaligned with any speech features, while
Eq. 10 indicates that all of the speech features in an
utterance must be assigned to a letter.
Algorithm 1 Block-sample ni and ci,p from Bt(i)
and B?t (i)
1: i? 0
2: t? 0
3: while i < L ? t < T do
4: nexti ? SampleFromBt(i)
5: if nexti > i+ 1 then
6: for k = i+ 1 to k = nexti ? 1 do
7: nk ? 0
8: end for
9: end if
10: d, ni, ?ci,p?, v ? SampleFromB?t (nexti)
11: t? t+ d
12: i? nexti
13: end while
B0(0) =
min{L,U+1}?
j=1
B?0(j)
j?1?
k=1
?~li,?(0) (8)
BT (i) ,
?
??
??
1 if i = L
?L
j=i+1 ?~li,?(0) if L? U ? i < L
0 if i < L? U
(9)
Bt(L) ,
{
1 if t = T
0 otherwise
(10)
Given Bt(i) and B?t (i), ni and ci,p for each letter
in the utterance can be sampled using Alg. 1. The
SampleFromBt(i) function in line 4 returns a ran-
dom sample from the relative probability distribu-
tion composed by entries of the summation in Eq. 5.
Line 5 to line 9 check whether li (and maybe li+1)
is mapped to zero phonetic units. nexti points to
the letter that needs to be aligned with 1 or 2 phone
segments starting from xt. The number of phonetic
units that lnexti maps to and the identities of the
units are sampled in SampleFromB?t (i). This sub-
routine generates a tuple of d, ni, ?ci,p? as well as
v (if ni = 2) from all the entries of the summation
shown in Eq. 73.
3We use ?ci,p? to denote that ?ci,p?may consist of two num-
bers, ci,1 and ci,2, when ni = 2.
186
4.2 Heuristic Phone Boundary Elimination
The variables d and v in Eq. 7 enumerate through
every frame index in a sentence, treating each fea-
ture frame as a potential boundary between acous-
tic units. However, it is possible to exploit acoustic
cues to avoid checking feature frames that are un-
likely to be phonetic boundaries. We follow the pre-
segmentation method described in Glass (2003) to
skip roughly 80% of the feature frames and greatly
speed up the computation of B?t (i).
Another heuristic applied to our algorithm to re-
duce the search space for d and v is based on the
observation that the average duration of phonetic
units is usually no longer than 300 ms. Therefore,
when computing B?t (i), we only consider speech
segments that are shorter than 300 ms to avoid align-
ing letters to speech segments that are too long to be
phonetic units.
4.3 Sampling ?~l? , pi~l?,ni,p, ? and ?c
Sampling ?~l? To compute the posterior distribu-
tion of ?~l? , we count how many times
~l? is mapped
to 0, 1 and 2 phonetic units from nmi . More specifi-
cally, we define N~l?(j) for 0 ? j ? 2 as follows:
N~l?(j) =
M?
m=1
Lm?
i=1
?(nmi , j)?(~l
m
i,?,~l?)
where we use ?(·) to denote the discrete Kronecker
delta. With N~l? , we can simply sample a new value
for ?~l? from the following distribution:
?~l? ? Dir(? +N~l?)
Sampling pi~l?,n,p and ? The posterior distribu-
tions of pi~l?,n,p and ? are constructed recursively due
to the hierarchical structure imposed on pi~l?,n,p and
?. We start with gathering counts for updating the
pi variables at the lowest level, i.e., pi~l2,n,p given that
? is set to 2 in our model implementation, and then
sample pseudo-counts for the pi variables at higher
hierarchies as well as ?. With the pseudo-counts, a
new ? can be generated, which allows pi~l?,n,p to be
re-sampled sequentially.
More specifically, we define C~l2,n,p(k) to be the
number of times that ~l2 is mapped to n units and
the unit in position p is the kth phonetic unit. This
value can be counted from the current values of cmi,p
as follows.
C~l2,n,p(k) =
M?
m=1
Lm?
i=1
?(~li,2,~l2)?(n
m
i , n)?(c
m
i,p, k)
To derive the posterior distribution of pi~l1,n,p an-
alytically, we need to sample pseudo-counts C~l1,n,p,
which is defined as follows.
C~l1,n,p(k) =
?
~l2?U~l1
C~l2,n,p
(k)
?
i=1
I[?i <
?2pi~l1,n,p(k)
i+ ?2pi~l1,n,p(k)
]
(11)
We use U~l1 = {
~l2|P(~l2) = ~l1} to denote the set of
~l2 whose parent is~l1 and ?i to represent random vari-
ables sampled from a uniform distribution between
0 and 1. Eq. 11 can be applied recursively to com-
pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts that
are applied to the conjugate priors of pi~l0,n,p and ?.
With the pseudo-count variables computed, new val-
ues for ? and pi~l?,n,p can be sampled sequentially as
shown in Eq. 12 to Eq. 14.
? ? Dir(? + C ,n,p) (12)
pi~l?,n,p ? Dir(??? + C~l?,n,p) for ? = 0 (13)
pi~l?,n,p ? Dir(??pi~l??1,n,p + C~l?,n,p) for ? ? 1
(14)
5 Experimental Setup
To test the effectiveness of our model for joint learn-
ing phonetic units and word pronunciations from an
annotated speech corpus, we construct speech rec-
ognizers out of the training results of our model.
The performance of the recognizers is evaluated and
compared against three baselines: first, a grapheme-
based speech recognizer; second, a recognizer built
by using an expert-crafted lexicon, which is referred
to as an expert lexicon in the rest of the paper for
simplicity; and third, a recognizer built by discover-
ing the phonetic units and L2S pronunciation rules
sequentially without using a lexicon. In this section,
we provide a detailed description of the experimen-
tal setup.
187
? ? ?0 ?1 ?2 ?0 ? K
?0.1?3 ?10?100 1 0.1 0.2 * 2 100
Table 1: The values of the hyperparameters of our
model. We use ?a?D to denote aD-dimensional vec-
tor with all entries being a. *We follow the proce-
dure reported in (Lee and Glass, 2012) to set up the
HMM prior ?0.
5.1 Dataset
All the speech recognition experiments reported in
this paper are performed on a weather query dataset,
which consists of narrow-band, conversational tele-
phone speech (Zue et al., 2000). We follow the ex-
perimental setup of McGraw et al. (2013) and split
the corpus into a training set of 87,351 utterances, a
dev set of 1,179 utterances and a test set of 3,497 ut-
terances. A subset of 10,000 utterances is randomly
selected from the training set. We use this subset of
data for training our model to demonstrate that our
model is able to discover the phonetic composition
and the pronunciation rules of a language even from
just a few hours of data.
5.2 Building a Recognizer from Our Model
The values of the hyperparameters of our model are
listed in Table 1. We run the inference procedure de-
scribed in Sec. 4 for 10,000 times on the randomly
selected 10,000 utterances. The samples of ?~l? and
pi~l?n,p from the last iteration are used to decode n
m
i
and cmi,p for each sentence in the entire training set by
following the block-sampling algorithm described
in Sec. 4.1. Since cmi,p is the phonetic mapping of
lmi , by concatenating the phonetic mapping of ev-
ery letter in a word, we can obtain a pronunciation
of the word represented in the labels of discovered
phonetic units. For example, assume that word w
appears in sentence m and consists of l3l4l5 (the
sentence index m is ignored for simplicity). Also,
assume that after decoding, n3 = 1, n4 = 2 and
n5 = 1. A pronunciation ofw is then encoded by the
sequence of phonetic labels c3,1c4,1c4,2c5,1. By re-
peating this process for each word in every sentence
for the training set, a list of word pronunciations can
be compiled and used as a stochastic lexicon to build
a speech recognizer.
In theory, the HMMs inferred by our model can be
directly used as the acoustic model of a monophone
speech recognizer. However, if we regard the ci,p
labels of each utterance as the phone transcription
of the sentence, then a new acoustic model can be
easily re-trained on the entire data set. More conve-
niently, the phone boundaries corresponding to the
ci,p labels are the by-products of the block-sampling
algorithm, which are indicated by the values of d and
v in line 10 of Alg. 1 and can be easily saved during
the sampling procedure. Since these data are readily
available, we re-build a context-independent model
on the entire data set. In this new acoustic model,
a 3-state HMM is used to model each phonetic unit,
and the emission probability of each state is modeled
by a 32-mixture GMM.
Finally, a trigram language model is built by using
the word transcriptions in the full training set. This
language model is utilized in all speech recogni-
tion experiments reported in this paper. Finite State
Transducers (FSTs) are used to build all the recog-
nizers used in this study. With the language model,
the lexicon and the context-independent acoustic
model constructed by the methods described in this
section, we can build a speech recognizer from
the learning output of the proposed model without
the need of a pre-defined phone inventory and any
expert-crafted lexicons.
5.2.1 Pronunciation Mixture Model Retraining
McGraw et al. (2013) presented the Pronuncia-
tion Mixture Model (PMM) for composing stochas-
tic lexicons that outperform pronunciation dictionar-
ies created by experts. Although the PMM frame-
work was designed to incorporate and augment ex-
pert lexicons, we found that it can be adapted to pol-
ish the pronunciation list generated by our model.
In particular, the training procedure for PMMs in-
cludes three steps. First, train a L2S model from
a manually specified expert-pronunciation lexicon;
second, generate a list of pronunciations for each
word in the dataset using the L2S model; and finally,
use an acoustic model to re-weight the pronuncia-
tions based on the acoustic scores of the spoken ex-
amples of each word.
To adapt this procedure for our purposes, we sim-
ply plug in the word pronunciations and the acous-
tic model generated by our model. Once we ob-
tain the re-weighted lexicon, we re-generate forced
188
phone alignments and retrain the acoustic model,
which can be utilized to repeat the PMM lexicon re-
weighting procedure. For our experiments, we it-
erate through this model refining process until the
recognition performance converges.
5.2.2 Triphone Model
Conventionally, to train a context-dependent
acoustic model, a list of questions based on the
linguistic properties of phonetic units is required
for growing decision tree classifiers (Young et al.,
1994). However, such language-specific knowledge
is not available for our training framework; there-
fore, our strategy is to compile a question list that
treats each phonetic unit as a unique linguistic class.
In other words, our approach to training a context-
dependent acoustic model for the automatically dis-
covered units is to let the decision trees grow fully
based on acoustic evidence.
5.3 Baselines
We compare the recognizers trained by following
the procedures described in Sec. 5.2 against three
baselines. The first baseline is a grapheme-based
speech recognizer. We follow the procedure de-
scribed in Killer et al. (2003) and train a 3-state
HMM for each grapheme, which we refer to as the
monophone grapheme model. Furthermore, we cre-
ate a singleton question set (Killer et al., 2003), in
which each grapheme is listed as a question, to train
a triphone grapheme model. Note that to enforce
better initial alignments between the graphemes and
the speech data, we use a pre-trained acoustic model
to identify the non-speech segments at the beginning
and the end of each utterance before starting training
the monophone grapheme model.
Our model jointly discovers the phonetic inven-
tory and the L2S mapping rules from a set of tran-
scribed data. An alternative of our approach is to
learn the two latent structures sequentially. We fol-
low the training procedure of Lee and Glass (2012)
to learn a set of acoustic models from the speech
data and use these acoustic models to generate a
phone transcription for each utterance. The phone
transcriptions along with the corresponding word
transcriptions are fed as inputs to the L2S model
proposed in Bisani and Ney (2008). A stochastic
lexicon can be learned by applying the L2S model
unit(%) Monophone
Our model 17.0
Oracle 13.8
Grapheme 32.7
Sequential model 31.4
Table 2: Word error rates generated by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3 on the weather query corpus.
and the discovered acoustic models to PMM. This
two-stage approach for training a speech recognizer
without an expert lexicon is referred to as the se-
quential model in this paper.
Finally, we compare our system against a rec-
ognizer trained from an oracle recognition system.
We build the oracle recognizer on the same weather
query corpus by following the procedure presented
in McGraw et al. (2013). This oracle recognizer is
then applied to generate forced-aligned phone tran-
scriptions for the training utterances, from which
we can build both monophone and triphone acous-
tic models. The expert-crafted lexicon used in the
oracle recognizer is also used in this baseline. Note
that for training the triphone model, we compose a
singleton question list (Killer et al., 2003) that has
every expert-defined phonetic unit as a question. We
use this singleton question list instead of a more so-
phisticated one to ensure that this baseline and our
system differ only in the acoustic model and the lex-
icon used to generate the initial phone transcriptions.
We call this baseline the oracle baseline.
6 Results and Analysis
6.1 Monophone Systems
Table 2 shows the WERs produced by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3. It can be seen that our model outper-
forms the grapheme and the sequential model base-
lines significantly while approaching the perfor-
mance of the supervised oracle baseline. The im-
provement over the sequential baseline demonstrates
the strength of the proposed joint learning frame-
work. More specifically, unlike the sequential base-
line, in which the acoustic units are discovered in-
dependently from the text data, our model is able to
exploit the L2S mapping constraints provided by the
word transcriptions to cluster speech segments.
189
By comparing our model to the grapheme base-
line, we can see the advantage of modeling the
pronunciations of a letter using a mixture model,
especially for a language like English which has
many pronunciation irregularities. However, even
for languages with straightforward pronunciation
rules, the concept of modeling letter pronunciations
using mixture models still applies. The main dif-
ference is that the mixture weights for letters of
languages with simple pronunciation rules will be
sparser and spikier. In other words, in theory, our
model should always perform comparable to, if not
better than, grapheme recognizers.
Last but not least, the recognizer trained with the
automatically induced lexicon performs similarly to
the recognizer initialized by an oracle recognition
system, which demonstrates the effectiveness of the
proposed model for discovering the phonetic inven-
tory and a pronunciation lexicon from an annotated
corpus. In the next section, we provide some in-
sights into the quality of the learned lexicon and
into what could have caused the performance gap
between our model and the conventionally trained
recognizer.
6.2 Pronunciation Entropy
The major difference between the recognizer that is
trained by using our model and the recognizer that
is seeded by an oracle recognition system is that
the former uses an automatically discovered lexicon,
while the latter exploits an expert-defined pronun-
ciation dictionary. In order to quantify, as well as
to gain insights into, the difference between these
two lexicons, we define the average pronunciation
entropy, Hˆ , of a lexicon as follows.
Hˆ ?
?1
|V |
?
w?V
?
b?B(w)
p(b) log p(b) (15)
where V denotes the vocabulary of a lexicon, B(w)
represents the set of pronunciations of a word w
and p(b) stands for the weight of a certain pronun-
ciation b. Intuitively, we can regard Hˆ as an in-
dicator of how much pronunciation variation that
each word in a lexicon has on average. Table 3
shows that the Hˆ values of the lexicon induced by
our model and the expert-defined lexicon as well as
Our model PMM iterations
(Discovered lexicon) 0 1 2
Hˆ (bit) 4.58 3.47 3.03
WER (%) 17.0 16.6 15.9
Oracle PMM iterations
(Expert lexicon) 0 1 2
Hˆ (bit) 0.69 0.90 0.92
WER (%) 13.8 12.8 12.4
Table 3: The upper-half of the table shows the aver-
age pronunciation entropies, Hˆ , of the lexicons in-
duced by our model and refined by PMM as well
as the WERs of the monophone recognizers built
with the corresponding lexicons for the weather
query corpus. The definition of Hˆ can be found in
Sec. 6.2. The first row of the lower-half of the ta-
ble lists the average pronunciation entropies, Hˆ , of
the expert-defined lexicon and the lexicons gener-
ated and weighted by the L2P-PMM framework de-
scribed in McGraw et al. (2013). The second row of
the lower-half of the table shows the WERs of the
recognizers that are trained with the expert-lexicon
and its PMM-refined versions.
their respective PMM-refined versions4. In Table 3,
we can see that the automatically-discovered lexi-
con and its PMM-reweighted versions have much
higher Hˆ values than their expert-defined counter-
parts. These higher Hˆ values imply that the lexicon
induced by our model contains more pronunciation
variation than the expert-defined lexicon. Therefore,
the lattices constructed during the decoding process
for our recognizer tend to be larger than those con-
structed for the oracle baseline, which explains the
performance gap between the two systems in Table 2
and Table 3.
As shown in Table 3, even though the lexicon
induced by our model is noisier than the expert-
defined dictionary, the PMM retraining framework
consistently refines the induced lexicon and im-
proves the performance of the recognizers5. To the
best of our knowledge, we are the first to apply
PMM to lexicons that are created by a fully unsu-
4We build the PMM-refined version of the expert-defined
lexicon by following the L2P-PMM framework described
in McGraw et al. (2013).
5The recognition results all converge in 2 ? 3 PMM retrain-
ing iterations.
190
pronunciations
pronunciation probabilities
Our model 1 PMM 2 PMM
93 56 87 39 19 0.125 - -
93 56 61 87 73 99 0.125 - -
11 56 61 87 73 99 0.125 0.400 0.419
93 20 75 87 17 27 52 0.125 0.125 0.124
55 93 56 61 87 73 84 19 0.125 0.220 0.210
93 26 61 87 49 0.125 0.128 0.140
63 83 86 87 73 53 19 0.125 - -
93 26 61 87 61 0.125 0.127 0.107
Table 4: Pronunciation lists of the word Burma pro-
duced by our model and refined by PMM after 1 and
2 iterations.
pervised method. Therefore, in this paper, we pro-
vide further analysis on how PMM helps enhance
the performance of our model.
We compare the pronunciation lists for the word
Burma generated by our model and refined itera-
tively by PMM in Table 4. The first column of Ta-
ble 4 shows all the pronunciations of Burma dis-
covered by our model, to which our model assigns
equal probabilities to create a stochastic list6. As
demonstrated in the third and the fourth columns of
Table 4, the PMM framework is able to iteratively
re-distribute the pronunciation weights and filter out
less-likely pronunciations, which effectively reduces
both the size and the entropy of the stochastic lexi-
con generated by our model. The benefits of using
the PMM to refine the induced lexicon are twofold.
First, the search space constructed during the recog-
nition decoding process with the refined lexicon is
more constrained, which is the main reason why the
PMM is capable of improving the performance of
the monophone recognizer that is trained with the
output of our model. Secondly, and more impor-
tantly, the refined lexicon can greatly reduce the size
of the FST built for the triphone recognizer of our
model. These two observations illustrate why the
PMM framework can be an useful tool for enhancing
the lexicon discovered automatically by our model.
6.3 Triphone Systems
The best monophone systems of the grapheme base-
line, the oracle baseline and our model are used to
6It is also possible to assign probabilities proportional to the
decoding scores of the word tokens.
Unit(%) Triphone
Our model 13.4
Oracle 10.0
Grapheme 15.7
Table 5: Word error rates of the triphone recogniz-
ers. The triphone recognizers are all built by us-
ing the phone transcriptions generated by their best
monohpone system. For the oracle initialized base-
line and for our model, the PMM-refined lexicons
are used to build the triphone recognizers.
generate forced-aligned phone transcriptions, which
are used to train the triphone models described in
Sec. 5.2.2 and Sec. 5.3. Table 5 shows the WERs
of the triphone recognition systems. Note that if a
more conventional question list, for example, a list
that contains rules to classify phones into different
broad classes, is used to build the oracle triphone
system, the WER can be reduced to 6.5%. However,
as mentioned earlier, in order to gain insights into
the quality of the induced lexicon and the discovered
phonetic set, we compare our model against an ora-
cle triphone system that is built by using a singleton
question set.
By comparing Table 2 and Table 5, we can see
that the grapheme triphone improves by a large mar-
gin compared to its monophone counterpart, which
is consistent with the results reported in (Killer et
al., 2003). However, even though the grapheme
baseline achieves a great performance gain with
context-dependent acoustic models, the recognizer
trained using the lexicon learned by our model and
subsequently refined by PMM still outperforms the
grapheme baseline. The consistently better perfor-
mance our model achieves over the grapheme base-
line demonstrates the strength of modeling the pro-
nunciation of each letter with a mixture model that
is presented in this paper.
Last but not least, by comparing Table 2 and
Table 5, it can be seen that the relative perfor-
mance gain achieved by our model is similar to
that obtained by the oracle baseline. Both Table 2
and Table 5 show that even without exploiting any
language-specific knowledge during training, our
recognizer is able to perform comparably with the
recognizer trained using an expert lexicon. The abil-
ity of our model to obtain such similar performance
191
further supports the effectiveness of the joint learn-
ing framework proposed in this paper for discover-
ing the phonetic inventory and the word pronuncia-
tions from simply an annotated speech corpus.
7 Conclusion
We present a hierarchical Bayesian model for si-
multaneously discovering acoustic units and learn-
ing word pronunciations from transcribed spoken ut-
terances. Both monophone and triphone recogniz-
ers can be built on the discovered acoustic units and
the inferred lexicon. The recognizers trained with
the proposed unsupervised method consistently out-
performs grapheme-based recognizers and approach
the performance of recognizers trained with expert-
defined lexicons. In the future, we plan to apply this
technology to develop ASRs for more languages.
Acknowledgements
The authors would like to thank Ian McGraw and
Ekapol Chuangsuwanich for their advice on the
PMM and recognition experiments presented in this
paper. Thanks to the anonymous reviewers for help-
ful comments. Finally, the authors would like to
thank Stephen Shum for proofreading and editing
the early drafts of this paper.
References
Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexi-
con, acoustic unit inventory and model design. Speech
Communication, 29:99 – 114.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451, May.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357–366.
Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and
Yoshinori Sagisaka. 1996. Speech recognition based
on acoustically derived segment units. In Proceedings
of ICSLP, pages 1077 – 1080.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949–952.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 – 152.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 – 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 – 556.
Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian
nonparametric hidden semi-markov models. Journal
of Machine Learning Research, 14:673–701, February.
Mirjam Killer, Sebastian Stu¨ker, and Tanja Schultz.
2003. Grapheme based speech recognition. In Pro-
ceeding of the Eurospeech, pages 3141–3144.
Chia-ying Lee and James Glass. 2012. A nonparamet-
ric Bayesian approach to acoustic model discovery. In
Proceedings of ACL, pages 40–49.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501–
504.
Ian McGraw, Ibrahim Badr, and James Glass. 2013.
Learning lexicons from speech using a pronunciation
mixture model. IEEE Trans. on Speech and Audio
Processing, 21(2):357–366.
Kevin P. Murphy. 2002. Hidden semi-Markov mod-
els (hsmms). Technical report, University of British
Columbia.
Kuldip Paliwal. 1990. Lexicon-building methods for an
acoustic sub-word based speech recognizer. In Pro-
ceedings of ICASSP, pages 729–732.
Man-hung Siu, Herbert Gish, Arthur Chan, William
Belfield, and Steve Lowe. 2013. Unsupervised train-
ing of an HMM-based self-organizing unit recgonizer
with applications to topic classification and keyword
discovery. Computer, Speech, and Language.
Sebastian Stu¨ker and Tanja Schultz. 2004. A grapheme
based speech recognition system for Russian. In Pro-
ceedings of the 9th Conference Speech and Computer.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165–168.
Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic mod-
elling. In Proceedings of HLT, pages 307–312.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-
herington. 2000. Jupiter: A telephone-based con-
versational interface for weather information. IEEE
Trans. on Speech and Audio Processing, 8:85–96.
192
