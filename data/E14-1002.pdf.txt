Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10–19,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics
Undirected Machine Translation with
Discriminative Reinforcement Learning
Andrea Gesmundo
Google Inc.
andrea.gesmundo@gmail.com
James Henderson
Xerox Research Centre Europe
james.henderson@xrce.xerox.com
Abstract
We present a novel Undirected Machine
Translation model of Hierarchical MT that
is not constrained to the standard bottom-
up inference order. Removing the order-
ing constraint makes it possible to condi-
tion on top-down structure and surround-
ing context. This allows the introduc-
tion of a new class of contextual features
that are not constrained to condition only
on the bottom-up context. The model
builds translation-derivations efficiently in
a greedy fashion. It is trained to learn
to choose jointly the best action and the
best inference order. Experiments show
that the decoding time is halved and forest-
rescoring is 6 times faster, while reaching
accuracy not significantly different from
state of the art.
1 Introduction
Machine Translation (MT) can be addressed as a
structured prediction task (Brown et al., 1993; Ya-
mada and Knight, 2001; Koehn et al., 2003). MT’s
goal is to learn a mapping function, f , from an in-
put sentence, x, into y = (t, h), where t is the
sentence translated into the target language, and
h is the hidden correspondence structure (Liang
et al., 2006). In Hierarchical MT (HMT) (Chi-
ang, 2005) the hidden correspondence structure is
the synchronous-tree composed by instantiations
of synchronous rules from the input grammar, G.
Statistical models usually define f as: f(x) =
argmax
y?Y
Score(x, y), where Score(x, y) is a
function whose parameters can be learned with a
specialized learning algorithm. In MT applica-
tions, it is not possible to enumerate all y ? Y .
HMT decoding applies pruning (e.g. Cube Prun-
ing (Huang and Chiang, 2005)), but even then
HMT has higher complexity than Phrase Based
MT (PbMT) (Koehn et al., 2003). On the other
hand, HMT improves over PbMT by introducing
the possibility of exploiting a more sophisticated
reordering model not bounded by a window size,
and producing translations with higher syntactic-
semantic quality. In this paper, we present the
Undirected Machine Translation (UMT) frame-
work, which retains the advantages of HMT and
allows the use of a greedy decoder whose com-
plexity is lower than standard quadratic beam-
search PbMT.
UMT’s fast decoding is made possible through
even stronger pruning: the decoder chooses a sin-
gle action at each step, never retracts that action,
and prunes all incompatible alternatives to that ac-
tion. If this extreme level of pruning was ap-
plied to the CKY-like beam-decoding used in stan-
dard HMT, translation quality would be severely
degraded. This is because the bottom-up infer-
ence order imposed by CKY-like beam-decoding
means that all pruning decisions must be based on
a bottom-up approximation of contextual features,
which leads to search errors that affect the qual-
ity of reordering and lexical-choice (Gesmundo
and Henderson, 2011). UMT solves this problem
by removing the bottom-up inference order con-
straint, allowing many different inference orders
for the same tree structure, and learning the in-
ference order where the decoder can be the most
confident in its pruning decisions.
Removing the bottom-up inference order con-
straint makes it possible to condition on top-down
structure and surrounding context. This undirected
approach allows us to integrate contextual features
such as the Language Model (LM) in a more flex-
10
ible way. It also allows us to introduce a new class
of undirected features. In particular, we introduce
the Context-Free Factor (CFF) features. CFF fea-
tures compute exactly and efficiently a bound on
the context-free cost of a partial derivation’s miss-
ing branches, thereby estimating the future cost of
partial derivations. The new class of undirected
features is fundamental for the success of a greedy
approach to HMT, because the additional non-
bottom-up context is sometimes crucial to have the
necessary information to make greedy decisions.
Because UMT prunes all but the single cho-
sen action at each step, both choosing a good in-
ference order and choosing a correct action re-
duce to a single choice of what action to take
next. To learn this decoding policy, we propose
a novel Discriminative Reinforcement Learning
(DRL) framework. DRL is used to train mod-
els that construct incrementally structured out-
put using a local discriminative function, with
the goal of optimizing a global loss function.
We apply DRL to learn the UMT scoring func-
tion’s parameters, using the BLEU score as the
global loss function. DRL learns a weight vector
for a linear classifier that discriminates between
decisions based on which one leads to a com-
plete translation-derivation with a better BLEU
score. Promotions/demotions of translations are
performed by applying a Perceptron-style update
on the sequence of decisions that produced the
translation, thereby training local decisions to op-
timize the global BLEU score of the final trans-
lation, while keeping the efficiency and simplic-
ity of the Perceptron Algorithm (Rosenblatt, 1958;
Collins, 2002).
Our experiments show that UMT with DRL re-
duces decoding time by over half, and the time to
rescore translations with the Language Model by
6 times, while reaching accuracy non-significantly
different from the state of the art.
2 Undirected Machine Translation
In this section, we present the UMT frame-
work. For ease of presentation, and following
synchronous-grammar based MT practice, we will
henceforth restrict our focus to binary grammars
(Zhang et al., 2006; Wang et al., 2007).
A UMT decoder can be formulated as a func-
tion, f , that maps a source sentence, x ? X , into
a structure defined by y = (t, h) ? Y , where t
is the translation in the target language, and h
is the synchronous tree structure generating the
input sentence on the source side and its trans-
lation on the target side. Synchronous-trees are
composed of instantiations of synchronous-rules,
r, from a grammar, G. A UMT decoder builds
synchronous-trees, h, by recursively expanding
partial synchronous-trees, ? . ? includes a partial
translation. Each ? is required to be a connected
sub-graph of some synchronous-tree h. Thus, ?
is composed of a subset of the rules from any h
that generates x on the source side, such that there
is a connected path between any two rules in ? .
Differently from the partial structures built by a
bottom-up decoder, ? does not have to cover a
contiguous span on x. Formally, ? is defined by:
1) The set of synchronous-rule instantiations in ? :
I ? {r
1
, r
2
, · · · , r
k
|r
i
? G, 1 ? i ? k};
2) The set of connections among the synchronous-
rule instantiations, C .
Let c
i
= (r
i
, r
j
i
) be the notation to represent the
connection between the i-th rule and the rule r
j
i
.
The set of connections can be expressed as:
C ? {(r
1
, r
j
1
), (r
2
, r
j
2
), · · · , (r
k?1
, r
j
k?1
)}
3) The postcondition set, P , which specifies
the non-terminals in ? that are available for
creating new connections. Each postcondition,
p
i
= (r
x
,X y )i, indicates that the rule rx has the
non-terminal X y available for connections. The
index y identifies the non-terminal in the rule. In
a binary grammar y can take only 3 values: 1 for
the first non-terminal (the left child of the source
side), 2 for the second non-terminal, and h for the
head. The postcondition set can be expressed as:
P?{(r
x
1
,X
y
1
)
1
, · · · , (r
x
m
,X
y
m
)
m
}
4) The set of carries, K . We define a different
carry, ?
i
, for each non-terminal available for
connections. Each carry stores the extra infor-
mation required to correctly score the non-local
interactions between ? and the rule that will be
connected at that non-terminal. Thus |K| = |P |.
Let ?
i
be the carry associated with the postcon-
dition p
i
. The set of carries can be expressed as:
K ? {?
1
, ?
2
, · · · , ?
m
}
Partial synchronous-trees, ? , are expanded by
performing connection-actions. Given a ? we can
connect to it a new rule, rˆ, using one available non-
terminal represented by postcondition, p
i
? P ,
and obtain a new partial synchronous-tree ?ˆ . For-
mally: ?ˆ ? ? ? ? aˆ ?, where, aˆ = [rˆ, p
i
],
represents the connection-action.
11
Algorithm 1 UMT Decoding
1: function Decoder (x; w, G) : (t,h)
2: ?.{I, C, P,K} ? {?, ?, ?, ?} ;
3: Q? LeafRules(G);
4: while |Q| > 0 do
5: [rˆ, p
i
]? PopBestAction (Q,w);
6: ? ? CreateConnection(?, rˆ , p
i
);
7: UpdateQueue(Q, rˆ, p
i
);
8: end while
9: Return(?);
10: procedure CreateConnection(? , rˆ, p
i
) : ?ˆ
11: ?ˆ .I ? ?.I + rˆ;
12: ?ˆ .C ? ?.C + (rˆ, r
p
i
);
13: ?ˆ .P ? ?.P ? p
i
;
14: ?ˆ .K ? ?.K ? ?
i
;
15: ?ˆ .K .UpdateCarries(rˆ, p
i
);
16: ?ˆ .P .AddAvailableConnectionsFrom(rˆ , p
i
);
17: ?ˆ .K .AddCarriesForNewConnections(rˆ , p
i
);
18: Return(?ˆ );
19: procedure UpdateQueue( Q, rˆ, p
i
) :
20: Q.RemoveActionsWith(p
i
);
21: Q.AddNewActions(rˆ, p
i
);
2.1 Decoding Algorithm
Algorithm 1 gives details of the UMT decoding
algorithm. The decoder takes as input the source
sentence, x, the parameters of the scoring func-
tion, w, and the synchronous-grammar, G. At
line 2 the partial synchronous-tree ? is initialized
by setting I , C , P and K to empty sets ?. At
line 3 the queue of candidate connection-actions
is initialized as Q ? { [r
leaf
, null] | r
leaf
is a
leaf rule}, where null means that there is no post-
condition specified, since the first rule does not
need to connect to anything. A leaf rule r
leaf
is
any synchronous rule with only terminals on the
right-hand sides. At line 4 the main loop starts.
Each iteration of the main loop will expand ? us-
ing one connection-action. The loop ends when
Q is empty, implying that ? covers the full sen-
tence and has no more missing branches or par-
ents. The best scoring action according to the
parameter vector w is popped from the queue at
line 5. The scoring of connection-actions is dis-
cussed in details in Section 3.2. At line 6 the se-
lected connection-action is used to expand ? . At
line 7 the queue of candidates is updated accord-
ingly (see lines 19-21). At line 8 the decoder it-
erates the main loop, until ? is complete and is
returned at line 9.
Lines 10-18 describe the CreateConnection(·)
procedure, that connects the partial synchronous-
tree ? to the selected rule rˆ via the postcondi-
tion p
i
specified by the candidate-action selected
in line 5. This procedure returns the resulting par-
tial synchronous-tree: ?ˆ ? ? ? ? [rˆ, p
i
] ?. At
line 11, rˆ is added to the rule set I . At line 12 the
connection between rˆ and r
p
i
(the rule specified
in the postcondition) is added to the set of connec-
tions C . At line 13, p
i
is removed from P . At
line 14 the carry k
i
matching with p
i
is removed
from K . At line 15 the set of carries K is updated,
in order to update those carries that need to pro-
vide information about the new action. At line 16
new postconditions representing the non-terminals
in rˆ that are available for subsequent connections
are added in P . At line 17 the carries associated
with these new postconditions are computed and
added to K . Finally at line 18 the updated partial
synchronous-tree is returned.
In the very first iteration, the
CreateConnection(·) procedure has nothing
to compute for some lines. Line 11 is not exe-
cuted since the first leaf rule needs no connection
and has nothing to connect to. lines 12-13 are
not executed since P and K are ? and p
i
is not
specified for the first action. Line 15 is not
executed since there are no carries to be updated.
Lines 16-17 only add the postcondition and carry
relative to the leaf rule head link.
The procedure used to update Q is reported in
lines 19-21. At line 20 all the connection-actions
involving the expansion of p
i
are removed from
Q. These actions are the incompatible alternatives
to the selected action. In the very first iteration,
all actions in Q are removed because they are all
incompatible with the connected-graph constraint.
At line 21 new connection-actions are added to
Q. These are the candidate actions proposing a
connection to the available non-terminals of the
selected action’s new rule rˆ. The rules used for
these new candidate-actions must not be in con-
flict with the current structure of ? (e.g. the rule
cannot generate a source side terminal that is al-
ready covered by ? ).
12
3 Discriminative Reinforcement
Learning
Training a UMT model simply means training the
parameter vector w that is used to choose the best
scoring action during decoding. We propose a
novel method to apply a kind of minimum error
rate training (MERT) to w. Because each ac-
tion choice must be evaluated in the context of
the complete translation-derivation, we formalize
this method in terms of Reinforcement Learning.
We propose Discriminative Reinforcement Learn-
ing as an appropriate way to train a UMT model to
maximize the BLEU score of the complete deriva-
tion. First we define DRL as a novel generic train-
ing framework.
3.1 Generic Framework of DRL
RL can be applied to any task, T , that can be for-
malized in terms of:
1) The set of states S1;
2) A set of actions A
s
for each state s ? S;
3) The transition function T : S × A
s
? S, that
specifies the next state given a source state and
performed action2;
4) The reward function, R : S ×A
s
? R;
5) The discount factor, ? ? [0, 1].
A policy is defined as any map ? : S ? A. Its
value function is given by:
V
pi
(s
0
) =
?
?
i=0
?
i
R(s
i
, ?(s
i
)) (1)
where path(s
0
|?)? ?s
0
, s
1
, · · · , s
?
|?? is the se-
quence of states determined by following policy ?
starting at state s
0
. The Q-function is the total fu-
ture reward of performing action a
0
in state s
0
and
then following policy ?:
Q
pi
(s
0
, a
0
) = R(s
0
, a
0
) + ?V
pi
(s
1
) (2)
Standard RL algorithms search for a policy that
maximizes the given reward.
Because we are taking a discriminative ap-
proach to learn w, we formalize our optimization
task similarly to an inverse reinforcement learning
problem (Ng and Russell, 2000): we are given in-
formation about the optimal action sequence and
we want to learn a discriminative reward func-
tion. As in other discriminative approaches, this
1
S can be either finite or infinite.
2For simplicity we describe a deterministic process. To
generalize to the stochastic process, replace the transition
function with the transition probability: P
sa
(s
?
), s
?
? S.
Algorithm 2 Discriminative RL
1: function Trainer (?,T ,D ) : w
2: repeat
3: s?SampleState(S);
4: aˆ? ?
w
(s);
5: a? ?SampleAction(A
s
);
6: if Qpiw(s, aˆ) < Qpiw(s, a?) in D then
7: w? w + ?w(s, a?)? ?w(s, aˆ);
8: end if
9: until convergence
10: Return(w);
approach simplifies the task of learning the re-
ward function in two respects: the learned reward
function only needs to be monotonically related
to the true reward function, and this property only
needs to hold for the best competing alternatives.
This is all we need in order to use the discrimina-
tive reward function in an optimal classifier, and
this simplification makes learning easier in cases
where the true reward function is too complicated
to model directly.
In RL, an optimal policy ?? is one which, at
each state s, chooses the action which maximizes
the future reward Qpi?(s, a). We assume that the
future discriminative reward can be approximated
with a linear function ˜Qpi(s, a) in some feature-
vector representation ? : S ×A
s
? R
d that maps
a state-action pair to a d-dimensional features vec-
tor:
˜
Q
pi
(s, a) = w ?(s, a) (3)
where w ? Rd. This gives us the following policy:
?
w
(s) = argmax
a?A
s
w ?(s, a) (4)
The set of parameters of this policy is the vec-
tor w. With this formalization, all we need to
learn is a vector w such that the resulting deci-
sions are compatible with the given information
about the optimal action sequence. We propose a
Perceptron-like algorithm to learn these parame-
ters.
Algorithm 2 describes the DRL meta-algorithm.
The Trainer takes as input ?, the task T , and a
generic set of data D describing the behaviors we
want to learn. The output is the weight vector w
of the learned policy that fits the data D. The al-
gorithm consists in a single training loop that is
repeated until convergence (lines 2-9). At line 3
a state, s, is sampled from S. At line 4, aˆ is set to
13
be the action that would be preferred by the cur-
rent w-policy. At line 5 an action, a?, is sampled
from A
s
such that a? 6= aˆ. At line 6 the algo-
rithm checks if preferring path(T (s, aˆ), ?
w
) over
path(T (s, a
?
), ?
w
) is a correct choice according
to the behaviors data D that the algorithm aims to
learn. If the current w-policy contradicts D, line 7
is executed to update the weight vector to promote
?
w
(s, a
?
) and penalize ?w(s, aˆ), where ?w(s, a)
is the summation of the features vectors of the en-
tire derivation path starting at (s, a) and following
policy ?
w
. This way of updating w has the ef-
fect of increasing the ˜Q(·) value associated with
all the actions in the sequence that generated the
promoted structure, and reducing the ˜Q(·) value
of the actions in the sequence that generated the
penalized structure3 .
We have described the DRL meta-algorithm to
be as general as possible. When applied to a spe-
cific problem, more details can be specified: 1) it
is possible to choose specific sampling techniques
to implement lines 3 and 5; 2) the test at line 6
needs to be detailed according to the nature of T
and D; 3) the update statement at line 7 can be re-
placed with a more sophisticated update approach.
We address these issues and describe a range of
alternatives as we apply DRL to UMT in Section
3.2.
3.2 Application of DRL to UMT
To apply DRL we formalize the task of translating
x with UMT as T ? {S, {A
s
}, T,R, ?}:
1) The set of states S is the space of all possible
UMT partial synchronous-trees, ? ;
2) The set A
?,x
is the set of connection-actions
that can expand ? connecting new synchronous-
rule instantiations matching the input sentence x
on the source side;
3) The transition function T is the connection
function ?ˆ ? ? ? ? a ? formalized in Section 2
and detailed by the procedure CreateConnection(·)
in Algorithm 1;
4) The true reward function R is the BLEU score.
BLEU is a loss function that quantifies the differ-
ence between the reference translation and the out-
put translation t. The BLEU score can be com-
puted only when a terminal state is reached and a
full translation is available. Thus, the rewards are
all zero except at terminal states, called a Pure De-
3Preliminary experiments with updating only the features
for aˆ and a? produced substantially worse results.
layed Reward function;
5) Considering the nature of the problem and re-
ward function, we choose an undiscounted setting:
? = 1.
Next we specify the details of the DRL algo-
rithm. The data D consists of a set of pairs of
sentences, D ? {(x, t?)}, where x is the source
sentence and t? is the reference translation. The
feature-vector representation function ? maps a
pair (?, a) to a real valued vector having any num-
ber of dimensions. Each dimension corresponds
to a distinct feature function that maps: {?} ×
A
?,x
? R. Details of the features functions im-
plemented for our model are given in Section 4.
Each loop of the DRL algorithm analyzes a single
sample (x, t?) ? D. The state s is sampled from a
uniform distribution over ?s
0
, s
1
, · · · , s
?
|??. The
action a? is sampled from a Zipfian distribution
over {A
?,x
? aˆ} sorted with the ˜Qpiw(s, a) func-
tion. In this way actions with higher score have
higher probability to be drawn, while actions at the
bottom of the rank still have a small probability to
be selected. The if at line 6 tests if the translation
produced by path(T (s, a?), ?
w
) has higher BLEU
score than the one produced by path(T (s, aˆ), ?
w
).
For the update statement at line 7 we use
the Averaged Perceptron technique (Freund and
Schapire, 1999). Algorithm 2 can be eas-
ily adapted to implement the efficient Averaged
Perceptron updates (e.g. see Section 2.1.1 of
(Daume´ III, 2006)). In preliminary experiments,
we found that other more aggressive update tech-
nique, such as Passive-Aggressive (Crammer et
al., 2006), Aggressive (Shen et al., 2007), or
MIRA (Crammer and Singer, 2003), lead to worst
accuracy. To see why this might be, consider that
a MT decoder needs to learn to construct struc-
tures (t, h), while the training data specifies the
gold translation t? but gives no information on the
hidden-correspondence structure h. As discussed
in (Liang et al., 2006), there are output structures
that match the reference translation using a wrong
internal structure (e.g. assuming wrong internal
alignment). While in other cases the output trans-
lation can be a valid alternative translation but gets
a low BLEU score because it differs from t?. Ag-
gressively promoting/penalizing structures whose
correctness can be only partially verified can be
expected to harm generalization ability.
14
4 Undirected Features
In this section we show how the features designed
for bottom-up HMT can be adapted to the undi-
rected approach, and we introduce a new feature
from the class of undirected features that are made
possible by the undirected approach.
Local features depend only on the action rule r.
These features can be used in the undirected ap-
proach without adaptation, since they are indepen-
dent of the surrounding structure. For our experi-
ments we use a standard set of local features: the
probability of the source phrase given the target
phrase; the lexical translation probabilities of the
source words given the target words; the lexical
translation probabilities of the target words given
the source words; and the Word Penalty feature.
Contextual features are dependent on the inter-
action between the action rule r and the avail-
able context. In UMT all the needed information
about the available context is stored in the carry
?
i
. Therefore, the computation of contextual fea-
tures whose carry’s size is bounded (like the LM)
requires constant time.
The undirected adaptation of the LM feature
computes the scores of the new n-grams formed
by adding the terminals of the action rule r to the
current partial translation ? . In the case that the
action rule r is connected to ? via a child non-
terminal, the carry is expressed as ?
i
? ([W
L
?
W
R
]). Where W
L
and W
R
are respectively the left
and right boundary target words of the span cov-
ered by ? . This notation is analogous to the stan-
dard star notation used for the bottom-up decoder
(e.g. (Chiang, 2007) Section 5.3.2). In the case
that r is connected to ? via the head non-terminal,
the carry is expressed as ?
i
? (W
R
]-[W
L
). Where
W
L
and W
R
are respectively the left and right
boundary target words of the surrounding context
provided by ? . The boundary words stored in the
carry and the terminals of the action rule are all the
information needed to compute and score the new
n-grams generated by the connection-action.
In addition, we introduce the Context-Free Fac-
tor (CFF) features. An action rule r is connected
to ? via one of r’s non-terminals, X
r,?
. Thus, the
score of the interaction between r and the context
structure attached to X
r,?
can be computed ex-
actly, while the score of the structures attached to
other r nonterminals (i.e. those in postconditions)
cannot be computed since these branches are miss-
ing. Each of these postcondition nonterminals
has an associated CFF feature, which is an upper
bound on the score of its missing branch. More
precisely, it is an upper bound on the context-free
component of this score. This upper bound can be
exactly and efficiently computed using the Forest
Rescoring Framework (Huang and Chiang, 2007;
Huang, 2008). This framework separates the MT
decoding in two steps. In the first step only the
context-free factors are considered. The output of
the first step is a hypergraph called the context-
free-forest, which compactly represents an expo-
nential number of synchronous-trees. The second
step introduces contextual features by applying a
process of state-splitting to the context-free-forest,
rescoring with non-context-free factors, and effi-
ciently pruning the search space.
To efficiently compute CFF features we run
the Inside-Outside algorithm with the (max,+)
semiring (Goodman, 1999) over the context-free-
forest. The result is a map that gives the maxi-
mum Inside and Outside scores for each node in
the context-free forest. This map is used to get the
value of the CFF features in constant time while
running the forest rescoring step.
5 Experiments
We implement our model on top of Cdec (Dyer et
al., 2010). Cdec provides a standard implemen-
tation of the HMT decoder (Chiang, 2007) and
MERT training (Och, 2003) that we use as base-
line.
We experiment on the NIST Chinese-English
parallel corpus. The training corpus contains
239k sentence pairs with 6.9M Chinese words and
8.9M English words. The test set contains 919
sentence pairs. The hierarchical translation gram-
mar was extracted using the Joshua toolkit (Li et
al., 2009) implementation of the suffix array rule
extractor algorithm (Callison-Burch et al., 2005;
Lopez, 2007).
Table 1 reports the decoding time measures.
HMT with beam1 is the fastest possible configu-
ration for HMT, but it is 71.59% slower than UMT.
This is because HMT b1 constructs O(n2) sub-
trees, many of which end up not being used in
the final result, whereas UMT only constructs the
rule instantiations that are required. HMT with
beam30 is the fastest configuration that reaches
state of the art accuracy, but increases the aver-
age time per sentence by an additional 131.36%
when compared with UMT. The rescoring time is
15
Model sent. t. sent. t. var. resc. t. resc. t. var.
UMT 135.2ms - 38.9 ms -
HMT b1 232.0ms +71.59% 141.3 ms +263.23%
HMT b30 312.8ms +131.36% 226.9 ms +483.29%
Table 1: Decoding speed comparison.
Model sent. t. sent. t. var.
UMT with DRL 267.4 ms -
HMT b1 765.2 ms +186.16%
HMT b30 1153.5 ms +331.37%
Table 2: Training speed comparison.
Model BLEU relative loss p-value
UMT with DRL 30.14 6.33% 0.18
HMT b1 30.87 4.07% 0.21
HMT b30 32.18 - -
Table 3: Accuracy comparison.
the average time spent on the forest rescoring step,
which is the only step where the decoders actu-
ally differ. This is the step that involves the inte-
gration of the Language Model and other contex-
tual features. For HMT b30, rescoring takes two
thirds of the total decoding time. Thus rescoring
is the most time consuming step in the pipeline.
The rescoring time comparison shows even bigger
gains for UMT. HMT b30 is almost 6 times slower
than UMT.
Table 2 reports the training time measures.
These results show HMT b30 training is more
than 4 times slower than UMT training with DRL.
Comparing with Table 1, we notice that the rela-
tive gain on average training time is higher than
the gain measured at decoding time. This is be-
cause MERT has an higher complexity than DRL.
Both of the training algorithms requires 10 train-
ing epochs to reach convergence.
Table 3 reports the accuracy measures. As ex-
pected, accuracy degrades the more aggressively
the search space is pruned. UMT trained with
DRL loses 2.0 BLEU points compared to HMT
b30. This corresponds to a relative-loss of 6.33%.
Although not inconsequential, this variation is
not considered big (e.g. at the WMT-11 Ma-
chine Translation shared task (Callison-Burch et
al., 2011)). To measure the significance of the
variation, we compute the sign test and measure
the one-tail p-value for the presented models in
comparison to HMT b30. From the values re-
ported in the fourth column, we can observe that
the BLEU score variations would not normally be
considered significant. For example, at WMT-11
two systems were considered equivalent if p >
0.1, as in these cases. The accuracy cannot be
compared in terms of search score since the mod-
els we are comparing are trained with distinct al-
gorithms and thus the search scores are not com-
parable.
To test the impact of the CFF features, we
trained and tested UMT with DRL with and with-
out these features. This resulted in an accuracy de-
crease of 2.3 BLEU points. Thus these features are
important for the success of the greedy approach.
They provide an estimate of the score of the miss-
ing branches, thus helping to avoid some actions
that have a good local score but lead to final trans-
lations with low global score.
To validate the results, additional experiments
were executed on the French to Italian portion
of the Europarl corpus v6. This portion contains
190k pairs of sentences. The first 186k sentences
were used to extract the grammar and train the two
models. The final tests were performed on the re-
maining 4k sentence pairs. With this corpus we
measured a similar speed gain. HMT b30 is 2.3
times slower at decoding compared to UMT, and
6.1 times slower at rescoring, while UMT loses
1.1 BLEU points in accuracy. But again the ac-
curacy differences are not considered significant.
We measured a p-value of 0.25, which is not sig-
nificant at the 0.1 level.
6 Related Work
Models sharing similar intuitions have been pre-
viously applied to other structure prediction tasks.
For example, Nivre et al. (2006) presents a linear
time syntactic dependency parser, which is con-
strained in a left-to-right decoding order. This
model offers a different accuracy/complexity bal-
ance than the quadratic time graph-based parser of
Mcdonald et al. (2005).
Other approaches learning a model specifically
for greedy decoding have been applied with suc-
16
cess to other less complex tasks. Shen et al. (2007)
present the Guided Learning (GL) framework for
bidirectional sequence classification. GL success-
fully combines the tasks of learning the order of
inference and training the local classifier in a sin-
gle Perceptron-like algorithm, reaching state of the
art accuracy with complexity lower than the ex-
haustive counterpart (Collins, 2002).
Goldberg and Elhadad (2010) present a simi-
lar training approach for a Dependency Parser that
builds the tree-structure by recursively creating
the easiest arc in a non-directional manner. This
model also integrates the tasks of learning the or-
der of inference and training the parser in a single
Perceptron. By “non-directional” they mean the
removal of the constraint of scanning the sentence
from left to right, which is typical of shift-reduce
models. However this algorithm still builds the
tree structures in a bottom-up fashion. This model
has a O(n log n) decoding complexity and accu-
racy performance close to the O(n2) graph-based
parsers (Mcdonald et al., 2005).
Similarities can be found between DRL and pre-
vious work that applies discriminative training to
structured prediction: Collins and Roark (2004)
present an Incremental Parser trained with the Per-
ceptron algorithm. Their approach is specific to
dependency parsing and requires a function to test
exact match of tree structures to trigger parameter
updates. On the other hand, DRL can be applied to
any structured prediction task and can handle any
kind of reward function. LASO (Daume´ III and
Marcu, 2005; Daume´ III et al., 2005) and SEARN
(Daume´ III et al., 2009; Daume´ III et al., 2006)
are generic frameworks for discriminative training
for structured prediction: LASO requires a func-
tion that tests correctness of partial structures to
trigger early updates, while SEARN requires an
optimal policy to initialize the learning algorithm.
Such a test function or optimal policy cannot be
computed for tasks such as MT where the hidden
correspondence structure h is not provided in the
training data.
7 Discussion and Future Work
In general, we believe that greedy-discriminative
solutions are promising for tasks like MT, where
there is not a single correct solution: normally
there are many correct ways to translate the same
sentence, and for each correct translation there
are many different derivation-trees generating that
translation, and each correct derivation tree can be
built greedily following different inference orders.
Therefore, the set of correct decoding paths is a
reasonable portion of UMT’s search space, giving
a well-designed greedy algorithm a chance to find
a good translation even without beam search.
In order to directly evaluate the impact of our
proposed decoding strategy, in this paper the only
novel features that we consider are the CFF fea-
tures. But to take full advantage of the power
of discriminative training and the lower decoding
complexity, it would be possible to vastly increase
the number of features. The UMT’s undirected na-
ture allows the integration of non-bottom-up con-
textual features, which cannot be used by stan-
dard HMT and PbMT. And the use of a history-
based model allows features from an arbitrarily
wide context, since the model does not need to be
factorized. Exploring the impact of this advantage
is left for future work.
8 Conclusion
The main contribution of this work is the pro-
posal of a new MT model that offers an accu-
racy/complexity balance that was previously un-
available among the choices of hierarchical mod-
els.
We have presented the first Undirected frame-
work for MT. This model combines advantages
given by the use of hierarchical synchronous-
grammars with a more efficient decoding algo-
rithm. UMT’s nature allows us to design novel
undirected features that better approximate con-
textual features (such as the LM), and to introduce
a new class of undirected features that cannot be
used by standard bottom-up decoders. Further-
more, we generalize the training algorithm into
a generic Discriminative Reinforcement Learning
meta-algorithm that can be applied to any struc-
tured prediction task.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263–311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
17
phrases. In ACL ’05: Proceedings of the 43rd Con-
ference of the Association for Computational Lin-
guistics, Ann Arbor, MI, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In WMT ’11:
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
’05: Proceedings of the 43rd Conference of the As-
sociation for Computational Linguistics, Ann Arbor,
MI, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ’04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ’02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
Hal Daume´ III and Daniel Marcu. 2005. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML ’05:
Proceedings of the 22nd International Conference
on Machine Learning, Bonn, Germany.
Hal Daume´ III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as clas-
sification. In ASLTSP ’05: Proceedings of the
NIPS Workshop on Advances in Structured Learn-
ing for Text and Speech Processing, Whistler, British
Columbia, Canada.
Hal Daume´ III, John Langford, and Daniel Marcu.
2006. Searn in practice. Technical report.
Hal Daume´ III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Hal Daume´ III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL ’10: Proceedings of the ACL 2010 System
Demonstrations, Uppsala, Sweden.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.
Andrea Gesmundo and James Henderson. 2011.
Heuristic Search for Non-Bottom-Up Tree Structure
Prediction. In EMNLP ’11: Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In NAACL ’10: Proceedings of the
11th Conference of the North American Chapter of
the Association for Computational Linguistics, Los
Angeles, CA, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573–605.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT ’05: Proceedings of the 9th Inter-
national Workshop on Parsing Technology, Vancou-
ver, British Columbia, Canada.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL ’07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Liang Huang. 2008. Forest-based algorithms in natu-
ral language processing. Ph.D. thesis, University of
Pennsylvania.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 4th Conference of
the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In WMT ’09: Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, Greece.
Percy Liang, Alexandre Bouchard-Coˆte´, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL ’06: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
Conference of the Association for Computational
Linguistics, Sydney, Australia.
18
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL ’07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Ryan Mcdonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL ’05: Proceedings of the
43rd Conference of the Association for Computa-
tional Linguistics, Ann Arbor, MI, USA.
Andrew Y. Ng and Stuart Russell. 2000. Algorithms
for inverse reinforcement learning. In ICML ’00:
Proceedings of the 17th International Conference on
Machine Learning, Stanford University, CA, USA.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In LREC ’06: Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genoa, Italy.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Conference of the Association
for Computational Linguistics, Sapporo, Japan.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386–
408.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In ACL ’07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL ’07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL ’01: Pro-
ceedings of the 39th Conference of the Association
for Computational Linguistics, Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In NAACL ’06: Proceedings of the
7th Conference of the North American Chapter of
the Association for Computational Linguistics, New
York, New York.
19
