Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1990–1995,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Reversibility reconsidered: finite-state factors for efficient probabilistic
sampling in parsing and generation
Marc Dymetman
†
, Sriram Venkatapathy
‡
, Chunyang Xiao
†
†
Xerox Research Centre Europe, Grenoble, France
‡
Amazon, Machine Learning Team, Bangalore, India
?
†
{first.last}@xrce.xerox.com,
‡
vesriram@amazon.com
Abstract
We restate the classical logical notion of
generation/parsing reversibility in terms of
feasible probabilistic sampling, and argue
for an implementation based on finite-state
factors. We propose a modular decompo-
sition that reconciles generation accuracy
with parsing robustness and allows the in-
troduction of dynamic contextual factors.
(Opinion Piece)
1 Introduction
The objective of Natural Language Understanding
(NLU) is to map linguistic utterances to semantic
representations, that of Natural Language Genera-
tion (NLG) to map semantic representations to lin-
guistic utterances. In most of NLP practice, these
two objectives are handled by different processes,
and computational linguists rarely operate at the
intersection of the two subdomains.
For a few years around the early nineties, based
both on cognitive, linguistic, and engineering con-
siderations, there was a surge of interest in so
called reversible grammar approaches to NLP,
where one and the same grammatical specification
could serve both for parsing utterance x into logi-
cal form z, but also for generating x from z (Strza-
lkowski, 1994).
We start by a brief review of this historical non-
probabilistic notion of reversibility and point out
certain of its weaknesses, in particular regarding
robustness; we then give in section 3 a new proba-
bilistic definition of reversibility; then, in section 4
we argue for a reversibility model based on modu-
lar weighted finite-state transducers. We end with
a discussion of recent related work.
?
Work done while at XRCE.
2 Classical reversibility
The most direct approaches to NLU attempt to de-
sign procedures for semantic parsing that, given
an input utterance x, produce a semantic repre-
sentation z, by following a number of interme-
diate steps where the surface form is gradually
transformed into semantic structure. Such “pro-
cedural” approaches to semantic parsing are typ-
ically very hard or impossible to invert: start-
ing from a semantic representation z, there is no
simple process that is able to find an x which,
when given to the parser, would produce z. For-
mally, a Boolean relation r(x, z) can be such that
the question ??z r(x, z) is decidable for all x’s,
while the reciprocal question ??x r(x, z) is unde-
cidable for some z’s (Dymetman, 1991).
1
One of
the motivations for the emerging paradigm of uni-
fication grammars at the end of the eighties was
the clean separation they promised between spec-
ifying well-formed linguistic structures, both on
the syntactic and semantic levels, through a for-
mal description of the relation r(x, z), and pro-
ducing efficient implementations of the specifi-
cation; in particular, there was much hope that
such formalisms would be conductive to effec-
tive reversibility (by contrast to variable assign-
ment, variable unification is inherently symmetri-
cal), that is, to feasible (and if possible efficient)
implementations of the parsing problem r(x, ?)
and of the generation problem r(?, z).
To some extent, this hope was validated through
a number of works at the time, mostly involving
machine translation applications, and constrain-
ing in more or less explicit ways the specifica-
tion of r (van Noord, 1990). However, for the
non-statistical approaches to parsing then strongly
dominant, robustness was an issue: a parser had to
1
Some intuition into the issue may be gained by consider-
ing typical techniques of public key cryptography, which rely
on the difficulty of inverting some simple arithmetic compu-
tations.
1990
either accept or reject a given input x, with no in-
termediary options, and in order to be able to parse
actual utterances, with all their empirical diversity,
parsers had to be rather tolerant. In the procedural
view of parsing, such robustness issues could of-
ten be mitigated through engineering tricks such as
ordering the rules from strict to lax, where gram-
matical constructions were given preference over
less conventional ones; however, when trying to
move to reversible grammars, these tricks could
not be reproduced: if the grammar was able to
parse an x into z, then, by design, it was also able
to generate x from z, and there was no obvious
way, in these non-probabilistic approaches, to dis-
tinguish between producing a linguistically correct
x or producing a deviant or incorrect one.
3 Probabilistic reversibility
In the classical non-probabilistic case, a (relative)
consensus existed around the fact that a reversible
grammar should be, as we indicated above, a for-
mal specification of the relation r(x, z) such that
the problems r(x, ?) and r(?, z) were effectively
solvable.
Transposing this to the probabilistic world, we
propose the following semi-formal Definition:
A probabilistic reversible grammar is a for-
mal specification of a joint probability distribu-
tion p(x, z) over logical forms z and utterance
strings x such that the conditional distributions
p(z|x)
def
=
p(x,z)?
z
?
p(x,z
?
)
(parsing) and p(x|z)
def
=
p(x,z)?
x
?
p(x
?
,z)
(generation) can be efficiently sampled
from.
2
Why such focus on sampling? We could have
chosen other definitions of parsing (and similarly
for generation), for instance the ability to re-
turn the most probable z given x, i.e. to return
argmax
z
p(z|x); however sampling is the most di-
rect way of providing a concrete view of the un-
derlying probabilistic distribution, and has many
applications to learning, so we think the definition
above is reasonable (see also footnote
4
).
2
We note the “semi-formal” aspect of this definition: con-
trarily to the classical case, which has a formal notion of ef-
fective computation, there is no universally accepted notion
of effective sampling from a probability distribution. For
many probability distributions, the only feasible sampling
approaches are the MCMC techniques (Robert and Casella,
2004), which typically do not come with convergence guar-
antees; in some situations, exact sampling techniques are ap-
plicable, which come with much better guarantees. We will
see that the approach proposed in section 4 allows such exact
sampling to take place.
4 Finite-state models for reversibility
Finite-state transducers have properties which
make them uniquely suited to implementing re-
versible linguistic specifications in the above
sense. Consider a simple weighted string-to-
string transducer ?(s, t), where s, t are strings, and
where the underlying semiring is the “probabilis-
tic semiring” over the nonnegative reals, addition
and multiplication having their usual interpreta-
tions. Such a transducer preserves regularity, both
in the forward (resp. reverse) directions, meaning
that the image through ? of any weighted regular
language over s (resp. over t) is again a weighted
regular language over t (resp. over s). In partic-
ular the forward (resp. reverse) image of a fixed
string s
0
(resp t
0
) can be computed in a compact
form as a weighted finite-state automaton (FSA)
over t (resp. s), which we can denote by ?(s
0
, ·)
(resp. ?(·, t
0
)). A weighted FSA can be easily nor-
malized into a probabilistic FSA
3
and, from this
probabilitic FSA exact samplers for the “parser”
?(s
0
, ·) and for the “generator” ?(·, t
0
)) are di-
rectly obtained.
4
In general, some of the properties that make
weighted FSAs and FSTs — over strings or trees
— specially relevant for probabilistic models of
language are the following: (i) they allow com-
pact representations of complex probability distri-
butions over linguistic objects (automata) or pairs
of linguistic objects (transducers), (ii) they permit
efficient exact sampling (and efficient optimiza-
tion over derivations (but not always over strings)),
(iii) they support modularity: intersection of au-
tomata, composition of transducers, projections of
an automaton through a transducer.
5
Conceptual architecture Armed with these
general considerations, let us now propose a con-
ceptual architecture based on a small number of
3
That is, into a weighted FSA such the weights of the tran-
sitions from each state sum to 1.
4
While sampling strings from a weighted finite-state au-
tomaton is simple, finding the most probable string (not path)
in a probabilistic FSA is an NP-hard problem (Casacuberta
and de la Higuera, 2000), and one has to resort to the so-
called Viterbi approximation (assuming that the most prob-
able path projects into the most probable string). Contrary
to popular belief, sampling can sometimes be simpler than
optimization.
5
Outside of the realm of finite-state machines, this modu-
larity is typically impossible to obtain. Thus, in general, the
availability of a sampler for a distribution p(x) (resp. a dis-
tribution q(x)) does not imply that we can efficiently sample
from the product (i.e. intersection) p(x).q(x), but we can in
case p and q are both represented by weighted FSAs.
1991
finite-state modules, which attempts to satisfy the
definition given above for probabilistic reversibil-
ity, to address the problem of robustness that we
described earlier, and can also support contex-
tual preferences. We illustrate the approach with
some simple examples of human-machine dia-
logues (between a customer and a virtual agent), a
domain for which reversibility has high relevance,
due to effects such as self-monitoring (Neumann,
1998; Levelt, 1983), interleaving of understand-
ing and generation (Otsuka and Purver, 2003), and
lexical entrainment (Brennan, 1996).
z y x
?

?

?

?

µ

?
Figure 1: Reversible specification through finite-
state factors.
The conceptual architecture is shown in Figure 1.
Formally, the figure represents a probabilistic
graphical model in so-called factor form, where
the factors are ?, ?, ?, ? (we have also indicated
for future reference the “contextual” factors ?, µ,
that we ignore for now). The factors take as argu-
ments three types of objects: z is a logical form,
that is, a structured object which can be naturally
represented as a tree, x is a surface string, and y
is a latent “underlying” string that corresponds to
one of a small collection of “canonical” texts for
realizing the logical form z (more about that later).
Each factor is realized through a weighted
finite-state machine (acceptor or transducer) over
strings or trees (Mohri, 2009; F¨ul¨op and Vogler,
2009; Maletti, 2010; Graehl et al., 2008).
The ? factor is a string automaton that repre-
sents a standard ngram language model (typically
specific to domain), in other words a probability
distribution over utterances x. Symmetrically, the
regular tree automaton ? represents a distribution
over logical forms z, which can be seen as play-
ing a similar role to the language model, but at the
semantic level, namely telling us what are the pos-
sible/likely logical forms in a certain domain.
6
The “canonical factor” ? is a weighted tree-
to-string transducer (Graehl et al., 2008), which
implements a relation between logical forms z
6
In particular, the ? factor makes explicit the notion of a
well-formed semantic representation, a notion often left im-
plicit in semantic parsing.
and a small number of latent “canonical” texts
y realizing these logical forms. For example, ?
may associate the logical form (dialog act) z =
wad(batLife, iphone6) — with wad an abbrevi-
ation for “what is the value of this attribute on this
device?”, and batLife an abbreviation for “bat-
tery life” —, with such a canonical text (among
a few others) as: What is the battery life of the
Iphone 6?.
The “similarity factor” ? is a weighted string-
to-string finite state transducer which gives scores
to x, y according to a notion of similarity. It has
the role of “bridging” the gap between the actual
utterances x and the latent canonical utterances y.
The intention behind the similarity factor is to “de-
couple” the task of modeling some possible real-
izations of a given logical form from the task of
recognizing that a given more or less well-formed
input is a variant of such a realization. This fac-
tor relates the two strings y and x, where y is a
possible canonical utterance in the limited reper-
tory produced by ?, and x is an actual utterance,
in particular any utterance that could be produced
by a human speaker. So for instance suppose that
the user’s utterance is x = What about battery du-
ration on this Iphone 6?, we would like this x to
have a significant similarity with the canonical ut-
terance y = What is the battery life of the Iphone
6? but a negligible similarity with another canon-
ical utterance such as y
?
= What is the screen size
of the Galaxy Trend?.
Overall, the canonical factor ?(z, y) concen-
trates more on a core “generation model”, namely
on producing some well-formed output y from a
logical form z, while the similarity factor ?(y, x)
allows relating an actual user input x to a possi-
ble output y of the ? model. The main import of ?
is then to allow to use the core generation model
defined by ? to be exploited for robust semantic
parsing.
Different instantiations of this scheme can be
employed. In some preliminary experiments that
we have performed,
7
? is a simple edit-distance
transducer (Mohri, 2003) which penalizes differ-
ently the discrepancies between x and y: strongly
for some salient content words or named entities of
the domain, weakly for less relevant content words
and for non-content words, with limited use of lo-
cal paraphrases (which can also be implemented
7
In these experiments, we used string-based approxima-
tions of the logical forms, and only employed string-based
transducers from the OpenFST library.
1992
through ?). This strategy seems to work reason-
ably well when the semantical repertory of the do-
main is restricted, because a large number of pos-
sible variants for x are “attracted” to the same un-
derlying semantics. In domains where small nu-
ances of expression may result in distinct seman-
tics, the division of work between ? and ? may be
different.
Parsing and Generation To understand the re-
versibility properties of the model of Figure 1,
let us first simplify the description by assuming
that z, instead of being a tree, is actually a string.
Then both ? and ? are string automata, and both
? and ? string-to-string transducers. Such a spec-
ification satisfies our definition of probabilistic re-
versibility, exploiting well-known compositional-
ity properties of weighted finite-state machines
over strings (Mohri, 2009). For parsing, we start
from a fixed x
0
, and can project it through ? into
a weighted FSA over y; in turn we can project
this automaton onto an FSA over z, and finally
intersect this automaton with ?, obtaining a fi-
nal weighted “x
0
-parser” automaton over z, rep-
resenting a probability distribution from which we
can draw exact samples as explained above.
8
Gen-
eration works in exactly the reverse way, starting
from a z
0
and eventually building a “z
0
-generator”
automaton over x.
In the actual proposal, z is a tree, meaning that
? is a tree automaton, and ? a tree-to-string trans-
ducer. While finite-state tree automata correspond
to a single concept, and share all the nice proper-
ties of string automata (Comon et al., 2007), the
situation with tree-to-tree or tree-to-string trans-
ducers is more complicated (Maletti, 2010; Graehl
et al., 2008): several variants exist, only some of
which support the operations that our conceptual
model requires (composition with the string trans-
ducer ? and intersection with the tree automaton
?). In particular, the “linear non-deleting top-
down tree transducers” defined in (Maletti, 2010)
9
have the requisite properties.
Contextual factors We now briefly come back
to the factors ? (tree automaton) and µ (string
automaton) of Figure 1, which highlight the use-
8
We could also have precompiled a generic parser for all
x’s by first marginalizing the latent variable y through a com-
position of the transducers ? and ?, and then intersecting the
resulting transducer with the automaton ?.
9
The paper only defines tree-to-tree transducers, but tree-
to-string variants can be derived easily.
fulness of our modular finite-state architecture.
These factors play similar roles to ? and ?, but
they evolve dynamically with the context. In dia-
logue applications, utterances can often only be in-
terpreted by reference to the current dialogue state
(e.g. “ten hours” in the context of a question about
battery life), and the ? factor can be used as a com-
pact representation of the current expectations of
the dialogue manager about the next logical form,
to be combined with the actual customer’s utter-
ance. Symmetrically, the µ factor can be used
to represent such phenomena as lexical entrain-
ment (Brennan, 1996), where the agent’s utterance
is oriented towards using similar wordings to the
customer’s.
5 Related work
The unique formal properties of finite-state ma-
chines, which favor modular decompositions of
complex tasks, have long been exploited in Com-
putational Linguistics. Tree transducers in partic-
ular have gained popularity in Statistical Machine
Translation, starting with (Yamada and Knight,
2001), as described in the surveys (Maletti, 2010;
Razmara, 2011).
The reversibility properties of finite-state trans-
ducers have been exploited to a more limited ex-
tent, starting with applications of non-weighted
string-to-string transducers to morphological anal-
ysis and generation (Beesley, 1996).
Concerning the application of weighted finite-
state tree machines to NLU/NLG reversibility, our
proposal is strongly related on the one hand to
the approach of (Jones et al., 2012), who ex-
plicitely proposes tree-to-string transducers as a
tool for modelling semantic parsing and for train-
ing on semantically annotated data, and on the
other hand to (Wong, 2007; Wong and Mooney,
2007), who focus more directly on the problem of
inverting a semantic parser into a generator. Wong
et al. do not explicitely use tree-based transducers,
but rather a formalism inspired by SCFGs (syn-
chronous context-free grammars), which essen-
tially corresponds to a form of tree-to-string trans-
ducer. In relation to reversibility considerations,
presentations in terms of synchronous formalisms
have the interest that they are intrinsically sym-
metrical. Such formalisms have tight relations to
tree-transducers (Shieber, 2004); one recently pro-
posed generalization, “Interpreted Regular Tree
Grammars” (Koller and Kuhlmann, 2011), allows
1993
multiple (possibly more than two) synchronized
views of an underlying abstract derivation tree,
and has the advantage of permitting a uniform
treatment of strings and trees.
One important aspect in which our proposal dif-
fers from these previous approaches is in propos-
ing to decouple the “core” task of mapping logical
forms to well-formed latent canonical realizations
from the task of relating these realizations to ac-
tual utterances, through an additional “similarity”
transducer acting as a bridge.
This idea of a bridge is however close to another
line of work in semantic parsing, not transducer
based, namely (Berant and Liang, 2014; Wang
et al., 2015). There, a simple generic grammar
is used to generate canonical realizations from a
repertory of possible logical forms (expressed in
a variant of lambda calculus). Given an input to
parse, simple heuristics are used to select a fi-
nite list of potential logical forms which are then
ranked according to the (paraphrase-based) simi-
larity of their associated canonical realization with
the input. Thus in this approach, a form of gener-
ation plays an important role, not for its own sake,
but as a tool for semantic parsing.
6 Conclusion
Because of their unique compositional properties,
finite-state modules are a natural choice for imple-
menting our definition of reversibility as efficient
bidirectional sampling from a common specifica-
tion. In this piece we have argued in favor of an
architecture realizing this definition and display-
ing robustness and contextuality.
Acknowledgments We thank the anonymous
reviewers for their detailed comments and for
pointing us to some relevant literature that we had
overlooked.
References
Kenneth Beesley. 1996. Arabic Finite-State Morpho-
logical Analysis and Generation. In Coling, pages
89–94.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1415–1425, Baltimore, Maryland, June. Association
for Computational Linguistics.
S.E. Brennan. 1996. Lexical entrainment in sponta-
neous dialog. In Proceedings of International Sym-
posium on Spoken Dialogue (ISSD-96).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15–24.
H. Comon, M. Dauchet, R. Gilleron, C. L¨oding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and appli-
cations. Available on: http://www.grappa.
univ-lille3.fr/tata. release October, 12th
2007.
Marc Dymetman. 1991. Inherently reversible gram-
mars, logic programming and computability. In
In Proceedings of the ACL Workshop: Reversible
Grammar in Natural Language Processing.
Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, Monographs in Theoretical
Computer Science. An EATCS Series, pages 313–
403. Springer Berlin Heidelberg.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391–427, September.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ’12, pages
488–496, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alexander Koller and Marco Kuhlmann. 2011. A
generalized view on parsing and translation. In
Proceedings of the 12th International Conference
on Parsing Technologies, IWPT ’11, pages 2–13,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
W.J.M Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14(1):41–104.
Andreas Maletti. 2010. Survey: Tree transducers in
machine translation. Technical report, Universitat
Rovira i Virgili.
Mehryar Mohri. 2003. Edit-Distance of Weighted Au-
tomata: General Definitions and Algorithms. Inter-
national Journal of Foundations of Computer Sci-
ence, 14:957–982.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, pages 213–254. Springer.
G¨unter Neumann. 1998. Interleaving natural language
parsing and generation through uniform processing.
Artificial Intelligence, 99(1):121–163.
1994
M. Otsuka and M. Purver. 2003. Incremental gener-
ation by incremental parsing. In Proceedings 6th
CLUK Colloquium.
Majid Razmara. 2011. Applications of tree transduc-
ers in statistical machine translation. Technical re-
port, Simon Fraser University.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
Stuart M. Shieber. 2004. Synchronous grammars
as tree transducers. In Proceedings of the Seventh
International Workshop on Tree Adjoining Gram-
mar and Related Formalisms (TAG+ 7), Vancouver,
Canada, 20–22 May.
Tomek Strzalkowski, editor. 1994. Reversible Gram-
mar in Natural Language Processing. Springer.
Gertjan van Noord. 1990. Reversible unification based
machine translation. In Proceedings of the 13th
Conference on Computational Linguistics - Volume
2, COLING ’90, pages 299–304, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Y. Wang, J. Berant, and P. Liang. 2015. Building a
semantic parser overnight. In Association for Com-
putational Linguistics (ACL).
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. In HLT-NAACL, pages
172–179.
Yuk Wah Wong. 2007. Learning for Semantic Pars-
ing and Natural Language Generation Using Statis-
tical Machine Translation Techniques. Ph.D. the-
sis, Department of Computer Sciences, University
of Texas at Austin, Austin, TX, August. Also ap-
pears as Technical Report AI07-343, Artificial Intel-
ligence Lab, University of Texas at Austin, August
2007.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics, ACL ’01, pages 523–530,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
1995
