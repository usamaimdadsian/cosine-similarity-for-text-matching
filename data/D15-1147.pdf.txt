Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270,
Lisbon, Portugal, 17-21 September 2015.
c
©2015 Association for Computational Linguistics.
How to Avoid Unwanted Pregnancies:
Domain Adaptation using Neural Network Models
Shafiq Joty, Hassan Sajjad, Nadir Durrani,
Kamla Al-Mannai, Ahmed Abdelali and Stephan Vogel
Qatar Computing Research Institute - Hamad Bin Khalifa University
{sjoty,hsajjad,ndurrani,kmannai,aabdelali,svogel}@qf.org.qa
Abstract
We present novel models for domain adap-
tation based on the neural network joint
model (NNJM). Our models maximize
the cross entropy by regularizing the loss
function with respect to in-domain model.
Domain adaptation is carried out by as-
signing higher weight to out-domain se-
quences that are similar to the in-domain
data. In our alternative model we take a
more restrictive approach by additionally
penalizing sequences similar to the out-
domain data. Our models achieve better
perplexities than the baseline NNJM mod-
els and give improvements of up to 0.5
and 0.6 BLEU points in Arabic-to-English
and English-to-German language pairs, on
a standard task of translating TED talks.
1 Introduction
Rapid influx of digital data has galvanized the use
of empirical methods in many fields including Ma-
chine Translation (MT). The increasing availabil-
ity of bilingual corpora has made it possible to
automatically learn translation rules that required
years of linguistic analysis previously. While ad-
ditional data is often beneficial for a general pur-
pose Statistical Machine Translation (SMT) sys-
tem, a problem arises when translating new do-
mains such as lectures (Cettolo et al., 2014),
patents (Fujii et al., 2010) or medical text (Bojar
et al., 2014), where either the bilingual text does
not exist or is available in small quantity. All do-
mains have their own vocabulary and stylistic pref-
erences which cannot be fully encompassed by a
system trained on the general domain.
Machine translation systems trained from a sim-
ple concatenation of small in-domain and large
out-domain data often perform below par be-
cause the out-domain data is distant or over-
whelmingly larger than the in-domain data. Ad-
ditional data increases lexical ambiguity by in-
troducing new senses to the existing in-domain
vocabulary. For example, an Arabic-to-English
SMT system trained by simply concatenating in-
and out-domain data translates the Arabic phrase
“PAJ



J
	
kCË Y

K@
	
QË @ ÉÒm
Ì
'@

éÊ¾

?Ó
	
á«” to “about the
problem of unwanted pregnancy”. This translation
is incorrect in the context of the in-domain data,
where it should be translated to “about the prob-
lem of choice overload”. The sense of the Ara-
bic phrase taken from out-domain data completely
changes the meaning of the sentence. In this paper,
we tackle this problem by proposing domain adap-
tation models that make use of all the data while
preserving the in-domain preferences.
A significant amount of research has been car-
ried out recently in domain adaptation. The com-
plexity of the SMT pipeline, starting from cor-
pus preparation to word-alignment, and then train-
ing a wide range of models opens a wide horizon
to carry out domain specific adaptations. This is
typically done using either data selection (Mat-
soukas et al., 2009) or model adaptation (Foster
and Kuhn, 2007). In this paper, we further re-
search in model adaptation using the neural net-
work framework.
In recent years, there has been a growing in-
terest in deep neural networks (NNs) and word
embeddings with application to numerous NLP
problems. A notably successful attempt on the
SMT frontier was recently made by Devlin et
al. (2014). They proposed a neural network
joint model (NNJM), which augments streams of
source with target n-grams and learns a NN model
over vector representation of such streams. The
model is then integrated into the decoder and used
as an additional language model feature.
Our aim in this paper is to advance the state-of-
the-art in SMT by extending NNJM for domain
adaptation to leverage the huge amount of out-
1259
domain data coming from heterogeneous sources.
We hypothesize that the distributed vector rep-
resentation of NNJM helps to bridge the lexical
differences between the in-domain and the out-
domain data, and adaptation is necessary to avoid
deviation of the model from the in-domain data,
which otherwise happens because of the large out-
domain data.
To this end, we propose two novel extensions
of NNJM for domain adaptation. Our first model
minimizes the cross entropy by regularizing the
loss function with respect to the in-domain model.
The regularizer gives higher weight to the training
instances that are similar to the in-domain data.
Our second model takes a more conservative ap-
proach by additionally penalizing data instances
similar to the out-domain data.
We evaluate our models on the standard task
of translating Arabic-English and English-German
language pairs. Our adapted models achieve bet-
ter perplexities (Chen and Goodman, 1999) than
the models trained on in- and in+out-domain data.
Improvements are also reflected in BLEU scores
(Papineni et al., 2002) as we compare these mod-
els within the SMT pipeline. We obtain gains of
up to 0.5 and 0.6 on Arabic-English and English-
German pairs over a competitive baseline system.
The remainder of this paper is organized as fol-
lows: Section 2 gives an account on related work.
Section 3 revisits NNJM model and Section 4 dis-
cusses our models. Section 5 presents the experi-
mental setup and the results. Section 6 concludes.
2 Related Work
Previous work on domain adaptation in MT can
be broken down broadly into two main categories
namely data selection and model adaptation.
2.1 Data Selection
Data selection has shown to be an effective way
to discard poor quality or irrelevant training in-
stances, which when included in an MT system,
hurts its performance. The idea is to score the out-
domain data using a model trained from the in-
domain data and apply a cut-off based on the re-
sulting scores. The MT system can then be trained
on a subset of the out-domain data that is closer
to in-domain. Selection based methods can be
helpful to reduce computational cost when train-
ing is expensive and also when memory is con-
strained. Data selection was done earlier for lan-
guage modeling using information retrieval tech-
niques (Hildebrand et al., 2005) and perplexity
measures (Moore and Lewis, 2010). Axelrod et
al. (2011) further extended the work of Moore and
Lewis (2010) to translation model adaptation by
using both source- and target-side language mod-
els. Duh et al. (2013) used a recurrent neural lan-
guage model instead of an ngram-based language
model to do the same. Translation model features
were used recently by (Liu et al., 2014; Hoang and
Sima’an, 2014) for data selection. Durrani et al.
(2015a) performed data selection using operation
sequence model (OSM) and NNJM models.
2.2 Model Adaptation
The downside of data selection is that finding an
optimal cut-off threshold is a time consuming pro-
cess. An alternative to completely filtering out
less useful data is to minimize its effect by down-
weighting it. It is more robust than selection since
it takes advantage of the complete out-domain data
with intelligent weighting towards the in-domain.
Matsoukas et al. (2009) proposed a
classification-based sentence weighting method
for adaptation. Foster et al. (2010) extended this
by weighting phrases rather than sentence pairs.
Other researchers have carried out weighting by
merging phrase-tables through linear interpolation
(Finch and Sumita, 2008; Nakov and Ng, 2009)
or log-linear combination (Foster and Kuhn,
2009; Bisazza et al., 2011; Sennrich, 2012)
and through phrase training based adaptation
(Mansour and Ney, 2013). Durrani et al. (2015a)
applied EM-based mixture modeling to OSM
and NNJM models to perform model weighting.
Chen et al. (2013b) used a vector space model for
adaptation at the phrase level. Every phrase pair is
represented as a vector, where every entry in the
vector reflects its relatedness with each domain.
Chen et al. (2013a) also applied mixture model
adaptation for reordering model.
Other work on domain adaptation includes but
not limited to studies focusing on topic models
(Eidelman et al., 2012; Hasler et al., 2014), dy-
namic adaptation without in-domain data (Sen-
nrich et al., 2013; Mathur et al., 2014) and sense
disambiguation (Carpuat et al., 2013).
In this paper, we do model adaptation using a
neural network framework. In contrast to pre-
vious work, we perform it at the (bilingual) n-
gram level, where n is sufficiently large to cap-
ture long-range cross-lingual dependencies. The
1260
generalized vector representation of the neural net-
work model reduces the data sparsity issue of tra-
ditional Markov-based models by learning better
word classes. Furthermore, our specially designed
loss functions for adaptation help the model to
avoid deviation from the in-domain data without
losing the ability to generalize.
3 Neural Network Joint Model
In recent years, there has been a great deal of ef-
fort dedicated to neural networks (NNs) and word
embeddings with applications to SMT and other
areas in NLP (Bengio et al., 2003; Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Gao et
al., 2014; Schwenk, 2012; Collobert et al., 2011;
Mikolov et al., 2013a; Socher et al., 2013; Hin-
ton et al., 2012). Recently, Devlin et al. (2014)
proposed a neural network joint model (NNJM)
and integrated it into the decoder as an additional
feature. They showed impressive improvements
in Arabic-to-English and Chinese-to-English MT
tasks. Let us revisit the NNJM model briefly.
Given a source sentence S and its correspond-
ing target sentence T , the NNJM model computes
the conditional probability P (T |S) as follows:
P (T |S) ?
|T |?
i
P (t
i
|t
i?1
. . . t
i?p+1
, s
i
) (1)
where, s
i
is a q-word source window for the tar-
get word t
i
based on the one-to-one (non-NULL)
alignment of T to S. As exemplified in Figure 1,
this is essentially a (p + q)-gram neural network
LM (NNLM) originally proposed by Bengio et al.
(2003). Each input word i.e. source or target word
in the context is represented by a D dimensional
vector in the shared look-up layer L ? R
|V
i
|×D
,
where V
i
is the input vocabulary.
1
The look-up
layer then creates a context vector x
n
representing
the context words of the (p+q)-gram sequence by
concatenating their respective vectors in L. The
concatenated vector is then passed through non-
linear hidden layers to learn a high-level represen-
tation, which is in turn fed to the output layer. The
output layer has a softmax activation over the
output vocabulary V
o
of target words. Formally,
the probability of getting k-th word in the output
given the context x
n
can be written as:
P (y
n
= k|x
n
, ?) =
exp (w
T
k
?(x
n
))
?
|V
o
|
m=1
exp (w
T
m
?(x
n
))
(2)
1
Note that L is a model parameter to be learned.
where ?(x
n
) defines the transformations of x
n
through the hidden layers, and w
k
are the weights
from the last hidden layer to the output layer.
For notational simplicity, henceforth we will use
(x
n
, y
n
) to represent a training sequence.
By setting p and q to be sufficiently large,
NNJM can capture long-range cross-lingual de-
pendencies between words, while still overcom-
ing the data sparseness issue by virtue of its dis-
tributed representations (i.e., word vectors). A ma-
jor bottleneck, however, is to surmount the com-
putational cost involved in training the model and
applying it for MT decoding. Devlin et al. (2014)
proposed two tricks to speed up computation in
decoding. The first one is to pre-compute the hid-
den layer computations and fetch them directly as
needed during decoding. The second technique is
to train a self-normalized NNJM to avoid compu-
tation of the softmax normalization factor (i.e., the
denominator in Equation 2) in decoding. How-
ever, self-normalization does not solve the compu-
tational cost of training the model. In the follow-
ing, we describe a method to address this issue.
3.1 Training by Noise Contrastive Estimation
The standard way to train NNLMs is to maximize
the log likelihood of the training data:
J(?) =
N?
n=1
|V
o
|?
k=1
y
nk
log P (y
n
= k|x
n
, ?) (3)
where, y
nk
= I(y
n
= k) is an indicator vari-
able (i.e., y
nk
=1 when y
n
=k, otherwise 0). Op-
timization is performed using first-order online
methods, such as stochastic gradient ascent (SGA)
with standard backpropagation algorithm. Unfor-
tunately, training NNLMs are impractically slow
because for each training instance (x
n
, y
n
), the
softmax output layer (see Equation 2) needs to
compute a summation over all words in the output
vocabulary.
2
Noise contrastive estimation or NCE
(Gutmann and Hyv¨arinen, 2010) provides an effi-
cient and stable way to avoid this repetitive com-
putation as recently applied to NNLMs (Vaswani
et al., 2013; Mnih and Teh, 2012). We can re-write
Equation 2 as follows:
P (y
n
= k|x
n
, ?) =
?(y
n
= k|x
n
, ?)
Z(?(x
n
),W)
(4)
where ?(.) is the un-normalized score and Z(.)
is the normalization factor. In NCE, we consider
2
This would take few weeks for a modern CPU machine
to train a single NNJM model on the whole data.
1261
Source token 1
Source token 2
Source token 3
Target token 1
Target token 2
Hidden
layer
?(x
n
)
Look-up
layer
x
n
Output
layer
C
y
m
n
y
n
M
W
U
?
pi
Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target
words (i.e., 2-words history) and a source context window of size 3. For illustration, the output y
n
is
shown as a single categorical variable (scalar) as opposed to the traditional one-hot vector representation.
Z(.) as an additional model parameter along with
the regular parameters, i.e., weights, look-up vec-
tors. However, it has been shown that fixing Z(.)
to 1 instead of learning it in training does not affect
the model performance (Mnih and Teh, 2012).
For each training instance (x
n
, y
n
), we add
M noise samples (x
n
, y
m
n
) by sampling y
m
n
from
a known noise distribution ? (e.g., unigram,
uniform)M many times (i.e.,m= 1 . . .M ); see
Figure 1. NCE loss is then defined to discriminate
a true instance from a noisy one. Let C ? {0, 1}
denote the class of an instance with C = 1 indicat-
ing true and C = 0 indicating noise. NCE maxi-
mizes the following conditional log likelihood:
J(?) =
N?
n=1
[
log[P (C = 1|y
n
,x
n
, ?)]
+
M?
m=1
log[P (C = 0|y
m
n
,x
n
, ?)]
]
(5)
=
N?
n=1
[
log [P (y
n
|C = 1,x
n
, ?)P (C = 1|pi)]
+
M?
m=1
log [(P (y
m
n
|C = 0,x
n
, ?))P (C = 0|pi)]
? (M + 1) log Q
]
(6)
where Q = P (y
n
, C = 1|x
n
, ?, pi) + P (y
m
n
, C =
0|x
n
, ?, pi) is a normalization constant. After re-
moving the constant terms, Equation 6 can be fur-
ther simplified as:
J(?) =
N?
n=1
|V
o
|?
k=1
[
y
nk
log ?
nk
+
M?
m=1
y
m
nk
log ?
nk
]
(7)
where ?
nk
=P (y
m
n
= k|x
n
, ?) is the noise dis-
tribution, ?
nk
=?(y
n
= k|x
n
, ?) is the unnormal-
ized score at the output layer (Equation 4), and y
nk
and y
m
nk
are indicator variables as defined before.
NCE reduces the number of computations needed
at the output layer from |V
o
| to M + 1, where M
is a small number in comparison with |V
o
|. In all
our experiments we use NCE loss with M = 100
samples as suggested by Mnih and Teh (2012).
4 Neural Domain Adaptation Models
The ability to generalize and learn complex se-
mantic relationships (Mikolov et al., 2013b) and
its compelling empirical results gives a strong mo-
tivation to use the NNJM model for the problem of
domain adaptation in machine translation. How-
ever, the vanilla NNJM described above is limited
in its ability to effectively learn from a large and
diverse out-domain data in the best favor of an in-
domain data. To address this, we propose two neu-
ral domain adaptation models (NDAM) extending
the NNJM model. Our models add regularization
to its loss function either with respect to in-domain
or both in- and out-domains. In both cases, we first
present the regularized loss function for the nor-
malized output layer with the standard softmax,
1262
followed by the corresponding un-normalized one
using the noise contrastive estimation.
4.1 NDAM
v1
To improve the generalization of word embed-
dings, NNLMs are generally trained on very large
datasets (Mikolov et al., 2013a; Vaswani et al.,
2013). Therefore, we aim to train our neural
domain adaptation models (NDAM) on in- plus
out-domain data, while restricting it to drift away
from in-domain. In our first model NDAM
v1
, we
achieve this by biasing the model towards the in-
domain using a regularizer (or prior) based on the
in-domain model. Let ?
i
be an NNJM model al-
ready trained on the in-domain data. We train an
adapted model ?
a
on the whole data, but regular-
izing it with respect to ?
i
. We redefine the normal-
ized loss function of Equation 3 as follows:
J(?
a
) =
N?
n=1
|V
o
|?
k=1
[
? y
nk
logP (y
n
= k|x
n
, ?
a
) + (1? ?)
y
nk
P (y
n
= k|x
n
, ?
i
) logP (y
n
= k|x
n
, ?
a
)
]
(8)
=
N?
n=1
|V
o
|?
k=1
[
? y
nk
log yˆ
nk
(?
a
) +
(1? ?) y
nk
p
nk
(?
i
) log yˆ
nk
(?
a
)
]
(9)
where yˆ
nk
(?
a
) is the softmax output and p
nk
(?
i
)
is the probability of the training instance accord-
ing to the in-domain model ?
i
. Notice that the loss
function minimizes the cross entropy of the cur-
rent model ?
a
with respect to the gold labels y
n
and the in-domain model ?
i
. The mixing param-
eter ? ? [0, 1] determines the relative strength of
the two components.
3
Similarly, we can re-define
the NCE loss of Equation 7 as:
J(?
a
) =
N?
n=1
|V
o
|?
k=1
[
? y
nk
log ?
nk
+ (1? ?) y
nk
p
nk
(?
i
) log ?
nk
+
M?
m=1
y
m
nk
log ?
nk
]
(10)
We use SGA with backpropagation to train this
model. The derivatives of J(?
a
) with respect to
the final layer weight vectors w
j
turn out to be:
?
w
j
J(?
a
) =
N?
n=1
[
? (y
nj
? ?
nj
) + (1? ?)
[p
nj
(?
i
)?
?
k
y
nk
p
nk
(?
i
) ?
nj
]
]
(11)
3
We used a balanced value ? = 0.5 for our experiments.
4.2 NDAM
v2
The regularizer in NDAM
v1
is based on an in-
domain model ?
i
, which puts higher weights to
the training instances (i.e., n-gram sequences) that
are similar to the in-domain ones. This might
work better when the out-domain data is similar
to the in-domain data. In cases where the out-
domain data is different, we might want to build
a more conservative model that penalizes training
instances for being similar to the out-domain ones.
Let ?
i
and ?
o
be the two NNJMs already trained
from the in- and out-domains, respectively, and ?
o
is trained using the same vocabulary as ?
i
. We de-
fine the new normalized loss function as follows:
J(?
a
) =
N?
n=1
|V
o
|?
k=1
[
? y
nk
log yˆ
nk
(?
a
) + (1? ?) y
nk
[p
nk
(?
i
)? p
nk
(?
o
)] log yˆ
nk
(?
a
)
]
(12)
where y
nk
, yˆ
nk
(?
a
), p
nk
(?
i
) and p
nk
(?
o
) are sim-
ilarly defined as before. This loss function min-
imizes the cross entropy of the current model ?
a
with respect to the gold labels y
n
and the differ-
ence between the in-domain model ?
i
and the out-
domain model ?
o
. Intuitively, the regularizer as-
signs higher weights to training instances that are
not only similar to the in-domain but also dissim-
ilar to the out-domain. The parameter ? ? [0, 1]
determines the strength of the regularization. The
corresponding NCE loss can be defined as follows:
J(?
a
) =
N?
n=1
|V
o
|?
k=1
[
? y
nk
log ?
nk
+ (1? ?) y
nk
log ?
nk
(p
nk
(?
i
)? p
nk
(?
o
)) +
M?
m=1
y
m
nk
log ?
nk
]
(13)
The derivatives of the above cost function with re-
spect to the final layer weight vectors w
j
are:
?
w
j
J(?
a
) =
N?
n=1
[
? (y
nj
? ?
nj
) + (1? ?)[p
nj
(?
i
)?
p
nj
(?
o
)?
?
k
y
nk
?
nj
(p
nk
(?
i
)? p
nk
(?
o
))]
]
(14)
In a way, the regularizers in our loss functions
are inspired from the data selection methods of
Axelrod et al. (2011), where they use cross entropy
between the in- and the out-domain LMs to score
out-domain sentences. However, our approach is
quite different from them in several aspects. First
1263
and most importantly, we take the scoring inside
model training and use it to bias the training to-
wards the in-domain model. Both the scoring and
the training are performed at the bilingual n-gram
level rather than at the sentence level. Integrating
scoring inside the model allows us to learn a robust
model by training/tuning the relevant parameters,
while still using the complete data. Secondly, our
models are based on NNs, while theirs utilize the
traditional Markov-based generative models.
4.3 Technical Details
In this section, we describe some implementation
details of NDAM that we found to be crucial,
such as: using gradient clipping to handle vanish-
ing/exploding gradient problem in SGA training
with backpropagation, selecting appropriate noise
distribution in NCE, and special handling of out-
domain words that are unknown to the in-domain.
4.3.1 Gradient Clipping
Two common issues with training deep NNs on
large data-sets are the vanishing and the exploding
gradients problems (Pascanu et al., 2013). The er-
ror gradients propagated by the backpropagation
may sometimes become very small or very large
which can lead to undesired (nan) values in weight
matrices, causing the training to fail. We also ex-
perienced the same problem in our NDAM quite
often. One simple solution to this problem is to
truncate the gradients, known as gradient clipping
(Mikolov, 2012). In our experiments, we limit the
gradients to be in the range [?5;+5].
4.3.2 Noise Distribution in NCE
Training with NCE relies on sampling from a
noise distribution (i.e., ? in Equation 5), and the
performance of the NDAM models varies consid-
erably with the choice of the distribution. We ex-
plored uniform and unigram noise distributions in
this work. With uniform distribution, every word
in the output vocabulary has the same probability
to be sampled as noise. The unigram noise dis-
tribution is a multinomial distribution over words
constructed by counting their occurrences in the
output (i.e., n-th word in the n-gram sequence).
In our experiments, unigram distribution delivered
much lower perplexity and better MT results com-
pared to the uniform one. Mnih and Teh (2012)
also reported similar findings on perplexity.
4.3.3 Handling of Unknown Words
In order to reduce the training time and to learn
better word representations, NNLMs are often
trained on most frequent vocabulary words only
and low frequency words are represented under a
class of unknown words, unk. This results in a
large number of n-gram sequences containing at
least one unk word and thereby, makes unk a
highly probable word in the model.
4
Our NDAM models rely on scoring out-domain
sequences (of word Ids) using models that are
trained based on the in-domain vocabulary. To
score out-domain sequences using a model, we
need to generate the sequences using the same vo-
cabulary based on which the model was trained.
In doing so, the out-domain words that are un-
known to the in-domain data map to the same unk
class. As a result, out-domain sequences contain-
ing unks get higher probability although they are
distant from the in-domain data.
A solution to this problem is to have an in-
domain model that can differentiate between its
own unk class, resulted from the reduced in-
domain vocabulary, and actual unknown words
that come from the out-domain data. We intro-
duce a new class unk
o
to represent the latter.
We train the in-domain model by adding a few
dummy sequences containing unk
o
occurring on
both source and target sides. This enables the
model to learn unk and unk
o
separately, where
unk
o
is a less probable class according to the
model. Later, the n-gram sequences of the out-
domain data contain both unk and unk
o
classes
depending on whether a word is unknown to only
pruned in-domain vocabulary (i.e., unk) or is un-
known to full in-domain vocabulary (i.e., unk
o
).
5 Evaluation
In this section, we describe the experimental
setup (i.e., data, settings for NN models and MT
pipeline) and the results. First we evaluate our
models intrinsically by comparing the perplexities
on a held-out in-domain testset against the base-
line NNJM model. Then we carry out an extrinsic
evaluation by using the NNJM and NDAM models
as features in machine translation and compare the
BLEU scores. Initial developmental experiments
were done on the Arabic-to-English language pair.
4
For our Arabic-English in-domain data, 30% of n-gram
sequences contain at least one unk word.
1264
We carried out further experiments on the English-
to-German pair to validate our models.
5.1 Data
We experimented with the data made publicly
available for the translation task of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT) (Cettolo et al., 2014). We used TED
talks as our in-domain corpus. For Arabic-to-
English, we used the QCRI Educational Domain
(QED) – A bilingual collection of educational lec-
tures
5
(Abdelali et al., 2014), the News, and the
multiUN (UN) (Eisele and Chen, 2010) as our
out-domain corpora. For English-to-German, we
used the News, the Europarl (EP), and the Com-
mon Crawl (CC) corpora made available for the
9
th
Workshop of Statistical Machine Translation.
6
Table 1 shows the size of the data used.
Training NN models is expensive. We, there-
fore, randomly selected subsets of about 300K
sentences from the bigger domains (UN, CC and
EP) to train the NN models.
7
The systems were
tuned on concatenation of the dev. and test2010
and evaluated on test2011-2013 datasets. The tun-
ing set was also used to measure the perplexities
of different models.
5.2 System Settings
NNJM & NDAM: The NNJM models were
trained using NPLM
8
toolkit (Vaswani et al.,
2013) with the following settings. We used a tar-
get context of 5 words and an aligned source win-
dow of 9 words, forming a joint stream of 14-
grams for training. We restricted source and tar-
get side vocabularies to the 20K and 40K most
frequent words. The word vector size D and the
hidden layer size were set to 150 and 750, respec-
tively. Only one hidden layer is used to allow
faster decoding. Training was done by the stan-
dard stochastic gradient ascent with NCE using
5
Guzm´an et al. (2013) showed that the QED corpus is
similar to IWSLT and adding it improves translation quality.
6
http://www.statmt.org/wmt14/translation-task.html
7
Concatenating all the data results in a corpus of ap-
proximately 4.5 million sentences which requires roughly
18 days of wall-clock time (18 hours/epoch on a Linux
Ubuntu 12.04.5 LTS running on a 16 Core Intel Xeon E5-
2650 2.00Ghz and 64Gb RAM) to train NNJM models on
our machines. We ran one baseline experiment with all the
data and did not find it better than the system trained on ran-
domly selected subset of the data. In the interest of time, we
therefore reduced the NN training to a subset (800K and 1M
sentences for AR-EN and EN-DE respectively).
8
http://nlg.isi.edu/software/nplm/
AR-EN EN-DE
Corpus Sent. Tok. Corpus Sent. Tok.
IWSLT 150k 2.8/3.0 IWSLT 177K 3.5/3.3
QED 150k 1.4/1.5 CC 2.3M 57/53
NEWS 203k 5.6/6.3 NEWS 200K 2.8/3.4
UN 3.7M 129/125 EP 1.8M 51/48
Table 1: Statistics of the Arabic-English and
English-German training corpora in terms of Sen-
tences and Tokens (Source/Target). Tokens are
represented in millions.
100 noise samples and a mini-batch size of 1000.
All models were trained for 25 epochs. We used
identical settings to train the NDAM models, ex-
cept for the special handling of unk tokens.
Machine Translation System: We trained a
Moses system (Koehn et al., 2007), with the
following settings: a maximum sentence length
of 80, Fast-Aligner for word-alignments (Dyer et
al., 2013), an interpolated Kneser-Ney smoothed
5-gram language model with KenLM (Heafield,
2011), lexicalized reordering model (Galley and
Manning, 2008), a 5-gram operation sequence
model (Durrani et al., 2015b) and other default pa-
rameters. We also used an NNJM trained with the
settings described above as an additional feature
in our baseline system. In adapted systems, we
replaced the NNJM model with the NDAM mod-
els. We used ATB segmentation using the Stanford
ATB segmenter (Green and DeNero, 2012) for
Arabic-to-English and the default tokenizer pro-
vided with the Moses toolkit (Koehn et al., 2007)
for the English-to-German pair. Arabic OOVs
were translated using an unsupervised transliter-
ation module in Moses (Durrani et al., 2014). We
used k-best batch MIRA (Cherry and Foster, 2012)
for tuning.
5.3 Intrinsic Evaluation
In this section, we compare the NNJM model and
our NDAM models in terms of their perplexity
numbers on the in-domain held-out dataset (i.e.,
dev+test2010). We choose Arabic-English lan-
guage pair for the development experiments and
train domain-wise models to measure the related-
ness of each domain with respect to the in-domain.
We later replicated selective experiments for the
English-German language pair.
The first part of Table 2 summarizes the results
for Arabic-English. The perplexity numbers in the
second column (NNJM
b
) show that NEWS is the
1265
Domain NNJM
b
NNJM
cat
NDAM
v1
NDAM
v2
Arabic-English
IWSLT 12.55 - - -
QED 61.34 11.72 11.14 11.15
NEWS 42.88 10.88 10.67 10.59
UN 111.11 11.25 10.83 10.74
ALL - 10.31 10.08 10.22
English-German
IWSLT 10.20 – – –
ALL - 6.71 6.21 6.37
Table 2: Comparing the perplexity of NNJM
and NDAM models. NNJM
b
represents the model
trained on each individual domain separately.
most related domain from the perspective of in-
domain data, whereas UN is the farthest having
the worst perplexity. The third column (NNJM
cat
)
shows results of the models trained from concate-
nating each domain to the in-domain data. The
perplexity numbers improved significantly in each
case showing that there is useful information avail-
able in each domain which can be utilized to im-
prove the baseline. It also shows the robustness of
neural network models. Unlike the n-gram model,
the NN-based model improves generalization with
the increase in data without completely skewing
towards the dominating part of the data.
Concatenating in-domain with the NEWS data
gave better perplexities than other domains. Best
results were obtained by concatenating all the
data together (See row ALL). The third and fourth
columns show results of our models (NDAM
v?
).
Both give better perplexities than NNJM
cat
in all
cases. However, it is unclear which of the two
is better. Similar observations were made for the
English-to-German pair, where we only did exper-
iments on the concatenation of all domains.
5.4 Extrinsic Evaluation
Arabic-to-English: For most language pairs,
the conventional wisdom is to train the system
with all available data. However, previously re-
ported MT results on Arabic-to-English (Mansour
and Ney, 2013) show that this is not optimal and
the results are often worse than only using in-
domain data. The reason for this is that the UN
domain is found to be distant and overwhelmingly
large as compared to the in-domain IWSLT data.
We carried out domain-wise experiments and also
found this to be true.
We considered three baseline systems: (i) B
in
,
SYS IWSLT QED NEWS UN ALL
B
in
26.1 - - - -
B
cat
- 26.2 26.7 25.8 26.5
B
cat,in
- 26.2 26.3 25.9 26.5
Table 3: Results of the baseline Arabic-to-English
MT systems. The numbers are averaged over
tst2011-2013.
which is trained on the in-domain data, (ii) B
cat
,
which is trained on the concatenation of in- and
out-domain data, and (ii) B
cat,in
, where the MT
pipeline was trained on the concatenation but the
NNJM model is trained only on the in-domain
data. Table 3 reports average BLEU scores across
three test sets on all domains. Adding QED and
NEWS domains gave improvements on top of the
in-domain IWSLT baseline. Concatenation of UN
with in-domain made the results worse. Concate-
nating all out-domain and in-domain data achieves
+0.4 BLEU gain on top of the baseline in-domain
system. We will use B
cat
systems as our baseline
to compare our adapted systems with.
Table 4 shows results of the MT systems S
v1
and S
v2
using our adapted models NDAM
v1
and
NDAM
v2
. We compare them to the baseline sys-
tem B
cat
, which uses the non-adapted NNJM
cat
as a feature. S
v1
achieved an improvement of up
to +0.4 and S
v2
achieved an improvement of up
to +0.5 BLEU points. However, S
v2
performs
slightly worse than S
v1
on individual domains.
We speculate this is because of the nature of the
NDAM
v2
, which gives high weight to out-domain
sequences that are liked by the in-domain model
and disliked by the out-domain model. In the case
of individual domains, NDAM
v2
might be over pe-
nalizing out-domain since the out-domain model
is only built on that particular domain and always
prefers it more than the in-domain model. In case
of ALL, the out-domain model is more diverse and
has different level of likeness for each domain.
We analyzed the output of the baseline system
(S
cat
) and spotted several cases of lexical ambigu-
ity caused by out-domain data. For example, the
Arabic phrase PAJ



J
	
kCË Y

K@
	
QË @ ÉÒm
Ì
'@ can be trans-
lated to choice overload or unwanted pregnancy.
The latter translation is incorrect in the context of
in-domain. The bias created due to the out-domain
data caused S
cat
to choose the contextually incor-
rect translation unwanted pregnancy. However,
the adapted systems S
v?
were able to translate it
1266
QED NEWS UN ALL
tst11 tst12 tst13 tst11 tst12 tst13 tst11 tst12 tst13 tst11 tst12 tst13
B
cat
25.0 27.3 26.2 25.4 27.6 27.1 24.7 27.0 25.8 25.0 27.5 27.0
S
v1
25.2 27.7 26.2 25.8 27.8 27.3 24.7 27.5 26.1 25.3 27.8 27.0
? +0.2 +0.4 0.0 +0.4 +0.2 +0.2 0.0 +0.5 +0.3 +0.3 +0.2 0.0
S
v2
25.1 27.6 26.2 25.6 27.9 27.2 24.6 27.2 26.1 25.5 27.9 26.9
? +0.1 +0.3 0.0 +0.2 +0.3 +0.1 -0.1 +0.2 +0.3 +0.5 +0.4 -0.1
Table 4: Arabic-to-English MT Results
SYS tst11 tst12 tst13 Avg
Baselines
B
in
25.0 22.5 23.2 23.6
B
cat
25.7 22.9 24.1 24.2
B
cat,in
26.0 22.4 23.6 24.0
Comparison against NDAM
B
cat
25.7 22.9 24.1 24.2
S
v1
26.3 23.1 24.5 24.6
? +0.6 +0.2 +0.4 +0.4
S
v2
26.2 23.0 24.6 24.6
? +0.5 +0.1 +0.5 +0.4
Table 5: English-to-German MT Results
correctly. In another example ?
	
àYJ
.
Ë @

é

¯AJ


Ë
	
á« @
	
XAÓ
(How about fitness?), the word

é

¯AJ


Ë is translated
to proprietary by S
cat
, a translation frequently ob-
served in the out-domain data. S
v?
translated it
correctly to fitness, as preferred by the in-domain.
English-to-German: Concatenating all training
data to train the MT pipeline has been shown to
give the best results for English-to-German (Birch
et al., 2014). Therefore, we did not do domain-
wise experiments, except for training a system on
the in-domain IWSLT data for the sake of com-
pleteness. We also tried B
cat,in
variation, i.e.
training an MT system on the entire data and using
in-domain data to train the baseline NNJM. The
baseline system B
cat
gave better results and was
used as our reference for comparison.
Table 5 shows the results of our systems, S
v1
and S
v2
, compared to the baselines, B
in
and B
cat
.
Unlike Arabic-to-English, the baseline system B
in
is much worse than B
cat
. Our adapted MT systems
S
v1
and S
v2
both outperformed the best baseline
system (B
cat
) with an improvement of up to 0.6
points. S
v2
performed slightly better than S
v1
on
one occasion and slightly worse in others.
Comparison with Data Selection: We also
compared our results with the MML-based data
SYS tst11 tst12 tst13 Avg
Arabic-to-English
B
cat
25.0 27.5 27.0 26.5
S
v1
25.3 27.8 27.0 26.7
B
mml
25.5 27.8 26.8 26.7
S
v1+mml
25.5 28.2 27.2 27.0
English-to-German
B
cat
25.7 22.9 24.1 24.2
S
v1
26.3 23.1 24.5 24.6
B
mml
25.1 22.7 23.9 23.9
S
v1+mml
25.4 22.8 23.9 24.0
Table 6: Comparison with Modified Moore-Lewis
selection approach as shown in Table 6. The
MML-based baseline systems (B
mml
) used 20%
selected data for training the MT system and the
NNJM. On Arabic-English, both MML-based se-
lection and our model (S
v1
) gave similar gains on
top of the baseline system (B
cat
). Further results
showed that both approaches are complementary.
We were able to obtain an average gain of +0.3
BLEU points by training an NDAM
v1
model over
the selected data (see S
v1+mml
).
However, on English-German, the MML-based
selection caused a drop in the performance (see
Table 6). Training an adapted NDAM
v1
model
over selected data gave improvements over MML
in two test sets but could not restore the baseline
performance, probably because the useful data has
already been filtered by the selection process.
6 Conclusion
We presented two novel models for domain adap-
tation based on NNJM. Adaptation is performed
by regularizing the loss function towards the in-
domain model and away from the unrelated out-
of-domain data. Our models show better perplex-
ities than the non-adapted baseline NNJM mod-
els. When integrated into a machine translation
system, gains of up to 0.5 and 0.6 BLEU points
were obtained in Arabic-to-English and English-
to-German systems over strong baselines.
1267
References
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, May.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash-
ington, USA, October.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, Edinburgh, United Kingdom.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Alexandra Birch, Matthias Huck, Nadir Durrani, Niko-
lay Bogoychev, and Philipp Koehn. 2014. Ed-
inburgh SLT and MT system description for the
IWSLT 2014 evaluation. In Proceedings of the 11th
International Workshop on Spoken Language Trans-
lation, IWSLT ’14, Lake Tahoe, CA, USA.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. In Marcello Fed-
erico, Mei-Yuh Hwang, Margit R¨odder, and Sebas-
tian St¨uker, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 136–143.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Ale?s
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, Maryland, USA, June.
Marine Carpuat, Hal Daume III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. Sensespotting: Never let your par-
allel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, August.
Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th IWSLT Evaluation Campaign. Proceedings
of the International Workshop on Spoken Language
Translation, Lake Tahoe, US.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359–393.
Boxing Chen, George Foster, and Roland Kuhn.
2013a. Adaptation of reordering models for statisti-
cal machine translation. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Atlanta, Georgia, June.
Boxing Chen, Roland Kuhn, and George Foster.
2013b. Vector space model for adaptation in sta-
tistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Sofia,
Bulgaria, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT ’12, Montr´eal, Canada.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. volume 12, pages 2493–2537. JMLR. org.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
Kevin Duh, Sudoh Katsuhito Neubig, Graham, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), Sofia, Bulgaria,
August.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April.
Nadir Durrani, Hassan Sajjad, Shafiq Joty, Ahmed Ab-
delali, and Stephan Vogel. 2015a. Using Joint Mod-
els for Domain Adaptation in Statistical Machine
Translation. In Proceedings of the Fifteenth Ma-
chine Translation Summit (MT Summit XV), Florida,
USA, To Appear. AMTA.
Nadir Durrani, Helmut Schmid, Alexander Fraser,
Philipp Koehn, and Hinrich Sch¨utze. 2015b. The
Operation Sequence Model – Combining N-Gram-
based and Phrase-based Statistical Machine Trans-
lation. Computational Linguistics, 41(2):157–186.
1268
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL’13.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Jeju Island,
Korea, July.
Andreas Eisele and Yu Chen. 2010. MultiUN: A Mul-
tilingual Corpus from United Nation Documents. In
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, Valleta,
Malta, May.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, Columbus, Ohio, June.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ’07.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, Athens, Greece.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, MA,
October.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293–
302.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 848–856, Honolulu, Hawaii, October.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Baltimore, Maryland, June.
Spence Green and John DeNero. 2012. A class-
based agreement model for generating accurately in-
flected translations. In Proceedings of the Associ-
ation for Computational Linguistics (ACL’12), Jeju
Island, Korea.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Y.W. Teh and
M. Titterington, editors, Proc. Int. Conf. on Artificial
Intelligence and Statistics (AISTATS), volume 9 of
JMLR W&CP, pages 297–304.
Francisco Guzm´an, Hassan Sajjad, Stephan Vogel, and
Ahmed Abdelali. 2013. The AMARA corpus:
Building resources for translating the web’s educa-
tional content. In Proceedings of the 10th Interna-
tional Workshop on Spoken Language Technology
(IWSLT-13), December.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, Gothenburg, Swe-
den, April.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland, United King-
dom, July.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Conference of the European Association for Ma-
chine Translation (EAMT), Budapest, May.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.
Cuong Hoang and Khalil Sima’an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In COLING 2014, 25th International
Conference on Computational Linguistics, Proceed-
ings of the Conference: Technical Papers, August
23-29, 2014, Dublin, Ireland.
Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing, Seattle, Washington,
USA, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Association for Computational
Linguistics (ACL’07), Prague, Czech Republic.
Le Liu, Yu Hong, Hao Liu, Xing Wang, and Jianmin
Yao. 2014. Effective selection of translation model
1269
training data. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), Baltimore, Mary-
land, June.
Saab Mansour and Hermann Ney. 2013. Phrase train-
ing based adaptation for statistical machine trans-
lation. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, June.
Prashant Mathur, Sriram Venkatapathy, and Nicola
Cancedda. 2014. Fast domain adaptation of smt
models without in-domain parallel data. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, Dublin, Ireland, August.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2, EMNLP ’09.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. HLT-NAACL, pages 746–
751.
Tomas Mikolov, 2012. Statistical Language Models
based on Neural Networks. PhD thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL’10), Uppsala, Sweden.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP’09),
Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics
(ACL’02), Philadelphia, PA, USA.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine trans-
lation. In Proceedings of COLING 2012: Posters,
Mumbai, India, December.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model frame-
work for statistical machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, August.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics, Avignon,
France, April.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465, Sofia, Bulgaria, August.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
1270
