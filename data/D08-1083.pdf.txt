Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Learning with Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Determining the polarity of a sentiment-
bearing expression requires more than a sim-
ple bag-of-words approach. In particular,
words or constituents within the expression
can interact with each other to yield a particu-
lar overall polarity. In this paper, we view such
subsentential interactions in light of composi-
tional semantics, and present a novel learning-
based approach that incorporates structural in-
ference motivated by compositional seman-
tics into the learning procedure. Our exper-
iments show that (1) simple heuristics based
on compositional semantics can perform bet-
ter than learning-based methods that do not in-
corporate compositional semantics (accuracy
of 89.7% vs. 89.1%), but (2) a method that
integrates compositional semantics into learn-
ing performs better than all other alterna-
tives (90.7%). We also find that “content-
word negators”, not widely employed in pre-
vious work, play an important role in de-
termining expression-level polarity. Finally,
in contrast to conventional wisdom, we find
that expression-level classification accuracy
uniformly decreases as additional, potentially
disambiguating, context is considered.
1 Introduction
Determining the polarity of sentiment-bearing ex-
pressions at or below the sentence level requires
more than a simple bag-of-words approach. One of
the difficulties is that words or constituents within
the expression can interact with each other to yield
a particular overall polarity. To facilitate our discus-
sion, consider the following examples:
1: [I did [not]¬ have any [doubt]? about it.]+
2: [The report [eliminated]¬ my [doubt]?.]+
3: [They could [not]¬ [eliminate]¬ my [doubt]?.]?
In the first example, “doubt” in isolation carries
a negative sentiment, but the overall polarity of the
sentence is positive because there is a negator “not”,
which flips the polarity. In the second example, both
“eliminated” and “doubt” carry negative sentiment
in isolation, but the overall polarity of the sentence
is positive because “eliminated” acts as a negator for
its argument “doubt”. In the last example, there are
effectively two negators – “not” and “eliminated” –
which reverse the polarity of “doubt” twice, result-
ing in the negative polarity for the overall sentence.
These examples demonstrate that words or con-
stituents interact with each other to yield the
expression-level polarity. And a system that sim-
ply takes the majority vote of the polarity of indi-
vidual words will not work well on the above exam-
ples. Indeed, much of the previous learning-based
research on this topic tries to incorporate salient in-
teractions by encoding them as features. One ap-
proach includes features based on contextual va-
lence shifters1 (Polanyi and Zaenen, 2004), which
are words that affect the polarity or intensity of sen-
timent over neighboring text spans (e.g., Kennedy
and Inkpen (2005), Wilson et al. (2005), Shaikh et
al. (2007)). Another approach encodes frequent sub-
sentential patterns (e.g., McDonald et al. (2007)) as
features; these might indirectly capture some of the
subsentential interactions that affect polarity. How-
1For instance, “never”, “nowhere”, “little”, “most”, “lack”,
“scarcely”, “deeply”.
793
ever, both types of approach are based on learning
models with a flat bag-of-features: some structural
information can be encoded as higher order features,
but the final representation of the input is still a flat
feature vector that is inherently too limited to ade-
quately reflect the complex structural nature of the
underlying subsentential interactions. (Liang et al.,
2008)
Moilanen and Pulman (2007), on the other hand,
handle the structural nature of the interactions more
directly using the ideas from compositional seman-
tics (e.g., Montague (1974), Dowty et al. (1981)). In
short, the Principle of Compositionality states that
the meaning of a compound expression is a func-
tion of the meaning of its parts and of the syntac-
tic rules by which they are combined (e.g., Mon-
tague (1974), Dowty et al. (1981)). And Moilanen
and Pulman (2007) develop a collection of compo-
sition rules to assign a sentiment value to individual
expressions, clauses, or sentences. Their approach
can be viewed as a type of structural inference, but
their hand-written rules have not been empirically
compared to learning-based alternatives, which one
might expect to be more effective in handling some
aspects of the polarity classification task.
In this paper, we begin to close the gap between
learning-based approaches to expression-level po-
larity classification and those founded on composi-
tional semantics: we present a novel learning-based
approach that incorporates structural inference mo-
tivated by compositional semantics into the learning
procedure.
Adopting the view point of compositional seman-
tics, our working assumption is that the polarity of a
sentiment-bearing expression can be determined in a
two-step process: (1) assess the polarities of the con-
stituents of the expression, and then (2) apply a rela-
tively simple set of inference rules to combine them
recursively. Rather than a rigid application of hand-
written compositional inference rules, however, we
hypothesize that an ideal solution to the expression-
level polarity classification task will be a method
that can exploit ideas from compositional seman-
tics while providing the flexibility needed to handle
the complexities of real-world natural language —
exceptions, unknown words, missing semantic fea-
tures, and inaccurate or missing rules. The learning-
based approach proposed in this paper takes a first
step in this direction.
In addition to the novel learning approach, this
paper presents new insights for content-word nega-
tors, which we define as content words that can
negate the polarity of neighboring words or con-
stituents. (e.g., words such as “eliminated” in the
example sentences). Unlike function-word nega-
tors, such as “not” or “never”, content-word nega-
tors have been recognized and utilized less actively
in previous work. (Notable exceptions include e.g.,
Niu et al. (2005), Wilson et al. (2005), and Moilanen
and Pulman (2007).2)
In our experiments, we compare learning- and
non-learning-based approaches to expression-level
polarity classification — with and without com-
positional semantics — and find that (1) simple
heuristics based on compositional semantics outper-
form (89.7% in accuracy) other reasonable heuris-
tics that do not incorporate compositional seman-
tics (87.7%); they can also perform better than sim-
ple learning-based methods that do not incorporate
compositional semantics (89.1%), (2) combining
learning with the heuristic rules based on compo-
sitional semantics further improves the performance
(90.7%), (3) content-word negators play an impor-
tant role in determining the expression-level polar-
ity, and, somewhat surprisingly, we find that (4)
expression-level classification accuracy uniformly
decreases as additional, potentially disambiguating,
context is considered.
In what follows, we first explore heuristic-based
approaches in §2, then we present learning-based ap-
proaches in §3. Next we present experimental results
in §4, followed by related work in §5.
2 Heuristic-Based Methods
This section describes a set of heuristic-based meth-
ods for determining the polarity of a sentiment-
bearing expression. Each assesses the polarity of the
words or constituents using a polarity lexicon that
indicates whether a word has positive or negative
polarity, and finds negators in the given expression
using a negator lexicon. The methods then infer the
expression-level polarity using voting-based heuris-
tics (§ 2.1) or heuristics that incorporate composi-
tional semantics (§2.2). The lexicons are described
2See §5. Related Work for detailed discussion.
794
VOTE NEG(1) NEG(N) NEGEX(1) NEGEX(N) COMPO
type of negators none function-word function-word & content-word
maximum # of negations applied 0 1 n 1 n n
scope of negators N/A over the entire expression compositional
Table 1: Heuristic methods. (n refers to the number of negators found in a given expression.)
Rules Examples
1 Polarity( not [arg1] ) = ¬ Polarity( arg1 ) not [bad]arg1.
2 Polarity( [VP] [NP] ) = Compose( [VP], [NP] ) [destroyed]VP [the terrorism]NP .
3 Polarity( [VP1] to [VP2] ) = Compose( [VP1], [VP2] ) [refused]V P1 to [deceive]V P2 the man.
4 Polarity( [adj] to [VP] ) = Compose( [adj], [VP] ) [unlikely]adj to [destroy]V P the planet.
5 Polarity( [NP1] [IN] [NP2] ) = Compose( [NP1], [NP2] ) [lack]NP1 [of]IN [crime]NP2 in rural areas.
6 Polarity( [NP] [VP] ) = Compose( [VP], [NP] ) [pollution]NP [has decreased]V P .
7 Polarity( [NP] be [adj] ) = Compose( [adj], [NP] ) [harm]NP is [minimal]adj .
Definition of Compose( arg1, arg2 )
Compose( arg1, arg2 ) =
For COMPOMC: if (arg1 is a negator) then ¬ Polarity( arg2 )
(COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )
else the majority polarity of data
Compose( arg1, arg2 ) =
For COMPOPR: if (arg1 is a negator) then ¬ Polarity( arg2 )
(COMPOsition with PRiority) else Polarity( arg1 )
Table 2: Compositional inference rules motivated by compositional semantics.
in §2.3.
2.1 Voting
We first explore five simple heuristics based on vot-
ing. VOTE is defined as the majority polarity vote
by words in a given expression. That is, we count
the number of positive polarity words and negative
polarity words in a given expression, and assign the
majority polarity to the expression. In the case of a
tie, we default to the prevailing polarity of the data.
For NEG(1), we first determine the majority polar-
ity vote as above, and then if the expression contains
any function-word negator, flip the polarity of the
majority vote once. NEG(N) is similar to NEG(1), ex-
cept we flip the polarity of the majority vote n times
after the majority vote, where n is the number of
function-word negators in a given expression.
NEGEX(1) and NEGEX(N) are defined similarly as
NEG(1) and NEG(N) above, except both function-
word negators and content-word negators are con-
sidered as negators when flipping the polarity of the
majority vote. See Table 1 for summary. Note that a
word can be both a negator and have a negative prior
polarity. For the purpose of voting, if a word is de-
fined as a negator per the voting scheme, then that
word does not participate in the majority vote.
For brevity, we refer to NEG(1) and NEG(N) col-
lectively as NEG, and NEGEX(1) and NEGEX(N) col-
lectively as NEGEX.
2.2 Compositional semantics
Whereas the heuristics above use voting-based in-
ference, those below employ a set of hand-written
rules motivated by compositional semantics. Table 2
shows the definition of the rules along with moti-
vating examples. In order to apply a rule, we first
detect a syntactic pattern (e.g., [destroyed]V P [the
terrorism]NP ), then apply the Compose function as
defined in Table 2 (e.g., Compose([destroyed], [the
terrorism]) by rule #2).3
3Our implementation uses part-of-speech tags and function-
words to coarsely determine the patterns. An implementation
795
Compose first checks whether the first argument is
a negator, and if so, flips the polarity of the second
argument. Otherwise, Compose resolves the polar-
ities of its two arguments. Note that if the second
argument is a negator, we do not flip the polarity of
the first argument, because the first argument in gen-
eral is not in the semantic scope of the negation.4 In-
stead, we treat the second argument as a constituent
with negative polarity.
We experiment with two variations of the Com-
pose function depending on how conflicting polari-
ties are resolved: COMPOMC uses a Compose func-
tion that defaults to the Majority Class of the po-
larity of the data,5 while COMPOPR uses a Compose
function that selects the polarity of the argument that
has higher semantic PRiority. For brevity, we refer
to COMPOPR and COMPOMC collectively as COMPO.
2.3 Lexicons
The polarity lexicon is initialized with the lexicon
of Wilson et al. (2005) and then expanded using the
General Inquirer dictionary.6 In particular, a word
contained in at least two of the following categories
is considered as positive: POSITIV, PSTV, POSAFF,
PLEASUR, VIRTUE, INCREAS, and a word contained
in at least one of the following categories is consid-
ered as negative: NEGATIV, NGTV, NEGAFF, PAIN,
VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRAN-
LOSS.
For the (function- and content-word) negator lex-
icon, we collect a handful of seed words as well as
General Inquirer words that appear in either NOTLW
or DECREAS category. Then we expand the list of
content-negators using the synonym information of
WordNet (Miller, 1995) to take a simple vote among
senses.
based on parse trees might further improve the performance.
4Moilanen and Pulman (2007) provide more detailed dis-
cussion on the semantic scope of negations and the semantic
priorities in resolving polarities.
5The majority polarity of the data we use for our experi-
ments is negative.
6Available at http://www.wjh.harvard.edu/?inquirer/.
When consulting the General Inquirer dictionary, senses with
less than 5% frequency and senses specific to an idiom are
dropped.
3 Learning-Based Methods
While we expect that a set of hand-written heuristic
rules motivated by compositional semantics can be
effective for determining the polarity of a sentiment-
bearing expression, we do not expect them to be per-
fect. Interpreting natural language is such a com-
plex task that writing a perfect set of rules would
be extremely challenging. Therefore, a more ideal
solution would be a learning-based method that can
exploit ideas from compositional semantics while
providing the flexibility to the rigid application of
the heuristic rules. To this end, we present a novel
learning-based approach that incorporates inference
rules inspired by compositional semantics into the
learning procedure (§3.2). To assess the effect of
compositional semantics in the learning-basedmeth-
ods, we also experiment with a simple classifica-
tion approach that does not incorporate composi-
tional semantics (§3.1). The details of these two
approaches are elaborated in the following subsec-
tions.
3.1 Simple Classification (SC)
Given an expression x consisting of n words x1,
..., xn, the task is to determine the polarity y ?
{positive, negative} of x. In our simple binary
classification approach, x is represented as a vec-
tor of features f(x), and the prediction y is given by
argmaxyw·f(x, y), wherew is a vector of parameters
learned from training data. In our experiment, we
use an online SVM algorithm called MIRA (Margin
Infused Relaxed Algorithm) (Crammer and Singer,
2003)7 for training.
For each x, we encode the following features:
• Lexical: We add every word xi in x, and also
add the lemma of xi produced by the CASS
partial parser toolkit (Abney, 1996).
• Dictionary: In order to mitigate the problem of
unseen words in the test data, we add features
that describeword categories based on theGen-
eral Inquirer dictionary. We add this feature for
each xi that is not a stop word.
• Vote: We experiment with two variations of
voting-related features: for SC-VOTE, we add
7We use the Java implementation of this algorithm
available at http://www.seas.upenn.edu/?strctlrn/StructLearn
/StructLearn.html.
796
Simple Classification Classification with Compositional Inference
y ? argmaxy score(y) Find K best z and denote them as Z = {z(1), ..., z(K)}
l? loss flat(y?, y) s.t. ? i < j, score(z(i)) > score(z(j))
w? update(w, l, y?, y) zbad ? mink z(k) s.t. loss compo(y?, z(k), x) > 0
(if such zbad not found in Z, skip parameter update for this.)
If loss compo(y?, z?, x) > 0
zgood ? mink z(k) s.t. loss compo(y?, z(k), x) = 0
z? ? zgood
(if such zgood not found in Z, stick to the original z?.)
l ? loss compo(y?, zbad, x)? loss compo(y?, z?, x)
w? update(w, l, z?, zbad)
Definitions of score functions and loss functions
score(y) := w · f(x, y) score(z) :=
?
i score(zi) :=
?
i w · f(x, zi, i)
loss flat(y?, y) := if (y? = y) 0 else 1 loss compo(y?, z, x) := if (y? = C(x, z)) 0 else 1
Figure 1: Training procedures. y? ? {positive, negative} denotes the true label for a given expression x = x1, ..., xn.
z? denotes the pseudo gold standard for hidden variables z.
a feature that indicates the dominant polarity of
words in the given expression, without consid-
ering the effect of negators. For SC-NEGEX,
we count the number of content-word nega-
tors as well as function-word negators to de-
termine whether the final polarity should be
flipped. Then we add a conjunctive feature that
indicates the dominant polarity together with
whether the final polarity should be flipped. For
brevity, we refer to SC-VOTE and SC-NEGEX
collectively as SC.
Notice that in this simple binary classification set-
ting, it is inherently difficult to capture the compo-
sitional structure among words in x, because f(x, y)
is merely a flat bag of features, and the prediction
is governed simply by the dot product of f(x, y) and
the parameter vector w.
3.2 Classification with Compositional
Inference (CCI)
Next, instead of determining y directly from x,
we introduce hidden variables z = (z1, ..., zn)
as intermediate decision variables, where zi ?
{positive, negative, negator, none}, so that zi
represents whether xi is a word with posi-
tive/negative polarity, or a negator, or none of the
above. For simplicity, we let each intermediate de-
cision variable zi (a) be determined independently
from other intermediate decision variables, and (b)
For each token xi,
if xi is a word in the negator lexicon
then z?i ? negator
else if xi is in the polarity lexicon as negative
then z?i ? negative
else if xi is in the polarity lexicon as positive
then z?i ? positive
else
then z?i ? none
Figure 2: Constructing Soft Gold Standard z?
depend only on the input x, so that zi = argmaxziw ·
f(x, zi, i), where f(x, zi, i) is the feature vector en-
coding around the ith word (described on the next
page). Once we determine the intermediate decision
variables, we apply the heuristic rules motivated by
compositional semantics (from Table 2) in order to
obtain the final polarity y of x. That is, y = C(x, z),
where C is the function that applies the composi-
tional inference, either COMPOPR or COMPOMC.
For training, there are two issues we need to
handle: the first issue is dealing with the hidden
variables z. Because the structure of composi-
tional inference C does not allow dynamic program-
ming, it is intractable to perform exact expectation-
maximization style training that requires enumerat-
ing all possible values of the hidden variables z. In-
stead, we propose a simple and tractable training
797
rule based on the creation of a soft gold standard for
z. In particular, we exploit the fact that in our task,
we can automatically construct a reasonably accu-
rate gold standard for z, denoted as z?: as shown in
Figure 2, we simply rely on the negator and polar-
ity lexicons. Because z? is not always correct, we
allow the training procedure to replace z? with po-
tentially better assignments as learning proceeds: in
the event that the soft gold standard z? leads to an in-
correct prediction, we search for an assignment that
leads to a correct prediction to replace z?. The exact
procedure is given in Figure 1, and will be discussed
again shortly.
Figure 1 shows how we modify the parameter up-
date rule of MIRA (Crammer and Singer, 2003) to
reflect the aspect of compositional inference. In the
event that the soft gold standard z? leads to an incor-
rect prediction, we search for zgood, the assignment
with highest score that leads to a correct prediction,
and replace z? with zgood. In the event of no such
zgood being found among the K-best assignments of
z, we stick with z?.
The second issue is finding the assignment of z
with the highest score(z) = ?i w · f(x, zi, i) that
leads to an incorrect prediction y = C(x, z). Be-
cause the structure of compositional inference C
does not allow dynamic programming, finding such
an assignment is again intractable. We resort to enu-
merating only over K-best assignments instead. If
none of the K-best assignments of z leads to an in-
correct prediction y, then we skip the training in-
stance for parameter update.
Features. For each xi in x, we encode the follow-
ing features:
• Lexical: We include the current word xi as well
as the lemma of xi produced by CASS partial
parser toolkit (Abney, 1996). We also add a
boolean feature to indicate whether the current
word is a stop word.
• Dictionary: In order to mitigate the problem
with unseen words in the test data, we add fea-
tures that describe word categories based on the
General Inquirer dictionary. We add this fea-
ture for each xi that is not a stop word. We
also add a number of boolean features that pro-
vide following properties of xi using the polar-
ity lexicon and the negator lexicon:
– whether xi is a function-word negator
– whether xi is a content-word negator
– whether xi is a negator of any kind
– the polarity of xi according to Wilson et
al. (2005)’s polarity lexicon
– the polarity of xi according to the lexicon
derived from the General Inquirer dictio-
nary
– conjunction of the above two features
• Vote: We encode the same vote feature that we
use for SC-NEGEX described in § 3.1.
As in the heuristic-based compositional semantics
approach (§ 2.2), we experiment with two variations
of this learning-based approach: CCI-COMPOPR
and CCI-COMPOMC, whose compositional infer-
ence rules are COMPOPR and COMPOMC respec-
tively. For brevity, we refer to both variations col-
lectively as CCI-COMPO.
4 Experiments
The experiments below evaluate our heuristic- and
learning-based methods for subsentential sentiment
analysis (§ 4.1). In addition, we explore the role
of context by expanding the boundaries of the
sentiment-bearing expressions (§ 4.2).
4.1 Evaluation with given boundaries
For evaluation, we use the Multi-Perspective Ques-
tion Answering (MPQA) corpus (Wiebe et al.,
2005), which consists of 535 newswire documents
manually annotated with phrase-level subjectivity
information. We evaluate on all strong (i.e., inten-
sity of expression is ‘medium’ or higher), sentiment-
bearing (i.e., polarity is ‘positive’ or ‘negative’) ex-
pressions.8 As a result, we can assume the bound-
aries of the expressions are given. Performance is
reported using 10-fold cross-validation on 400 doc-
uments; a separate 135 documents were used as a
development set. Based on pilot experiments on the
development data, we set parameters for MIRA as
follows: slack variable to 0.5, and the number of
incorrect labels (constraints) for each parameter up-
date to 1. The number of iterations (epochs) for
training is set to 1 for simple classification, and to 4
8We discard expressions with confidencemarked as ‘uncer-
tain’.
798
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
(1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
Table 3: Performance (in accuracy) on MPQA dataset.
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
Data (1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
[-0,+0] 86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
[-1,+1] 86.4 81.0 81.2 87.2 87.2 89.3 89.0 88.3 88.4 89.5 89.4
[-5,+5] 85.9 79.0 79.4 85.7 85.6 88.2 88.0 86.4 87.1 88.7 88.7
[-?,+?] 85.3 75.8 76.9 83.9 83.9 87.0 86.9 85.8 85.8 87.3 87.5
Table 4: Performance (in accuracy) on MPQA data set with varying boundaries of expressions.
for classification with compositional inference. We
use K = 20 for classification with compositional
inference.
Results. Performance is reported in Table 3. In-
terestingly, the heuristic-based methods NEG (?
82.2%) that only consider function-word negators
perform even worse than VOTE (86.5%), which does
not consider negators. On the other hand, theNEGEX
methods (87.7%) that do consider content-word
negators as well as function-word negators perform
better than VOTE. This confirms the importance of
content-word negators for determining the polari-
ties of expressions. The heuristic-based methods
motivated by compositional semantics COMPO fur-
ther improve the performance over NEGEX, achiev-
ing up to 89.7% accuracy. In fact, these heuris-
tics perform even better than the SC learning-based
methods (? 89.1%). This shows that heuristics that
take into account the compositional structure of the
expression can perform better than learning-based
methods that do not exploit such structure.
Finally, the learning-based methods that in-
corporate compositional inference CCI-COMPO (?
90.7%) perform better than all of the previous
methods. The difference between CCI-COMPOPR
(90.7%) and SC-NEGEX (89.1%) is statistically sig-
nificant at the .05 level by paired t-test. The dif-
ference between COMPO and any other heuristic that
is not based on computational semantics is also
statistically significant. In addition, the difference
between CCICOMPOPR (learning-based) and COM-
POMC (non-learning-based) is statistically signifi-
cant, as is the difference between NEGEX and VOTE.
4.2 Evaluation with noisy boundaries
One might wonder whether employing additional
context outside the annotated expression boundaries
could further improve the performance. Indeed, con-
ventional wisdom would say that it is necessary to
employ such contextual information (e.g., Wilson et
al. (2005)). In any case, it is important to determine
whether our results will apply to more real-world
settings where human-annotated expression bound-
aries are not available.
To address these questions, we gradually relax
our previous assumption that the exact boundaries of
expressions are given: for each annotation bound-
ary, we expand the boundary by x words for each
direction, up to sentence boundaries, where x ?
{1, 5,?}. We stop expanding the boundary if it
will collide with the boundary of an expression with
a different polarity, so that we can consistently re-
cover the expression-level gold standard for evalua-
tion. This expansion is applied to both the training
and test data, and the performance is reported in Ta-
ble 4. From this experiment, we make the following
observations:
• Expanding the boundaries hurts the perfor-
799
mance for any method. This shows that most of
relevant context for judging the polarity is con-
tained within the expression boundaries, and
motivates the task of finding the boundaries of
opinion expressions.
• The NEGEX methods perform better than VOTE
only when the expression boundaries are rea-
sonably accurate. When the expression bound-
aries are expanded up to sentence boundaries,
they perform worse than VOTE. We conjecture
this is because the scope of negators tends to be
limited to inside of expression boundaries.
• The COMPO methods always perform better
than any other heuristic-based methods. And
their performance does not decrease as steeply
as the NEGEX methods as the expression
boundaries expand. We conjecture this is be-
cause methods based on compositional seman-
tics can handle the scope of negators more ade-
quately.
• Among the learning-based methods, those that
involve compositional inference (CCI-COMPO)
always perform better than those that do not
(SC) for any boundaries. And learning with
compositional inference tend to perform bet-
ter than the rigid application of heuristic rules
(COMPO), although the relative performance
gain decreases once the boundaries are relaxed.
5 Related Work
The task focused on in this paper is similar to that
of Wilson et al. (2005) in that the general goal of the
task is to determine the polarity in context at a sub-
sentence level. However, Wilson et al. (2005) for-
mulated the task differently by limiting their evalua-
tion to individual words that appear in their polarity
lexicon. Also, their approach was based on a flat bag
of features, and only a few examples of what we call
content-word negators were employed.
Our use of compositional semantics for the task
of polarity classification is preceded by Moilanen
and Pulman (2007), but our work differs in that
we integrate the key idea of compositional seman-
tics into learning-based methods, and that we per-
form empirical comparisons among reasonable al-
ternative approaches. For comparison, we evalu-
ated our approaches on the polarity classification
task from SemEval-07 (Strapparava and Mihalcea,
2007). We achieve 88.6% accuracy with COMPOPR,
90.1% with SCNEGEX, and 87.6% with CCICOM-
POMC.9 There are a number of possible reasons for
our lower performance vs. Moilanen and Pulman
(2007) on this data set. First, SemEval-07 does not
include a training data set for this task, so we use
400 documents from the MPQA corpus instead. In
addition, the SemEval-07 data is very different from
the MPQA data in that (1) the polarity annotation
is given only at the sentence level, (2) the sentences
are shorter, with simpler structure, and not as many
negators as the MPQA sentences, and (3) there are
many more instances with positive polarity than in
the MPQA corpus.
Nairn et al. (2006) also employ a “polarity” prop-
agation algorithm in their approach to the semantic
interpretation of implicatives. However, their notion
of polarity is quite different from that assumed here
and in the literature on sentiment analysis. In partic-
ular, it refers to the degree of “commitment” of the
author to the truth or falsity of a complement clause
for a textual entailment task.
McDonald et al. (2007) use a structured model
to determine the sentence-level polarity and the
document-level polarity simultaneously. But deci-
sions at each sentence level does not consider struc-
tural inference within the sentence.
Among the studies that examined content-word
negators, Niu et al. (2005) manually collected a
small set of such words (referred as “words that
change phases”), but their lexicon was designed
mainly for the medical domain and the type of nega-
tors was rather limited. Wilson et al. (2005) also
manually collected a handful of content-word nega-
tors (referred as “general polarity shifters”), but not
extensively. Moilanen and Pulman (2007) collected
a more extensive set of negators semi-automatically
using WordNet 2.1, but the empirical effect of such
words was not explicitly investigated.
9For lack of space, we only report our performance on in-
stances with strong intensities as defined in Moilanen and Pul-
man (2007), which amounts to only 208 test instances. The
cross-validation set of MPQA contains 4.9k instances.
800
6 Conclusion
In this paper, we consider the task of determining
the polarity of a sentiment-bearing expression, con-
sidering the effect of interactions among words or
constituents in light of compositional semantics. We
presented a novel learning-based approach that in-
corporates structural inference motivated by compo-
sitional semantics into the learning procedure. Our
approach can be considered as a small step toward
bridging the gap between computational semantics
and machine learning methods. Our experimen-
tal results suggest that this direction of research is
promising. Future research includes an approach
that learns the compositional inference rules from
data.
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0624277 and IIS-0535099
and by Department of Homeland Security Grant
N0014-07-1-0152. We also thank Eric Breck, Lil-
lian Lee, Mats Rooth, the members of the Cornell
NLP reading seminar, and the EMNLP reviewers for
insightful comments on the submitted version of the
paper.
References
Steven Abney. 1996. Partial parsing via finite-state
cascades. Journal of Natural Language Engineering,
2(4):337344.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR
3:951.
David R. Dowty, Robert E. Wall and Stanley Peters.
1981. Introduction to Montague Semantics.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. In Proceedings of 5th Conference on Lan-
guage Resources and Evaluation (LREC),.
Percy Liang, Hal Daume´ III and Dan Klein. 2008. Struc-
ture Compilation: Trading Structure for Features. In
International Conference on Machine Learning.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (KDD-2004).
Alistair Kennedy and Diana Inkpen. 2005. Senti-
ment Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Nego-
tiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. In Proceedings of
Association for Computational Linguistics (ACL) .
George A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11):3941
Richard Montague. 1974. Formal Philosophy; Selected
papers of Richard Montague. Yale University Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen
2006. Computing relative polarity for textual infer-
ence. In Inference in Computational Semantics (ICoS-
5).
Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst.
2005. Analysis of polarity information inmedical text.
In Proceedings of the American Medical Informatics
Association 2005 Annual Symposium (AMIA).
Livia Polanyi and Annie Zaenen. 2004. Contextual lex-
ical valence shifters. In Exploring Attitude and Affect
in Text: Theories and Applications: Papers from the
2004 Spring Symposium, AAAI.
Mostafa Shaikh, Helmut Prendinger and Mitsuru
Ishizuka. 2007. Assessing sentiment of text by se-
mantic dependency and contextual valence analysis.
In Proc 2nd Int’l Conf on Affective Computing and In-
telligent Interaction (ACII’07).
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of Se-
mEval.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. In Language Resources and Evalua-
tion (formerly Computers and the Humanities), 39(2-
3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
801
