Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284–1295,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics
Morpho-syntactic Lexical Generalization
for CCG Semantic Parsing
Adrienne Wang
Computer Science & Engineering
University of Washington
Seattle, WA
axwang@cs.washington.edu
Tom Kwiatkowski
Allen Institute for AI
Seattle, WA
tomk@allenai.org
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
In this paper, we demonstrate that
significant performance gains can be
achieved in CCG semantic parsing
by introducing a linguistically moti-
vated grammar induction scheme. We
present a new morpho-syntactic fac-
tored lexicon that models systematic
variations in morphology, syntax, and
semantics across word classes. The
grammar uses domain-independent
facts about the English language to
restrict the number of incorrect parses
that must be considered, thereby
enabling effective learning from less
data. Experiments in benchmark
domains match previous models with
one quarter of the data and provide
new state-of-the-art results with all
available data, including up to 45%
relative test-error reduction.
1 Introduction
Semantic parsers map sentences to formal
representations of their meaning (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011). One common approach is
to induce a probabilistic CCG grammar, which
defines the meanings of individual words and
phrases and how to best combine them to an-
alyze complete sentences. There has been re-
cent work developing learning algorithms for
CCG semantic parsers (Kwiatkowski et al.,
2010; Artzi and Zettlemoyer, 2011) and using
them for applications ranging from question
answering (Cai and Yates, 2013b; Kwiatkowski
et al., 2013) to robot control (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013).
One key learning challenge for this style
of learning is to induce the CCG lexicon,
which lists possible meanings for each phrase
and defines a set of possible parses for
each sentence. Previous approaches have
either hand-engineered a small set of lexi-
cal templates (Zettlemoyer and Collins, 2005,
2007) or automatically learned such tem-
plates (Kwiatkowski et al., 2010, 2011). These
methods are designed to learn grammars that
overgenerate; they produce spurious parses
that can complicate parameter estimation.
In this paper, we demonstrate that signif-
icant gains can instead be achieved by using
a more constrained, linguistically motivated
grammar induction scheme. The grammar
is restricted by introducing detailed syntac-
tic modeling of a wider range of constructions
than considered in previous work, for example
introducing explicit treatments of relational
nouns, Davidsonian events, and verb tense.
We also introduce a new lexical generalization
model that abstracts over systematic morpho-
logical, syntactic, and semantic alternations
within word classes. This includes, for exam-
ple, the facts that verbs in relative clauses and
nominal constructions (e.g., “flights departing
NYC” and “departing flights”) should be in-
finitival while verbs in phrases (e.g., “What
flights depart at noon?”) should have tense.
These grammar modeling techniques use uni-
versal, domain-independent facts about the
English language to restrict word usage to ap-
propriate syntactic contexts, and as such are
potentially applicable to any semantic parsing
application.
More specifically, we introduce a new
morpho-syntactic, factored CCG lexicon that
imposes our grammar restrictions during
learning. Each lexical entry has (1) a word
stem, automatically constructed from Wik-
tionary, with part-of-speech and morpholog-
ical attributes, (2) a lexeme that is learned
1284
and pairs the stem with semantic content that
is invariant to syntactic usage, and (3) a lexi-
cal template that specifies the remaining syn-
tactic and semantic content. The full set of
templates is defined in terms of a small set of
base templates and template transformations
that model morphological variants such as pas-
sivization and nominalization of verbs. This
approach allows us to efficiently encode a gen-
eral grammar for semantic parsing while also
eliminating large classes of incorrect analyses
considered by previous work.
We perform experiments in two benchmark
semantic parsing datasets: GeoQuery (Zelle
and Mooney, 1996) and ATIS (Dahl et al.,
1994). In both cases, our approach
achieves state-of-the-art performance, includ-
ing a nearly 45% relative error reduction on
the ATIS test set. We also show that the gains
increase with less data, including matching
previous model’s performance with less than
25% of the training data. Such gains are par-
ticularly practical for semantic parsers; they
can greatly reduce the amount of data that is
needed for each new application domain.
2 Related Work
Grammar induction methods for CCG seman-
tic parsers have either used hand-engineered
lexical templates, e.g. (Zettlemoyer and
Collins, 2005, 2007; Artzi and Zettlemoyer,
2011), or algorithms to learn such templates
directly from data, e.g. (Kwiatkowski et al.,
2010, 2011). Here, we extend the first ap-
proach, and show that better lexical general-
ization provides significant performance gains.
Although CCG is a common choice
for semantic parsers, many other for-
malisms have been studied, including DCS
trees (Liang et al., 2011), integer linear pro-
grams (Clarke et al., 2010), and synchronous
grammars (Wong and Mooney, 2007; Jones
et al., 2012; Andreas et al., 2013). All of these
approaches build complete meaning represen-
tations for individual sentences, but the data
we use has also been studied in related work on
cross-sentence reasoning (Miller et al., 1996;
Zettlemoyer and Collins, 2009) and model-
ing semantic interpretation as a tagging prob-
lem (Tur et al., 2013; Heck et al., 2013). Al-
though we focus on full analysis with CCG,
the general idea of using linguistic constraints
to improve learning is broadly applicable.
Semantic parsers are also commonly learned
from a variety of different types of supervision,
including logical forms (Kate and Mooney,
2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al., 2012), question-
answer pairs (Clarke et al., 2010; Liang et al.,
2011), conversational logs (Artzi and Zettle-
moyer, 2011), distant supervision (Krishna-
murthy and Mitchell, 2012; Cai and Yates,
2013b), sentences paired with system behav-
ior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013b),
and even from database constraints with no
explicit semantic supervision (Poon, 2013).
We learn from logical forms, but CCG learn-
ing algorithms have been developed for each
case above, making our techniques applicable.
There has been significant related work that
influenced the design of our morpho-syntactic
grammars. This includes linguistics stud-
ies of relational nouns (Partee and Borschev,
1998; de Bruin and Scha, 1988), Davidsonian
events (Davidson, 1967), parsing as abduc-
tion (Hobbs et al., 1988), and other more gen-
eral theories for lexicons (Pustejovsky, 1991)
and CCG (Steedman, 2011). It also includes
work on using morphology in CCG syntac-
tic parsing (Honnibal et al., 2010) and more
broad-coverage semantics in CCG (Bos, 2008;
Lewis and Steedman, 2013). However, our
work is unique in studying the use of related
ideas for semantic parsing.
Finally, there has also been recent progress
on semantic parsing against large, open do-
main databases such as Freebase (Cai and
Yates, 2013a; Kwiatkowski et al., 2013; Berant
et al., 2013). Unfortuantely, existing Freebase
datasets are not a good fit to test our approach
because the sentences they include have rela-
tively simple structure and can be interepreted
accurately using only factoid lookups with no
database joins (Yao and Van Durme, 2014).
Our work focuses on learning more syntacti-
cally rich models that support compositional
reasoning.
3 Background
Lambda Calculus We represent the mean-
ings of sentences, words and phrases with
1285
list one way flights from various cities
S/N N/N N PP/NP NP/N N
?f.f ?f?x.oneway(x) ? f(x) ?x.flight(x) ?x?y.from(y, x) ?fAx.f(x) ?x.city(x)
>
NP
Ax.city(x)
>
PP
?x.from(x,Ay.city(y))
>T
N\N
?x.from(x,Ay.city(y))
<
N
?x.flight(x) ? from(x,Ay.city(y))
>
N
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
>
S
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
Figure 1: An example CCG parse.
lambda calculus logical expressions. We use a
version of the typed lambda calculus (Carpen-
ter, 1997), in which the basic types include en-
tities, events, truth values and numbers. Func-
tion types are assigned to lambda expressions.
The expression ?x.flight(x) with type ?e, t?
takes an entity and returns a truth value, and
represents a set of flights.
Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a formalism
that tightly couples syntax and semantics,
and can be used to model a wide range of
linguistic phenomena. A traditional CCG
grammar includes a lexicon ? with lexical
entries like the following:
flights ` N :?x.flight(x)
from ` PP/NP :?y.?x.from(x, y)
cities ` N :?x.city(x)
where a lexical item w `X : h has words w,
syntactic category X, and logical expression h.
CCG uses a small set of combinatory rules
to jointly build syntactic parses and semantic
representations. Two common combinatory
rules are forward (>) and backward (<)
application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
CCG also includes combinatory rules of for-
ward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ? X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ? X\Z : ?x.f(g(x)) (< B)
These rules apply to build syntactic and se-
mantic derivations concurrently.
In this paper, we also implement type
raising rules for compact representation of
PP (prepositional phrase) and AP (adverbial
phrase).
PP : g ? N\N : ?f?x.f(x) ? g(x) (T)
AP : g ? S\S : ?f?e.f(e) ? g(e) (T)
AP : g ? S/S : ?f?e.f(e) ? g(e) (T)
Figure 1 shows an example CCG
parse (Steedman, 1996, 2000) where the
lexical entries are listed across the top and
the output lambda-calculus meaning repre-
sentation is at the bottom. This meaning is a
function (denoted by ?x...) that defines a set
of flights with certain properties and includes
a generalized Skolem constant (Steedman,
2011) (Ay...) that performs existential quan-
tification. Following recent work (Artzi and
Zettlemoyer, 2013b), we use meaning repre-
sentations that model a variety of linguistic
constructions, for example including Skolem
constants for plurals and Davidson quantifiers
for events, which we will introduce briefly
throughout this paper as they appear.
Weighted CCGs A weighted CCG gram-
mar is defined as G = (?,?), where ? is a
CCG lexicon and ? ? R
d
is a d-dimensional
parameter vector, which will be used to rank
the parses allowed under ?.
For a sentence x, G produces a set of candi-
1286
date parse trees Y = Y(x;G). Given a feature
vector ? ? R
d
, each parse tree y for sentence
x is scored by S(y; ?) = ? ·?(x, y). The output
logical form zˆ is then defined to be at the root
of the highest-scoring parse yˆ:
yˆ = arg max
y?Y(x;G)
S(y; ?) (1)
We use existing CKY-style parsing algo-
rithms for this computation, implemented
with UW SPF (Artzi and Zettlemoyer, 2013a).
Section 7 describes the set of features we use
in the learned models.
Learning with GENLEX We will also
make use of an existing learning algo-
rithm (Zettlemoyer and Collins, 2007) (ZC07).
We first briefly review the ZC07 algorithm,
and describe our modifications in Section 7.
Given a set of training examples D =
{(x
i
, z
i
) : i = 1...n}, x
i
being the ith sentence
and z
i
being its annotated logical form, the al-
gorithm learns a set of parameters ? for the
grammar, while also inducing the lexicon ?.
The ZC07 learning algorithm uses a function
GENLEX(x, z) to define a set of lexical entries
that could be used to parse the sentence x to
construct the logical form z. For each training
example (x, z), GENLEX(x, z) maps all sub-
strings x to a set of potential lexical entries,
generated by exhaustively pairing the logical
constants in z using a set of hand-engineered
templates. The example is then parsed with
this much bigger lexicon and lexical entries
from the highest scoring parses are added to ?.
The parameters ? used to score parses are up-
dated using a perceptron learning algorithm.
4 Morpho-Syntactic Lexicon
This section defines our morpho-syntactic lex-
ical formalism. Table 1 shows examples of how
lexemes, templates, and morphological trans-
formations are used to build lexical entries for
example verbs. In this section, we formally de-
fine each of these components and show how
they are used to specify the space of possible
lexical entries that can be built for each input
word. In the following two sections, we will
provide more discussion of the complete sets
of templates (Section 5) and transformations
(Section 6).
Verb, Noun, Preposition, Pronoun, Adjective,
Adverb, Conjunction, Numeral, Symbol,
Proper Noun, Interjection, Expression
Table 2: Part-of-Speech types
POS Attribute Values
Noun Number singular, plural
Verb Person first, second, third
Verb Voice active, passive
Verb Tense present, past
Verb Aspect simple, progressive, perfect
Verb Participle present participle,
past participle
Adj, Degree of comparative, superlative
Adv, comparison
Det
Table 3: Morphological attributes and values.
We build on the factored CCG lexicon in-
troduced by Kwiatkowski et al. (2011) but (a)
further generalize lexemes to represent word
stems, (b) constrain the use of templates with
widely available syntactic information, and (c)
efficiently model common morphological vari-
ations between related words.
The first step, given an input word w, is
to do morphological and part-of-speech analy-
sis with the morpho-syntactic function F .
F maps a word to a set of possible morpho-
syntactic representations, each containing a
triple (s, p,m) of word stem s, part-of-speech
p and morphological category m. For exam-
ple, F maps the word flies to two possible
representations:
F (flies) = {(fly,Noun, (plural)),
(fly,Verb, (third, singular, simple, present))}
for the plural noun and present-tense verb
senses of the word. F is defined based on the
stems, part-of-speech types, and morpholog-
ical attributes marked for each definition in
Wiktionary.
1
The full sets of possible part-of-
speech and morphological types required for
our domains are shown in Table 2 and Table 3.
Each morpho-syntactic analysis a ? F (w)
is then paired with lexemes based on stem
match. A lexeme (s,~c) pairs a word stem
s with a list of logical constants ~c = [c
1
. . . c
k
].
Table 1 shows the words ‘depart’, ‘departing’,
‘departure’, which are all assigned the lex-
eme (depart, [depart]). In general, there can
1
www.wiktionary.com
1287
Word Lexeme : Base Template Trans Lexical entry
depart
(depart, [depart]) :
I depart `S\NP :?x?e.depart(e, x)
departing I departing `S\NP :?x?e.depart(e, x)
departing
? `S\NP :?x?e.v
1
(e, x)
f
pres
departing `PP :?x?e.depart(e, x)
departure f
nom
departure `N :?x?e.depart(e, x)
use
(use, [airline]) :
I use `S\NP/NP :?x?y?e.airline(e, y, x)
using I using `S\NP/NP :?x?y?e.airline(e, y, x)
using
? `S\NP/NP :?x?y?e.v
1
(e, y, x)
f
pres
using `PP/NP :?x?e.airline(e, y, x)
use f
nom
use `N/NP :?x?y?e.airline(e, y, x)
Trans Template Transformation
f
pres
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
f
nom
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
Table 1: Lexical entries constructed by combining a lexeme, base template, and transformation
for the intransitive verb ‘depart’ and the transitive verb ‘use’.
be many different lexemes for each stem, that
vary in the selection of which logical constants
are included.
Given analysis (s, p,m) and lexeme (s,~c), we
can use a lexical template to construct a
lexical entry. Each template has the form:
?(?,~v).[? `X : h
~v
]
where ? and ~v are variables that abstract over
the words and logical constants that will be
used to define a lexical entry with syntax X
and templated logical form h
~v
.
To instantiate a template, ? is filled with the
original word w and the constants in ~c replace
the variables ~v. For example, the template
?(?,~v).[? ` S\NP : ?x?e.v
1
(e, x)] could be
used with the word ‘departing’ and the lexeme
(depart, [depart]) to produce the lexical entry
departing ` S\NP : ?x?e.depart(e, x). When
clear from context, we will omit the function
signature ?
p
(?,~v). for all templates, as seen in
Table 1.
In general, there can be many applicable
templates, which we organize as follows. Each
final template is defined by applying a mor-
phological transformation to one of a small
set of possible base templates. The pairing
is found based on the morphological analysis
(s, p,m), where each base template is associ-
ated with part-of-speech p and each transfor-
mation is indexed by the morphology m. A
transformation f
m
is a function:
f
m
(?
p
(?,~v).[? `X : h
~v
]) = ?
p
(?,~v).[? `X
?
: h
?
~v
]
that takes the base template as input and pro-
duces a new template to model the inflected
form specified by m.
For example, both base templates in Ta-
ble 1 are for verbs. The template ? `
S\NP : ?x?e.v
1
(e, x) can be translated into
three other templates based on the transfor-
mations I, f
pres
, and f
nom
, depending on the
analysis of the original words. These transfor-
mations generalize across word type; they can
be used for the transitive verb ‘use’ as well as
the intransitive ‘depart.’ Each resulting tem-
plate, potentially including the original input
if the identity transformation I is available,
can then be used to make an output lexical
entry, as we described above.
5 Lexical Templates
The templates in our lexicon, as introduced
in Section 4, model the syntactic and seman-
tic aspects of lexical entries that are shared
within each word class. Previous approaches
have also used hand-engineered lexical tem-
plates, as described in Section 2, but we dif-
fer by (1) using more templates allowing for
more fine grained analysis and (2) using word
class information to restrict template use, for
example ensuring that words which cannot be
verbs are never paired with templates designed
for verbs. This section describes the templates
used during learning, first presenting those de-
signed to model grammatical sentences and
then a small second set designed for more el-
liptical spoken utterances.
Base Forms Table 4 lists the primary tem-
plate set, where each row shows an example
with a sentence illustrating its use. Templates
are also grouped by the word classes, including
adjectives, adverbs, prepositions, and several
types of nouns and verbs. While there is not
enough space to discuss each row, it is worth
1288
word class example usage base template
Noun phrase Boston ? `NP : v
Noun (regular) What flight is provided by delta? ? `N : ?x.v(x)
Noun (relation) I need fares of flights ? `N/PP : ?x?y.v(x, y)
delta schedule ? `N\(N/N) : ?f?x.v(Ay.f(?z.true, y), x)
Noun (function) size of California ? `NP/NP : ?x.v(x)
V
intrans
What flights depart from New York? ? `S\NP : ?x?e.v(e, x)
V
trans
Which airlines serve Seattle (active verb) ? `S\NP/NP :?x?y?e.v(e, y, x)
What airlines have flights (passive verb) ? `S\NP/NP :?x?y?e.v(e, x, y)
V
ditrans
They give him a book ? `S\NP/NP/NP : ?x?y?z?e.v(e, z, y, x)
V
imperson
It costs $500 to fly to Boston ? `S\NP/NP/NP :?x?y?z?e.v(e, y, x)
V
aux
The flights have arrived at Boston ? `S\NP/(S\NP ) :?f.f
? `S/NP/(S/NP ) :?f.f
Does delta provide flights from Seattle? ? `S/S :?f.f
V
copula
The flights are from Boston ? `S\NP/PP :?f?x.f(x)
What flight is cheap? ? `S\NP/(N/N) :?f?x.f(?y.true, x)
Alaska is the state with the most rivers ? `S\NP/NP :?x?y.equals(y, x)
Adjective I need a one way flight ? `N/N :?f?x.f(x) ? v(x)
Boston flights round trip ? `PP :?x.v(x)
How long is mississippi? ? `DEG :?x.v(x)
Preposition List flights from Boston ? `PP/NP :?x?y.v(y, x)
List flights that go to Dallas ? `AP/NP :?x?e.v(e, x)
List flights between Dallas and Boston ? `PP/NP/NP :?x?y?z.v
1
(z, x) ? v
2
(z, y)
What flights leave between 8am and 9am? ? `AP/NP/NP :?x?y?e.v
1
(e, x) ? v
2
(e, y)
Adverb Which flight departs daily? ? `AP :?e.v(e)
How early does the flight arrive? ? `DEG :?x.v(x)
Determiner Which airline has a flight from Boston? ? `NP/N :?fAx.f(x)
Table 4: Base templates that define different syntactic roles.
type example usage base template
t
elliptical
flights Newark to Cleveland ? `PP :?x.P (x, v)
flights arriving 2pm ? `AP :?e.P (e, v)
american airline from Denver ? `N :?x.P (x, v)
t
metonymy
List airlines from Seattle ? `N/PP :?f?x.v(x) ? P (Ay.f(y), x))
Shat airlines depart from Seattle? ? `N/(S\NP ) :?f?x.v(x) ? P (Ay.f(y), x)
fares from miami to New York ? `N/PP :?f?x.v(Ay.f(y), x)
Table 5: Base templates for ungrammatical linguistic phenomena
considering nouns as an illustrative example.
We model nouns as denoting a set of entities
that satisfy a given property. Regular nouns
are represented using unary predicates. Rela-
tional nouns syntactically function as regular
nouns but semantically describe sets of enti-
ties that have some relationship with a comple-
ment (Partee and Borschev, 1998). For exam-
ple, the relational noun fare describes a binary
relationship between flights and their price in-
formation, as we see in this parse:
fares of flights
N/PP PP/NP N
?x?y.fare(x, y) ?x.x ?x.flight(x)
>T
NP
Ax.flight(x)
>
PP
Ax.flight(x)
>
N
?x.fare(Ay.flight(y), x)
This analysis differs from previous ap-
proaches (Zettlemoyer and Collins, 2007),
where relational nouns were treated as regu-
lar nouns and prepositions introduced the bi-
nary relationship. The relational noun model
reduces lexical ambiguity for the prepositions,
which are otherwise highly polysemous.
Adjectives are nominal modifiers that take
a noun or a noun phrase as an argument and
add properties through conjunction. Preposi-
tions take nominal objects and function as ad-
jectival modifiers for nouns or adverbial modi-
fiers for verbs. Verbs can be subcategorized
by their grammatical structures into transi-
tive (V
trans
), intransitive (V
intrans
), imper-
sonal (V
imperson
), auxiliary (V
aux
) and copula
(V
copula
). Adverbs are verb modifiers defin-
ing aspects like time, rate and duration. The
adoption of event semantics allows adverbial
modifiers to be represented by predicates and
1289
linked by the shared events. Determiners pre-
cede nouns or noun phrases and distinguish
a reference of the noun. Following the gen-
eralized Skolem terms, we model determiners,
including indefinite and definite articles, as a
??e, t?, e? function that selects a unique indi-
vidual from a ?e, t?-typed function defining a
singleton set.
Missing Words The templates presented so
far model grammatically correct input. How-
ever, in dialogue domains such as ATIS, speak-
ers often omit words. For example, speak-
ers can drop the preposition “from” in “flights
from Newark to Cleveland” to create the ellip-
tical utterance “flights Newark to Cleveland”.
We address this issue with the templates
t
elliptical
illustrated in Table 5. Each of these
adds a binary relation P to a lexeme with a
single entity typed constant. For our example,
the word “Newark” could be assigned the lexi-
cal item Newark `PP : ?x.from(x, newark)
by selecting the first template and P = from.
Another common problem is the use of
metonymy. In the utterance “What airlines
depart from New York?”, the word “airlines”
is used to reference flight services operated by
a specific airline. This is problematic because
the word “depart” needs to modify an event of
type flight. We solve this with the t
metonymy
templates in Table 5. These introduce a binary
predicate P that would, in the case of our ex-
ample, map airlines on to the flights that they
operate.
The templates in Table 5 handle the ma-
jor cases of missing words seen in our data
and are more efficient than the approach taken
by (Zettlemoyer and Collins, 2007) who intro-
duced complex type shifting rules and relaxed
the grammar to allow every word order.
6 Morphological Transformations
Finally, the morpho-syntactic lexicon intro-
duces morphological transformations, which
are functions from base lexical templates to
lexical templates that model the syntactic and
semantic variation as the word is inflected.
These transformations allow us to compactly
model, for example, the facts that argument
order is reversed when moving from active to
passive forms of the same verb, and that the
subject can be omitted. To the best of our
knowledge, we are the first to study such trans-
formations for semantic parsing.
Table 6 shows the transformations. Each
row groups a set of transformations by linguis-
tic category, including singular vs. plural num-
ber, active vs. passive voice, and so on, and
also includes example sentences where the out-
put templates could be used. Again, for space,
we do not detail the motivation for every class,
but it is worth looking at some of the alterna-
tions for verbs and nouns as our prototypical
example.
Some verbs can act as noun modifiers. For
example, the present participle “using” mod-
ifies “flights” in “flights using twa”. To
model this variation, we use the transforma-
tion f
present part
, a mapping that changes the
root of the verb signature S\NP to PP :
f
present part
: ? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
)
? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
where T = [,NP,NP/NP ] instantiates this
rule for verbs that take different sets of argu-
ments, effectively allowing any verb that is in
its finite or -ing form to behave syntactically
like a prepositional phrase.
Intransitive present participles can also act
as prenominal adjectival modifiers as in “the
departing flight”. We add a second mapping
that maps the intransitive category S\NP to
the noun modifier N/N .
f
present part
: ? `S\NP :?x?e.v(e, x)
? ? `N/N : ?f?x?e.f(x) ? v(e, x)
Finally, verbal nouns have meanings derived
from actions typically described by verbs but
syntactically function as nouns. For example,
landing in the phrase “landing from jfk” is the
gerundive use of the verb land. We add the
following mapping to f
present part
and f
nominal
:
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ?
? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
with T from above. This allows for reuse of the
same meaning across quite different syntac-
tic constructs, including for example “flights
that depart from Boston” and “departure from
Boston.”
1290
Template transformations f
m
Example usage
Plural Number (f
plural
)
I flight ? early flights
? `N : ?x.v(x) ? ? `NP :Ax.v(x) city ? flights to cities
Singular Number (f
singular
)
I flight ? flight
Possessive (f
possess
)
? `NP : v ? ? `N/N :?f?x.f(x) ? P (x, v) delta ? delta’s flights
? `N : ?x.v(x) ? ? `N/N :?f?x.f(x) ? P (x,Ay.v(y)) airline ? airline’s flights
Passive Voice (f
passive
)
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
..x
n
) ? ? `Y/PP : ?x
1
..x
n
?e.v(e, x
n
..x
1
) serves ?is served by
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
, .., x
n
) ? ? `Y : ?x
1
..x
n?1
?e.v(e, x
n?1
..x
1
) name ?city named Austin
Present Participle (f
present
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) use ?flights using twa
? `S\NP :?x?e.v(e, x) ? ? `N/N : ?f?x?e.f(x) ? v(e, x) arrive ?arriving flights
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) land ? landings at jfk
Past Participle (f
past
)
? `S\NP/NP :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/PP : ?x
1
..x
n
?e.v(e, x
1
..x
n
) use ? plane used by
Nominalization (f
nominal
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) depart ? departure
Comparative (f
comp
)
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) < v(x) short ? shorter
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) > v(x) long ? longer
Superlative (f
super
)
? `DEG :?x.v(x) ? ? `NP/N :?f.argmin(?x.f(x), ?x.v(x)) short ? shortest
? `DEG :?x.v(x) ? ? `NP/N :?f.argmax(?x.f(x), ?x.v(x)) long ? longest
Table 6: Morphological transformations with examples. T = [,NP,NP/NP ] and Y =
[S\NP,S\NP/NP ] allow a single transformation to generalize across word type.
Nouns can be inflected by number to de-
note singular and plural forms or by adding
an apostrophe to mark a possessive case. The
transformation function f
singular
is an identity
transformation. Plurals may have different in-
terpretations: one is the generic ?e, t? set rep-
resentation, which requires no transformation
on the base, or plurals can occur without overt
determiners (bare plurals), but semantically
imply quantification. We create a plural to
singular type shifting rule which implements
the ??e, t?, e? skolem function to select a unique
individual from the set. The possessive trans-
formation f
possess
transfers the base template
to a noun modifier, and adds a binary predi-
cate P that encodes the relation.
There are also a number of instances of the
identity transformation function I, which does
not change the base template. Because the se-
mantics we are constructing was designed to
answer questions against a static database, it
does not need to represent certain phenomena
to return the correct answer. This includes
more advanced variants of person, tense, as-
pect, and potentially many others. Ideally,
these morphological attributes should add se-
mantic modifiers to the base meaning, for ex-
ample, tense can constrain the time at which
an event occurs. However, none of our do-
mains support such reasoning, so we assign the
identity transformation, and leave the explo-
ration of these issues to future work.
7 Learning
One advantage of our morpho-syntactic, fac-
tored lexicon is that it can be easily learned
with small modifications to existing algo-
rithms (Zettlemoyer and Collins, 2007). We
only need to modify the GENLEX proce-
dure that defines the space of possible lexi-
cal entries. For each training example (x, z),
GENLEX(x, z, F ) first maps each substring in
the sentence x into the morphological repre-
sentation (s, p, c) using F introduced in Sec-
tion 4. A candidate lexeme set L
?
is then gen-
erated by exhaustively pairing the word stems
with all subsets of the logical constants from
z. Lexical templates are applied to the lexemes
in L
?
to generate candidate lexical entries for
x. Finally, the lexemes that participate in the
top scoring correct parse of x are added to the
permanent lexicon.
Initialization Following standard practice,
we compile an initial lexicon ?
0
, which con-
sists of a list of domain independent lexical
1291
items for function words, such as interrogative
words and conjunctions. These lexical items
are mostly semantically vacuous and serve par-
ticular syntactic functions that are not gener-
alizable to other word classes. We also initial-
ize the lexemes with a list of NP entities com-
plied from the database, e.g., (Boston, [bos]).
Features We use two types of features in
the model for discriminating parses. Four lex-
ical features are fired on each lexical item:
?
(s,~c)
for the lexeme, ?
t
p
for the base tem-
plate, ?
t
m
for the morphologically modified
template, and ?
l
for the complete lexical
item. We also compute the standard logical
expression features (Zettlemoyer and Collins,
2007) on the root semantics to track the pair-
wise predicate-argument relations and the co-
occuring predicate-predicate relations in con-
junctions and disjunctions.
8 Experimental Setup
Data and Metrics We evaluate perfor-
mance on two benchmark semantic pars-
ing datasets, Geo880 and ATIS. We use
the standard data splits, including 600/280
train/test for Geo880 and 4460/480/450
train/develop/test for ATIS. To support the
new representations in Section 5, we sys-
tematically convert annotations with existen-
tial quantifiers, temporal events and relational
nouns to new logical forms with equivalent
meanings. All systems are evaluated with ex-
act match accuracy, the percentage of fully
correct logical forms.
Initialization We assign positive initial
weights to the indicator features for entries in
the initial lexicon, as defined in Section 7, to
encourage their use. The elliptical template
and metonymy template features are initial-
ized with negative weights to initially discour-
age word skipping.
Comparison Systems We compare perfor-
mance with all recent CCG grammar induc-
tion algorithms that work with our datasets.
This includes methods that used a limited
set of hand-engineered templates for inducing
the lexicon, ZC05 (Zettlemoyer and Collins,
2005) and ZC07 (Zettlemoyer and Collins,
2007), and those that learned grammar struc-
ture by automatically splitting the labeled log-
System Test
ZC05 79.3
ZC07 86.1
UBL 87.9
FUBL 88.6
DCS 87.9
FULL 90.4
DCS
+
91.1
Table 7: Exact-match Geo880 test accuracy.
System Dev Test
ZC07 74.4 84.6
UBL 65.6 71.4
FUBL 81.9 82.8
GUSP - 83.5
TEMP-ONLY 85.5 87.2
FULL 87.5 91.3
Table 8: Exact-match accuracy on the ATIS
development and test sets.
ical forms, UBL (Kwiatkowski et al., 2010)
and FUBL (Kwiatkowski et al., 2011). We
also compare the state-of-the-art for Geo880
(DCS (Liang et al., 2011) and DCS+ which in-
cludes an engineered seed lexicon) and ATIS
(which is ZC07). Finally, we include results
for GUSP (Poon, 2013), a recent unsupervised
approach for ATIS.
System Variants We report results for a
complete approach (Full), and variants which
use different aspects of the morpho-syntactic
lexicon. The TEMP-ONLY variant learned
with the templates from Section 5 but, like
ZC07, does not use any word class information
to restrict their use. The TEMP-POS removes
morphology from the lexemes, but includes the
word class information from Wiktionary. Fi-
nally, we also include DCS
+
, which initialize a
set of words with POS tag JJ, NN, and NNS.
9 Results
Full Models Tables 7 and 8 report the
main learning results. Our approach achieves
state-of-the-art accuracies on both datasets,
demonstrating that our new grammar induc-
tion scheme provides a type of linguistically
motivated regularization; restricting the algo-
rithm to consider a much smaller hypothesis
space allows to learn better models.
1292
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 100  500  1000  2000  4460
Re
cal
l
Training samples
TEMP_ONLY
TEMP_POS
FULL
FUBL
Figure 2: ATIS Learning Curve
On Geo880 the full method edges out the
best systems by 2% absolute on the test set,
as compared to other systems with no domain-
specific lexical initialization. Although DCS
requires less supervision, it also uses external
signals including a POS tagger.
We see similarly strong results for ATIS,
outperforming FUBL on the ATIS develop-
ment set by 6.8%, and improving the accu-
racy on the test set by 7.9% over the previous
best system ZC07. Unlike FUBL, which excels
at the development set but trails ZC07’s tem-
plated grammar by almost 2 points on the test
set, our approach demonstrates consistent im-
provements on both. Additionally, although
the unsupervised model (GUSP) rivals previ-
ous approaches, we are able to show that more
careful use of supervision open a much wider
performance gap.
Learning Curve with Ablations Figure 2
presents a learning curve for the ATIS domain,
demonstrating that the learning improvements
become even more dramatic for smaller train-
ing set sizes. Our model outperforms FUBL by
wide margins, matching its final accuracy with
only 22% of the total training examples. Our
full model also consistently beats the variants
with fewer word class restrictions, although
by smaller margins. Again, these results fur-
ther highlight the benefit of importing external
syntactic resources and enforcing linguistically
motivated constraints during learning.
Learned Lexicon The learned lexicon is
also more compact. Table 9 summarizes
statistics on unique lexical entries required
to parse the ATIS development set. The
System Lexical Entries Lexemes
FUBL 1019 721
Our Approach 818 495
Table 9: Lexicon size comparison on the ATIS
dev set (460 unique tokens).
morpho-syntactic model uses 80.3% of the lex-
ical entries and 63.7% of the lexemes that
FUBL needs, while increase performance by
nearly 7 points. Upon inspection, our model
achieves better lexical decomposition by learn-
ing shorter lexical units, for example, the
adoption of Davidsonian events allows us to
learn unambiguous adverbial modifiers, and
the formal modeling of nominalized nouns and
relational nouns treats prepositions as syntac-
tic modifiers, instead of being encoded in the
semantics. Such restrictions generalize to a
much wider variety of syntactic contexts.
10 Summary and Future Work
We demonstrated that significant performance
gains can be achieved in CCG semantic pars-
ing by introducing a more constrained, linguis-
tically motivated grammar induction scheme.
We introduced a morpho-syntactic factored
lexicon that uses domain-independent facts
about the English language to restrict the
number of incorrect parses that must be con-
sidered and demonstrated empirically that it
enables effective learning of complete parsers,
achieving state-of-the-art performance.
Because our methods are domain indepen-
dent they should also benefit other semantic
parsing applications and other learning algo-
rithms that use different types of supervision,
as we hope to verify in future work. We would
also like to study how to generalize these gains
to languages other than English, by inducing
more of the syntactic structure.
Acknowledgements
The research was supported by the NSF (IIS-
1115966, IIS-1252835) and the Intel Center
for Pervasive Computing at the Univeristy
of Washington. The authors thank Robert
Gens, Xiao Ling, Xu Miao, Mark Yatskar and
the UW NLP group for helpful discussions,
and the anonymous reviewers for helpful com-
ments.
1293
References
Andreas, J., Vlachos, A., and Clark, S. (2013).
Semantic parsing as machine translation.
Artzi, Y. and Zettlemoyer, L. (2011). Boot-
strapping semantic parsers from conversa-
tions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing.
Artzi, Y. and Zettlemoyer, L. (2013a). UW
SPF: The University of Washington Seman-
tic Parsing Framework.
Artzi, Y. and Zettlemoyer, L. (2013b). Weakly
supervised learning of semantic parsers for
mapping instructions to actions. Transac-
tions of the Association for Computational
Linguistics, 1(1):49–62.
Berant, J., Chou, A., Frostig, R., and Liang, P.
(2013). Semantic parsing on freebase from
question-answer pairs. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Bos, J. (2008). Wide-coverage semantic anal-
ysis with boxer. In Proceedings of the Con-
ference on Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale
semantic parsing via schema matching and
lexicon extension. In Proceedings of the An-
nual Meeting of the Association for Compu-
tational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic pars-
ing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Con-
ference on Lexical and Computational Se-
mantics.
Carpenter, B. (1997). Type-Logical Semantics.
The MITPress.
Chen, D. and Mooney, R. (2011). Learning
to interpret natural language navigation in-
structions from observations. In Proceedings
of the National Conference on Artificial In-
telligence.
Clarke, J., Goldwasser, D., Chang, M., and
Roth, D. (2010). Driving semantic parsing
from the world’s response. In Proceedings
of the Conference on Computational Natural
Language Learning.
Dahl, D. A., Bates, M., Brown, M., Fisher,
W., Hunicke-Smith, K., Pallett, D., Pao, C.,
Rudnicky, A., and Shriberg, E. (1994). Ex-
panding the scope of the atis task: The atis-
3 corpus. In Proceedings of the workshop on
Human Language Technology.
Davidson, D. (1967). The logical form of
action sentences. Essays on actions and
events, pages 105–148.
de Bruin, J. and Scha, R. (1988). The interpre-
tation of relational nouns. In Proceedings of
the Conference of the Association of Com-
putational Linguistics, pages 25–32. ACL.
Goldwasser, D. and Roth, D. (2011). Learning
from natural instructions. In Proceedings of
the International Joint Conference on Arti-
ficial Intelligence.
Heck, L., Hakkani-Tu¨r, D., and Tur, G.
(2013). Leveraging knowledge graphs for
web-scale unsupervised semantic parsing. In
Proc. of the INTERSPEECH.
Hobbs, J. R., Stickel, M., Martin, P., and Ed-
wards, D. (1988). Interpretation as abduc-
tion. In Proceedings of the Association for
Computational Linguistics.
Honnibal, M., Kummerfeld, J. K., and Cur-
ran, J. R. (2010). Morphological analysis
can improve a ccg parser for english. In Pro-
ceedings of the International Conference on
Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S.
(2012). Semantic parsing with bayesian tree
transducers. In Proceedings of Association
of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In
Proceedings of the Conference of the Asso-
ciation for Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013).
Jointly learning to parse and perceive: Con-
necting natural language to the physical
world. Transactions of the Association for
Computational Linguistics, 1(2).
Krishnamurthy, J. and Mitchell, T. (2012).
Weakly supervised training of semantic
parsers. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Computational Natu-
ral Language Learning.
Kwiatkowski, T., Choi, E., Artzi, Y., and
1294
Zettlemoyer, L. (2013). Scaling semantic
parsers with on-the-fly ontology matching.
Kwiatkowski, T., Goldwater, S., Zettlemoyer,
L., and Steedman, M. (2012). A probabilis-
tic model of syntactic and semantic acquisi-
tion from child-directed utterances and their
meanings. Proceedings of the Conference of
the European Chapter of the Association of
Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2010). Induc-
ing probabilistic CCG grammars from log-
ical form with higher-order unification. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2011). Lexical
generalization in CCG grammar induction
for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Lewis, M. and Steedman, M. (2013). Com-
bined distributional and logical semantics.
Transactions of the Association for Compu-
tational Linguistics, 1:179–192.
Liang, P., Jordan, M., and Klein, D. (2011).
Learning dependency-based compositional
semantics. In Proceedings of the Conference
of the Association for Computational Lin-
guistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L.,
Bo, L., and Fox, D. (2012). A joint model
of language and perception for grounded at-
tribute learning. In Proceedings of the Inter-
national Conference on Machine Learning.
Miller, S., Stallard, D., Bobrow, R., and
Schwartz, R. (1996). A fully statistical ap-
proach to natural language interfaces. In
Proceedings Association for Computational
Linguistics.
Muresan, S. (2011). Learning for deep lan-
guage understanding. In Proceedings of the
International Joint Conference on Artificial
Intelligence.
Partee, B. H. and Borschev, V. (1998). Inte-
grating lexical and formal sematics: Gen-
itives, relational nouns, and type-shifting.
In Proceedings of the Second Tbilisi Sympo-
sium on Language, Logic, and Computation.
Poon, H. (2013). Grounded unsupervised se-
mantic parsing. In Association for Compu-
tational Linguistics (ACL).
Pustejovsky, J. (1991). The generative lexicon.
volume 17.
Steedman, M. (1996). Surface Structure and
Interpretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process.
The MIT Press.
Steedman, M. (2011). Taking Scope. The MIT
Press.
Tur, G., Deoras, A., and Hakkani-Tur, D.
(2013). Semantic parsing using word con-
fusion networks with conditional random
fields. In Proc. of the INTERSPEECH.
Wong, Y. and Mooney, R. (2007). Learning
synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the
Conference of the Association for Computa-
tional Linguistics.
Yao, X. and Van Durme, B. (2014). Informa-
tion extraction over structured data: Ques-
tion answering with freebase. In Association
for Computational Linguistics (ACL).
Zelle, J. and Mooney, R. (1996). Learning to
parse database queries using inductive logic
programming. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic cate-
gorial grammars. In Proceedings of the Con-
ference on Uncertainty in Artificial Intelli-
gence.
Zettlemoyer, L. and Collins, M. (2007). On-
line learning of relaxed CCG grammars for
parsing to logical form. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and Com-
putational Natural Language Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of
the Joint Conference of the Association
for Computational Linguistics and Interna-
tional Joint Conference on Natural Lan-
guage Processing.
1295
