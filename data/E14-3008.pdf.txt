Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 65–75,
Gothenburg, Sweden, April 26-30 2014.
c©2014 Association for Computational Linguistics
Multi-class Animacy Classification with Semantic Features
Johannes Bjerva
Center for Language and Cognition Groningen
University of Groningen
The Netherlands
j.bjerva@rug.nl
Abstract
Animacy is the semantic property of nouns
denoting whether an entity can act, or is
perceived as acting, of its own will. This
property is marked grammatically in var-
ious languages, albeit rarely in English.
It has recently been highlighted as a rele-
vant property for NLP applications such as
parsing and anaphora resolution. In order
for animacy to be used in conjunction with
other semantic features for such applica-
tions, appropriate data is necessary. How-
ever, the few corpora which do contain
animacy annotation, rarely contain much
other semantic information. The addition
of such an annotation layer to a corpus al-
ready containing deep semantic annotation
should therefore be of particular interest.
The work presented in this paper contains
three main contributions. Firstly, we im-
prove upon the state of the art in multi-
class animacy classification. Secondly, we
use this classifier to contribute to the anno-
tation of an openly available corpus con-
taining deep semantic annotation. Finally,
we provide source code, as well as trained
models and scripts needed to reproduce
the results presented in this paper, or aid
in annotation of other texts.
1
1 Introduction
Animacy is the semantic property of nouns de-
noting whether, or to what extent, the referent
of that noun is alive, human-like or even cogni-
tively sophisticated. Several ways of characteris-
ing the animacy of such referents have been pro-
posed in the literature, the most basic distinction
being between animate and inanimate entities. In
1
https://github.com/bjerva/animacy
such a binary scheme, examples of animate nouns
might include author and dog, while examples
of inanimate nouns might include table and rock.
More elaborate schemes tend to represent a hier-
archy or continuum typically ranging from HU-
MAN ? NON-HUMAN ? INANIMATE (cf. Com-
rie (1989)), with other categories in between.
In various languages, animacy affects linguis-
tic phenomena such as case marking and argument
realization. Furthermore, hierarchical restrictions
are often imposed by animacy, e.g. with subjects
tending to be higher in an animacy hierarchy than
objects (Dahl and Fraurud, 1996). Even though
animacy is rarely overtly marked in English, it still
influences the choice of certain grammatical struc-
tures, such as the choice of relative pronouns (e.g.
who vs. which).
The aims of this work are as follows: (i) to im-
prove upon the state of the art in multi-class an-
imacy classification by comparing and evaluating
different classifiers and features for this task, (ii) to
investigate whether a corpus of spoken language
containing animacy annotation can be used as a
basis to annotate animacy in a corpus of written
language, (iii) to use the resulting classifier as part
of the toolchain used to annotate a corpus contain-
ing deep semantic annotation.
The remainder of this paper is organized as fol-
lows: In Section 2 we go through the relevance of
animacy for Natural Language Processing (NLP)
and describe some corpora which contain animacy
annotation. Previous attempts and approaches to
animacy classification are portrayed in Section 3.
Section 4 contains an overview of the data used
in this study, as well as details regarding the man-
ual annotation of animacy carried out as part of
this work. The methods employed and the results
obtained are presented in Sections 5 and 6. The
discussion is given in Section 7. Finally, Section 8
contains conclusions and some suggestions for fu-
ture work in multi-class animacy classification.
65
2 Background
2.1 Relevance of animacy for NLP
Although seemingly overlooked in the past, ani-
macy has recently been shown to be an impor-
tant feature for NLP. Øvrelid & Nivre (2007)
found that the accuracy of a dependency parser for
Swedish could be improved by incorporating a bi-
nary animacy distinction. Other work has high-
lighted animacy as relevant for anaphora and co-
reference resolution (Or?asan and Evans, 2007; Lee
et al., 2013) and verb argument disambiguation
(Dell’Orletta et al., 2005).
Furthermore, in English, the choices for dative
alternation (Bresnan et al., 2007), between geni-
tive constructions (Stefanowitsch, 2003), and be-
tween active and passive voice (Rosenbach, 2008)
are also affected by the animacy of their con-
stituent nouns. With this in mind, Zaenen et al.
(2004) suggest that animacy, for languages such
as English, is not a matter of grammatical and un-
grammatical sentences, but rather of sentences be-
ing more and less felicitous. This highlights anno-
tation of animacy as potentially particularly useful
for applications such as Natural Language Gener-
ation.
In spite of this, animacy appears to be rarely an-
notated in corpora, and thus also rather rarely used
in tools and algorithms for NLP (although some
recent efforts do exist, cf. Moore et al. (2013)).
Furthermore, the few corpora that do include ani-
macy in their annotation do not contain much other
semantic annotation, making them less interesting
for computational semanticists.
2.2 Annotation of animacy
Resources annotated with animacy are few and
far between. One such resource is the MC160
dataset which has recently been labelled for bi-
nary animacy (Moore et al., 2013). The distinc-
tion between animate and inanimate was based on
whether or not an entity could “move under its
own will”. Although interesting, the size of this
data set (approximately 8,000 annotated nouns)
limits its usefulness, particularly with the methods
used in this paper.
Talbanken05 is a corpus of Swedish spoken lan-
guage which includes a type of animacy annota-
tion (Nivre et al., 2006). However, this annotation
is better described as a distinction between human
and non-human, than between animate and inani-
mate (Øvrelid, 2009). Although the work in this
paper focusses on English, a potential application
of this corpus is discussed at the end of this paper
(see Section 8).
The NXT Switchboard corpus represents a
larger and more interesting resource for our pur-
poses (Calhoun et al., 2010). This spoken lan-
guage corpus contains high quality manual anno-
tation of animacy for nearly 200,000 noun phrases
(Zaenen et al., 2004). Furthermore, the annota-
tion is fairly fine-grained, as a total of ten animacy
categories are used (see Table 1), with a few addi-
tional tags for mixed animacy and cases in which
annotators were uncertain. This scheme can be
arranged hierarchically, so that the classes Con-
crete, Non-concrete, Place and Time are grouped
as inanimate, while the remaining classes are
grouped as animate. The availability of this data
allows us to easily exploit the annotation for a su-
pervised learning approach (see Section 5).
3 Related work
In this section we will give an overview of previ-
ous work in animacy classification, some of which
has inspired the approach presented in this paper.
3.1 Exploiting corpus frequencies
A binary animacy classifier which uses syntactic
and morphological features has been previously
developed for Norwegian and Swedish (Øvrelid,
2005; Øvrelid, 2006; Øvrelid, 2009). The fea-
tures used are based on frequency counts from the
dependency-parsed Talbanken05 corpus. These
frequencies are counted per noun lemma, mean-
ing that this classifier is not context sensitive. In
other words, cases of e.g. polysemy where head is
inanimate in the sense of human head, but animate
in the sense of head of an organization, are likely
to be problematic. Intuitively, by taking context or
semantically motivated features into account, such
cases ought to be resolved quite trivially.
This classifier performs well, as it reaches an
accuracy for 96.8% for nouns, as compared to a
baseline of 90.5% when always picking the most
common class (Øvrelid, 2009). Furthermore, it is
shown that including the binary distinction from
this classifier as a feature in dependency parsing
can significantly improve its labelled attachment
score (Øvrelid and Nivre, 2007).
A more language-specific system for animacy
classification has also been developed for Japanese
(Baker and Brew, 2010). In this work, vari-
66
Table 1: Overview of the animacy tag set from Zaenen et al. (2004) with examples from the GMB.
Tag Description Examples
HUM Human Mr. Calderon said Mexico has become a worldwide leader ...
ORG Organization Mr. Calderon said Mexico has become a worldwide leader ...
ANI Animal There are only about 1,600 pandas still living in the wild in China.
LOC Place There are only about 1,600 pandas still living in the wild in China.
NCN Non-concrete There are only about 1,600 pandas still living in the wild in China.
CNC Concrete The wind blew so much dust around the field today.
TIM Time The wind blew so much dust around the field today.
MAC Machine The astronauts attached the robot, called Dextre, to the ...
VEH Vehicle Troops fired on the two civilians riding a motorcycle ...
ous language-specific heuristics are used to im-
prove coverage of, e.g., loanwords from English.
The features used are mainly frequency counts of
nouns as subjects or objects of certain verbs. This
is then fed to a Bayesian classifier, which yields
quite good results on both Japanese and English.
Taking these works into account, it is clear that
the use of morphosyntactic features can provide
relevant information for the task of animacy clas-
sification. However, both of these approaches use
binary classification schemes. It is therefore not
clear whether acceptably good results could be ob-
tained for more elaborate schemes.
3.2 Exploiting lexico-semantic resources
Or?asan & Evans (2007) present an animacy classi-
fier which is based on knowledge obtained from
WordNet (Miller, 1995). In one approach, they
base this on the so-called unique beginners at the
top of the WordNet hierarchy. The fact that some
of these are closely related to animacy is then used
to infer the animacy of their hyponyms. The inclu-
sion of the classifications obtained by this system
for the task of anaphora resolution is shown to im-
prove its results.
An animacy classifier based on exploiting syn-
onymy relations in addition to hyponymy and hy-
peronymy has been described for Basque (de Il-
laraza et al., 2002). In this work, a small set con-
sisting of 100 nouns was manually annotated. Us-
ing an electronic dictionary from which semantic
relations could be inferred, they then further auto-
matically annotated all common nouns in a 1 mil-
lion word corpus.
An approach to animacy classification for
Dutch is presented in Bloem & Bouma (to ap-
pear). This approach exploits a lexical semantic
resource, from which word-senses were obtained
and merged per lemma. This is done, as they pos-
tulate that ambiguity in animacy per lemma ought
to be relatively rare. Each lemma was then as-
signed a simplified animacy class depending on
its animacy category – either human, non-human
or inanimate. Similarly to Baker & Brew (2010),
they also use dependency features obtained from
an automatically parsed corpus for Dutch. This
type-based approach obtains accuracies in the low
90% range, compared to a most frequent class
baseline of about 81%.
Based on the three aforementioned works, it is
clear that the use of semantic relations obtained
from lexico-semantic resources such as WordNet
are particularly informative for the classification
of animacy.
3.3 Multi-class animacy classification
An animacy classifier which distinguishes be-
tween ten different classes of animacy has been
developed by Bowman & Chopra (2012). They
use a simple logistic regression classifier and
quite straight-forward bag-of-words and PoS fea-
tures, as well as subject, object and PP dependen-
cies. These are obtained from the aforementioned
Switchboard corpus, for which they obtain quite
good results.
A quite involved system for animacy classifi-
cation based on using an ensemble of voters is
presented by Moore et al. (2013). This system
draws its strengths from the fact that it, rather
than defining and using a large number of features
and training one complex classifier, uses more in-
terpretable voting models which differ depending
on the class in question. They distinguish be-
tween three categories, namely person, animal and
67
inanimate. The voters comprise a variety of sys-
tems, based on the n-gram list method of Ji and
Lin (2009), a WordNet-based approach similar to
Or?asan & Evans (2007), and several others. Their
results yield animacy detection rates in the mid-
90% range, and can therefore be seen as an im-
provement upon the state of the art. However,
comparison between animacy classification sys-
tems is not all that straight-forward, considering
the disparity between the data sets and classifica-
tion schemes used.
These two works show that multi-class animacy
classification can be successfully done both with
syntactic and semantic features.
4 Data
Two annotated corpora are used in this work. A
further data source is concreteness ratings ob-
tained through manual annotation (Brysbaert et
al., 2013), and is used as a feature in the classifier.
These ratings were obtained for approximately
40,000 English words and two-word expressions,
through the use of internet crowd-sourcing. The
rating was given on a five-point scale, ranging
from abstract, or language based, to concrete, or
experience based (Brysbaert et al., 2013).
4.1 The NXT Switchboard Corpus
Firstly, the classifier is trained and evaluated on the
Switchboard corpus, as this allows for direct com-
parison of results to at least one previous approach
(i.e. Bowman & Chopra (2012)).
4.1.1 Pre-processing of spoken data
The fact that the Switchboard corpus consists of
transcribed spoken data presents challenges for
some of the tools used in the feature extraction
process. The primary concern identified, apart
from the differing form of spoken language as
compared to written language, is the presence of
disfluency markers in the transcribed texts. As a
preprocessing step, all disfluencies were removed
using a simple automated script. Essentially, this
consisted of removing all words tagged as interjec-
tions (labelled with the tag UH), as this is the tag
assigned to disfluencies in the Switchboard cor-
pus. Although interjections generally can be in-
formative, the occurrences of interjections within
NPs was restricted to usage as disfluencies.
4.2 The Groningen Meaning Bank
There are several corpora of reasonable size which
include semantic annotation on some level, such as
PropBank (Palmer et al., 2005), FrameNet (Baker
et al., 1998), and the Penn Discourse TreeBank
(Prasad et al., 2005). The combination of sev-
eral levels of semantic annotation into one formal-
ism are not common, however. Although some ef-
forts exist, they tend to lack a level of formally
grounded “deep” semantic representation which
combines these layers.
The Groningen Meaning Bank (GMB) contains
a substantial collection of English texts with such
deep semantic annotation (Basile et al., 2012a).
One of its goals is to combine semantic phenom-
ena into a single formalism, as opposed to deal-
ing with single phenomena in isolation. This pro-
vides a better handle on explaining dependencies
between various ambiguous linguistic phenomena.
Manually annotating a comprehensive corpus
with gold-standard semantic representations is ob-
viously a hard and time-consuming task. There-
fore, a sophisticated bootstrapping approach is
used. Existing NLP tools are used to get a rea-
sonable approximation of the target annotations
to start with. Pieces of information coming from
both experts (linguists) and crowd sourcing meth-
ods are then added in to improve the annotation.
The addition of animacy annotation is done in the
same manner. First, the animacy classifier will
be incorporated into this toolchain. We then cor-
rect the tags for a subset of the corpus, which
is also used to evaluate the classifier. Note that
the classifier used in the toolchain uses a different
model from the conditions where we evaluate on
the Switchboard corpus. For the GMB, we include
training data obtained through the crowd-sourcing
game Wordrobe, which uses a subset of the data
from the GMB (Venhuizen et al., 2013).
4.2.1 Annotation
So as to allow for evaluation of the classifier on
a widely used semantically annotated corpus, one
part (p00) of the GMB was semi-manually anno-
tated for animacy, although this might lead to a
bias with potentially overly good results for our
classifier, if annotators are affected by its out-
put. We use the tagset presented by Zaenen et
al. (2004), which is given in Table 1. This tagset
was chosen for the addition of animacy annota-
tion to the GMB. Including this level of annotation
68
Figure 1: A tagged document in the GMB.
in a resource which already contains other seman-
tic annotation should prove particularly useful, as
this allows animacy to be used in conjunction with
other semantically based features in NLP tools and
algorithms. This annotation was done using the
GMB’s interface for expert annotation (Basile et
al., 2012b). A total of 102 documents, contain-
ing approximately 15,000 tokens, were annotated
by an expert annotator, who corrected the tags as-
signed by the classifier. We assign animacy tags
to all nouns and pronouns. Similarly to our tag-
ging convention for named entities, we assign the
same tag to the whole NP, so that wagon driver
is tagged with HUM, although wagon in isolation
would be tagged with CNC. This has the added
advantage that this is the manner in which NPs are
annotated in the Switchboard corpus, making eval-
uation and comparison with Bowman & Chopra
(2012) somewhat more straight-forward. An ex-
ample of a tagged document can be seen in Fig-
ure 1. Table 2 shows the amount of annotated
nouns per class. In order to verify the integrity of
this annotation, two other experts annotated a ran-
dom selection of ten documents. Inter-annotator
agreement was calculated using Fleiss’ kappa on
this selection, yielding a score of ? = .596.
Table 2: Annotation statistics for p00 of the GMB
HUM NCN CNC TIM ORG LOC ANI VEH MAC
1436 2077 79 500 887 512 67 28 0
5 Method
5.1 Classifiers
We experiment using four different classifiers (see
Table 3). All classifiers used are obtained from
the implementations provided by SciKit-learn (Pe-
dregosa et al., 2011). For each type of classifier,
we train one classifier for each class in a one-
versus-all fashion. For source code, trained mod-
els and scripts to run the experiments in this paper,
please see https://github.com/bjerva/
animacy.
The classifiers are trained on a combination of
the Switchboard corpus and data gathered from
Wordrobe, depending on the experimental condi-
tion. In addition to the features explained below,
the classifier exploits named entity tags, in that
these override the proposed animacy tag where ap-
plicable. That is to say, if a named entity has al-
ready been identified and tagged as, e.g., a person,
this is reflected in the animacy layer with the HUM
tag.
Considering that the balance between samples
per class is quite skewed, an attempt was made at
placing lower weights on the samples from the ma-
jority classes. Although this did lead to a marginal
increase in accuracy for the minority classes, over-
all accuracy dropped to such an extent that this
weighting was not used for the results presented
in this work.
5.2 Features
In this section, an overview of the features used by
the classifiers is given.
5.2.1 Bag-of-words feature
The simplest feature used consists of looking at
each lemma in the NP to be classified, and their
corresponding PoS tags. We also experimented
with using whole sentences as context for classi-
fication, but as this worsened results on our devel-
opment data, it was not used for the evaluations
later in the paper.
5.2.2 Concreteness ratings
Considering that two of the categories in our
tag set discriminate between concrete and non-
concrete entities, we include concreteness ratings
69
Table 3: Overview of the classifiers used in the experiments.
Classifier Reference Parameter settings
Logistic Regression (MaxEnt) (Berger et al., 1996) `2 regularization
Support Vector Machine (SVM) (Joachims, 1998) linear kernel
Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge loss
Bernoulli Naive Bayes (B-NB) (McCallum et al., 1998) –
as a feature in the classifier (Brysbaert et al.,
2013). In its original form, these ratings are quite
fine-grained as they are provided with the average
concreteness score given by annotators on a scale.
We experimented with using different granulari-
ties of these scores as a feature. A simple binary
distinction where anything with a score of c > 2.5
being represented as concrete, and c ? 2.5 be-
ing represented as non-concrete yielded the best
results, and is used in the evaluations in this paper.
5.2.3 WordNet distances
We also include a feature based on WordNet dis-
tances. In this work, we use the path distance sim-
ilarity measure provided in NLTK (Bird, 2006).
In essence, this measure provides a score based
on the shortest path that connects the senses in a
hypernym/hyponym taxonomy. First, we calcu-
late the distance to each hypernym of every given
word. These distances are then summed together
for each animacy class. Taking the most fre-
quent hypernym for each animacy class gives us
the following hypernyms: person.n.01, abstrac-
tion.n.06, city.n.01, time period.n.01, car.n.01, or-
ganization.n.01, artifact.n.01, animal.n.01, ma-
chine.n.01, buddy.n.01. The classifier then uses
whichever of these words is closest as its Word-
Net feature.
5.2.4 Thematic roles
The use of thematic roles for animacy annotation
constitutes a novel contribution from this work.
Intuitively this makes sense, as e.g. agents tend to
be animate. Although the GMB contains an anno-
tation layer with thematic roles, the Switchboard
corpus does not. In order to use this feature, we
therefore preprocessed the latter using Boxer (Bos,
2008). We use the protoroles obtained from Boxer,
namely agent, theme and patient. Although auto-
matic annotation does not provide 100% accuracy,
especially on such a particular data set, this feature
proved somewhat useful (see Section 6.1.2).
6 Results
6.1 Evaluation on the Switchboard corpus
We employ 10-fold cross validation for the evalua-
tions on the Switchboard corpus. All NPs were au-
tomatically extracted from the pre-processed cor-
pus, put into random order and divided into ten
equally-sized folds. In each of the ten cross valida-
tion iterations, one of these folds was left out and
used for evaluation. For the sake of conciseness,
averaged results over all classes are given in the
comparisons of Section 6.1.1 and Section 6.1.2,
whereas detailed results are only given for the best
performing classifier. Note that the training data
from Wordrobe is not used for the evaluations on
the Switchboard corpus, as this would prohibit fair
evaluation with previous work.
6.1.1 Classifier evaluation
We first ran experiments to evaluate which of the
classifiers performed the best on this task. Figure 2
shows the average accuracy for each classifier, us-
ing 10-fold cross validation on the Switchboard
corpus. Table 4 contains the per-class results from
the cross validation performed with the best per-
forming classifier, namely the Logistic Regression
classifier. The remaining evaluations in this pa-
per are all carried out with this classifier. Aver-
age accuracy over the 10 folds was 85.8%. This
is well above the baseline of always picking the
most common class (HUM), which results in an ac-
curacy of 45.3%. More interestingly, this is some-
what higher than the best results for this dataset
reported in the literature (84.9% without cross val-
idation (Bowman and Chopra, 2012)).
6.1.2 Feature evaluation
Using the best performing classifier, we ran exper-
iments to evaluate how different features affect the
results. These experiments were also performed
using 10-fold cross validation on the Switchboard
corpus. Table 5 shows scores from using only one
70
MaxEnt
SGD SVM
B-NB
40
50
60
70
80
90
85.8
82
84.2
80.8
Figure 2: Accuracy of the classifiers, using 10-
fold cross validation on the Switchboard corpus.
The dashed line represents the most frequent class
baseline.
feature in addition to the lemma and PoS of the
head of the NP to be classified. Although none of
the features in isolation add much to the perfor-
mance of the classifier, some marginal gains can
be observed.
Table 5: Comparison of the effect of including sin-
gle features, from cross validation on the Switch-
board corpus. All conditions consist of the fea-
ture named in the condition column in addition to
Lemma+PoS.
Condition Precision Recall F-score
Lemma+PoS 0.846 0.850 0.848
Bag of Words 0.851 0.856 0.853
Concreteness 0.847 0.851 0.849
WordNet 0.849 0.855 0.852
Thematic Roles 0.847 0.851 0.849
All features 0.851 0.857 0.854
6.1.3 Performance on unknown words
For a task such as animacy classification, where
many words can be reliably classified based solely
on their lemma and PoS tag, it is particularly in-
teresting to investigate performance on unknown
words. As in all other conditions, this was evalu-
ated using 10-fold cross validation on the Switch-
board corpus. It should come as no surprise that
the results are substantially below those for known
words, for every single class. The average accu-
racy for this condition was 59.2%, which can be
compared to the most frequent class (NCN) base-
line at 43.0%.
6.2 Evaluation on the GMB
Since one of the purposes of the development of
this classifier was to include it in the tools used in
the tagging of the GMB, we also present the first
results in the literature for the animacy annotation
of this corpus. Due to the limited size of the por-
tion of this corpus for which animacy tags have
been manually corrected, no cross-validation was
performed. However, due to the high differences
in the training data from the Switchboard corpus,
and the evaluation data in the GMB, the results
could be seen as a lower bound for this classifier
on this data set. Table 4 contains the results from
this evaluation. The accuracy on this dataset was
79.4%, which can be compared to a most frequent
class baseline of 37.2%.
6.3 Excluding pronouns
The discrepancy between the results obtained
from the Switchboard corpus and the GMB does
call for some investigation. Considering that the
Switchboard corpus consists of spoken language,
it contains a relatively large amount of personal
pronouns compared to, e.g., news text. Taking into
account that these pronouns are rarely ambiguous
as far as animacy is concerned, it seems feasible
that this may be why the results for the Switch-
board corpus are better than those of the GMB.
To evaluate this, a separate experiment was run
in which all pronouns were excluded. As a large
amount of pronouns are tagged as HUM, the F-
scores for this class dropped by 8% and 5% for
the Switchboard corpus and GMB respectively.
For the GMB, results for other classes remained
fairly stable, most likely due to there not being
many pronouns present which affect the remain-
ing classes. For the Switchboard corpus, however,
an increase in F-score was observed for several
classes. This might be explained by that the ex-
clusion of pronouns lowered the classifier’s pre-
existing bias for the HUM class, as the number
of annotated examples was lowered from approxi-
mately 85,000 to 15,000.
Animacy classification of pronouns can be con-
sidered trivial, as there is little or no ambiguity of
that the referent of e.g. he is HUM. Even so, pro-
nouns were included in the main results provided
71
Table 4: Results from 10-fold cross validation on the Switchboard corpus and evaluation on the GMB.
Switchboard GMB
Class Count Precision Recall F-score Count Precision Recall F-score
HUM 82596 0.91 0.97 0.94 1436 0.82 0.79 0.80
NCN 62740 0.82 0.94 0.88 2077 0.76 0.88 0.82
CNC 12425 0.75 0.43 0.55 79 0.48 0.13 0.20
TIM 7179 0.88 0.85 0.87 500 0.77 0.95 0.85
ORG 6847 0.71 0.26 0.38 887 0.85 0.68 0.75
LOC 5592 0.71 0.66 0.69 512 0.89 0.71 0.79
ANI 2362 0.89 0.36 0.51 67 0.63 0.22 0.33
VEH 1840 0.89 0.45 0.59 28 1.00 0.39 0.56
MAC 694 0.80 0.34 0.47 - - - -
MIX 34 0.00 0.00 0.00 - - - -
here, as this is the standard manner of reporting
results in prior work.
6.4 Summary of results
Table 6 contains a brief overview of the most es-
sential results from this work. For the Switchboard
corpus, this constitutes the current best results in
the literature. As for the GMB, this constitutes the
first results in the literature for animacy classifica-
tion.
Table 6: Main results from all conditions. B&C
(2012) refers to Bowman & Chopra (2012).
Corpus Condition Accuracy
Switchboard
B&C (2012) 0.849
Unknown words 0.592
Known words 0.860
All words 0.858
GMB
Unknown words 0.764
Known words 0.831
All words 0.794
7 Discussion
The work presented in this paper constitutes a mi-
nor improvement to the previously best results for
multi-class animacy classification on the Switch-
board corpus (Bowman and Chopra, 2012). Ad-
ditionally, we also present the first results in the
literature for animacy classification on the GMB,
allowing for future research to use this work as a
point of comparison. It is, however, important to
note that the results obtained for the GMB in this
paper are prone to bias, as the annotation proce-
dure was done in a semi-automatic fashion. If an-
notators were affected by the output of the clas-
sifier, this is likely to have improved the results
presented here.
A striking factor when observing the results, is
the high discrepancy in performance between the
GMB and the Switchboard corpus. This is, how-
ever, not all that surprising. Considering that the
Switchboard corpus consists of spoken language,
and the GMB contains written language, one can
easily draw the conclusion that the domain dif-
ferences pose a substantial obstacle. This can,
for instance, be seen in the differing vocabulary.
In the cross-validation conditions for the Switch-
board corpus, approximately 1% of the words to
be classified in each fold are unknown to the clas-
sifier. As for the GMB, approximately 10% of
the words are unknown. As mentioned in Sec-
tion 6.1.2, the lemma of the head noun in an NP
is a very strong feature, which naturally can not be
used in the case of unknown words. As seen in Ta-
ble 6, performance on known words in the GMB
is not far away from that of known words in the
Switchboard corpus.
Although a fairly good selection of classifiers
were tested in this work, there is room for im-
provement in this area. The fact that the Logistic
Regression classifier outperformed all other clas-
sifiers is likely to have been caused by that not
enough effort was put into parameter selection for
the other classifiers. More sophisticated classi-
fiers, such as Artificial Neural Networks, ought to
72
at the very least replicate the results achieved here.
Quite likely, results should even improve, seeing
that the added computational power of ANNs al-
lows us to capture more interesting/deeper statisti-
cal patterns, if they exist in the data.
The features used in this paper mainly revolved
around semantically oriented ones, such as seman-
tic relations from WordNet, thematic roles and, ar-
guably, concreteness ratings. Better results could
most likely be achieved if one also incorporated
more syntactically oriented features, such as fre-
quency counts from a dependency parsed corpus,
as done by e.g. Bowman & Chopra (2012) and
Øvrelid (2009). Other options include the use of
more linguistically motivated features, such as ex-
ploiting relative pronouns (i.e. who vs. which).
8 Conclusions and future work
At the beginning of this paper, we set out three
aims. Firstly, we wanted to improve upon the
state of the art in multi-class animacy classifica-
tion. A conclusive statement to that effect is hard
to make, considering that comparison was only
made directly to one previous work. However, as
our performance compared to this work was some-
what higher, this work certainly marks some sort
of improvement. Secondly, we aimed at investi-
gating whether a corpus of spoken language con-
taining animacy annotation could be used to anno-
tate a corpus of written language. As our results
for the GMB are well above the baseline, we con-
clude that this is indeed feasible, in spite of the
disparities between language form and vocabulary.
Lastly, we aimed at using the resulting classifier as
a part of the toolchain used to annotate the GMB.
This goal has also been met.
As for future work, the fact that animacy is
marked explicitly in many languages presents a
golden opportunity to alleviate the annotation of
this semantic property for languages in which it
is not explicitly marked. By identifying these
markers, the annotation of animacy in such a lan-
guage should be relatively trivial through the use
of parallel texts. Alternatively, one could look
at using existing annotated corpora, such as Tal-
banken05 (Nivre et al., 2006), as a source of an-
notation. One could then look at transferring this
annotation to a second language. Although intu-
itively promising, this approach has some poten-
tial issues, as animacy is not represented univer-
sally across languages. For instance, fluid contain-
ers (e.g. cups, spoons) represent a class of nouns
which are considered grammatically animate in
Algonquian (Quinn, 2001). Annotating such items
as animate in English would most likely not be
considered correct, neither by native speakers nor
by most experts. Nevertheless, if a sufficiently
large amount of languages have some manner of
consensus as to where a given entity is in an ani-
macy hierarchy, this problem ought to be solvable
by simply hand-picking such languages.
73
References
Kirk Baker and Chris Brew. 2010. Multilingual an-
imacy classification by sparse logistic regression.
OSUWPL, 59:52–75.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In 36th An-
nual Meeting of the Association for Computational
Linguistics and 17th International Conference on
Computational Linguistics. Proceedings of the Con-
ference, pages 86–90, Universit´e de Montr´eal, Mon-
treal, Quebec, Canada.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012a. Developing a large semantically
annotated corpus. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196–3200, Istan-
bul, Turkey.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012b. A platform for collaborative se-
mantic annotation. In Proceedings of the Demon-
strations at the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 92–96, Avignon, France.
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39–71.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69–72. Asso-
ciation for Computational Linguistics.
Jelke Bloem and Gosse Bouma. to appear. Automatic
animacy classification for dutch. Computational
Linguistics in the Netherlands Journal, 3, 12/2013.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Samuel R Bowman and Harshit Chopra. 2012. Au-
tomatic animacy classification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies: Student Research
Workshop, pages 7–10. Association for Computa-
tional Linguistics.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, R Harald
Baayen, et al. 2007. Predicting the dative alterna-
tion. Cognitive foundations of interpretation, pages
69–94.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1–8.
Sasha Calhoun, Jean Carletta, Jason M Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The nxt-format switchboard corpus:
a rich resource for investigating the syntax, seman-
tics, pragmatics and prosody of dialogue. Language
Resources and Evaluation, 44(4):387–419.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. Univer-
sity of Chicago press.
¨
Osten Dahl and Kari Fraurud. 1996. Animacy in gram-
mar and discourse. PRAGMATICS AND BEYOND
NEW SERIES, pages 47–64.
Arantza D´?az de Illaraza, Aingeru Mayor, and Kepa
Sarasola. 2002. Semiautomatic labelling of seman-
tic features. In Proceedings of the 19th International
Conference on Computational Linguistics.
Felice Dell’Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the path
to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Work-
shop on Psychocomputational Models of Human
Language Acquisition, pages 72–81. Association for
Computational Linguistics.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In PACLIC,
pages 220–229.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Springer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Andrew McCallum, Kamal Nigam, et al. 1998. A
comparison of event models for naive bayes text
classification. In AAAI-98 workshop on learning for
text categorization, volume 752, pages 41–48. Cite-
seer.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Joshua L. Moore, Christopher J.C. Burges, Erin Ren-
shaw, and Yih Wen-tau. 2013. Animacy detec-
tion with voting models. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 55–60. Association for
Computational Linguistics.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006.
Talbanken05: A swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the fifth International Conference on Language Re-
sources and Evaluation (LREC), pages 1392–1395.
74
Constantin Or?asan and Richard Evans. 2007. Np ani-
macy identification for anaphora resolution. J. Artif.
Intell. Res.(JAIR), 29:79–103.
Lilja Øvrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough–Swedish
dependency parsing with rich linguistic features.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP), pages 447–451.
Lilja Øvrelid. 2005. Animacy classification based on
morphosyntactic corpus frequencies: some experi-
ments with norwegian nouns. In Proc. of the Work-
shop on Exploring Syntactically Annotated Corpora.
Lilja Øvrelid. 2006. Towards robust animacy clas-
sification using morphosyntactic distributional fea-
tures. In Proceedings of the Eleventh Conference of
the European Chapter of the Association for Com-
putational Linguistics: Student Research Workshop,
pages 47–54. Association for Computational Lin-
guistics.
Lilja Øvrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 630–638. Association
for Computational Linguistics.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proc. of the Corpus
Linguistics Workshop on Using Corpora for Natural
Language Generation, pages 25–32.
Conor Quinn. 2001. A preliminary survey of animacy
categories in penobscot. In Papers of the 32nd. Al-
gonquian Conference, pages 395–426.
Anette Rosenbach. 2008. Animacy and grammatical
variation–findings from English genitive variation.
Lingua, 118(2):151–171.
Anatol Stefanowitsch. 2003. Constructional semantics
as a limit to grammatical alternation: The two gen-
itives of English. TOPICS IN ENGLISH LINGUIS-
TICS, 43:413–444.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, volume 1, pages 477–485.
Association for Computational Linguistics.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense
labeling. Proc. 10th International Conference on
Computational Semantics (IWCS-2013), pages 397–
403.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O’Connor, and Tom Wasow.
2004. Animacy encoding in english: why and how.
In Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 118–125. Association for
Computational Linguistics.
75
