Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603–611,
Athens, Greece, 30 March – 3 April 2009. c©2009 Association for Computational Linguistics
Deterministic shift-reduce parsing for unification-based grammars by 
using default unification 
 
 
Takashi Ninomiya 
Information Technology Center 
University of Tokyo, Japan 
ninomi@r.dl.itc.u-tokyo.ac.jp 
Takuya Matsuzaki 
Department of Computer Science 
University of Tokyo, Japan 
matuzaki@is.s.u-tokyo.ac.jp 
  
Nobuyuki Shimizu 
Information Technology Center 
University of Tokyo, Japan 
shimizu@r.dl.itc.u-tokyo.ac.jp 
Hiroshi Nakagawa 
Information Technology Center 
University of Tokyo, Japan 
nakagawa@dl.itc.u-tokyo.ac.jp
 
 
Abstract 
Many parsing techniques including pa-
rameter estimation assume the use of a 
packed parse forest for efficient and ac-
curate parsing.  However, they have sev-
eral inherent problems deriving from the 
restriction of locality in the packed parse 
forest.  Deterministic parsing is one of 
solutions that can achieve simple and fast 
parsing without the mechanisms of the 
packed parse forest by accurately choos-
ing search paths.  We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first 
shift-reduce parsing with beam threshold-
ing for unification-based grammars.  De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard 
constraints.  Therefore, it is developed by 
using default unification, which almost 
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars. 
1 Introduction 
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively.  Previous studies (Abney, 
1997; Johnson et al., 1999; Kaplan et al., 2004; 
Malouf and van Noord, 2004; Miyao and Tsujii, 
2005; Riezler et al., 2000) defined a probabilistic 
model of unification-based grammars, including 
head-driven phrase structure grammar (HPSG), 
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum 
entropy model (Berger et al., 1996).  Geman and 
Johnson (Geman and Johnson, 2002) and Miyao 
and Tsujii (Miyao and Tsujii, 2002) proposed a 
feature forest, which is a dynamic programming 
algorithm for estimating the probabilities of all 
possible parse candidates.  A feature forest can 
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.  
Feature forests have been used successfully 
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its 
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and 
Curran, 2004a; Ninomiya et al., 2007; Ninomiya 
et al., 2006).  Both estimation and parsing with 
the packed parse forest, however, have several 
inherent problems deriving from the restriction 
of locality.  First, feature functions can be de-
fined only for local structures, which limit the 
parser’s performance.  This is because parsers 
segment parse trees into constituents and factor 
equivalent constituents into a single constituent 
(edge) in a chart to avoid the same calculation.  
This also means that the semantic structures must 
be segmented.  This is a crucial problem when 
we think of designing semantic structures other 
than predicate argument structures, e.g., syn-
chronous grammars for machine translation.  The 
size of the constituents will be exponential if the 
semantic structures are not segmented.  Lastly, 
we need delayed evaluation for evaluating fea-
ture functions.  The application of feature func-
tions must be delayed until all the values in the 
603
segmented constituents are instantiated.  This is 
because values in parse trees can propagate any-
where throughout the parse tree by unification.  
For example, values may propagate from the root 
node to terminal nodes, and the final form of the 
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree.  Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.  
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and 
Matsumoto, 2003), search optimization learning 
(Daumé and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa, 
2007), were studied. 
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars 
without the mechanisms of the packed parse for-
est.  Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and 
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003), 
through approaches based essentially on deter-
ministic parsing.  These techniques, however, 
cannot simply be applied to unification-based 
grammar parsing because it can fail as a result of 
its hard constraints in the grammar.  Therefore, 
in this study, we propose deterministic parsing 
for unification-based grammars by using default 
unification, which almost always succeeds in 
unification by overwriting inconsistent con-
straints in the grammars.  We further pursue 
best-first shift-reduce parsing for unification-
based grammars. 
Sections 2 and 3 explain unification-based 
grammars and default unification, respectively.  
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4.  Section 5 dis-
cusses our experiments, and Section 6 concludes 
the paper. 
2 Unification-based grammars 
A unification-based grammar is defined as a pair 
consisting of a set of lexical entries and a set of 
phrase-structure rules.  The lexical entries ex-
press word-specific characteristics, while the 
phrase-structure rules describe constructions of 
constituents in parse trees.  Both the phrase-
structure rules and the lexical entries are 
represented by feature structures (Carpenter, 
1992), and constraints in the grammar are forced 
by unification.  Among the phrase-structure rules, 
a binary rule is a partial function: ? × ? ? ? , 
where ? is the set of all possible feature struc-
tures.  The binary rule takes two partial parse 
trees as daughters and returns a larger partial 
parse tree that consists of the daughters and their 
mother.  A unary rule is a partial function: 
? ? ?, which corresponds to a unary branch. 
In the experiments, we used an HPSG (Pollard 
and Sag, 1994), which is one of the sophisticated 
unification-based grammars in linguistics.  Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries.  Figure 1 shows an example of HPSG pars-
ing of the sentence, “Spring has come.”  The up-
per part of the figure shows a partial parse tree 
for “has come,” which is obtained by unifying 
each of the lexical entries for “has” and “come” 
with a daughter feature structure of the head-
complement rule.  Larger partial parse trees are 
obtained by repeatedly applying phrase-structure 
rules to lexical/phrasal partial parse trees.  Final-
ly, the parse result is output as a parse tree that 
dominates the sentence. 
3 Default unification 
Default unification was originally investigated in 
a series of studies of lexical semantics, in order 
to deal with default inheritance in a lexicon.  It is 
also desirable, however, for robust processing, 
because (i) it almost always succeeds and (ii) a 
feature structure is relaxed such that the amount 
of information is maximized (Ninomiya et al., 
2002).  In our experiments, we tested a simpli-
fied version of Copestake’s default unification.  
Before explaining it, we first explain Carpenter’s 
 
Figure 1: Example of HPSG parsing. 
 
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
HEAD  noun
SUBJ  <      SUBJ  <>     >
COMPS <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
head-comp
Spring has come
1
1 12
2
HEAD  verb
SUBJ  <>
COMPS <>
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
subject-head
head-comp
Spring has come
1
1 11 2
2
604
two definitions of default unification (Carpenter, 
1993). 
 
(Credulous Default Unification) 
? ?? ? ? =  ?? ? ????
? ? ? is maximal such
that ? ? ? ?is defined ? 
 
(Skeptical Default Unification) 
? ?? ? ? =  ?(? ?
?
? ?) 
 
?  is called a strict feature structure, whose in-
formation must not be lost, and ? is called a de-
fault feature structure, whose information can be 
lost but as little as possible so that ? and ? can 
be unified. 
Credulous default unification is greedy, in that 
it tries to maximize the amount of information 
from the default feature structure, but it results in 
a set of feature structures.  Skeptical default un-
ification simply generalizes the set of feature 
structures resulting from credulous default unifi-
cation.  Skeptical default unification thus leads to 
a unique result so that the default information 
that can be found in every result of credulous 
default unification remains.  The following is an 
example of skeptical default unification: 
 
[F: ?]  ?? ? ?
F: 1 ?
G: 1
H: ?
? =  ???
F: ?
G: ?
H: ?
? , ?
F: 1 ?
G: 1
H: ?
?? = ?
F: ?
G: ?
H: ?
?. 
 
Copestake mentioned that the problem with 
Carpenter’s default unification is its time com-
plexity (Copestake, 1993).  Carpenter’s default 
unification takes exponential time to find the op-
timal answer, because it requires checking the 
unifiability of the power set of constraints in a 
default feature structure.  Copestake thus pro-
posed another definition of default unification, as 
follows. Let ??(?) be a function that returns a 
set of path values in ?, and let ??(?) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ?. 
 
(Copestake’s default unification) 
? ?? ? ? =  ? ? ? ???
? ? ??(?)and there is no ?? ? ??(?)
such that ? ? ??is defined and
? ? ? ? ??is not defined
?, 
where ? = ? ? ???(?). 
 
Copestake’s default unification works effi-
ciently because all path equations in the default 
feature structure are unified with the strict fea-
ture structures, and because the unifiability of 
path values is checked one by one for each node 
in the result of unifying the path equations.  The 
implementation is almost the same as that of 
normal unification, but each node of a feature 
structure has a set of values marked as “strict” or 
“default.”  When types are involved, however, it 
is not easy to find unifiable path values in the 
default feature structure.  Therefore, we imple-
mented a more simply typed version of Corpes-
take’s default unification. 
Figure 2 shows the algorithm by which we 
implemented the simply typed version.  First, 
each node is marked as “strict” if it belongs to a 
strict feature structure and as “default” otherwise. 
The marked strict and default feature structures 
procedure forced_unification(p, q) 
   queue := {?p, q?}; 
   while( queue is not empty ) 
      ?p, q? := shift(queue); 
      p := deref(p); q := deref(q); 
      if p ? q 
         ?(p) ?  ?(p) ? ?(q); 
         ?(q) ? ptr(p); 
         forall f ? feat(p)? feat(q) 
            if f ? feat(p) ? f ? feat(q) 
               queue := queue ? ??(f, p), ?(f, q)?; 
            if f ? feat(p) ? f ? feat(q) 
               ?(f, p) ?  ?(f, q); 
procedure mark(p, m) 
   p := deref(p); 
   if p has not been visited 
      ?(p) := {??(p),m?}; 
      forall f ? feat(p) 
         mark(?(f, p), m); 
procedure collapse_defaults(p) 
   p := deref(p); 
   if p has not been visited 
      ts := ?; td := ?; 
      forall ?t, ??????? ? ?(p) 
         ts := ts ? t; 
      forall ?t, ???????? ? ?(p) 
         td := td ? t; 
      if ts is not defined 
         return false; 
      if ts ? td is defined 
         ?(p) := ts ? td; 
      else 
         ?(p) := ts; 
      forall f ? feat(p) 
         collapse_defaults(?(f, p)); 
procedure default_unification(p, q) 
   mark(p, ??????); 
   mark(q, ???????); 
   forced_unification(p, q); 
   collapse_defaults(p); 
 
?(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of 
types and markers in the feature structure node p. 
A marker indicates that the types in a feature structure node 
originally belong to the strict feature structures or the default 
feature structures. 
A pointer indicates that the node has been unified with other 
nodes and it points the unified node.  A function deref tra-
verses pointer nodes until it reaches to non-pointer node. 
?(f, p) returns a feature structure node which is reached by 
following a feature f from p. 
 
Figure 2: Algorithm for the simply typed ver-
sion of Corpestake’s default unification. 
605
are unified, whereas the types in the feature 
structure nodes are not unified but merged as a 
set of types.  Then, all types marked as “strict” 
are unified into one type for each node.  If this 
fails, the default unification also returns unifica-
tion failure as its result.  Finally, each node is 
assigned a single type, which is the result of type 
unification for all types marked as both “default” 
and “strict” if it succeeds or all types marked 
only as “strict” otherwise. 
4 Shift-reduce parsing for unification-
based grammars 
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).  
Their algorithm works non-deterministically with 
the mechanism of the packed parse forest, and 
hence it has the problem of locality in the packed 
parse forest.  This section explains our shift-
reduce parsing algorithms, which are based on 
deterministic shift-reduce CFG parsing (Sagae 
and Lavie, 2005) and best-first shift-reduce CFG 
parsing (Sagae and Lavie, 2006).  Sagae’s parser 
selects the most probable shift/reduce actions and 
non-terminal symbols without assuming explicit 
CFG rules.  Therefore, his parser can proceed 
deterministically without failure.  However, in 
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar.  We propose two new 
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and 
beam search.  The major difference between our 
algorithm and Sagae’s algorithm is that we use 
default unification.  First, we explain the deter-
ministic shift-reduce parsing algorithm, and then 
we explain the shift-reduce parsing with back-
tracking and beam search. 
4.1 Deterministic shift-reduce parsing for 
unification-based grammars 
The deterministic shift-reduce parsing algorithm 
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.  
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the 
whole input sentence.  Items in W are words and 
POSs in the input sentence.  The algorithm de-
fines two types of parser actions, shift and reduce, 
as follows. 
• Shift: A shift action removes the first item 
(a word and a POS) from W.  Then, one 
lexical entry is selected from among the 
candidate lexical entries for the item.  Fi-
nally, the selected lexical entry is put on 
the top of the stack. 
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i), 
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1, 
pi+2, pi+3 
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr, 
hwr, hpr, hlr 
Unary reduce features: sy, hw, hp, hl 
 
Sw(i) … head word of i-th item from the top of the stack 
Sp(i) … head POS of i-th item from the top of the stack 
Shw(i) … head word of the head daughter of i-th item from the 
top of the stack 
Shp(i) … head POS of the head daughter of i-th item from the 
top of the stack 
Snw(i) … head word of the non-head daughter of i-th item 
from the top of the stack 
Snp(i) … head POS of the non-head daughter of i-th item from 
the top of the stack 
Ssy(i) … symbol of phrase category of the i-th item from the 
top of the stack 
Shsy(i) … symbol of phrase category of the head daughter of 
the i-th item from the top of the stack 
Snsy(i) … symbol of phrase category of the non-head daughter 
of the i-th item from the top of the stack 
d … distance between head words of daughters 
c … whether a comma exists between daughters and/or inside 
daughter phrases 
sp … the number of words dominated by the phrase 
sy … symbol of phrase category 
hw … head word 
hp … head POS 
hl … head lexical entry 
 
Figure 3: Feature templates. 
Shift Features 
  [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, 
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] 
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi, 
pi+1] [pi+1, pi+2] 
 
Binary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, 
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, 
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d, 
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy] 
 
Unary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl] 
[sy, hw] [hp, hl] [hp] [hl] [sy]
 
Figure 4: Combinations of feature templates. 
606
• Binary Reduce: A binary reduce action 
removes two items from the top of the 
stack.  Then, partial parse trees are derived 
by applying binary rules to the first re-
moved item and the second removed item 
as a right daughter and left daughter, re-
spectively.  Among the candidate partial 
parse trees, one is selected and put on the 
top of the stack. 
• Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.  
Then, partial parse trees are derived by 
applying unary rules to the removed item.  
Among the candidate partial parse trees, 
one is selected and put on the top of the 
stack. 
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end).  Parsing is considered suc-
cessfully finished when W is empty and S has 
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty.  Parsing is considered a 
non-sentential success when W is empty and S 
has only one item but it does not satisfy the sen-
tential condition. 
In our experiments, we used a maximum en-
tropy classifier to choose the parser’s action.  
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates.  Many of these features were tak-
en from those listed in (Ninomiya et al., 2007), 
(Miyao and Tsujii, 2005) and (Sagae and Lavie, 
2005), including global features defined over the 
information in the stack, which cannot be used in 
parsing with the packed parse forest.  The fea-
tures for selecting shift actions are the same as 
the features used in the supertagger (Ninomiya et 
al., 2007).  Our shift-reduce parsers can be re-
garded as an extension of the supertagger. 
The deterministic parsing can fail because of 
its grammar’s hard constraints.  So, we use de-
fault unification, which almost always succeeds 
in unification.  We assume that a head daughter 
(or, an important daughter) is determined for 
each binary rule in the unification-based gram-
mar.   Default unification is used in the binary 
rule application in the same way as used in Ni-
nomiya’s offline robust parsing (Ninomiya et al., 
2002), in which a binary rule unified with the 
head daughter is the strict feature structure and 
the non-head daughter is the default feature 
structure, i.e.,  (? ? ?) ?? ??, where R is a bi-
nary rule, H is a head daughter and NH is a non-
head daughter.  In the experiments, we used the 
simply typed version of Copestake’s default un-
ification in the binary rule application1.  Note 
that default unification was always used instead 
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation.  Although Copestake’s default unification 
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified 
with the head daughter, or inconsistency is 
caused by path equations in the default feature 
structures.  If the rule application fails for all the 
binary rules, backtracking or beam search can be 
used for its recovery as explained in Section 4.2.  
In the experiments, we had no failure in the bi-
nary rule application with default unification. 
4.2 Shift-reduce parsing by backtracking 
and beam-search 
Another approach for recovering from the pars-
ing failure is backtracking.  When parsing fails 
or ends with non-sentential success, the parser’s 
state goes back to some old state (backtracking), 
and it chooses the second best action and tries 
parsing again.  The old state is selected so as to 
minimize the difference in the probabilities for 
selecting the best candidate and the second best 
candidate.  We define a maximum number of 
backtracking steps while parsing a sentence.  
Backtracking repeats until parsing finishes with 
sentential success or reaches the maximum num-
ber of backtracking steps.  If parsing fails to find 
a parse tree, the best continuous partial parse 
trees are output for evaluation. 
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first 
search algorithms.  Another possibility is to use 
the best-first search algorithm.  The best-first 
parser has a state priority queue, and each state 
consists of a tree stack and a word queue, which 
are the same stack and queue explained in the 
shift-reduce parsing algorithm.  Parsing proceeds 
by applying shift-reduce actions to the best state 
in the state queue.  First, the best state is re-
                                                 
1 We also implemented Ninomiya’s default unification, 
which can weaken path equation constraints.  In the prelim-
inary experiments, we tested binary rule application given 
as (? ? ?) ?? ?? with Copestake’s default unification, 
(? ? ?) ?? ?? with Ninomiya’s default unification, and 
(? ? ??) ?? ? with Ninomiya’s default unification.  How-
ever, there was no significant difference of F-score among 
these three methods.  So, in the main experiments, we only 
tested (? ? ?) ?? ?? with Copestake’s default unification 
because this method is simple and stable. 
607
moved from the state queue, and then shift-
reduce actions are applied to the state.  The new-
ly generated states as results of the shift-reduce 
actions are put on the queue.  This process re-
peats until it generates a state satisfying the sen-
tential condition.  We define the probability of a 
parsing state as the product of the probabilities of 
selecting actions that have been taken to reach 
the state.  We regard the state probability as the 
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities 
is always chosen in the algorithm.  However, the 
best-first algorithm with this objective function 
searches like the breadth-first search, and hence, 
parsing is very slow or cannot be processed in a 
reasonable time.  So, we introduce beam thre-
sholding to the best-first algorithm.  The search 
space is pruned by only adding a new state to the 
state queue if its probability is greater than 1/b of 
the probability of the best state in the states that 
has had the same number of shift-reduce actions.  
In what follows, we call this algorithm beam 
search parsing. 
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation.  Note that, the beam search parsing for 
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam 
search.  This is because we have to copy parse 
trees, which consist of a large feature structures, 
in every step of searching to keep many states on 
the state queue.  In the case of backtracking, co-
pying is not necessary. 
5 Experiments 
We evaluated the speed and accuracy of parsing 
with Enju 2.3?, an HPSG for English (Miyao and 
Tsujii, 2005).  The lexicon for the grammar was 
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences).  The grammar consisted 
of 2,302 lexical entries for 11,187 words.  Two 
probabilistic classifiers for selecting shift-reduce 
actions were trained using the same portion of 
the treebank.  One is trained using normal unifi-
cation, and the other is trained using default un-
ification. 
We measured the accuracy of the predicate ar-
gument relation output of the parser.  A predi-
cate-argument relation is defined as a tuple 
??, ??, ?, ???, where ? is the predicate type (e.g., 
  Section 23 (Gold POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non- 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - - 
(Ninomiya et al., 2007) 89.78 89.28 89.53 234 - - - - - 
Ours 
det 76.45 82.00 79.13 122 0 - 867 35 1514 
det+du 87.78 87.45 87.61 256 0 - 0 117 2299 
back40 81.93 85.31 83.59 519 18986 - 386 23 2007 
back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371 
beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017 
beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400 
beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331 
           
  Section 23 (Auto POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - - 
(Ninomiya et al., 2007) 87.28 87.05 87.17 260 - - - - - 
(Matsuzaki et al., 2007)  86.93 86.47 86.70 30 - - - - - 
(Sagae et al., 2007)  88.50 88.00 88.20 - - - - - - 
Ours 
det 74.13 80.02 76.96 127 0 - 909 31 1476 
det+du 85.93 85.72 85.82 252 0 - 0 124 2292 
back40 78.71 82.86 80.73 568 21068 - 438 27 1951 
back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370 
beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962 
beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395 
beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311 
 
Table 1: Experimental results for Section 23. 
608
adjective, intransitive verb), ?? is the head word 
of the predicate, ? is the argument label (MOD-
ARG, ARG1, …, ARG4), and ??  is the head 
word of the argument.  The labeled precision 
(LP) / labeled recall (LR) is the ratio of tuples 
correctly identified by the parser, and the labeled 
F-score (LF) is the harmonic mean of the LP and 
LR. This evaluation scheme was the same one 
used in previous evaluations of lexicalized 
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005).  The expe-
riments were conducted on an Intel Xeon 5160 
server with 3.0-GHz CPUs. Section 22 of the 
Penn Treebank was used as the development set, 
and the performance was evaluated using sen-
tences of ? 100 words in Section 23.  The LP, 
LR, and LF were evaluated for Section 23. 
Table 1 lists the results of parsing for Section 
23.  In the table, “Avg. time” is the average pars-
ing time for the tested sentences.  “# of backtrack” 
is the total number of backtracking steps that oc-
curred during parsing.  “Avg. # of states” is the 
average number of states for the tested sentences.  
“# of dead end” is the number of sentences for 
which parsing failed.  “# of non-sentential suc-
cess” is the number of sentences for which pars-
ing succeeded but did not generate a parse tree 
satisfying the sentential condition.  “det” means 
the deterministic shift-reduce parsing proposed 
in this paper.  “back?” means shift-reduce pars-
ing with backtracking at most ? times for each 
sentence.  “du” indicates that default unification 
was used.  “beam?” means best-first shift-reduce 
parsing with beam threshold ?.  The upper half 
of the table gives the results obtained using gold 
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger.  The max-
imum number of backtracking steps and the 
beam threshold were determined by observing 
the performance for the development set (Section 
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except 
“beam(403.4)”). The performance of 
“beam(403.4)” was evaluated to see the limit of 
the performance of the beam-search parsing. 
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around 
79.1% (Section 23, gold POS).  With backtrack-
ing, the LF increased to 83.6%.  Figure 5 shows 
the relation between LF and parsing time for the 
development set (Section 22, gold POS).  As 
seen in the figure, the LF increased as the parsing 
time increased.  The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.  
Table 1 also shows that deterministic parsing 
with default unification achieved higher accuracy, 
with an LF of around 87.6% (Section 23, gold 
POS), without backtracking.  Default unification 
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal 
unification.  The beam-search parsing without 
default unification achieved high accuracy, with 
an LF of around 87.0%, but is still worse than 
deterministic parsing with default unification.  
However, with default unification, it achieved 
the best performance, with an LF of around 
88.5%, in the settings of parsing time less than 
500ms/sentence for Section 22. 
For comparison with previous studies using 
the packed parse forest, the performances of 
Miyao’s parser, Ninomiya’s parser, Matsuzaki’s 
parser and Sagae’s parser are also listed in Table 
1.  Miyao’s parser is based on a probabilistic 
model estimated only by a feature forest.  Nino-
miya’s parser is a mixture of the feature forest 
 
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). 
 
82.00%
83.00%
84.00%
85.00%
86.00%
87.00%
88.00%
89.00%
90.00%
0 1 2 3 4 5 6 7 8
LF
Avg. parsing time (s/sentence)
back
back+du
beam
beam+du
609
and an HPSG supertagger.  Matsuzaki’s parser 
uses an HPSG supertagger and CFG filtering.  
Sagae’s parser is a hybrid parser with a shallow 
dependency parser.  Though parsing without the 
packed parse forest is disadvantageous to the 
parsing with the packed parse forest in terms of 
search space complexity, our model achieved 
higher accuracy than Miyao’s parser. 
“beam(403.4)” in Table 1 and “beam” in Fig-
ure 5 show possibilities of beam-search parsing.  
“beam(403.4)” was very slow, but the accuracy 
was higher than any other parsers except Sagae’s 
parser. 
Table 2 shows the behaviors of default unifi-
cation for “det+du.”  The table shows the 20 
most frequent path values that were overwritten 
by default unification in Section 22.  In most of 
the cases, the overwritten path values were in the 
selection features, i.e., subcategorization frames 
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee 
specification (MOD:).  The column of ‘Default 
type’ indicates the default types which were 
overwritten by the strict types in the column of 
‘Strict type,’ and the last column is the frequency 
of overwriting.  ‘cons’ means a non-empty list, 
and ‘nil’ means an empty list.  In most of the 
cases, modifiee and subcategorization frames 
were changed from empty to non-empty and vice 
versa.  From the table, overwriting of head in-
formation was also observed, e.g., ‘noun’ was 
changed to ‘verb.’ 
6 Conclusion and Future Work 
We have presented shift-reduce parsing approach 
for unification-based grammars, based on deter-
ministic shift-reduce parsing.  First, we presented 
deterministic parsing for unification-based 
grammars.  Deterministic parsing was difficult in 
the framework of unification-based grammar 
parsing, which often fails because of its hard 
constraints.  We introduced default unification to 
avoid the parsing failure.  Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification.  The 
experiments revealed that deterministic parsing 
with default unification achieved high accuracy, 
with a labeled F-score (LF) of 87.6% for Section 
23 of the Penn Treebank with gold POSs.  
Second, we also presented the best-first parsing 
with beam search for unification-based gram-
mars.  The best-first parsing with beam search 
achieved the best accuracy, with an LF of 87.0%, 
in the settings without default unification.  De-
fault unification further increased LF from 
87.0% to 88.5%.  By widening the beam width, 
the best-first parsing achieved an LF of 90.0%. 
References 
Abney, Steven P. 1997. Stochastic Attribute-Value 
Grammars. Computational Linguistics, 23(4), 597-
618. 
Path Strict 
type 
Default 
type 
Freq
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: 
HEAD:MOD: 
cons nil 101
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:
hd:LOCAL:CAT:HEAD:MOD: 
cons nil 96
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
… … … …
Total   10,598
 
Table 2: Path values overwritten by default unification in Section 22. 
610
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71. 
Briscoe, Ted and John Carroll. 1993. Generalized 
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59. 
Carpenter, Bob. 1992. The Logic of Typed Feature 
Structures: Cambridge University Press. 
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates 
and Inheritance. In Inheritance, Defaults, and the 
Lexicon. Cambridge: Cambridge University Press. 
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative 
Reranking. In proc. of ACL'05, pp. 173-180. 
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG 
parsing. In proc. of COLING-04, pp. 282-288. 
Clark, Stephen and James R. Curran. 2004b. Parsing 
the WSJ using CCG and log-linear models. In proc. 
of ACL'04, pp. 104-111. 
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon. 
Cambridge: Cambridge University Press. 
Daumé, Hal III and Daniel Marcu. 2005. Learning as 
Search Optimization: Approximate Large Margin 
Methods for Structured Prediction. In proc. of 
ICML 2005. 
Geman, Stuart and Mark Johnson. 2002. Dynamic 
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of 
ACL'02, pp. 279-286. 
Hockenmaier, Julia. 2003. Parsing with Generative 
Models of Predicate-Argument Structure. In proc. 
of ACL'03, pp. 359-366. 
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi 
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ``Unification-Based'' Grammars. In proc. of 
ACL '99, pp. 535-541. 
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell 
III, and A. Vasserman. 2004. Speed and accuracy 
in shallow and deep stochastic parsing. In proc. of 
HLT/NAACL'04. 
Malouf, Robert and Gertjan van Noord. 2004. Wide 
Coverage Parsing with Stochastic Attribute Value 
Grammars. In proc. of IJCNLP-04 Workshop 
``Beyond Shallow Analyses''. 
Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp. 
1671-1676. 
Miyao, Yusuke and Jun'ichi Tsujii. 2002. Maximum 
Entropy Estimation for Feature Forests. In proc. of 
HLT 2002, pp. 292-297. 
Miyao, Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In proc. of ACL'05, pp. 83-90. 
Nakagawa, Tetsuji. 2007. Multilingual dependency 
parsing using global features. In proc. of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007, pp. 915-932. 
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke 
Miyao, and Jun'ichi Tsujii. 2007. A log-linear 
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp. 
60-68. 
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa 
Tsuruoka, Yusuke Miyao, and Jun'ichi Tsujii. 2006. 
Extremely Lexicalized Models for Accurate and 
Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 
155-163. 
Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2002. Lenient Default Unification for Robust 
Processing within Unification Based Grammar 
Formalisms. In proc. of COLING 2002, pp. 744-
750. 
Nivre, Joakim and Mario Scholz. 2004. Deterministic 
dependency parsing of English text. In proc. of 
COLING 2004, pp. 64-70. 
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar: University of Chicago 
Press. 
Ratnaparkhi, Adwait. 1997. A linear observed time 
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP'97. 
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and 
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of 
ACL'00, pp. 480-487. 
Sagae, Kenji and Alon Lavie. 2005. A classifier-based 
parser with linear run-time complexity. In proc. of 
IWPT 2005. 
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp. 
691-698. 
Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG parsing with shallow dependency 
constraints. In proc. of ACL 2007, pp. 624-631. 
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector 
Machines. In proc. of IWPT-2003. 
 
611
