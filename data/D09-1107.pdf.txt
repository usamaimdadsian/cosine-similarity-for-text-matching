Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1027–1036,
Singapore, 6-7 August 2009.
c©2009 ACL and AFNLP
Sinuhe — Statistical Machine Translation using a Globally Trained
Conditional Exponential Family Translation Model
Matti K
¨
a
¨
ari
¨
ainen
Department of Computer Science
FI-00014 University of Helsinki, Finland
matti.kaariainen@cs.helsinki.fi
Abstract
We present a new phrase-based con-
ditional exponential family translation
model for statistical machine translation.
The model operates on a feature repre-
sentation in which sentence level transla-
tions are represented by enumerating all
the known phrase level translations that
occur inside them. This makes the model
a good match with the commonly used
phrase extraction heuristics. The model’s
predictions are properly normalized prob-
abilities. In addition, the model automati-
cally takes into account information pro-
vided by phrase overlaps, and does not
suffer from reference translation reacha-
bility problems.
We have implemented an open source
translation system Sinuhe based on the
proposed translation model. Our experi-
ments on Europarl and GigaFrEn corpora
demonstrate that finding the unique MAP
parameters for the model on large scale
data is feasible with simple stochastic gra-
dient methods. Sinuhe is fast and mem-
ory efficient, and the BLEU scores ob-
tained by it are only slightly inferior to
those of Moses.
1 Introduction
In current phrase-based statistical machine transla-
tion systems such as Moses
1
(Koehn et al., 2007),
the translation model is defined in terms of phrase
pairs (biphrases) extracted from a bilingual cor-
pus as follows. The corpus is first word-aligned
using a word alignment heuristic (Och and Ney,
1
Throughout this paper, we refer to Moses for concrete-
ness, but most of the discussion applies to other standard
phrase-based statistical machine translation systems as well.
2003). The phrase extraction heuristic then ex-
tracts all the biphrases that are compatible with
the word alignment (Och et al., 1999). This way,
each sentence pair may generate any number of
potentially overlapping biphrases. However, when
defining the phrase-based sentence level transla-
tion model, phrase overlaps are explicitly disal-
lowed: The source sentence is segmented into dis-
joint phrases, which are translated independently
using conditional phrase-level translation models
that have been estimated from extracted biphrase
counts.
The disparity between the phrase extraction
heuristic and the use of the extracted biphrases
can be addressed in at least three ways. One
approach is to simply ignore the disparity as is
done, e.g., in Moses. While empirically succes-
ful, this approach is hard to justify theoretically,
and begs the question of whether more principled
methods might lead to better translation results.
The other extensively studied approach is to re-
place the phrase extraction heuristic with a method
that better matches the use of the extracted phrases
(see, e.g., (Marcu and Wong, 2002; DeNero et al.,
2008) and the references therein). While theo-
retically sound, this approach is computationally
challenging both in practice (DeNero et al., 2008)
and in theory (DeNero and Klein, 2008), may suf-
fer from reference reachability problems (DeNero
et al., 2006), and in the end may lead to inferior
translation quality (Koehn et al., 2003).
In this paper, we study a third alternative. We
propose a new translation model that is compati-
ble with the phrase extraction heuristic. The pro-
posed machine learning inspired translation model
takes the form of a conditional exponential family
probability distribution over a feature representa-
tion for word-aligned sentence pairs. The feature
representation represents a word-aligned sentence
pair by essentially enumerating the (multi)set of
biphrases that would have been extracted from it,
1027
together with the source positions at which the
biphrases occur. The model’s predictions are con-
ditional probabilities for such sets of biphrases
given the source sentence.
The chosen feature representation has many ad-
vantages. Since all word-aligned sentence pairs
can be represented, reference reachability prob-
lems are automatically circumvented. For exam-
ple, if the translation of a sentence consisted solely
of words that do not occur in the phrase table, then
the feature vector for the translation would be the
all zero vector. As the training data receives non-
zero probability, maximum likelihood or maxi-
mum a posteriori (MAP) parameters for the model
can be estimated in a principled way without re-
sorting to pseudo-references. The fact that the
model is not restricted to using disjoint biphrases
means that the information in biphrase overlaps is
automatically taken into account. This may help
in smoothing the model’s predictions on long and
rare phrases, and in enhancing fluency at places
that otherwise would be phrase boundaries. Also,
the model can be extended in a principled way by
introducing additional features (e.g., translations
from a dictionary, biphrases with gaps, biphrases
over POS tags,. . . ).
The proposed model has one parameter per
biphrase feature, so the total number of parame-
ters is easily millions or more. Still, the model
structure is designed so that feature expectations
and related quantities can be computed efficiently
by dynamic programming. It is thus feasible to
compute the gradient of the MAP objective, and
simple gradient ascent can be used to efficiently
find the globally optimal model parameters (with
respect to a suitably scaled Gaussian prior used for
regularization). Exact inference is also possible by
dynamic programming when translations are pre-
dicted by the translation model alone. When other
features like a language model are included, one
needs to resort to beam search type approximate
dynamic programming for decoding.
We have implemented a translation system
called Sinuhe based on the proposed translation
model. The system has been released under the
GPLv3 open source license (K¨a¨ari¨ainen, 2009).
Our experiments on Europarl and GigaFrEn cor-
pora demonstrate that the proposed translation
model scales well to large data, and offers trans-
lation quality that is only slightly worse than that
of the baseline system Moses. In terms of trans-
lation speed, Sinuhe is already clearly better.
The rest of this paper is organized as follows.
After briefly reviewing related work in Section 2,
we describe the proposed translation model in Sec-
tion 3. Finally, experimental results are presented
in Section 4, and conclusions in Section 5.
2 Related work
The proposed translation model is strongly influ-
enced by machine learning techniques for solv-
ing sequence prediction tasks, most notably the
work on conditional random fields (Lafferty et al.,
2001). The modelling task in machine translation
is, however, more complicated than sequence la-
belling (not one-to-one, reorderings), so the stan-
dard methods cannot be directly applied here.
The model we propose is also related to standard
phrase-based translation models through the use of
the same phrase-level translation features. How-
ever, the way we use the features is quite different.
There exists a number of discriminative ap-
proaches whose model structure, training crite-
ria, or both, are similar to ours. However, to our
knowledge, none of the other systems operates
directly on biphrase features, scales up to bilin-
gual corpora with millions of sentence pairs, and
achieves translation quality comparable to fully
tuned standard phrase-based systems. The ap-
proach most closely resembling ours is the in-
dependently developed global discriminative log-
linear model based on synchronous context-free
grammars (Blunsom and Osborne, 2008; Blun-
som et al., 2008). The version presented in (Blun-
som and Osborne, 2008) operates on millions of
rule count features analogous to our biphrase fea-
tures, and integrates a language model into train-
ing and decoding. The system can be trained on
tens of thousands of short sentences yielding bet-
ter translations than a baseline system Hiero on
this data. The version presented in (Blunsom et
al., 2008) scales to more than a hundred thousand
short training sentences, but does not integrate a
language model and thus has performance that im-
proves upon Hiero without a language model
only. Both versions deal with derivational ambi-
guity by treating derivations as a latent variables
that are integrated out to get conditional proba-
bilities for translations
2
. The downside of this
2
In our translation model, coping with multiple deriva-
tions is not needed as there is just one derivation for each
word-aligned sentence pair. However, dealing with alterna-
tive word alignments might be beneficial, though as argued
1028
is that approximations are needed in computing
the maximum probability translation in decoding,
and also in computing model expectations in train-
ing when a language model is used. In addi-
tion, since the models operate directly on transla-
tions, using probabilistic training criteria for learn-
ing the model parameters is possible only if all
reference translations in the training data can be
generated by the model. In practice, this problem
can be circumvented by discarding the training
sentence pairs with unreachable reference transla-
tions, but this may mean a significant reduction in
the amount of training data (24% in (Blunsom et
al., 2008)).
Another closely related approach is the in-
dependently developed discriminative block bi-
gram prediction model presented in (Tillmann
and Zhang, 2007). This work proposes a global
phrase-based translation model very similar to
ours, but due to computational reasons, resorts to a
localized approximation thereof, and is restricted
to biphrases of length at most two. In (Liang et
al., 2006) a standard phrase-based model is aug-
mented with more than a million features whose
weights are trained discriminatively by a variant
of the perceptron algorithm. Reference reachabil-
ity is again a problem, and the method has not been
scaled up to use biphrase features directly.
3 The proposed translation model
3.1 Biphrase extraction
The biphrases used in Sinuhe are extracted from
the training data with the Moses phrase extraction
heuristics. The sentence-aligned training corpus S
is first word-aligned by running Giza++ in both
directions and then symmetrizing the alignments.
This maps the original aligned sentence pairs
(x, y) into word-aligned sentence pairs (x, a, y),
where a is a many-to-many alignment between the
words in x and y. Second, using a heuristic pro-
posed in (Och et al., 1999), all the aligned phrase
pairs (x
?
, a
?
, y
?
) satisfying the following criteria
are extracted: (1) x
?
and y
?
consist of consecutive
words of x and y, and both have length at most k,
(2) a
?
is the alignment between words of x
?
and y
?
induced by a, (3) a
?
contains at least one link, and
(4) there are no links in a that have just one end in
x
?
or y
?
. Each aligned training sentence (x, a, y)
thus generates a number of potentially overlapping
in (DeNero et al., 2006), the ambiguity in word alignment is
less prevalent than in phrase segmentation.
aligned biphrase features (x
?
, a
?
, y
?
). In our exper-
iments, we chose k = 7 which is the default in
Moses. Unlike in Moses, we do not map the
aligned biphrases (x
?
, a
?
, y
?
) back to non-aligned
biphrases (x
?
, y
?
).
To reduce the number of extracted biphrases, for
each source phrase x
?
, only biphrases (x
?
, a
?
, y
?
)
whose occurrence count is among the top K = 20
in the training data are retained (rank ties bro-
ken by including all biphrases with rank equal to
the limit K). For technical reasons related to our
dynamic programming algorithms, we also drop
biphrases whose source phrase begins or ends with
unlinked words. Finally, we drop all biphrases
that occur only once in the training data. This can
be motivated by a leave-one-out argument (cf the
derivation of Good-Turing estimates): Dropping
the biphrases that occur only once in the train-
ing data means that the feature representation for
a training sentence pair (see Section 3.2) contains
only biphrases that occur also in other training ex-
amples. Without the leave-one-out pruning, the
feature vectors for training sentence pairs would
be maximally dense, whereas such feature density
cannot be expected on test data. Our system can
also be used without the leave-one-out pruning,
but according to our preliminary experiments this
has little effect on translation quality. An excep-
tion seems to be morphologically rich languages
with scarce training data on which pruning seems
to reduce translation quality.
All the pruning steps combined reduce the
phrase table size considerably, but in our experi-
ments, millions of biphrases per language pair still
remain (2-4 million for Europarl data and over 95
million for GigaFrEn data).
3.2 Features
Our primary feature representation is a binary fea-
ture vector that indicates which aligned biphrases
in the phrase table occur in an aligned sentence
pair and where. More specifically, a source sen-
tence x aligned to a target sentence y by an align-
ment a is represented by a binary feature vector
?(x, a, y) whose component ?(x, a, y)
(x
?
,a
?
,y
?
),i
is
1 iff the aligned biphrase (x
?
, a
?
, y
?
) occurs at
source position i in (x, a, y), and 0 otherwise.
Here, (x
?
, a
?
, y
?
) occurs in (x, a, y) at source posi-
tion i iff the phrase extraction process described in
Section 3.1 would have extracted it from (x, a, y)
at source position i.
1029
The weights for the aligned biphrases are tied
together by mapping the binary feature vector ?
(indexed by pairs of an aligned biphrase and a
source position) to an integral feature vector
˜
? (in-
dexed by aligned biphrases only) using the for-
mula
˜
?
(x
?
,a
?
,y
?
)
=
?
i
?
(x
?
,a
?
,y
?
),i
. The “real” fea-
tures that drive the translation process are thus the
lowest level binary features ?, whereas the higher
level representation
˜
? is convenient in defining the
conditional probabilities given by the translation
model.
3.3 The model
Instead of modelling the conditional distribution
P (y|x) directly, we model the conditional dis-
tribution P (?(x, a, y)|x) by the following condi-
tional exponential model:
P (?(x, a, y)|x) =
exp(w ·
˜
?(x, a, y))
?
???
x
exp(w ·
˜
?)
.
Here, w is a parameter vector with one component
for each aligned biphrase feature in the phrase ta-
ble. The set ?
x
defines the set of possible predic-
tions given x, and includes all feature vectors ?
satisfying the following criteria:
1. There exists a translation y
?
and an alignment
a
?
such that all active features in ? occur in
(x, a
?
, y
?
)
2. Features corresponding to aligned biphrases
that occur inside aligned biphrases whose
features are active in ? are also active in ?.
Thus, the set ?
x
has a feature representation for
all possible aligned sentence pairs (x, a
?
, y
?
) that
have x as the source side, so all reference trans-
lations y
?
word-aligned to x in any way a
?
are
representable by features in ?
x
. By condition
1, the predictions given by the model never con-
tain conflicting biphrases, so given any prediction
of the model, there always exists a translation y
?
where all the predicted biphrases do occur. How-
ever, since our dynamic programming algorithms
can only force active super-phrases implying ac-
tive sub-phrases (condition 2) but not active sub-
phrases implying active super-phrases, the set ?
x
also contains some feature vectors in which the lat-
ter type of implications are not enforced. Having
such redundant representations for some transla-
tions is a waste of probability mass, but we hope it
has little effect in practice.
The choice of modelling P (?(x, a, y)|x) in-
stead of modelling P (y|x) directly is crucial, both
from a modelling and from a computational per-
spective. From the modelling perspective, the cru-
cial point is that in our approach, any aligned sen-
tence pair (x, a, y) has an associated feature vec-
tor ?(x, a, y) ? ?
x
that is reachable (i.e., re-
ceives non-zero probability) by the model. This
means it is straightforward to use probabilistic cri-
teria in learning the model parameters. In con-
trast, systems modelling P (y|x) directly are often
plagued by the reference reachability problem. To
use probabilistic training criteria for such systems
one needs to circumvent the reference reachability
problem, e.g., by using pseudo-references or by
dropping out the non-reachable portion of training
data.
Working with the feature vectors ?(x, a, y) in-
stead of working with a and y directly means that
we model the ordering and choice of words in y
only partially. This way, when computing the nor-
malizing constants and feature expectations, we
can partition the unbounded set of potential trans-
lations y and alignments a into a smaller set of
equivalence classes given by ?(x, a, y). Though
the number of feature vectors ? ? ?
x
may be
large (exponential in length of x), all the necessary
computations can be done exactly and efficiently
by dynamic programming. For more details, see
Section 3.4.3.
3.4 Learning the model parameters
3.4.1 The objective
We use maximum a posteriori (MAP) estimation
to estimate the model parameters w. To con-
trol overfitting, we regularize the parameters by a
suitably scaled Gaussian prior. This can be also
viewed as L2 regularization. The prior guarantees
that the MAP parameters are unique, and mod-
els our belief that the observed feature occurrence
counts randomly deviate from their “true” values
roughly proportionally to the standard deviations
of the occurrence count distributions. The prior
variance ?
2
(x
?
,a
?
,y
?
)
for feature (x
?
, a
?
, y
?
) is given
by the formula ?
2
(x
?
,a
?
,y
?
)
= ?/?
(x
?
,a
?
,y
?
)
, where
? > 0 is a free regularization parameter, and
?
2
(x
?
,a
?
,y
?
)
is an empirical estimate of the variance
of the occurrence count of (x
?
, a
?
, y
?
) in the train-
ing data. This is similar to (Chen and Rosenfeld,
2000), except that we use standard deviations in
place of variances. As the estimate for the vari-
1030
ance of a feature we use the occurrence count of
the corresponding biphrase in the training data.
This could be justified by assuming that the oc-
currence counts follow a Poisson distribution. We
have also run preliminary experiments with other
forms of regularization (different ways of comput-
ing ?
(x
?
,a
?
,y
?
)
, exponential priors corresponding to
L1 regularization, no regularization), and it looks
like the system is not very sensitive to the chosen
prior.
Combining the prior with the model, we see that
the negative log-posteriorL(w) is given by the for-
mula
?
(x
?
,a
?
,y
?
)
w
2
(x
?
,a
?
,y
?
)
2?
2
(x
?
,a
?
,y
?
)
?
?
(x,a,y)?S
logP (?(x, a, y)|x) + C,
where the sum over (x
?
, a
?
, y
?
) is understood to
go over all aligned biphrase features in the model.
This is our criterion for learning w.
3.4.2 Optimization
We solve the optimization problem related to
learning w by first order gradient ascent methods.
The gradient ?L(w) of L(w) with respect to w
can be written as
?
(x
?
,a
?
,y
?
)
w
(x
?
,a
?
,y
?
)
?
2
(x
?
,a
?
,y
?
)
?
?
(x,a,y)?S
[
˜
?(x, a, y)?E
w
[
˜
?|x]
]
,
where E
w
[
˜
?|x] denotes the conditional expecta-
tion of the aligned biphrase occurrence count fea-
tures given x with respect to model parameters w.
Feature expectations can be computed by combin-
ing the results of a left-to-right and right-to-left
dynamic programming sweep over the source sen-
tence. For more details, see Section 3.4.3.
Inspired by the empirical results in (Vish-
wanathan et al., 2006), we use classic stochas-
tic gradient ascent to solve the optimization prob-
lem. At each step t, we sample with replacement
a batch S
t
of b examples from S. We start from
w
0
= 0, and use the update rule
w
t+1
= w
t
? ?
t
?L
t
(w
t
), (1)
where ?
t
> 0 is the learning rate, and ?L
t
(w)
is the stochastic gradient of the negative log-
posterior
L
t
(w) =
|S
t
|
|S|
?
i
w
2
i
2?
2
i
?
?
(x,a,y)?S
t
logP (?(x, a, y)|x)
restricted to batch S
t
. The second term of the
stochastic gradient involves only biphrases whose
source sides match the source sentences in the
batch. Though the gradient of the regularizer is
non-zero for all non-zero biphrase features, the
updates of features that are not active in the sec-
ond term of the gradient can be postponed until
they become active again. Due to feature sparsity,
the number of features that are active in a small
batch is small, and thus also the updates are sparse.
Hence, it is possible to handle even feature vectors
that do not fit into memory.
Another advantage of the stochastic gradient
method is that many processes can apply updates
(1) to a weight vector asynchronously in parallel.
We have implemented two strategies for dealing
with this. The simpler one is to store the weight
vector in a database that takes care of the neces-
sary concurrency control. This way, no process
needs to store the entire weight vector in memory.
The downside is that all training processes must
be able to mmap() to the common file-system
due to limitations in the underlying Berkeley DB
database system. We have also implemented a
client-server architecture in which a server process
stores w in memory and manages read and update
requests to its components that come from train-
ing clients. In this approach, the degree of paral-
lelism is limited only by the number of available
machines and server capacity. The server could
be further distributed for managing models that do
not fit into the memory of a single server.
3.4.3 Computing gradients etc
The computationally most challenging part in
learning the model parameters is computing
? logP (?(x, a, y)|x), i.e., the vector of differ-
ences between the observed occurrence counts of
biphrase features in (x, a, y) and their conditional
expectations under the current model parameters.
The conditional feature expectations can be
computed by a dynamic programming procedure
similar to the one used in training conditional ran-
dom fields. We combine the results of a left-
to-right and right-to-left dynamic programming
1031
sweep over x. In the left-to-right sweep, we
have for each biphrase feature (x
?
, a
?
, y
?
), i a state
s
(x
?
,a
?
,y
?
),i
for translations starting from the be-
ginning of x and ending in an occurrence of the
biphrase (x
?
, a
?
, y
?
) at source position i. This state
records the contribution of all partial translations
whose right-most active biphrase feature on the
source side is (x
?
, a
?
, y
?
), i to the conditional ex-
pectation of feature (x
?
, a
?
, y
?
), i (in log scale).
The score for s
empty,0
= 0, and s
(x
?
,a
?
,y
?
),i
is ob-
tained from the recurrence
score
(
s
(x
?
,a
?
,y
?
),i
)
=
?
(x
??
,a
??
,y
??
),i
??
?A((x
?
,a
?
,y
?
),i)
[
score
(
s
(x
??
,a
??
,y
??
),i
??
)
+
?
(x
???
,a
???
,y
???
),i
???
?B
w
(x
???
,a
???
,y
???
),i
???
]
Here, A((x
?
, a
?
, y
?
), i) is the set of prede-
cessor states of s
(x
?
,a
?
,y
?
),i
and includes all
states s
(x
??
,a
??
,y
??
),i
?? such that a proper suffix of
(x
??
, a
??
, y
??
), i
??
(i.e., a biphrase whose source and
target are proper suffices of x
??
and y
??
respec-
tively) is equal to a prefix of (x
?
, a
?
, y
?
), i. As
a special case, A includes all states s
(x
??
,a
??
,y
??
),i
??
for which (x
??
, a
??
, y
??
), i
??
ends before or at posi-
tion i. This takes care of translation paths that
leave some words in x untranslated. Since the
starting position of a proper suffix of a biphrase
is always after the biphrase’s original starting po-
sition, going through the states in order of in-
creasing i guarantees that the scores for biphrases
in A((x
?
, a
?
, y
?
), i) are available when computing
score
(
s
(x
?
,a
?
,y
?
),i
)
.
The set B that depends on ((x
?
, a
?
, y
?
), i) and
((x
??
, a
??
, y
??
), i
??
) is defined by the formula
B = sub
(
(x
?
, a
?
, y
?
), i
)
\ sub
(
(x
??
, a
??
, y
??
), i
??
)
,
where sub ((x
?
, a
?
, y
?
), i) denotes the set of sub-
biphrases of (x
?
, a
?
, y
?
), i (including the biphrase
(x
?
, a
?
, y
?
), i itself). Thus, summing over the
weights of biphrases in B adds the contribution of
features introduced by extending translation paths
ending in (x
??
, a
??
, y
??
), i
??
by (x
?
, a
?
, y
?
), i
?
.
From the right-to-left dynamic programming,
we get analogously the contribution of right-to-
left partial translations whose left-most active
biphrase is (x
?
, a
?
, y
?
), i. The partition function
used for normalizing the expectations can be ob-
tained as a side product of either of the sweeps.
In conditional random fields, the (unnormal-
ized) expectations for the feature can be ob-
tained by multiplying the scores of the states
corresponding to the same feature in the left-
to-right and right-to-left dynamic programming
memories. In our case, combining the two val-
ues stored in the states for a feature (x
?
, a
?
, y
?
), i
only gives the contribution of the translation paths
where (x
?
, a
?
, y
?
), i is active but not covered by
any longer biphrase that extends (x
?
, a
?
, y
?
), i both
left and right. To include the contribution of
the remaining translation paths, we need to go
through states corresponding to super-biphrases of
(x
?
, a
?
, y
?
), i. Special care has to be taken in order
to include the contribution of all feature vectors
in which such super-biphrases are active exactly
once. An efficient way to do this is to process the
states for super-biphrases in topological order with
respect to biphrase inclusion, and to include only
the contributions of states for super-biphrases that
extend the previously included states both left and
right.
Another complication in the dynamic program-
ming is that a biphrase can extend the source side
of another overlapping biphrase to the right, but
the target side to the left, or visa versa. Such
overlaps are not directly covered by our dynamic
programming. To deal with them, we construct
new virtual biphrases that correspond to the re-
sults of such overlaps in a pre-processing step. The
number of such virtual combinations can in theory
grow exponentially, but in practice only a small
number of virtual biphrases seems to suffice.
3.5 Prediction with translation model alone
Prediction is done in two phases. First, we find (by
a dynamic programming procedure similar to the
one outlined in Section 3.4.3) the highest proba-
bility feature vector
ˆ
?(x) defined by
ˆ
?(x) = argmax
???
x
:x covered by biphrases in ?
P (?|x).
Note that we restrict the search to feature vec-
tors that cover the whole of x, i.e., to feature vec-
tors ?(x, a, y) in which each word in x is covered
by at least one active aligned biphrase (x
?
, a
?
, y
?
).
This forces the system to translate all words in the
source sentence even if the translation model pre-
dicts that none of the translations are very likely.
To translate words that are not covered by any
aligned biphrase feature in the model, we use the
following strategy: If the word is found from an
1032
optional out-of-vocabulary dictionary, we use the
translation from the dictionary, and otherwise re-
sort to an implicit zero weight aligned biphrase
that copies the input word to the output as is.
In our experiments, the out-of-vocabulary dictio-
nary is constructed from the word translations that
occur once in the training data, so the out-of-
vocabulary dictionary only compensates for the
word translations lost in phrase table pruning. If
available, a real dictionary could be used as well.
The second step in predicting a translation is
solving the pre-image problem, i.e., constructing
a translation y from the predicted feature vector
ˆ
?(x). Since
ˆ
?(x) ? ?
x
, there always exists an
alignment a and a translation y such that all the
aligned biphrases in
ˆ
?(x) occur in ?(x, a, y), but
the a and y may not be unique. We choose the a
and y given by concatenating the target sides of the
biphrases active in
ˆ
?(x) in the order induced by
their positions in the source sentence. Thus, there
is no phrase-level reordering, and the fluency of
the target language output is induced by the phrase
overlaps only.
3.6 Predicting with an integrated LM
The prediction strategy outlined in the previous
section is simple and conceptually clean. How-
ever, biphrase overlaps alone may not be enough to
enforce fluent output, especially given that bilin-
gual data is typically more scarce than monolin-
gual data. Also, the lack of a reverse transla-
tion model means the system is unable to iden-
tify phrase extraction errors in which rarely seen
source phrases are translated to common target
phrases by chance.
To address these shortcomings, we augment the
translation model with the following additional
features that have been observed to enhance trans-
lation quality in other SMT systems.
1. Language model: logP (y), where P (y) is
given by a smoothed n-gram language model
2. Lexical translation model (reverse direc-
tion): logP (x|y, a) given by a word-level re-
verse translation model
3. Translation length: number of words in y
4. Distortion: number of source words in
phrases with swapped translations
The final score driving the translation process
is given by a linear combination of the trans-
lation model score logP (?(x, a, y)|x) and these
features. Besides the translation model, the lan-
guage model feature is clearly the most influential,
while the lexical translation feature has only a mi-
nor positive effect on translation quality.
We use an approximate dynamic program-
ming variant of the commonly used beam search
procedure to find the highest scoring candidate
translation. We compute the translation model
log-probability logP (?(x, a, y)|x) incrementally
while building up the corresponding candidate
translation y and word alignment a from left to
right. We allow phrase-level distortions given
by swapping the order of translations of consec-
utive non-overlapping source phrases. Unlike in
Moses, our beam search is structured around state
transitions, not around states. This means that we
apply each biphrase (state transition) simultane-
ously to all applicable partial translations (states).
This strategy is in our experience more efficient,
does not rely on future score estimates, and is im-
plementationally very similar to the dynamic pro-
gramming procedures that we use in training the
model parameters and in prediction with a lan-
guage model alone.
The weights of the features are tuned by opti-
mizing the BLEU score of development set trans-
lations with amoeba search. This simplistic strat-
egy is feasible given our system’s fast translation
speed, and extends easily to cover non-linear fea-
ture combinations. The reason for using amoeba is
that it is simpler to implement—we do not believe
amoeba yields any better values for the parameters
in the end.
4 Experiments
4.1 Experimental setup
Our experiments are on the Europarl translation
tasks following the setup used in the shared trans-
lation task of the ACL 2008 Third Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2008), and on the French-to-English transla-
tion task of the EACL 2009 Fourth Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2009). The size of the Europarl training cor-
pora is about 1M sentence pairs per language pair,
while the larger GigaFrEn corpus contains about
22M sentence pairs. The corpora were used for
biphrase extraction and translation model training.
Decoder feature weights were tuned on the pro-
vided development sets. In case of Europarl, lan-
guage models were trained on the target sides of
1033
es-en en-es fr-en en-fr de-en en-de time
Sinuhe 31.38 30.94 31.50 28.91 25.03 19.26 338.0
Moses 32.18 31.88 32.63 29.92 27.30 20.57 3729.5
Sinuhe
trans
29.14 27.12 28.74 26.06 22.38 17.14 44.2
Moses
trans
24.32 22.75 23.84 21.22 19.62 13.59 1321.5
Table 1: Left: The translation quality of the SMT systems as measured by the BLEU score. Translations
were detokenized but not recased before evaluating their quality against lowercased reference translations
by the mteval-v11b.pl script. Right: Average total translation time in seconds.
the bilingual corpora. In the GigaFrEn experi-
ments we used the provided monolingual news do-
main data. All data was tokenized and lowercased
using the tools in the Moses distribution.
We experimented with four translation sys-
tems: Sinuhe
trans
, Sinuhe, Moses
trans
, and
Moses. Sinuhe
trans
uses only the translation
model in producing translations (see Section 3.5),
while the full system Sinuhe uses also a lan-
guage model and some additional features (see
Section 3.6). As a baseline, we used the Moses
translation system, which is known to be very
competitive on the Europarl translation tasks as
evidenced by the University of Edinburgh entries
in the translation challenge (Callison-Burch et al.,
2008). The other comparison point Moses
trans
was obtained from Moses by disabling distortions
and setting setting the weights of all features ex-
cept the forward translation model to 0. By com-
paring Sinuhe
trans
and Moses
trans
, we hope to
indirectly compare the performance of the under-
lying translation models. A more direct compar-
ison was not possible as it is not feasible to nor-
malize the “probabilities” predicted by the Moses
translation model.
4.1.1 Training the models
We trained Moses exactly as suggested
in (Callison-Burch et al., 2008), except that
we used the -unk option for SRILM in training
the language models (both for Sinuhe and
Moses). The translation model for Sinuhe
(and Sinuhe
trans
) was built from the phrases
extracted by Moses as described in Section 3.1.
We chose ? = 1.0 and set batch size to 1. The
learning rate was initially set to 0.1, and decayed
proportional to 1/t after 2M or 100M iterations
of training for Europarl and GigaFrEn tasks,
respectively. These choices may not be optimal
as we did not experiment with other choices
yet. In case of Europarl, training was run for
70-100M iterations using the Berkeley DB based
distribution strategy (4 CPU cores per language
pair). This took 10 days. For GigaFrEn, we
used the client-server architecture, and trained the
model for 620M stochastic gradient iterations on
about 200 CPUs. This took 2 days, which is a lot
less than the time needed to run (parallel) Giza
on this data. The number of biphrase features in
Sinuhe’s model was 2-4 million on the Europarl
tasks, and about 95 million on the GigaFrEn task.
The decoder parameters for Sinuhe were
tuned on the development sets by amoeba, and for
Moses by MERT. As both amoeba and MERT try
to solve the same optimization problem, we be-
lieve the difference in optimization methods has
little influence on the results.
4.1.2 Translation results
Europarl tasks The systems were tested on
the 2000 sentence Europarl domain develop-
ment test sets provided for the shared translation
task (Callison-Burch et al., 2008). The resulting
BLEU scores and total translation times averaged
over the datasets are reported in Table 1. While
Moses has the highest BLEU score for all the
language pairs, the BLEU score for Sinuhe is
worse by only at most 1.31 BLEU points except
on the de-en task, where the difference is 2.27.
Sinuhe
trans
is clearly inferior to Sinuhe but
equally clearly superior to Moses
trans
.
It takes less than a minute to translate the
development test set by the fastest system
Sinuhe
trans
. The slowest system Moses needs
around an hour for the same task. Memory usage
follows a similar pattern. For example, Sinuhe
requires roughly one tenth of the memory used
by Moses. Thus, in terms of resource usage,
Sinuhe
trans
and Sinuhe seem clearly supe-
rior to Moses. The quantitative results would
change if the systems’ parameters were optimized
for speed rather than quality, but the differences
1034
are so clear that the general pattern would proba-
bly remain the same. For example, Moses with
no distortion is still clearly slower than Sinuhe.
GigaFrEn task The fr-en model was tested on
the 2525 sentence news domain test data used in
the preliminary evaluation of the translation chal-
lenge results. The BLEU scores for Sinuhe and
Moses were 26.32 and 26.98, respectively. The
total translation time was 13m 50s with Sinuhe,
whereas Moses needed 82m 28s. Thus, the pat-
tern that was observed on Europarl tasks is re-
peated here: The translation quality of Moses is
slightly better, but Sinuhe is significantly faster.
Surprisingly, both Sinuhe and Moses fare well
in comparison to the participants of the actual
challenge: According to the preliminary results
on the same test data we used (Koehn, 2009),
the Moses baseline would have been beaten only
by Google, and Sinuhe would have been sixth
among the 23 participating systems with a differ-
ence of only 0.57 BLEU points to the second best
entry. A partial explanation for the good relative
performance could be that the challenge partici-
pants had only a week to train their models on
the full version of GigaFrEn data, so they may not
have had time to take full advantage of it. On the
other hand, many of the top ranked systems relied
on external resources that were not available for
us.
Based on an informal human evaluation of the
outputs of Sinuhe and Moses, it looks like the
translations of Sinuhe are slightly more accu-
rate in conveying the meaning of the original sen-
tences, but especially the translations of long rare
expressions (e.g., multi-word names of institu-
tions) are less fluent. This hints that the parame-
ters for (rare) biphrases may have been regularized
too heavily — it looks like Sinuhe is underfitting
rather than overfitting. We will conduct more ex-
periments to see how much the translation quality
can be improved by a better choice of ? or by us-
ing a different prior for regularization. Of course,
there is room for tuning elsewhere, too. For exam-
ple, it would be a surprise if the phrase extraction
pipeline that has been optimized for Moseswould
be optimal for Sinuhe.
5 Conclusions
In this paper, we have shown that phrase-based
SMT can be viewed as an instance of structural
prediction. The word alignment and phrase ex-
traction heuristics serve as a strategy for feature
extraction, and the translation task can be mod-
elled as a structural prediction problems over these
features. Our methods scale to large corpora and
are fast at predicting translations. While speed is
not the primary goal, the faster translation times
may be a key to success in applications where the
amount of text that needs to be translated is large.
In terms of BLEU scores, the results do not im-
prove the state-of-the-art so far. However, fine-
tuning the standard phrase-based approach over
the years has increased its performance signifi-
cantly, and we see no reason why the same would
not happen with the proposed approach, especially
if the model is augmented with additional features
like gapped biphrases and biphrases over POS
tags.
The fact that our translation model is a prop-
erly normalized conditional probability distribu-
tion opens up many new possibilities. For in-
stance, instead of predicting translations, it is pos-
sible to efficiently compute the expected num-
ber of times each word would appear in them.
Such output might be useful, e.g., if the transla-
tions are to be post-processed by models relying
on bag-of-words representation. Another research
direction we are currently looking into is train-
ing the proposed translation model in the reverse
direction, and then predicting translations using
the noisy channel approach, i.e., by maximizing
P (x|y)P (y). The key difference to previous work
here is that since P (x|y) is properly normalized,
the noisy channel approach would not in our case
suffer from the potentially negative effects caused
by ignoring the normalizer that depends on y. Be-
sides being a viable (though computationally de-
manding) alternative criterion for predicting trans-
lations, the noisy channel approach could easily be
used for, e.g., reranking n-best lists and for system
combination.
Acknowledgments
This work has been partially funded by the
SMART EU project. We wish to thank the anony-
mous reviewers for their constructive comments
and especially for pointing out the related pa-
per (Blunsom and Osborne, 2008) that we had
missed. Special thanks to Vladimir Poroshin for
all the bugs he found in beta-testing and to Esther
Galbrun for her comments on a draft version of
this paper.
1035
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In EMNLP.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Josh Schroeder, and Cameron Shaw Fordyce.
2008. ACL 2008 third workshop on statistical ma-
chine translation. http://www.statmt.org/
wmt08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. EACL 2009 fourth
workshop on statistical machine translation. http:
//www.statmt.org/wmt09.
Stanley Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions Speech and Audio Processing,
8(1):37–50.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In ACL.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models un-
derperform surface heuristics. In Workshop on SMT
at NAACL.
John DeNero, Alex Bouchard, and Dan Klein. 2008.
Sampling alignment structure under a bayesian
translation model. In EMNLP.
Matti K¨a¨ari¨ainen. 2009. Sinuhe source code dis-
tribution (v1.2). Website. http://www.cs.
helsinki.fi/u/mtkaaria/sinuhe.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, and
Chris Callison-Burch et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Philipp Koehn. 2009. BLEU/NIST scores for sub-
missions. http://groups.google.com/
group/WMT09/browse thread/thread/
bfbce7b219648a4c.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In ACL.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:2003.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statisti-
cal machine translation. In EMNLP, pages 20–28.
Cristoph Tillmann and Tong Zhang. 2007. A block bi-
gram prediction model for statistical machine trans-
lation. ACM Transacions on Speech and Language
Processing, 4(3).
S. V. N. Vishwanathan, Nicol N. Schraudolph, MarkW.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML.
1036
