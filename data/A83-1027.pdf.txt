AN APPL ICAT ION OF MONTAGUE GRAMMAR TO ENGL ISH- JAPANESE MACHINE TRANSLAT ION 
Toyoak i  N ISH IDA and  Shu j i  DOSHITA  
Dept .  of In fo rmat ion  Sc ience  
Facu l ty  of  Eng ineer ing ,  Kyoto  Un ivers i ty  
Sakyo-ku ,  Kyoto  606, JAPAN 
ABSTRACT 
English-Japanese machine translation 
requires a large amount of structural transfor- 
mations in both grammatical and conceptual level. 
In order to make its control structure clearer 
and more understandable, this paper proposes a 
model based on Montague Gramamr. Translation 
process is modeled as a data flow computation 
process. Formal description tools are developed 
and a prototype system is constructed. Various 
problems which arise in this modeling and their 
so lu t ions  are  descr ibed .  Resu l t s  of exper iments  
are shown and it i s  d i scussed  how far initial 
goals are achieved. 
I .  GOAL OF INTERMEDIATE REPRESENTATION DESIGN 
Differences between English and Japanese 
exist not only in grammatical level but also in 
conceptual level. Examples are illustrated in 
Fig.l. Accordingly, a large amount of transfor- 
mations in various levels are required in order 
to obtain high quality translation. The goal of 
this research is to provide a good framework for 
carrying out those operations systematically. 
The solution depends on the design of intermedi- 
ate representation (IR). Basic requirements to 
intermediate representation design are listed 
below. 
a) Accuracy: IR should retain logical conclu- 
sion of natural language expression. The follow- 
ing distinctions, for example, should be made in 
IR level: 
- partial/total negation 
- any-ex is t /ex i s t -any  
"- act ive/pass ive  
- restrictive use/ nonrestrictive use, etc. 
In other words, scope of operators should be 
represented precisely. 
GRAMMATICAL difference 
a) Case Marking: 
<E>: (relative position) + preposition 
<J>: postposition (called JOSHI) 
b) Word Order 
\[) simple sentence 
<E>: S+V+O : ~ a ~  
<J>: s÷o÷v : WAT-ASHi ~ O  ~f f~ET~ 
ii) preposition vs postposit\[on 
<E>: PREP+NP : ,in, Lthe refrigerator, 
<J>: NP÷JOS~I : q ~ - { I ? N ~  
iii) order of modification 
<E>: NP÷POSTMODIF\[ER: an apple on the box, 
<J>: PRENOMINAL MODIFIER+NP: HAKO NO UE NO RINGO 
LEXICAL difference 
<E> <J> 
translate HONYAKU SURU 
interpret ~ KAISHAKU SURU 
understand RIKAI SURU 
grasp / TSUKAMU 
hold ~ TAMOTSU 
keep MAMORU 
. , ,  
CONCEPTUAL difference 
<E2 her arrival makes him happy 
~.. \[s needed paraphrasing 
<j> KARE WA KANOJO GA TOUCHAKU SHITA NODE 
URESHII. 
(he becomes happy because she has arrived) 
Fig.l. Examples of Differences between English 
and Japanese. 
<E>: English; <J>: Japanese. 
156 
b) Ab i l i ty  of represent ing  semantic re la t ions :  
In  Eng l i sh - Japanese  t rans la t ion ,  i t  is of ten  the 
case that  a g iven Engl ish  word must be t rans la ted  
in to  d i f fe rent  Japanese words or  phrases  i f  i t  
has more than one word meanings. But i t  is  not 
reasonable to capture th is  problem so le ly  as a 
problem of word meaning d isambiguat ion  in ana ly -  
s i s  phase; the needed depth of  d i samb£iuat ion  
depends on ta rget  language.  So i t  i s  a l so  
handled in t rans fer  phase.  In genera l ,  meaning 
of  • given word is  recognized based on the re la -  
t ion  to o ther  const i tuents  in the sentence or 
text  vhicb is semant ica l ly  re la ted  to the given 
word. To make th i s  poaslble in t rans fer  phase,  
IR must provide a l ink  to semant ica l ly  re la ted  
const i tuents  of a g iven item. For example, an 
ob jec t  of a verb should be access ib le  in IR leve l  
from the verb,  even i f  the re la t ion  is  imp l i c i t  
~n the sur face  s t ruc ture  (as . ,  pass ives ,  re la t ive  
claus=a, and the i r  combinations,  e tc . )  
¢) Pred ic t ion  of cont ro l :  g iven an IR expres -  
s ion ,  the model should be able to pred ic t  
exp l i c i t I y  what operat ions  are co be done in what 
o rder .  
d) Lexicon dr iven:  some sor t  of  t rans forma-  
t ion  ru les  ere word spec i f i c .  The IR in terpreta -  
t ion  system should be designed Co deal with those 
word spec i f i c  ru les eas i ly .  
e) Computabi l i ty :  A l l  processing= should be 
e f fec t ive ly  computable. Any IR is useless i f  i t  
is not computable. 
2. PRINCIPLE OF TP, ANSLATION 
This sect ion  out l ines  our so lu t ion  Co the 
requ i rements  posed in the preceding sect ion .  We 
employ MonCague Gram=mr (HonCague 1974, Dowry 
1981) as a theoret i ca l  bas i s  of t rans la t ion  model. 
I n te r~ed la te  representat ion  is designed based on 
i n tens iona l  logic. I n termed ia te  representat ion  
for  a given natura l  language expression is 
obtained by what we call functional analysis. 
2.1 Funct ional  Analys is  
In funct iona l  ana lys i s ,  input sentence is 
decomposed into groups of const i tuents  and 
in ter re la t ionsh ip  among those groups are analyzed 
in terms of function-argument relationships. 
Suppose a sentence: 
I don't have a book. (l) 
The funct iona l  analys is  makes fo l low ing  two 
po ints :  
a) (L) is decomposed as: 
" I  have a book" ÷ "nOt". (2) 
b) In the decomposi t ion (2) ,  "not"  is  an 
operator  or funct ion  co " I  have a book." 
The resu l t  of  th is  analys is  can be depicted as 
fo l lows:  
~ ""I have a book" I (3) 
wherel >denotes a function and\[ Idenotes 
en argument. The ro le  of  "not"  as a funct ion  i s :  
"not"  as a semantic opers tor :  
i t  negates a given propos i t ion ;  
"not"  i s  a syntact i c  operator :  
i t  inserts an appropr ia te  auxiliary verb 
and = l ex ica l  i tem "not"  i n to  appropr ia te  
pos i t ion  of i t s  argument.  (4) 
This kind of analysis goes on further with 
embedded sentence unt i l  i t  is  decomposed into 
lex ica l  un i ts  or even morphemes. 
2.2 Montague Grammar as a Basic Theory 
Montague Grammar (MG) gives a basis of func- 
tlonel analysis. One of the advantages of MG 
cons ists  in its interpretation system of function 
form (or intensional logical form). In MG, inter- 
pretation of an intenelonal logical formula is a 
mapping I from incenaional logical formulas to 
set  theoret i ca l  domain. Important proper ty  is 
chat this ampping I is defined under the cons- 
trainC of compositlonality, that is, I satisfies: 
Z\ [ f (a ,b  . . . .  ) \ ] ' I \ [ f l (Ha \ ] ,Z \ [b \ ]  . . . .  ) ,  (5) 
w i thout  regard to what f ,  a, b, e tc .  are.  This 
property s impl i f ies  control structure and it also 
spec i f ies  what operat ions are done in what order .  
For example, suppose input data has a s t ruc ture  
l i ke :  
A 
For the sake of  property  (5) ,  ~he in terpreta t ion  
of (6) is done as a data flow computation process 
as followa: 
A ~I \ [A \ ]  , | 
A "I 
I t s  c O } 
~7) 
By th is  p roper ty ,  we can eas i ly  grasp the process- 
ing stream. In par t i cu la r ,  we can eas i ly  ~hooc 
t rouble and source of  abnormal i ty  when debugging 
a system. 
Due to the above property  and others ,  Ln 
par t i cu la r  due to i ts  r igorous framework based .)n 
Logic, MG has been studied in ~nformation science 
f ie ld  (Hobbs 1978, Friedman |978, Yonezaki \[980, 
157 
Nish ida 1980, Landsbergen 1980, Moran 1982, Moore 
1981, Rosenschein 1982, . . . ) .  Application of MG 
to machine t rans la t ion  was a lso  attempted 
(Hauenschild 1979, Landsbergen 1982), but those 
systems have only partially utilized the power of 
MG. Our approach attempts to utilize the full 
power of MGo 
2.3 Application of Montague Grammar to 
Machine Translation 
In  o rder  to obta in  the syntact i c  s t ruc ture  
in Japanese from an in tens iona l  log ica l  form, in 
the same way as in terpretat ion  process  of MC, we 
change the semant ic  domain from set  theoret i ca l  
domain to conceptua l  domain for  Japanese .  Each 
conceptual unit contains its syntactic expression 
in Japanese. Syntactic aspect is stressed for 
generating syntact i c  structure in Japanese. 
Conceptual information is utilized for semantic 
based word choice end paraphras ing .  
For example,  the fo l low ing  funct ion  in 
Japanese syntact i c  domain is  ass igned to • 
logical i tem "not": 
(LAMBDA (x) (SENTENCE x \[AUX "NAI"\])). (8) 
3.1 Definition of Formal Tools 
e) English oriented Formal Representation (EFR) 
is a version of intensional logic, and gives a 
rigorous formalism for describing the results of 
functional analysis. I t  is based on Cresswell's 
lambda deep s t ructure  (Cresawel l  1973). Each 
express ion  has a un ique ly  def ined type.  Lambda 
form is  employed to denote funct ion  i t se l f .  
b) Conceptual  Phrase St ructure  (CPS) is  a data 
s t ruc ture  in which syntact i c  and semant ic  in fo rma-  
t ion  of a Japanese lex ice l  un i t  or phrase s t ruc -  
tu re  are  packed. 
i) example of CPS for a lexical item: 
EIGO:\[NP "EIGO" with,ZSAmLANGUAGE; ...,\] (9) 
category; lexical item; conceptual info. 
; "EIGO" means English" language. 
ii) example of CPS for phrase structure: 
\[NP \[ADJ "AKAI" with ... \] 
\[NOUN "RINGO" with ... \] with ... \] (i0) 
Trans fer -generat ion  process  for  the sentence  (1) 
looks l i ke :  
"I don't have a book" 
~ ' , , I  have a book" I 
// 
• TRANSFER / 
(LAMBDA (x) 
{SENTENCE x \[AUX "NAI"\]}) 
TRANS FE R, GENE RAT I ON 
S 
WATASHI-WA HON-WO MOTSU ,,-..._./ 
S 
S AUX 
WATASHI-WA HON-WO MOTSU NAI 
MOTANAI 
; "AKAI" means red, and "RINGO" means apple. 
c) CPS Form (CPSF) is a form which denotes 
operation or function on CPS domain. It is used 
to give descriptions to mappings from EFR to CPS. 
Const i tuents of CPSF are: 
i) Constants: CPS. 
ii) Variables: x, y, ... . 
(indicated by lower case strings). 
iii) Variables with constraints: 
e.g., (! SENTENCE x). 
; variable x which must be 
of category SENTENCE. 
iv) Transformations: 
e.g., (? TENSE (TENSE-PAST~ x). 
indicator; operator-name; PARAMs; argumen~ 
v) CPS construction: 
e.g., <SENTENCE (x y) with ... 7. 
/ \ 
new category; descendents 
vi) Conditionals: 
\[ <condition> I -> <CPSF>I; ... \]. 
vii) Lambda form: 
e.g., (LAMBDA (x) (+ PASSIVE () x)) 
Using those description tools, translation 
process is modeled as a three staged process: 
3. FORMAL TOOLS 
Formal description tools have been developed 
co provide a precise description of the idea men- 
tioned Ln the last section. 
stage I (analysis): anlyzes English 
sentence and extracts EFR form, 
stage 2 (transfer): substitutes CPSF to 
each lexical item in the EFR form, 
158 
not 
) 
+NEG 
He does not always cOme late. 
always ( he ( late ( comes ) ) ) ) 
lq, kp 
S 
\[p< ~llIrF+V I A~V ,rll,l xIN\[p4 P IIII 
L IJl L 
itS \ Z.a'¢# z comes  
.A. 
L?e22::'J 
ADV 
aZzoays ~=e comes ZaCe 4/: /.s not; Cite case  thuC 
~cotr4s ~Ce 
. .  EFR  . .  
~,RAN$FER) 
.. CPSF .. 
• . CPS  . .  
Fig.2. Example of Translation Process / /  Prefix notation is used for CPSF, 
described using Formal Tools. / and syntactic aspect is emphasized. 
stage 3 (generation): evaluates the CPSF to 
get CPS; generation of surface s t ructure  
from CPS is straightforward. 
In order to give readers an overa l l  pers-  
pect ive ,  we i l l us t ra te  an example in F ig .2 .  
Note that the example i l l us t ra ted  includes 
par t ia l  negat ion.  Thus operator  "not"  is 
given a wider scope than "always". 
In the remaining part of this section 
we will describe how to extract EFR expression 
from a given sentence. Then we will discuss the 
problem which ar ises in eva luat ing CPSF, and give 
its possible solution. 
3.2 Extracting EFR Expression from Input Sentence  
Rules for translating English into EFR form 
in .~ssociated with each phrase structure rules. 
159 
For example, the rule looks l l ke :  
NP -> DET+NOUN where <NP>-<DET>(<NOUN>) (ii) 
where, <NP> stands for an EFR form assigned tu 
~he NP node, etc. Rule (II) says chat EFR for an 
NP is a form whose function section is EFR for a 
DET node and whose argument sect ion is  EFR for  a 
NOUN node. This ru le  can be incorporated into 
convent iona l  natural language parser.  
3.3 Evaluation of CPSF 
Evaluation process of CPSF is a sequence of 
lambda conversions and tree ~ransformations. 
Evaluation of CPSF is done by a LISP ~ncerpreter- like algorithm. Aproblem which we call higher 
order problem arose in designing the evaluation 
algorithm. 
Higher Order Problem 
By higher order property we mean that there 
exist functions which take other functions as 
arguments (Henderson 1980). CPSF in fact has 
this property. For example, an adjective "large" 
i s  modeled as a funct ion  which takes  a noun as 
i t s  a rgument .  For example ,  
la rge(database) ,  
"large database"  (12) 
On the o ther  hand,  adverbs  a re  modeled as 
functions to ad jec t ives ,  For example ,  
very(large), extremely(large), 
comparatively(large), etc. (13) 
The difficulty with higher order functions 
consists in modifiction to function. For explana- 
tion, l e t  our temporal goal be regeneration of 
English from EFR. Suppose we assign to "large" a 
lambde form like: 
(LAMBDA (x) (NOUN \[ADJ "LARGE"\] x>) (14) 
which takes  a noun and re turns  a complex noun by 
a t tach ing  an ad jec t ive  " la rge" .  I f  the ad jec t ive  
is modified by an adverb, say "very", we have to 
modify (14); we have to transform (14) into a 
lambda form like: 
(LASBDA (x) 
(NOUN \[ADJ \[ADV "VERY"\] 
\[ADJ "LARGE"\]\] x}), (15) 
which attaches a complex adjective "very large" 
to a given noun. As is easily expected, it is 
too ted ious  or even imposs ib le  to do th i s  task  
in  genera l .  Accord ing ly ,  we take  an a l te rnat ive  
ass ignment  ins tead  of (14) ,  namely:  
large <- \[ADJ "LARGE"\]. (16) 
Since this decision cuases a form: 
\[ADJ "LARGE"\](\[NOUN "DATABASE"\]), (17) 
to be created in the course of evaluation, we 
specify what to do in such case. The rule is 
defiend as follows: 
\[ADj\](\[NOUN\]) - \[NOUN \[ADJI \[NOUN\]\]. (18) 
~y\[(the(table)) 
(Ax\[(((*ap(on))(y))(block))(x)\])\], (20)
; which may read: is y:\[there is a uniquely 
specified ob ject  y referred to by an NP "the 
table", such that y is a block which is 
res t r i c ted  to be located on x . \ ]  
This lambda form is too complicated for tree 
transformation procedure to manipulate. So it 
should be transformed into equivalent CPS if it 
exists. The type of the lambda form is known 
from the context, namely one-place predicate. So 
if we apply the lambda form (20) to "known" 
entity, say "it", we can obtain sentence struc- 
ture like: 
SENTENCE 
UN PRED 
NOUN 
l 
NP NP JOSHI I / ' , ,  /'x I 
SORE WA TSUKUE NO UE NO BLOCK DEARU 
it a block on the ~able is 
(it is a block on the table) (21) 
From this result, we can infer that the lambda 
form (20) is equivalent to a noun: 
NOUN 
MODIFIER NOUN 
NP f  SN, I 
TSUKUE NO UE NO BLOCK 
(block on the table) 
(22) 
The extraction rule can be written as a pattern 
matching rule like: 
SENTENCE 
NP NP PRED 
I \ 
SORE WA x:NOUN DEARU 
(It is ~ z) 
x 
(23) 
This rule is called an application rule. 
In general, evaluation of \[ambda form 
itself results in a function value (function as a 
value). This causes difficulty as mentioned 
above. Unfortunately, we can't dispense with 
lambda forms; lambda variables are needed to link 
gap and its antecedent in relative clause, verb 
and its dependants (subject, object, etc), pre- 
position and its object, etc. For example, in 
our model, an complex noun modified by a PP: 
"block on the table" (19) 
£s assigned a following EFR: 
Of course, this way of processing is not 
desirable; it introduces extra complexity. But 
this i s  a trade off of employing formal seman- 
tics; the same sort of processing is also done 
rather opaque procedures in conventional MT 
system. 
4. MODELING TRANSLATION PROCESS 
This section illustrates how English- 
Japanese translation process is modeled using 
formal tools. Firstly, how several basic 
linguistic constructions are treated is described 
and then mechanism for word choice is presented. 
160 
4.1 Trans la t ing  Basic Const ruct ions  of English 
a) Sentence: sentence cons is ts  of an NP and a 
VF. VP is  analyzed as a one-p lace  pred icate ,  
which const ructs  a propos i t ion  out of an ind iv i -  
dual re fer red  Co by the sub jec t .  VP i s  fu r ther  
decomposed in to  in t rans i t ive  verb or  c rana l t ive  
verb + ob jec t .  In t rans i t ive  verbs and t rans i t ive  
verbs ere analyzed as one-p lace  pred icates  and 
two-place pred icate ,  respect ive ly .  One-place 
pred icate  and two-p lace pred icate  are ass igned a 
CFSF function which generates a sentence ouc of 
an ind iv idua l  and chat which generates  a sentence 
out  of a pa i r  of ind iv idua ls ,  respect ive ly .  Thus, 
a t rans i t i ve  verb "const ructs"  i s  ass igned a CPSF 
form: 
(LAMBDA (x y) 
(SENTENCE 
(÷ CASE-MAR/~R (CASE=AGENT) x) 
(+ C~SE-MARi~R (CASE=OBJ) y) 
\[ FRED ICATE \[ VERB "SAKUSEI-SURU" \] J )), (24) 
; given two i nd iv idua ls ,  th i s  funct ion  a t taches  
co each argument a case marker (cor respond ing  
to JOSHI or  Japanese post f ix )  and then gener -  
ates a sentence s t ruc ture .  
The ass ignment  (24) may be extended l a te r  to 
incorporate  word choice mechanism. 
Treatment of  NP in MonCague-besed semant ics  
i s  s ign i f i cant  in  chat EFR express ion  fo r  an NP i s  
g iven a wider scope then Chat for  a VP. Thus the 
EFR form for  an ~P-VP const ruct ion  looks l lke:  
<~>(<w>) ,  (25) 
where <x> means EFR form for  x, x=NP, . . .  . 
The reason is  Co provide an appropriate model for  
English quantifier which is syntact i ca l ly  local 
but semant ica l ly  g loba l .  For example, f i r s t  
order  logical form for a sentence:  
"this command needs no operand" (267 
looks Like: 
nor(there-exists x 
\[needs("chis-command",x) & 
operand(x) \ ] ) ,  (27) 
where operator "not", which comes from a deter- 
miner "no", is given a wider scope than "needs". 
This translation is straightforward in our model; 
the following EFR is extracted from (26): 
( th i s ( round) )  
Ax \ [ (no(operand) ) ( ly \ [needs(x ,y ) \ ] ) \ ] ) .  (28) 
\[f we make appropriate assignment including: 
no <= (LAMBDA (p) 
(LAMBDA (q) 
"nor ( there  ex is ts  x 
\ [p(x)  & q(x)\])")), (29) 
we can get (27) from (28). 
161 
In  Eng l l sh - Japanese  -,-'chine t rans la t ion ,  
th i s  t reatment  gives an e legant  so lu t ion  to the 
: rana la t ion  of prenominal  negat ion ,  par t ia l  nega- 
t ion ,  etc. Since Japanese language does not have 
a synCactlc device for prenominal negation, "no" 
must be translated into asainly two separate 
constituents: one is a RENTAISHI (Japanese decer- 
miner)  and another  is an aux i l i a ry  verb of  nega-  
tion. One possible assignment of CFSF looks like: 
no <= (LAMBDA (p) 
(U~NgDA (q) 
( ,  NEG () 
(q (~  "DONNA" (t NOUN p) " ,~0") ) ) ) ) .  
(30) 
In  genera l ,  correspondence of ~P and ind iv i -  
dual is ind i rec t  in EFR. The assoc ia t ion  of an 
NF with i t s  re ferent  x is i nd icated  as fo l lows :  
<~>(Ix{ ... x ... ;). 
i',enCence type 
one-p lace  predlcaCe type 
; <NP> stands  for  EFR express ion  for  NP. 
(31) 
Most of ocher  NP's cor respond co ice 
re ferent  more d i rec t ly .  The app l i ca t ion  ru le  
re f lec t ing  this fact is: 
\[NFJ(\[O~-eU~CE-PREDI) - \[ONE-PU~CE-FREOI(\[NP*\]), 
(32) 
where,  ix\] s tands  fo r  a CPS for  x. 
b) Internal structure of NP: the below illus- 
trates the s t ruc ture  of EFR express ion  ass igned 
CO an NP: 
<DET>(<MOD\[FIER>(.. .(<MDDIFIER>(<NOUN>)) ...)). 
(33) 
By <MOD£FIER> we mean mod i f i ca t ion  to noun by 
ad jec t ives ,  p repos i t iona l  phrases, in f in i t i ves ,  
present/past par t i c les ,  e tc .  The t rans la t ion  
process is determined by a CPSF assigned co <DET>, 
En cases of "the" or "a/an", translation process 
is ab ic  compl icated.  Et is almost the same as 
the process described in deta i l  in sect ion  3: 
firstly the <MODIFIER>s and <NOUN> are applied Co 
an individual like "the chinE" (the) or "some- 
chinE" (a/an) and a sentence will be obtained; 
then a noun structure is extracted and appro- 
priate RENTAISHI or Japanese determiner is 
attached. 
c) Other cases: some ocher cases are illust- 
rated by examples in Fig.3. 
4.2"Word Choice Mechanism 
• In order to obtain high quality translation, 
word choice .~chanism must be incorporated at 
least  for  handling the cases l i ke :  
i) subordinate clause: 
"When SI, S2" 
& 
(when (<SI >) ) (<$2>) 
"TOKI" \[$I\] 
\[\[SI\] "TOKI 's\] \[$2\] 
\[\[Sl\] "TOKI" \[S2\]\] 
2) tense, aspect, modal: 
"I bought a car" 
did(<I buy a car>) 
"TA" "WATASHI-WA JIDOUSHA-WO KAU" 
"WATASHI-WA JIDOUSHA-WO KAU TA" 
KATTA 
3) passive: 
" ... is broken ... " 
& 
... en(break) ... 
C~x~ ,,.GA,, y ,,.WO KOWASU ,,} .... 
~y{y "-GA KOWA SARERU" } .... 
; function "en" transforms a CPSF for 
a transitive verb into intransitive. 
4) interrogative: 
"Do ~ou have a car?" 
#ques(whether(<you have a car>)) 
+MKSENTENCE "KADOUKA .... ANATA-WA JIDOUSHA-WO MOTSU" 
-WA JIDOUSHA-W0 MOTSU-KADOUKA" 
"ANATA-WA JIDOUSHA-WO MOTSU-KA" 
"Which car do you have?" 
#ques((which(car))(~y\[<you havey>~)) 
+MKSENT~NCE I . . . . . .  { 
(Xp{p("DON0-JIDOUSHA) KA } I , ,, 
kk,,  .wA, y .wo 
~IDOUSHA-W0 MOTSU-KA" 
"ANATA-WA DONO-JIDOUSHA-WO MOTSU-KA" 
; indirect question is generated first, then it is 
transformed into a sentence. 
Fig.3. Examples of Translation of Basic English 
Construction. <x>, {x}, \[x\] and "x" stand 
for EFR for x, CPSF for x, CPS for x, and 
CPB for Japanese string x, respectively. 
verb in accordance with its object or its agent, 
adjective-noun, 
adverb-verb, and 
preposition. 
Word choice is partially solved in the analysis 
phase as a word meaning disambiguation. So the 
design problem \[s to determine to what degree 
word sense is disamblguated in the analysis phase 
and what kind of ambiguities is left until 
transfer-generation phase. Suppose we are to 
translate a given preposition. The occurence of 
a preposition \[s classified as: 
(a) when it is governed by verbs or nouns: 
(a-l) when governmant is strong: 
e.g., study on, belong to, provide for; 
(a-2) when govern.ment is weak: 
e.g., buy ... at store; 
(b) otherwise: 
(b-I) idiomatic: 
e.g., in particular, in addition; 
(b-2) related to its object: 
e.g., by bus, with high probability, 
without÷ING. 
We treat (a) and (b-l) as an analysis problem and 
handle them in the analysis phase. (b-2) is more 
difficult and is treated in the transfer- 
generation phase where partial semantic interpre- 
tation \[s done. 
162 
Word choice in t rans fer -generat lon  phase is 
done by using, cond i t iona l  expression and a t t r i -  
but ive information included in CPS. For example, 
a transitive verb "develop" is translated differ- 
ently according to its object: 
develop ~ (* system) ... KAINATSU-SURU 
t (+ film) GENZOU-SURU. (34) 
The following assignment of CPSF makes this choice 
poss ib le : 
deve lop 
<= (LAMBDA (x y) 
\[(CLASS y)=SYSTEM -> 
("x-GA y-WO KAIHATSU-SURU"} ; 
(CLASS y)-FILM -> 
("x-GA y-WO GENZOU-SURU"}; 
• .. \]), (35) 
operating-syStem 
<- \[NOUN "OS" with CLASS-system; ... \], (36) 
film 
<- \[NOUN "FUILUMU" with CLASS-film; ... 1. 
(37) 
To make this type of processing possible in the 
cases where the deep object is moved from surface 
object  position by transformations, link infor- 
mation between verb and its (deep) object should 
be represented  exp l i c i t l y .  The below shows bow 
i t  is  done in the case of  re la t ive  c lause .  
Phrase Structu~ (for restrictive use): 
NP 
(which(Xx\[ .. x .. .  \]))(<noun>) 
l i nk  from head noun to 
place ho lder  
y activity 
gent: GA ~agent :  NO 
ocatl on: NI \ [1  ocati on: E-NO 
HONYAKU ~'~ 
r e s - o b J / ~ T s  Z=~7:°~/ 
/ / "~Ct i  vi ty { (agent: NO/NIYORU 
f ' ' '~=-~ ~Jobj :NO 
\[NONYAKU SURU~ ad j -ab le  _ ~source:KARANO 
((agent:C~ activitv-'~7 HONYAKU ~NOU NA~ 
/source:KARA I '~ .  . . . . . . .  , -=  I ~ 
Ldest :E /N!  L°D3~'su°3 -J (,fsubj :WA 
-)source: ~RA 
Ldest:E/NI 
5 • EXPERIMENTS 
CPSF assignment: 
whtch ~ (LAHBOA (P) (LAMBOA (Q) 
{NOUN (+ HK-HODIFIER () 
(P (+ MK-NULL-NP () O))) 
Q})), 
In EFR leve l ,  lambda variable x is exp l i c i t -  
l y  used as a place ho lder  fo r  the gap. 
A functor "which" dominates both the EFR 
for the embedded sentence and that for 
the head noun. A CPSF assigned to the 
functor "which" sends conceptual informa- 
tion of the head noun to the gap as 
follows: firstly it creates a null NF 
out of the head noun, then the null NP 
is substituted into the lambda variable 
for the gap. 
In word choice or semantic based translation 
in general, various kinds of transformations are 
carried out on target language structure. For 
example, 
her a r r iva l  makes him happy, (38) 
must be paraphrased into:  
he becomes happy because she has arrived (39) 
since inanimate agent is unnatural in Japanese. 
In order to re t r ieve  appropr iate lex ica l  item of 
ta rget  language for transformation, mutual rela- 
tions among lexlcal items are organized using 
network formalism (lexical net). The node repre- 
sents a lexicel item and a link represents an 
association with spec i f i ca t ion  of what operation 
causes that link t() be passed through. \[t also 
contains description of case ~ransformation 
needed Ln order co map case structure appropr ia te -  
ly .  The below i l l us t ra te  s part of Lexical net: 
We have constructed a prototype  system. 
I t  is  s lmp l i f ied  then pract i ca l  system in: 
- i t  has only l imi ted  vocabu lary ,  
- interactive disembiguation is done instead 
of automatic disambiguaCion, and 
- word choice mmchenism is limited to typical 
cases since overall definition of rules 
have not yet  been completed. 
Sample texts  are taken from rea l  computer 
manuals or abst rac ts  of computer journa ls .  
Initially, four sample texts (40 sentences) are 
chosen. Currently it is extended to I0 texts (72 
sentences). 
Add i t iona l  features are introduced Ln order 
to make the system more pract i ca l .  
a) Parser: declarative rules are inefficient 
for dealing with sentences in real cexts. The 
parser  uses production type rules each of which 
is classified according to its invocation condi- 
tion. Declarative rules are manually converted 
into this rule type. 
b) Automatic postedicor: transfer process 
defined so far  concentrates on local processings. 
Even if certain kinds of ambiguities are re- 
solved in this phase, there s t i l l  remains a 
poss ib i l i ty  that new ambiguity is introduced in 
generat ion phase. Instead of incorporat ing into 
the transfer-generation phase a sophist icated 
mechanism for  filtering out ambiguities, we 
attach a postprocessor which will "reform" a 
phrase structure y ie ld ing  ambiguous output.  Tree- 
tree transformation rules are utilized here. 
Current resu l t  of our machine cransLacion 
system is shown in Appendix. 
163 
6. DISCUSSION 
Having completed initial experiments, it is 
shown that our framework is applicable to real 
texts  under plausible assumption. The prototype  
system has a clear architecture. Central rule 
interpreter contains no complicated parts. 
Although several errors occured in the implementa- 
tion of translation rules, they were easily 
detected and eliminated for the sake of data flow 
proper ty .  
The initial requ i rement  for i n te rmed ia te  
representation are filled in the following way: 
Requirement a: prec ise  representat ion  based 
on in tens ioua l  logic, 
Requirement b: using lambda variables and 
scope ru les ,  
Requirement c: data flow computing model 
based on compositionality, 
Requirement d: any CPSF can be assigned 
to a given lexical item 
if type is agreed, 
Requirement e: fact that computer model 
has been implemented. 
Some essent ia l  problems are left unsolved. 
I) Scope analysis: correct analysis of scope of 
words are cruc ia l  but difficult. For example,  
scope relation of auxiliary and "not" d i f fe rs  
case by case: 
he can't swim 
-> not(can(<he>,<swim>)) (A0) 
you should not eat the banana 
-> should(not(<eat the banana>)) (41) 
it may not be h im 
-> may(not( <it-he> )) (42) 
you may not eat the banana 
-> not(may( <you eat banana>)) (43) 
2) Logic vs machine translation: The sentence 
(44) is logically equivalent to (45), but 
that paraphrasing is bad in machine translation. 
he reads and writes English. (44) 
he reads English and he writes English. (45) 
7. CONCLUSION 
Application of formal semantics to machine 
translation brings about new phase of machine 
translation. It makes the translation process 
clearer than conventional systems. The theory 
has been tested by implementing a prototype, 
which can translate real texts  with plausible 
human assist. 
REFERENCES 
Cresswell, M.J. (1973): Logic and Languages, 
Methuen and Company. 
Dowry, R. et al (1981): Introduction to Montague 
Semantics, Reide l .  
Friedman, J. (1978): Evaluating English Sentences 
in a Logical Model, Abstract 16, COLING 78. 
Hauenschild, C., e ta l .  (1979): SALAT: Machine 
Translation Via Semantic Representation, 
Bauerle et al.(eds.): Semantics From Different 
Points of View, Springer-Verlag, 324-352. 
Henderson, P.(1980): Functional Programming -- 
Application and Implementation, Prentice/Hall. 
Hobbs, J.R. and Rosenschein, S.J. (1978): Making 
Computational Sense of Montague's Incensional 
Logic,  AI 9, 287-306. 
Landsbergen, J. (1980): Adaptation of Montague 
Grammar to the Requirement of Question Answer- 
ing, Proc° COLING 80, 211-212. 
Landsbergen, J. (1982): Machine Translation based 
on Logically Isomorphic Montague Gra=mars, 
Proc. COLING 82. 
Montague, R. (1974): Proper Treatment of Quantifi- 
cation in Ordinary English, Thompson (ed.) 
Formal Philosophy, Yale University, 247-270. 
Moore, R.C. (1981): Problems in Logical Form, 
Froc. 19th Annual Meeting of the ACL, I17-124. 
Moran, D.B. (1982): The Representation of 
Inconsistent Information in a Dynamic Model- 
Theoretic Semantics, Proc. 20th Annual Meeting 
of the ACL, 16-18. 
Nishida, T. and Doshita, S. (1980): Hierarchical 
Meaning Representation and Analysis of 
Natural Language Documents, Proc. COLING 80, 
85-92. 
Rosenschein, S.J. and Shieber, S.M.(1982): Trans- 
lating English into Logical Form, Proc. 20th 
Annual Meeting of the ACL, l-S. 
Yonezaki, H. and Enomoto, H. (1980): Database 
System Based on Intensional Logic, Proc. COLING 
80, 220-227. 
164 
INPUT TEXT 
APPENDIX: Translation of a Sample Text. 
}((h¢.*ne: ,, a %~qem (or IOcai communlcat,on among computing statiOns Our experlmcn\[ai 
E.thcrnc; u~;.: ~ppcc coaxial eabl~ Io c~rn ~urlaoie-len~th dlgltal data packets among, for example, 
pcrsonai minicomputers, pr~nung f'aciliues, iar~¢ ~ie s~orage de,.~ces, magnetic r~pe backup stauons. 
lar~er cenlra! computers, and longer-haul communlcauor~ equzpment. 
The ,~hared communicauon facilit.~, a branchm8 E~er. ~s passive. A sIauons E~heme~ interface 
connecL~ b,-sonalb through an interface cabie to a Lranscezver which in turn ~ps mLo the passing 
F/her 4 packet is hmadcas{ onto the F:'ther. is heard b.~ all smr/ons, and is cop~ed from the Er.her 
b.~ desunauons ~.hich soiL'c: ~! accorain~ to the packe:s leadm8 address bits. This ,s 0madc.~l 
packe: s~tching alld shouic be disunguzshec~ from s(ore-and-t'or~ard packe( switchin 8 m wh,ch 
muun9 ~ nerformed h~ mtermedmte pruccssm~ elements. To handle {he demand~ of ~,rowth. an 
F/heine! can be ex~ended usm@ packet repeaters (or signaJ regeneration, packe{ filters t'or crar~c 
locaJzzauon, an(~ p~ket gate~a.vs /'or intcmetwurk address extension. 
Control is completeb dnstrioutea among stauons with packet transmissions coordinated ',nmugh 
sr, austical arbitration. Transmissions inl~ated b) a s~aoon defer ~o an)' which may' alread.~ be m 
progress. Once s~arted, if interference v,.:d~ ocher packe~ ~s detected, a transmission ts aborted and 
reschedu\[ed b;, ~LS source s~auon. A~er a certain period of interference-tree transmission, a packet 
is heard b.v all s\[aoons and will run to completion without interference. E~ernet controllers m 
colliding sauons each generate random retransrniss~on inten-ab to avoid repeated co\[iismns. The 
mean of a packer's retransmission inter, aiS is adjusted as a f~ncUon of co\]hsion histon. to keep 
Ether uulizauon near ~le opumum v.-,.h changing network load. 
E~en ~,nen transmuted w~thout source-detected interference, a packet may still not reach z~ 
destination w~thouz error: thus. packets are delivered only ~.~th high probabilio'. Scauons requmng a 
residual error rate lower than thai provided b.~ the bare Ethemet packet transport mechanism muSl 
follo~ mutually agreed upon packet protOcols. 
cCted from: MeCcalfe, R.M. and 8oggs, D.R. (1975): E~hernec: D1scrlbuced Packec~ 
Swi~chin 8 for Local Computer Networks, CSL-75-7, Xerox. 
OUTPUT TEXT 
cranslaglon is carr ied 
oue sengence by sentence; 
the result is assembled 
by hand. 
=-9 ,  E I \ ] ~ . . ~ 7  ~ d Ju09x ~ U - 9"o~'~.  ~ll~'-~,'~':, ~' "," J '2"x~ = - ~ 3 
I%~ ='r  ~ ~, ~- n,,~- -- ~a ,  ~ ;~o 
097-  xx9  = - :~ ~ > C2 o ( f f .~ I~1~T ~ 6 ~,  ~7.~ ">'_~ -~ ~ ~-.3, ~;o'~?;~, 
& ~ i v -- x ~ T ~ , ~ / ~ : ~  L C,~ "~ ~ .9,'f~.~ ~."  L I~ '<.  ~ ~ ~ ~.r . -~ -- .~' : 
t~ ;~.. ~ ¢) l~¢) E THI-= RNET,  ~'r v b ~ I~/~IL  " • --, T t~f~ ~ tl "5 L er)j: ;) 'L ~"  ,~f~.~r) 
\[65 
