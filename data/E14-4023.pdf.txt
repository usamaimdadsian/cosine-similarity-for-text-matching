Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 117–122,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics
Comparing methods for deriving intensity scores for adjectives
Josef Ruppenhofer?, Michael Wiegand†, Jasper Brandes?
?Hildesheim University
Hildesheim, Germany
{ruppenho|brandesj}@uni-hildesheim.de
†Saarland University
Saarbru¨cken, Germany
michael.wiegand@lsv.uni-saarland.de
Abstract
We compare several different corpus-
based and lexicon-based methods for the
scalar ordering of adjectives. Among
them, we examine for the first time a low-
resource approach based on distinctive-
collexeme analysis that just requires a
small predefined set of adverbial modi-
fiers. While previous work on adjective in-
tensity mostly assumes one single scale for
all adjectives, we group adjectives into dif-
ferent scales which is more faithful to hu-
man perception. We also apply the meth-
ods to both polar and non-polar adjectives,
showing that not all methods are equally
suitable for both types of adjectives.
1 Introduction
Ordering adjectives by strength (e.g. good < great
< excellent) is a task that has recently received
much attention due to the central role of intensity
classification in sentiment analysis. However, the
need to assess the relative strength of adjectives
also applies to non-polar adjectives. We are thus
interested in establishing prior or lexical intensity
scores and rankings for arbitrary sets of adjectives
that evoke the same scale.1 We do not address con-
textualized intensity, i.e. the fact that e.g. negation
and adverbs such as very or slightly impact the per-
ceived intensity of adjectives.
We work with four scales of adjectives (cf. Ta-
ble 1). Our polar adjectives include 29 adjectives
referring to quality and 18 adjectives relating to
intelligence. Our non-polar adjectives include 8
dimensional adjectives denoting size and 22 de-
noting duration. The adjectives are taken, in part,
from FrameNet’s (Baker et al., 1998) frames for
1As there has been previous work on how to group adjec-
tives into scales (Hatzivassiloglou and McKeown, 1993), we
consider this grouping as given.
DESIRABILITY, MENTAL PROPERTY, SIZE and
DURATION DESCRIPTION. These scales are used
because they are prototypical and have multiple
members on the positive and negative half-scales.
We evaluate several corpus- and resource-based
methods that have been used to assign intensity
scores to adjectives. We compare them to a new
corpus-based method that is robust and of low
complexity, and which directly uses information
related to degree modification of the adjectives to
be orderered. It rests on the observation that ad-
jectives with different types of intensities co-occur
with different types of adverbial modifiers.2
POLAR ADJECTIVES
Intelligence Adjs. Intensity Level
brilliant very high positive
ingenious high positive
brainy, intelligent medium positive
smart low positive
bright very low positive
daft very low negative
foolish low negative
inane lower medium negative
dim upper medium negative
dim-witted, dumb, mindless high negative
brainless, idiotic, imbecillic, moronic, stupid very high negative
Quality Adjs. Intensity Level
excellent, extraordinary, first-rate, great, outstand-
ing, super, superb, superlative, tip-top, top-notch
very high positive
good high positive
decent upper medium positive
fine, fair lower medium positive
okay, average low positive
so-so very low positive
mediocre very low negative
second-rate, substandard low negative
inferior lower medium negative
bad, crappy, lousy, poor, third-rate medium negative
rotten upper medium negative
awful high negative
shitty very high negative
DIMENSIONAL ADJECTIVES
Size Adjs. Intensity Level
colossal, enormous, gargantuan, giant, gigantic, gi-
normous, humongous
high positive
big, huge, immense, large, oversize, oversized, vast medium positive
outsize, outsized low positive
diminutive, little, puny, small low negative
tiny medium negative
microscopic high negative
Duration Adjs. Intensity Level
long high positive
lengthy medium positive
extended low positive
momentaneous low negative
brief, fleeting, momentary medium negative
short high negative
Table 1: Adjectives used grouped by human gold
standard intensity classes
2The ratings we collected and our scripts are avail-
able at www.uni-hildesheim.de/ruppenhofer/
data/DISA_data.zip.
117
2 Data and resources
Table 2 gives an overview of the different corpora
and resources that we use to produce the different
scores and rankings that we want to compare. The
corpora and ratings will be discussed alongside the
associated experimental methods in §4.1 and §4.2.
Corpora Tokens Reference
BNC ?112 M (Burnard, 2007)
LIU reviews ?1.06 B (Jindal and Liu, 2008)
ukWaC ?2.25 B (Baroni et al., 2009)
Resources Entries Reference
Affective norms ?14 K (Warriner et al., 2013)
SoCAL ? 6.5 K (Taboada et al., 2011)
SentiStrength ? 2.5 K (Thelwall et al., 2010)
Table 2: Corpora and resources used
3 Gold standard
We collected human ratings for our four sets of ad-
jectives. All items were rated individually, in ran-
domized order, under conditions that minimized
bias. Participants were asked to use a horizontal
slider, dragging it in the desired direction, repre-
senting polarity, and releasing the mouse at the de-
sired intensity, ranging from ?100 to +100 .
Through Amazon Mechanical Turk (AMT), we
recruited subjects with the following qualifica-
tions: US residency, a HIT-approval rate of at least
96% (following Akkaya et al. (2010)), and 500
prior completed HITs. We collected 20 ratings for
each item but had to exclude some participants’
answers as unusable, which reduced our sample to
17 subjects for some items. In the raw data, all ad-
jectives had different mean ratings and their stan-
dard deviations overlapped. We therefore trans-
formed the data into sets of equally strong adjec-
tives as follows. For a given pair of adjectives of
identical polarity, we counted how many partici-
pants rated adjective A more intense than adjective
B; B more intense than A; or A as intense as B.
Whenever a simple majority existed for one of the
two unequal relations, we adopted that as our rela-
tive ranking for the two adjectives.3 The resulting
rankings (intensity levels) are shown in Table 1.
4 Methods
Our methods to determine the intensity of adjec-
tives are either corpus- or lexicon-based.
3In our data, there was no need to break circular rankings,
so we do not consider this issue here.
4.1 Corpus-based methods
Our first method, distinctive-collexeme analysis
(Collex) (Gries and Stefanowitsch, 2004) assumes
that adjectives with different types of intensities
co-occur with different types of adverbial modi-
fiers (Table 3). End-of-scale modifiers such as ex-
tremely or absolutely target adjectives with a par-
tially or fully closed scale, such as brilliant or out-
standing, which occupy extreme positions on the
intensity scale. “Normal” degree modifiers such
as very or rather target adjectives with an open
scale structure (in the sense of Kennedy and Mc-
Nally (2005)), such as good or decent, which oc-
cupy non-extreme positions.
To determine an adjective’s preference for one
of the two constructions, the Fisher exact test
(Pedersen, 1996) is used. It makes no distribu-
tional assumptions and does not require a min-
imum sample size. The direction in which ob-
served values differ from expected ones indicates a
preference for one construction over the other and
the p-values are taken as a measure of the prefer-
ence strength. Our hypothesis is that e.g. an adjec-
tive A with greater preference for the end-of-scale
construction than adjective B has a greater inher-
ent intensity than B. We ran distinctive-collexeme
analysis on both the ukWaC and the BNC. We re-
fer to the output as Collex
ukWaC
and Collex
BNC
.
Note that this kind of method has not yet been ex-
amined for automatic intensity classification.
end-of-scale “normal”
100%, fully, totally, absolutely,
completely, perfectly, entirely,
utterly, almost, partially, half,
mostly
all, as, awfully, enough, extremely,
fairly, highly, how, least, less, much,
pretty, quite, rather, so, somewhat,
sort of, terribly, too, very, well
Table 3: Domain independent degree modifiers (3
most freq. terms in the BNC; 3 most freq. terms
in the ukWaC)
Another corpus-based method we consider em-
ploys Mean star ratings (MeanStar) from prod-
uct reviews as described by Rill et al. (2012). Un-
like Collex, this method uses no linguistic prop-
erties of the adjectives themselves. Instead, it de-
rives intensity from the star rating scores that re-
viewers (manually) assign to reviews. We count
how many instances of each adjective i (of the set
of adjectives to classify) occur in review titles with
a given star rating (score) S
j
within a review cor-
pus. The intensity score is defined as the weighted
mean of the star ratings SR
i
=
?
n
j=1
S
i
j
n
.
Horn (1976) proposes pattern-based diagnos-
118
Pattern Any Int. Qual. Size Dur.
X or even Y 4118 1 34 9 3
X if not Y 3115 1 0 29 0
be X but not Y 2815 0 74 3 1
not only X but Y 1114 0 3 0 0
X and in fact Y 45 0 0 0 0
not X, let alone Y 4 0 0 0 0
not Y, not even X 4 0 1 0 0
Table 4: Phrasal patterns in the ukWaC
tics for acquiring information about the scalar
structure of adjectives. This was validated on ac-
tual data by Sheinman and Tokunaga (2009). A
pattern such as not just/only X but Y implies that
[Y] must always be stronger than [X] (as in It’s
not just good but great.).
The pattern-based approach has a severe cover-
age problem. Table 4 shows the results for 7 com-
mon phrasal patterns in the larger of our two cor-
pora, the ukWaC. The slots in the patterns are typ-
ically not filled by adjectives from the same scale.
For example, the most frequent pattern X or even
Y has 4118 instances in the ukWaC. Only 34 of
these have quality adjectives in both slots. Though
de Melo and Bansal (2013) have shown that the
coverage problems can be overcome and state-of-
the-art results obtained using web scale data in the
form of Google n-grams, we still set aside this
method here because of its great resource need.
4.2 Manually compiled lexical resources
In addition to the corpus methods, we also con-
sider some manually compiled resources. We want
to know if the polarity and intensity information in
them can be used for ordering polar adjectives.
One resource we consider are the affective rat-
ings (elicited with AMT) for almost 14,000 En-
glish words collected by Warriner et al. (2013).
They include scores of valence (unhappy to
happy), arousal (calm to aroused) and dominance
(in control to controlled) for each word in the list.
This scoring system follows the dimensional the-
ory of emotion by Osgood et al. (1957). We will
interpret each of these dimensions as a separate in-
tensity score, i.e. War
V al
, War
Aro
and War
Dom
.
Beyond Warriner’s ratings, we consider the two
polarity lexicons SentiStrength (Thelwall et al.,
2010) and SoCAL (Taboada et al., 2011) which
also assign intensity scores to polar expressions.
5 Experiments
For our evaluation, we compute the similarity be-
tween the gold standard and every other ranking
we are interested in in terms of Spearman’s rank
correlation coefficient (Spearman’s ?).
Polar Dimensional
Data set Intelligence Quality Duration Size
MeanStar 0.886 0.935 0.148 -0.058
SoCAL 0.848 0.953 NA 0.776
SentiStrength 0.874 0.880 NA NA
Collex
ukWaC
0.837 0.806 0.732 0.808
Collex
ukWaC
? 0.845 0.753 0.732 0.940
Collex
BNC
0.834 0.790 0.732 0.733
Collex
BNC
? 0.705 0.643 0.834 0.700
War
V al
0.779 0.916 -0.632 -0.031
War
Aro
0.504 -0.452 0.316 0.717
War
Dom
0.790 0.891 0.632 0.285
Table 5: Spearman rank correlations with the hu-
man gold standard (?: only the 3 most frequent
modifiers are used (see Table 3))
5.1 Data transformation
For the word lists with numeric scores (MeanStar
(§4.1); SentiStrength, SoCAL, War
V al
, War
Aro
and War
Dom
(§4.2)) we did as follows: Adjectives
not covered by the word lists were ignored. Ad-
jectives with equal scores were given tied ranks.
For the experiments involving distinctive
collexeme analysis in our two corpora (§4.1) we
proceeded as follows: The adjectives classified
as distinctive for the end-of-scale modification
constructions were put at the top and bottom of
the ranking according to polarity; the greater the
collostructional strength for the adjective as de-
noted by the p-value, the nearer it is placed to the
top or bottom of the ranking. The adjectives that
are distinctive for the normal degree modification
construction are placed between those adjectives
distinctive for the end-of-scale modification
construction, again taking polarity and collostruc-
tional strength into account. This time, the least
distinctive lemmas for the normal modification
construction come to directly join up with the
least distinctive lemmas for the end-of-scale
construction. In between the normal modifiers,
we place adjectives that have no preference for
one or the other construction, which may result
from non-occurrence in small data sets (see §5.2).
5.2 Results
The results of the pairwise correlations between
the human-elicited gold standard and the rankings
derived from various methods and resources are
shown in Table 5. For polar adjectives, most rank-
ings correlate fairly well with human judgments.
Warriner’s arousal list, however, performs poorly
on quality adjectives, whereas MeanStar and War-
riner’s dominance and valence lists perform bet-
ter on quality than on intelligence adjectives. For
MeanStar, this does not come as a surprise as qual-
ity adjectives are much more frequent in prod-
119
uct reviews than intelligence adjectives. Overall,
it seems that MeanStar most closely matches the
human judgments that we elicited for the intel-
ligence adjectives. SentiStrength also produces
high scores. However, we do not have full confi-
dence in that result since SentiStrength lacks many
of our adjectives, thus leading to a possibly higher
correlation than would have been achieved if ranks
(scores) had been available for all adjectives.
The picture is very different for the dimensional
(non-polar) adjectives. While Collex still gives
very good results, especially on the ukWaC, the
MeanStar method and most Warriner lists produce
very low positive or even negative correlations.
This shows that estimating the intensity of non-
polar adjectives from metadata or ratings elicited
in terms of affect is not useful. It is much better to
consider their actual linguistic behavior in degree
constructions, which Collex does. SentiStrength
has no coverage for size or duration adjectives.
SoCAL covers 14 of the 22 size adjectives.
Although it never gives the best result, Collex
produces stable results across both corpora and
the four scales. It also requires the least human
effort by far. While all other rankings are pro-
duced with the help of heavy human annotation
(even MeanStar is completely dependent on manu-
ally assigned review scores), one has only to spec-
ify some domain-independent degree and end-of-
scale modifiers. Table 5 also shows that normally
a larger set of modifiers is necessary: only consid-
ering the 3 most frequent terms (Table 3) results in
a notably reduced correlation. As there is no con-
sistent significant difference between Collex
BNC
and Collex
ukWaC
even though the ukWaC is 20
times larger than the BNC (Table 2), we may
conclude that the smaller size of the BNC is al-
ready sufficient. This, however, raises the question
whether even smaller amounts of data than the full
BNC could already produce a reasonable intensity
ranking. Figure 1 plots the Spearman correlation
for our adjectives using various sizes of the BNC
corpus.4 It shows that further reducing the size of
the corpus causes some deterioration, most signifi-
cantly on the intelligence adjectives. The counter-
intuitive curve for duration adjectives is explained
as follows. Collex produces ties in the middle of
the scale when data is lacking (see §5.1). Because
the smallest corpus slices contain no or very few
instances and because the gold standard does in-
4For each size, we average across 10 samples.
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  20  40  60  80  100
Sp
ea
rm
an
’s 
rh
o
% Size of BNC
Intelligence
Quality
Size
Duration
Figure 1: Reducing the size of the BNC
clude several ties, the results for duration adjec-
tives are inflated initially, when data is lacking.
6 Related work
Sentiment analysis on adjectives has been exten-
sively explored in previous work, however, most
work focussed on the extraction of subjective ad-
jectives (Wiebe, 2000; Vegnaduzzo, 2004; Wie-
gand et al., 2013) or on the detection of polar ori-
entation (Hatzivassiloglou and McKeown, 1997;
Kamps et al., 2004; Fahrni and Klenner, 2008).
Intensity can be considered in two ways, as a
contextual strength analysis (Wilson et al., 2004)
or as an out-of-context analysis, as in this paper.
Our main contribution is that we compare sev-
eral classification methods that include a new
effective method based on distinctive-collexeme
analysis requiring hardly any human guidance and
which moreover can solve the problem of intensity
assignment for all, not only polar adjectives.
7 Conclusion
We compared diverse corpus-based and lexicon-
based methods for the intensity classification of
adjectives. Among them, we examined for the first
time an approach based on distinctive-collexeme
analysis. It requires only a small predefined set
of adverbial modifiers and relies only on infor-
mation about individual adjectives rather than co-
occurrences of adjectives within patterns. As a re-
sult, it can be used with far less data than e.g. the
Google n-grams provide. Unlike the mean star ap-
proach, it needs no extrinsic meta-data and it can
handle both polar and non-polar adjectives. Ac-
cordingly, it appears to be very promising for cases
where only few resources are available and as a
source of evidence to be used in hybrid methods.
120
Acknowledgments
Michael Wiegand was funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under grant no. 01IC12SO1X. The authors would
like to thank Maite Taboada for providing her sen-
timent lexicon (SoCAL) to be used for the experi-
ments presented in this paper.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazon’s Mechanical
Turk, pages 195–203, Los Angeles, CA, USA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley Framenet Project.
In Proceedings of the International Conference
on Computational Linguistics and Annual Meeting
of the Association for Computational Linguistics
(COLING/ACL), pages 86–90, Montre´al, Quebec,
Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetti. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209–226.
Lou Burnard, 2007. Reference Guide for the British
National Corpus. Research Technologies Service
at Oxford University Computing Services, Oxford,
UK.
Gerard de Melo and Mohit Bansal. 2013. Good, Great,
Excellent: Global Inference of Semantic Intensities.
Transactions of the Association for Computational
Linguistics, 1:279–290.
Angela Fahrni and Manfred Klenner. 2008. Old Wine
or Warm Beer: Target Specific Sentiment Analysis
of Adjectives. In Proceedings of the Symposium on
Affective Language in Human and Machine, pages
60–63, Aberdeen, Scotland, UK.
Stefan Th. Gries and Anatol Stefanowitsch. 2004.
Extending collostructional analysis: a corpus-based
perspective on ‘alternations’. International Journal
of Corpus Linguistics, 9(1):97–129.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales: Clustering Adjectives According to
Meaning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 172–182, Columbus, OH, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of the Conference on Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 174–181, Madrid, Spain.
Laurence Robert Horn. 1976. On the Semantic Prop-
erties of Logical Operators in English. Indiana Uni-
versity Linguistics Club.
Nitin Jindal and Bing Liu. 2008. Opinion Spam
and Analysis. In Proceedings of the international
conference on Web search and web data mining
(WSDM), pages 219–230, Palo Alto, USA.
Jaap Kamps, M.J. Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using Wordnet to Mea-
sure Semantic Orientations of Adjectives. In Pro-
ceedings of the Conference on Language Resources
and Evaluation (LREC), pages 1115–1118, Lisbon,
Portugal.
Christopher Kennedy and Louise McNally. 2005.
Scale Structure, Degree Modification, and the
Semantics of Gradable Predicates. Language,
81(2):345–338.
Charles E. Osgood, George Suci, and Percy Tannen-
baum. 1957. The Measurement of Meaning. Uni-
versity of Illinois Press.
Ted Pedersen. 1996. Fishing for exactness. In
Proceedings of the South-Central SAS Users Group
Conference, Austin, TX, USA.
Sven Rill, Johannes Drescher, Dirk Reinel, Joerg
Scheidt, Oliver Schuetz, Florian Wogenstein, and
Daniel Simon. 2012. A Generic Approach to Gen-
erate Opinion Lists of Phrases for Opinion Mining
Applications. In Proceedings of the KDD-Workshop
on Issues of Sentiment Discovery and Opinion Min-
ing (WISDOM), Beijing, China.
Vera Sheinman and Takenobu Tokunaga. 2009. Ad-
jScales: Differentiating between Similar Adjectives
for Language Learners. CSEDU, 1:229–235.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267 – 307.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
and Di Cai. 2010. Sentiment Strength Detec-
tion in Short Informal Text. Journal of the Ameri-
can Society for Information Science and Technology,
61(12):2544–2558.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings
of the AAAI Spring Symposium on Exploring Atti-
tude and Affect in Text: Theories and Applications,
Stanford, CA, USA.
Amy Warriner, Victor Kuperman, and Marc Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior Research Meth-
ods, Online First:1–17.
Janyce M. Wiebe. 2000. Learning Subjective Adjec-
tives from Corpora. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), pages
735–740, Austin, TX, USA.
121
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative Adjectives: An Unsu-
pervised Criterion to Extract Subjective Adjectives.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT/NAACL), pages 534–539, Atlanta, GA,
USA.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI),
pages 761–767, San Jose, CA, USA.
122
