Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1522–1533, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Characterizing Stylistic Elements in Syntactic Structure
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Much of the writing styles recognized in
rhetorical and composition theories involve
deep syntactic elements. However, most
previous research for computational sty-
lometric analysis has relied on shallow
lexico-syntactic patterns. Some very re-
cent work has shown that PCFG models
can detect distributional difference in syn-
tactic styles, but without offering much in-
sights into exactly what constitute salient
stylistic elements in sentence structure
characterizing each authorship. In this
paper, we present a comprehensive ex-
ploration of syntactic elements in writing
styles, with particular emphasis on inter-
pretable characterization of stylistic ele-
ments. We present analytic insights with
respect to the authorship attribution task
in two different domains.
1 Introduction
Much of the writing styles recognized in rhetor-
ical and composition theories involve deep syn-
tactic elements in style (e.g., Bain (1887), Kem-
per (1987) Strunk and White (2008)). However,
previous research for automatic authorship at-
tribution and computational stylometric analy-
sis have relied mostly on shallow lexico-syntactic
patterns (e.g., Mendenhall (1887), Mosteller
and Wallace (1984), Stamatatos et al. (2001),
Baayen et al. (2002), Koppel and Schler (2003),
Zhao and Zobel (2007), Luyckx and Daelemans
(2008)).
Some very recent works have shown that
PCFG models can detect distributional differ-
ence in sentence structure in gender attribution
(Sarawgi et al., 2011), authorship attribution
(Raghavan et al., 2010), and native language
identification (Wong and Dras, 2011). However,
still very little has been understood exactly what
constitutes salient stylistic elements in sentence
structures that characterize each author. Al-
though the work of Wong and Dras (2011) has
extracted production rules with highest informa-
tion gain, their analysis stops short of providing
insight any deeper than what simple n-gram-
level analysis could also provide.1 One might
even wonder whether PCFG models are hing-
ing mostly on leaf production rules, and whether
there are indeed deep syntactic differences at all.
This paper attempts to answer these questions.
As an example of syntactic stylistic elements
that have been much discussed in rhetorical the-
ories, but have not been analyzed computation-
ally, let us consider two contrasting sentence
styles: loose (cumulative) and periodic:2 a loose
sentence places the main clause at the begin-
ning, and then appends subordinate phrases and
clauses to develop the main message. In con-
trast, a periodic sentence starts with subordi-
nate phrases and clauses, suspending the most
1For instance, missing determiners in English text
written by Chinese speakers, or simple n-gram anomaly
such as frequent use of “according to” by Chinese speak-
ers (Wong and Dras, 2011).
2Periodic sentences were favored in classical times,
while loose sentences became more popular in the modern
age.
1522
Hobbs Joshi Lin McDon
SˆROOT ? S , CC S PPˆPRN ? IN NP NPˆS ? NN CD NPˆNP ? DT NN POS
NPˆPP ? DT NPˆPP ? NP PRN SBAR NPˆNP ? DT NN NNS WHNPˆSBAR ? IN
VPˆVP ? TO VP SˆROOT ? PP NP VP . SˆROOT ? SBAR , NP VP . NPˆPP ? NP SBAR
PPˆPP ? IN S PRNˆNP ? -LRB- PP -RRB- NPˆPP ? NP : NP SBARˆPP ? WHADVP S
NPˆPP ? NP , PP NPˆNP ? NNP SˆROOT ? PP , NP VP . SBARˆS ? WHNP S
VPˆS ? VBZ ADJP S SˆSBAR ? PP NP VP NPˆNP ? PDT DT NNS PPˆNP ? IN SBAR
VPˆSINV ? VBZ SˆROOT ? LST NP VP . NPˆVP ? DT NN SBAR SBARˆNP ? WHNP S
VPˆS ? VBD S CONJPˆNP ? RB RB IN SBARˆS ? WHADVP S SBARˆPP ? SBAR CC SBAR
VPˆS ? VBG PP NPˆPP ? NP PRN PP PRNˆNP ? -LRB- NP -RRB- PPˆVP ? IN
ADVPˆVP ? RB PP NPˆNP ? NP , NP NPˆPP ? NN NN SˆSBAR ? VP
Table 1: Top 10 most discriminative production rules for each author in the scientific domain.
loose Christopher Columbus finally
reached the shores of San Salvador
after months of uncertainty at
sea, the threat of mutiny, and a
shortage of food and water.
periodic After months of uncertainty at sea,
the threat of mutiny, and a short-
age of food and water, Christopher
Columbus finally reached the shores
of San Salvador.
Table 2: Loose/Periodic sentence with identical set
of words and POS tags
important part to the end. The example in Ta-
ble 2 highlights the difference:
Notice that these two sentences comprise of an
identical set of words and part-of-speech. Hence,
shallow lexico-syntactic analysis will not be able
to catch the pronounced stylistic difference that
is clear to a human reader.
One might wonder whether we could gain in-
teresting insights simply by looking at the most
discriminative production rules in PCFG trees.
To address this question, Table 1 shows the
top ten most discriminative production rules
for authorship attribution for scientific articles,3
ranked by LIBLINEAR (Fan et al., 2008).4 Note
that terminal production rules are excluded so
as to focus directly on syntax.
It does provide some insights, but not to a sat-
isfactory degree. For instance, Hobbs seems to
favor inverted declarative sentences (SINV) and
adverbs with prepositions (RB PP). While the
latter can be easily obtained by simple part-of-
3See Section 2 for the description of the dataset.
4We use Berkeley PCFG parser (Petrov and Klein,
2007) for all experiments.
speech analysis, the former requires using parse
trees. We can also observe that none of the
top 10 most discriminative production rules for
Hobbs includes SBAR tag, which represents sub-
ordinate clauses. But examining discriminative
rules alone is limited in providing more compre-
hensive characterization of idiolects.
Can we unveil something more in deep syntac-
tic structure that can characterize the collective
syntactic difference between any two authors?
For instance, what can we say about distribu-
tional difference between loose and periodic sen-
tences discussed earlier for each author? As can
be seen in Table 1, simply enumerating most dis-
criminative rules does not readily answer ques-
tions such as above.
In general, production rules in CFGs do not
directly map to a wide variety of stylistic el-
ements in rhetorical and composition theories.
This is only as expected however, partly because
CFGs are not designed for stylometric analysis
in the first place, and also because some syntac-
tic elements can go beyond the scope of context
free grammars.
As an attempt to reduce this gap between
modern statistical parsers and cognitively recog-
nizable stylistic elements, we explore two com-
plementary approaches:
1. Translating some of the well known stylistic
elements of rhetorical theories into PCFG
analysis (Section 3).
2. Investigating different strategies of analyz-
ing PCFG trees to extract author charac-
teristics that are interesting as well as in-
terpretable (Sections 4 & 5).
1523
Algorithm 1 Sentence Type-1 Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
if S ? Ltop then
if SBAR /? ?(Nr) then
return COMPOUND
else
return COMPLEX-COMPOUND
else
if VP ? Ltop then
if SBAR /? ?(Nr) then
return SIMPLE
else
return COMPLEX
return OTHER
We present analytic insights with respect to
the authorship attribution task in two distinct
domains.
2 Data
For the empirical analysis of authorship attri-
bution, we use two different datasets described
below. Sections 3, 4 & 5 provide the details of
our stylometric analysis.
Scientific Paper We use the ACL Anthol-
ogy Reference Corpus (Bird et al., 2008). Since
it is nearly impossible to determine the gold-
standard authorship of a paper written by multi-
ple authors, we select 10 authors who have pub-
lished at least 8 single-authored papers. We in-
clude 8 documents per author, and remove cita-
tions, tables, formulas from the text using sim-
ple heuristics.5
Novels We collect 5 novels from 5 English au-
thors: Charles Dickens, Edward Bulwer-Lytton,
Jane Austen, Thomas Hardy and Walter Scott.
We select the first 3000 sentences from each
novel and group every 50 consecutive sentences
into 60 documents per novel per author.
5Some might question whether the size of the dataset
used here is relatively small in comparison to typical
dataset comprised of thousands of documents in conven-
tional text categorization. We point out that authorship
attribution is fundamentally different from text catego-
rization in that it is often practically impossible to collect
more than several documents for each author. Therefore,
it is desirable that the attribution algorithms to detect
the authors based on very small samples.
Algorithm 2 Sentence Type-II Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
k ? 1
while k ? ? do
if Ltopk 6= VP then
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return PERIODIC
else
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return LOOSE
return OTHER
3 Sentence Types
In this section, we examine well-known sentence
types that are recognized in the literature, but
have not been analyzed computationally.
Type-I Identification – Simple/Complex/
Compound/Complex-Compound: PCFG
trees do not provide this information directly,
hence we must construct an algorithm to derive
it. The key to identifying these sentences is the
existence of dependent and independent clauses.
For the former, we rely on the SBAR tag, while
for the latter, we first define the sequence of
nodes right below the root (e.g., [NP VP .] shown
in the horizontal box in Figure 1). We call this
the top structural level. We then check whether
S (in addition to the root S) appears in this
sequence.
Formally, let Ltop = {Ni} be the set of nodes
in the top structural level, and ? = |Ltop|. Let
t(Nr) be the tree rooted at Nr, and ?(Nr) de-
note the set of nodes in t(Nr). Algorithm 1
shows the procedure to determine the type-I
class of a sentence based on its PCFG tree.6
Type-II Identification – Loose/Periodic:
A sentence can also be classified as loose or
periodic, and we present Algorithm 2 for this
identification. We perform a mini-evaluation on
20 previously unseen sentences for each type7.
Our algorithm was able to perform type-I iden-
tification on all sentences correctly. In type-II
6Note that Algorithm 1 & 2 rely on the use of Berkeley
parser (Petrov and Klein, 2007).
7These were gathered from several online quizzes
for English learners. E.g., http://grammar.about.com,
http://a4esl.org
1524
Type Hobbs Joshi Lin McDon
simple 40.0 41.7 50.2 27.9
cplex 40.8 40.7 37.6 48.4
cpnd 7.9 5.6 3.9 5.5
cpxnd 8.5 9.2 7.7 15.5
other 2.8 2.8 0.6 2.7
loose 27.6 26.4 26.9 30.8
perio 11.1 11.7 15.2 16.4
other 61.3 61.9 57.9 52.8
Table 3: Sentence Types (%) in scientific data.
identification, it labeled all loose sentences cor-
rectly, and achieved 90% accuracy on periodic
sentences.
Discussion Tables 3 & 4 show the sentence
type distribution in scientific data and novels,
respectively.8 We see that different authors are
characterized by different distribution of sen-
tence types. For instance, in Table 3, Lin is
a prolific user of simple sentences while McDon
prefers employing complex sentences. McDon
also uses complex-compound sentences quite of-
ten (15.5%), more than twice as frequently as
Lin. Notice that all authors use loose sen-
tences much more often than periodic sentences,
a known trend in modern English.
In Table 4, we see the opposite trend among
19th-century novels: with the exception of Jane
Austen, all authors utilize periodic sentences
comparatively more often. We also notice
that complex and complex-compound sentences
abound, as expected from classic literary proses.
Can we determine authorship solely based on the
distribution of sentence types?
We experiment with a SVM classifier using just
6 features (one feature for each sentence type in
Table 3), and we achieve accuracy 36.0% with
the scientific data. Given that a random base-
line would achieve only about 10% accuracy, this
demonstrates that the distribution of sentence
types does characterize an idiolect to some de-
gree.
8Due to space limitation, we present analyses based
on 4 authors from the scientific data.
Type Dickens B-Lyt Austen Hardy Scott
simple 26.0 21.2 23.9 25.6 17.5
cplex 24.4 21.8 24.8 25.6 31.8
cpnd 15.3 15.2 12.6 16.3 11.7
cpxnd 20.8 23.3 31.1 18.9 28.7
other 13.5 18.5 7.6 13.6 10.3
loose 11.5 10.8 17.9 14.5 15.3
perio 19.5 13.6 14.0 16.2 18.0
other 69.0 75.6 68.1 69.3 66.7
Table 4: Sentence Types (%) in Novels
4 Syntactic Elements Based on
Production Rules
In this section, we examine three different as-
pects of syntactic elements based on production
rules.
4.1 Syntactic Variations
We conjecture that the variety of syntactic
structure, which most previous research in com-
putational stylometry has not paid much atten-
tion to, provides an interesting insight into au-
thorship. One way to quantify the degree of syn-
tactic variations is to count the unique produc-
tion rules. In Tables 5, we show the extent of
syntactic variations employed by authors using
the standard deviation ? and the coverage of an
author:
C(a) :=
|R(a)|
| ?a R(a)|
× 100
whereR(a) denotes the set of unique production
rules used by author a, and ?a iterates over all
authors. In order to compare among authors,
we also show these parameters normalized with
respect to the highest value. Our default setting
is to exclude all lexicalized rules in the produc-
tions to focus directly on the syntactic varia-
tions. In our experiments (Section 6), however,
we do augment the rules with (a) ancestor nodes
to capture deeper syntactic structure and (b)
lexical (leaf) nodes.
As hypothesized, these statistics provide us
new insights into the authorship. For instance,
we find that McDon employs a wider variety of
syntactic structure than others, while Lin’s writ-
ing exhibits relatively the least variation. More-
over, comparing Joshi and Hobbs, it is inter-
esting to see the standard deviation differ a lot
1525
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
C 36.0 37.6 32.8 42.6 30.9 28.8 36.2 30.0 24.1
Cnorm 0.84 0.88 0.77 1.0 0.85 0.79 1.0 0.83 0.67
? 51.5 39.2 63.3 44.4 88.3 81.6 98.0 125.3 114.7
?norm 0.81 0.62 1.0 0.7 0.7 0.65 0.78 1.0 0.92
Table 5: Syntactic variations of different authors in the scientific domain.
Hobbs Joshi Lin McDon
# 136 # 142 # 124 # 161
S ? S CC S . S ? ADVP PP NP VP . S ? SBAR NP VP . S ? S NP VP .
S ? CC NP VP . S ? PP NP ADVP VP . FRAG ? NP : S . S ? S : S .
S ? S VP . S ? NP VP S ? NP VP . S ? SBAR VP .
S ? NP NP VP . S ? S S CC S . S ? PP VP . S ? SBAR S CC S .
S ? PP NP VP . S ? ADVP NP VP . S ? NP ADVP VP . S ? NP PP VP .
Table 6: Most discriminative sentence outlines in the scientific data. #N shows the number of unique
sentence outlines of each author.
(51.5 and 39.2), in spite of their C scores being
similar: 36.0% and 37.6%, respectively. This
indicates that Hobbs tends to use a certain sub-
set production rules much more frequently than
Joshi. Lin exhibits the highest standard devia-
tion in spite of having least syntactic variation,
indicating that he uses a much smaller subset of
productions regularly, while ocassionally deviat-
ing to other rules.
Similarly, among novels, Jane Austen’s writ-
ing has the highest amount of variation, while
Walter Scott’s writing style is the least varied.
Even though authors from both datasets display
similar C scores (Table 5), the difference in ? is
noteworthy. The significantly higher linguistic
variation is to be expected in creative writing
of such stature. It is interesting to note that
the authors with highest coverage – Austen and
Dickens – have much lower deviation in their
syntactic structure when compared to Hardy
and Scott. This indicates that while Austen and
Dickens consistently employ a wider variety of
sentence structures in their writing, Hardy and
Scott follow a relatively more uniform style with
sporadic forays into diverse syntactic constructs.
4.2 Sentence Outlines
Although the approach of Section 4.1 give us a
better and more general insight into the char-
acteristics of each author, its ability to provide
insight on deep syntactic structure is still lim-
ited, as it covers production rules at all levels of
the tree. We thus shift our focus to the top level
of the trees, e.g., the second level (marked in a
horizontal box) in Tree (1) of Figure 1, which
gives us a better sense of sentence outlines.
Tables 6 and 7 present the most discrimina-
tive sentence outlines of each author in the scien-
tific data and novels, respectively. We find that
McDon is a prolific user of subordinate clauses,
indicating his bias towards using complex sen-
tences. The rule “S ? SBAR S CC S” shows
his inclination towards complex-compound sen-
tences as well. These inferences are further sup-
ported by the observations in Table 3. Another
observation of possible interest is the tendency
of Joshi and Lin to begin sentences with prepo-
sitional phrases.
In comparing Table 6 and Table 7, notice
the significantly higher presence of complex and
compound-complex structures in the latter9.
The most discriminating sentence outlines for
Jane Austen, for instance, are all indicative of
complex-compound sentences. This is further
supported by Table 4.
5 Syntactic Elements Based on Tree
Topology
In this section, we investigate quantitative tech-
niques to capture stylistic elements in the tree
9The presence of “FRAG” is not surprising. Inten-
tional use of verbless sentence fragments, known as sce-
sis onomaton, was often employed by authors such as
Dickens and Bulwer-Lytton (Quinn, 1995).
1526
Dickens Bulwer-Lytton Austen Hardy Scott
# 1820 # 1696 # 2137 # 1772 # 1423
SQ ? NNP . SBARQ ? WHNP S . S ? S : CC S . S ? S NP VP . S ? NP PRN VP .
FRAG ? NP . FRAG ? INTJ NP . S ? S CC S : CC S . S ? ADVP NP VP . S ? PP NP VP .
SINV ? NP VP NP . S ? S : S CC S . S ? S : CC S : CC S . S ? FRAG : S . S ? S S : S .
INTJ ? UH . FRAG ? CC NP . S ? S : S : CC S . S ? INTJ NP VP . S ? NP PP VP .
SBARQ ? WHNP SQ . FRAG ? NP ADJP . S ? SBAR S : CC S . S ? NP VP . S ? ADVP PRN NP VP .
Table 7: Most discriminative sentence outlines in the novel data. #N shows the number of unique sentence
outlines of each author.
Metrics Scientific Data Novels
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
sen-len avg 23.7 26.0 21.0 32.2 24.1 26.7 31.4 21.5 34.1
hT avg 5.8 5.3 5.9 4.8 4.7 5.0 5.4 4.9 5.9
hF avg 2.4 2.1 2.5 1.9 1.9 1.9 2.1 1.9 2.1
wL avg 5.0 4.8 5.5 4.2 4.1 4.4 4.7 3.8 4.9
?H avg 1.2 1.1 1.1 1.0 1.1 1.1 1.3 1.2 1.4
?S avg 1.9 1.8 1.8 1.7 1.0 1.1 1.2 1.0 1.4
Table 8: Tree topology metrics for scientific data and novels.
topology. Figure 1 shows three different parse
trees to accompany our discussion.10 Notice
that sentence (1) is a loose sentence, and sen-
tence (2) is periodic. In general, loose sentences
grow deep and unbalanced, while periodic sen-
tences are relatively more balanced and wider.
For a tree t rooted at NR with a height n, let
T be the set of leaf nodes, and let F be the set
of furcation nodes, and let ?(Ni, Nj) denote the
length of the shortest path from Ni to Nj . In-
spired by the work of Shao (1990), we analyze
tree topology with the following four measure-
ments:
• Leaf height (hT = {hTi , Ni ? T }), where
hTi = ?(Ni, NR) Ni ? T . For instance, the
leaf height of “free” of Tree (2) in Fig. 1
is 6.
• Furcation height (hF = {hFi , Ni ? F}),
where hFi is the maximum leaf height within
the subtree rooted at Ni. In Figure 1, for
example, the furcation height of the VP in
Tree (2) (marked in triangle) is 3.
• Level width (wL = {wl, 1 ? l ? n}),
where wl = |{Ni : ?(Ni, NR) = l}|. E.g., w4
of Tree (1) in Figure 1 is 6.
10Example sentences are taken from Lin (1997), Joshi
(1992), and Lin (1995).
• Horizontal ?H = {?Hi , Ni ? F} , and
Vertical Imbalance ?S = {?Si , Ni ? F}.
Let C be the set of child nodes of Nk. If
|C| ? 2, then
?Hk =
?
?
?
? 1
n
|C|?
i=1
(hFi ?H)
2
where H = 1|C|
?|C|
i=1 h
F
i . Similarly,
?Sk =
?
?
?
? 1
n
|C|?
i=1
(s(Ni)? S)2
where S = 1|C|
?|C|
i=1 s(Ni) and s(Ni) is the
number of leaf nodes of tree rooted at Ni.
As shown in Figure 1, the imbalance of the
internal node VP in Tree (2) (marked in
triangle) is 0.5 horizontally, and 0.5 verti-
cally.
To give an intuition on the relation between
these measurements and different tree struc-
tures, Table 9 provides the measurements of the
three trees shown in Figure 1.
Note that all three sentences are of similar
length but show different tree structures. Tree
(1) and Tree (2) differ in that Tree (1) is
highly unbalanced and grows deep, while Tree
1527
Figure 1: Parsed trees
Metrics Tree (1) Tree (2) Tree (3)
# of tokens 15 13 13
maxi {hTi } 11 6 6
maxi {wLi } 6 9 9
maxi {?Hi } 4.97 1.6 1.7
maxi {?Si } 4 1.5 4.7
Table 9: Tree Topology Statistics for Figure 1.
(2) is much better balanced and grows shorter
but wider. Comparing Tree (2) and Tree (3),
they have the same max Leaf height, Level
width, and Horizontal Imbalance, but the
latter has bigger Vertical Imbalance, which
quantifies the imbalance in terms of the text
span covered by subtrees.
We provide these topological metrics for au-
thors from both datasets in Table 8.
6 Experiments & Evaluation
In our experiments, we utilize a set of features
motivated by PCFG trees. These consist of sim-
ple production rules and other syntactic features
based on tree-traversals. Table 10 describes
these features with examples from Tree (2), us-
ing the portion marked by the triangle.
These sets of production rules and syntax fea-
tures are used to build SVM classifiers using LI-
BLINEAR (Fan et al., 2008), wherein all fea-
ture values are encoded as term-frequencies nor-
malized by document size. We run 5-fold cross-
validation with training and testing split first as
80%/20%, and then as 20%/80%.
We would like to point out that the latter con-
figuration is of high practical importance in au-
thorship attribution, since we may not always
have sufficient training data in realistic situa-
tions, e.g., forensics (Luyckx and Daelemans,
2008).
Lexical tokens provide strong clues by creat-
ing features that are specific to each author: re-
search topics in the scientific data, and proper
nouns such as character names in novels. To
lessen such topical bias, we lemmatize and rank
words according to their frequency (in the entire
dataset), and then consider the top 2,000 words
only. Leaf-node productions with words outside
this set are disregarded.
Our experimental results (Tables 11 & 12)
show that not only do deep syntactic features
perform well on their own, but they also signif-
icantly improve over lexical features. We also
show that adding the style11 features further
improves performance.
1528
Features
pr Rules excluding terminal productions.
E.g., VP ? VBG NP
synv Traversal from a non-leaf node to its grand-
parent (embedded rising).
E.g., VPˆS ? PP
synh Left-to-right traversal in the set of all non-
leaf children of a node.
E.g., VBG ? NP (for node VP)
synv+h synv ? synh
syn0 No tree traversal. Feature comprises inte-
rior nodes only.
syn? Union of all edges to child nodes, except
when child is a leaf node.
E.g., {VP ? VBG, VP ? NP}
synl syn? ? { edge to parent node}
style11 The set of 11 extra stylistic features. 6 val-
ues from the distribution of sentence types
(Section 3), and 5 topological metrics (Sec-
tion 5) characterizing the height, width and
imbalance of a tree.
Variations
pˆr Each production rule is augmented with the
grandparent node.
? Terminal (leaf) nodes are included.
Table 10: Features and their lexico-syntactic varia-
tions. Illustration: pˆr? denotes the set of production
rules pr (including terminal productions) that are
augmented with their grandparent nodes.
To quantify the amount of authorship infor-
mation carried in the set style11, we experi-
ment with a SVM classifier using only 11 fea-
tures (one for each metric), and achieve accu-
racy of 42.0% and 52.0% with scientific data
and novels, respectively. Given that a random-
guess baseline would achieve only 10% and 20%
(resp.), and that the classification is based on
just 11 features, this experiment demonstrates
how effectively the tree topology statistics cap-
ture idiolects. In general, lexicalized features
yield higher performance even after removing
topical words. This is expected since tokens
such as function words play an important role
in determining authorship (e.g., Mosteller and
Wallace (1984), Garcia and Martin (2007), Arg-
amon et al. (2007)).
A more important observation, however, is
that even after removing the leaf production
rules, accuracy as high as 93% (scientific) and
92.2% (novels) are obtained using syntactic fea-
Features Scientific Novels
+style11 +style11
style11 20.6 – 43.1 –
Unigram 56.9 – 69.3 –
synh 53.7 53.7 68.3 67.9
syn0 22.9 31.1 57.8 62.5
syn? 43.4 44.0 63.6 65.7
synl 51.1 51.7 71.3 72.8
synv+h 54.0 55.7 72.0 73.2
syn?h 63.1 64.0 72,1 73.2
syn?0 56.6 56.0 73.1 74.1
syn?? 56.3 57.2 74.0 74.9
syn?l 64.6 65.4 74.9 75.3
syn?v+h 64.0 67.7 74.0 74.7
pr 50.3 53.4 67.0 66.7
pˆr 59.1 60.6 69.7 68.7
pr? 63.7 65.1 71.5 73.2
pˆr? 66.3 69.4 73.6 74.9
Table 11: Authorship attribution with 20% train-
ing data. Improvement with addition of style11
shown in bold.
tures, which demonstrates that there are syn-
tactic patterns unique to each author. Also no-
tice that using only production rules, we achieve
higher accuracy in novels (90.1%), but the ad-
dition of style11 features yields better results
with scientific data (93.0%).
Using different amounts of training data pro-
vides insight about the influence of lexical clues.
In the scientific dataset, increasing the amount
of training data decreases the average perfor-
mance difference between lexicalized and unlex-
icalized features: 13.5% to 11.6%. In novels,
however, we see the opposite trend: 6.1% in-
creases to 8.1%.
We further observe that with scientific data,
increasing the amount of training data improves
the average performance across all unlexicalized
feature-sets from 50.0% to 82.9%, an improve-
ment of 32.8%. For novels, the corresponding
improvement is small in comparison: 17.0%.
This difference is expected. While authors
such as Dickens or Hardy have their unique writ-
ing styles that a classifier can learn based on few
documents, capturing idiolects in the more rigid
domain of scientific writing is far from obvious
with little training data.
1529
Features Scientific Novels
+style11 +style11
style11 42.0 – 52.0 –
Unigram 88.0 – 92.7 –
synh 85.0 85.0 87.6 88.9
syn0 40.0 53.0 66.4 72.3
syn? 78.0 82.0 80.3 82.3
synl 85.0 92.0 89.3 92.2
synv+h 89.0 93.0 90.1 91.2
syn?h 93.0 93.0 93.7 93.9
syn?0 92.0 94.0 92.1 93.2
syn?? 93.0 94.0 93.4 94.5
syn?l 93.0 95.0 94.9 95.2
syn?v+h 94.0 96.0 94.7 94.8
pr 85.0 86.0 86.7 86.7
pˆr 87.0 89.0 88.2 89.3
pr? 93.0 94.0 92.1 93.2
pˆr? 94.0 95.0 94.5 95.1
Table 12: Authorship attribution with 80% train-
ing data.
Turning to lexicalized features, we note that
with more training data, lexical cues perform
better in scientific domain than in novels. With
80% data used for training, the average per-
formance of lexicalized feature-sets with science
data is 94.4%, and slightly lower at 94.3% for
novels. With less training data, however, these
figures are 63.5% and 74.3% respectively.
Finally, we point out that adding the style
features derived from sentence types and tree
topologies almost always improves the perfor-
mance. In scientific data, syn?v+h with style11
features shows the best performance (96%),
while syn?l yields the best results for novels
(95.2%). For unlexicalized features, adding
style11 to synv+h and synl yields respective
improvements of 4.0% and 2.9% in the two
datasets.
7 Related Work
There are several hurdles in authorship attribu-
tion. First and foremost, writing style is ex-
tremely domain-dependent. Much of previous
research has focused on several domains of writ-
ing, such as informal modern writing in blogs
and online messages (Zheng et al., 2006), rela-
tively formal contemporary texts such as news
articles (Raghavan et al., 2010), or classical lit-
erature like novels and proses (e.g., (Burrows,
2002), (Hoover, 2004)).
The nature of these features have also var-
ied considerably. Character level n-grams have
been used by several researchers; most notably
by Peng et al. (2003), by Houvardas and Sta-
matatos (2006) for feature selection, and by Sta-
matatos (2006) in ensemble learning. Keselj et
al. (2003) employed frequency measures on n-
grams for authorship attribution.
Others, such as Zhao and Zobel (2005), Arg-
amon and Levitan (2004), Garcia and Martin
(2007), have used word-level approaches instead,
incorporating the differential use of function
words by authors.
More sophisticated linguistic cues have been
explored as well: parts-of-speech n-grams
(Diederich et al., 2003), word-level statistics to-
gether with POS-sequences (Luyckx and Daele-
mans, 2008), syntactic labels from partial pars-
ing (Hirst and Feiguina, 2007), etc. The use
of syntactic features from parse trees in au-
thorship attribution was initiated by Baayen et
al. (1996), and more recently, Raghavan et al.
(2010) have directly employed PCFG language
models in this area.
Syntactic features from PCFG parse trees
have also been used for gender attribution
(Sarawgi et al., 2011), genre identification (Sta-
matatos et al., 2000), native language identifi-
cation (Wong and Dras, 2011) and readability
assessment (Pitler and Nenkova, 2008). The
primary focus of most previous research, how-
ever, was to attain better classification accuracy,
rather than providing linguistic interpretations
of individual authorship and their stylistic ele-
ments.
Our work is the first to attempt authorship
attribution of scientific papers, a contemporary
domain where language is very formal, and the
stylistic variations have limited scope. In ad-
dition to exploring this new domain, we also
present a comparative study expounding the
role of syntactic features for authorship attri-
bution in classical literature. Furthermore, our
work is also the first to utilize tree topological
1530
features (Chan et al., 2010) in the context of
stylometric analysis.
8 Conclusion
In this paper, we have presented a comprehen-
sive exploration of syntactic elements in writing
styles, with particular emphasis on interpretable
characterization of stylistic elements, thus dis-
tinguishing our work from other recent work on
syntactic stylometric analysis. Our analytical
study provides novel statistically supported in-
sights into stylistic elements that have not been
computationally analyzed in previous literature.
In the future, we plan to investigate the use of
syntactic feature generators for text categoriza-
tion (e.g., Collins and Duffy (2002), Moschitti
(2008), Pighin and Moschitti (2009)) for stylom-
etry analysis.
Acknowledgments Yejin Choi is partially
supported by the Stony Brook University Office
of the Vice President for Research. We thank
reviewers for many insightful and helpful com-
ments.
References
Shlomo Argamon and Shlomo Levitan. 2004. Mea-
suring the usefulness of function words for author-
ship attribution. Literary and Linguistic Comput-
ing, pages 1–3.
Shlomo Argamon, Casey Whitelaw, Paul Chase,
Sobhan Raj Hota, Navendu Garg, and Shlomo
Levitan. 2007. Stylistic text classification using
functional lexical features: Research articles. J.
Am. Soc. Inf. Sci. Technol., 58(6):802–822.
H. Baayen, H. Van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: Using syntactic an-
notation to enhance authorship attribution. Lit-
erary and Linguistic Computing, 11(3):121.
H. Baayen, H. van Halteren, A. Neijt, and
F. Tweedie. 2002. An experiment in authorship
attribution. In 6th JADT. Citeseer.
A. Bain. 1887. English Composition and Rhetoric:
Intellectual elements of style. D. Appleton and
company.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and
Y.F. Tan. 2008. The acl anthology reference
corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proc.
of the 6th International Conference on Language
Resources and Evaluation Conference (LREC08),
pages 1755–1759.
J. Burrows. 2002. Delta: A measure of stylistic dif-
ference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267–287.
Samuel W. K. Chan, Lawrence Y. L. Cheung, and
Mickey W. C. Chong. 2010. Tree topological fea-
tures for unlexicalized parsing. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics: Posters, COLING ’10, pages
117–125, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ’02,
pages 263–270, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Diederich, J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence,
19(1):109–123.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–
1874.
Antonion Miranda Garcia and Javier Calle Mar-
tin. 2007. Function words in authorship attribu-
tion studies. Literary and Linguistic Computing,
22(1):49–66.
Graeme Hirst and Olga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405–417.
D. L. Hoover. 2004. Testing burrow’s delta. Literary
and Linguistic Computing, 19(4):453–475.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proc.
of the 12th International Conference on Artificial
Intelligence: Methodology, Systems and Applica-
tions, volume 4183 of LNCS, pages 77–86, Varna,
Bulgaria. Springer.
Aravind K. Joshi. 1992. Statistical language mod-
eling. In Proceedings of a Workshop Held at Har-
riman, New York, February 23-26, 1992. Associa-
tion for Computational Linguistics.
S. Kemper. 1987. Life-span changes in syntactic
complexity. Journal of gerontology, 42(3):323.
Vlado Keselj, Fuchun Peng, Nick Cercone, and
Calvin Thomas. 2003. N-gram-based author pro-
files for authorship attribution. In Proc. of the
1531
Pacific Association for Computational Linguistics,
pages 255–264.
M. Koppel and J. Schler. 2003. Exploiting stylistic
idiosyncrasies for authorship attribution. In Pro-
ceedings of IJCAI, volume 3, pages 69–72. Cite-
seer.
D. Lin. 1995. University of manitoba: descrip-
tion of the pie system used for muc-6. In Pro-
ceedings of the 6th conference on Message under-
standing, pages 113–126. Association for Compu-
tational Linguistics.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 64–71.
Association for Computational Linguistics.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship attribution and verification with many au-
thors and limited data. In COLING ’08, pages
513–520.
T.C. Mendenhall. 1887. The characteristic curves of
composition. Science, ns-9(214S):237–246.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceedings of the 17th ACM conference on In-
formation and knowledge management, CIKM ’08,
pages 253–262, New York, NY, USA. ACM.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent au-
thorship attribution using character level language
models. In Proceedings of the tenth conference on
European chapter of the Association for Compu-
tational Linguistics - Volume 1, EACL ’03, pages
267–274, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404–411.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, EMNLP ’09, pages 111–120,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: a unified framework for predicting
text quality. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 186–195, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arthus Quinn. 1995. Figures of Speech: 60 Ways To
Turn A Phrase. Routledge.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using
probabilistic context-free grammars. In Proceed-
ings of the ACL 2010 Conference Short Papers,
pages 38–42, Uppsala, Sweden. Association for
Computational Linguistics.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin
Choi. 2011. Gender attribution: tracing stylo-
metric evidence beyond topic and genre. In Pro-
ceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ’11,
pages 78–86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
K.T. Shao. 1990. Tree balance. Systematic Biology,
39(3):266.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis. 2000. Automatic text catego-
rization in terms of genre and author. Comput.
Linguist., 26(4):471–495.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001. Computer-based authorship attribution
without lexical measures. Computers and the Hu-
manities, 35(2):193–214.
E. Stamatatos. 2006. Ensemble-based author iden-
tification using character n-grams. ReCALL, page
4146.
W. Strunk and E.B. White. 2008. The elements of
style. Penguin Group USA.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1600–1610, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ying Zhao and Justin Zobel. 2005. Effective
and scalable authorship attribution using func-
tion words. In Proceedings of the Second Asia
conference on Asia Information Retrieval Technol-
ogy, AIRS’05, pages 174–189, Berlin, Heidelberg.
Springer-Verlag.
Y. Zhao and J. Zobel. 2007. Searching with style:
Authorship attribution in classic literature. In
Proceedings of the thirtieth Australasian confer-
ence on Computer science-Volume 62, pages 59–
68. Australian Computer Society, Inc.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship identi-
fication of online messages: Writing-style features
1532
and classification techniques. J. Am. Soc. Inf. Sci.
Technol., 57(3):378–393.
1533
