Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627–632,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Non-linear Mapping for Improved Identification of 1300+ Languages
Ralf D. Brown
Carnegie Mellon University Language Technologies Institute
5000 Forbes Avenue, Pittsburgh PA 15213 USA
ralf@cs.cmu.edu
Abstract
Non-linear mappings of the form
P (ngram)
?
and
log(1+?P (ngram))
log(1+?)
are applied to the n-gram probabilities
in five trainable open-source language
identifiers. The first mapping reduces
classification errors by 4.0% to 83.9%
over a test set of more than one million
65-character strings in 1366 languages,
and by 2.6% to 76.7% over a subset of 781
languages. The second mapping improves
four of the five identifiers by 10.6% to
83.8% on the larger corpus and 14.4% to
76.7% on the smaller corpus. The subset
corpus and the modified programs are
made freely available for download at
http://www.cs.cmu.edu/?ralf/langid.html.
1 Introduction
Language identification, particularly of short
strings, is a task which is becoming quite impor-
tant as a preliminary step in much automated pro-
cessing of online data streams such as microblogs
(e.g. Twitter). In addition, an increasing num-
ber of languages are represented online, so it is
desireable that performance remain high as more
languages are added to the identifier.
In this paper, we stress-test five open-source
n-gram-based language identifiers by presenting
them with 65-character strings (about one printed
line of text in a book) in up to 1366 languages. We
then apply a simple modification to their scoring
algorithms which improves the classification ac-
curacy of all five of them, three quite dramatically.
2 Method
The selected modification to the scoring algorithm
is to apply a non-linear mapping which spreads
out the lower probability values while compact-
ing the higher ones. This low-end spreading of
values is the opposite of what one sees in a Zip-
fian distribution (Zipf, 1935), where the proba-
bilities of the most common items are the most
spread out while the less frequent items become
ever more crowded as there are increasing num-
bers of them in ever-smaller ranges. The hypoth-
esis is that regularizing the spacing between val-
ues will improve language-identification accuracy
by avoiding over-weighting frequent items (from
having higher probabilities in the training data and
also occurring more frequently in the test string).
Two functions were selected for experiments:
x = P (ngram)
gamma:
y = x
?
loglike:
y =
log(1 + 10
?
x)
log(1 + 10
?
)
The first simply raises the n-gram probabil-
ity to a non-unity power; this exponent is named
“gamma” as in image processing (Poynton, 1998).
The second mapping function is a normalized vari-
ant of the logarithm function; the normalization
provides fixed points at 0 and 1, as is the case for
gamma. Each of the functions gamma and loglike
has one tunable parameter, ? and ? , respectively.
3 Related Work
Although n-gram statistics as a basis for language
identification has been in use for two decades since
Cavnar and Trenkle (1994) and Dunning (1994),
little work has been done on trying to optimize
the values used for those n-gram statistics. Where
some form of frequency mapping is used, it is of-
ten implicit (as in Cavnar and Trenkle’s use of
ranks instead of frequencies) and generally goes
unremarked as such.
Vogel and Tresner-Kirsch (2012) use the log-
arithm of the frequency for some experimental
runs, reporting that it improved accuracy in some
cases. Gebre et al (2013) used logarithmic term-
frequency scaling of words in an English-language
627
essay to classify the native language of the writer,
reporting an improvement from 82.36% accuracy
to 84.55% in conjunction with inverse document
frequency (IDF) weighting, and from 79.18% ac-
curacy to 80.82% without IDF.
4 Programs
4.1 LangDetect
LangDetect, version 2011-09-13 (Shuyo,
2014), uses the Naive Bayes approach. Inputs are
split into a bag of character n-grams of length 1
through 3; each randomly-drawn n-gram’s prob-
ability in each of the trained models is multiplied
by the current score for that model. After 1000
n-grams, or when periodic renormalization into
a probability distribution detects that one model
has accumulated an overwhelming probability
mass, the iteration is terminated. After averaging
seven randomized iterations, each with a random
gaussian offset (mean 5×10
?6
, standard deviation
0.5× 10
?6
) that is added to each probability prior
to multiplication (to avoid multiplication by zero),
the highest-scoring model is declared to be the
language of the input.
The mapping function is applied to the model’s
probability before adding the randomized off-
set. To work around the limitation of one model
per language code, disambiguating digits are ap-
pended to the language code during training and
removed from the output prior to scoring.
4.2 libtextcat
libtextcat, version 2.2-9 (Hugueney, 2011),
is a C reimplementation of the Cavnar and Tren-
kle (1994) method. It compiles “fingerprints” con-
taining a ranked list of the 400 (by default) most
frequent 1- through 5-grams in the training data.
An unknown text is classified by forming its fin-
gerprint and comparing that fingerprint against the
trained fingerprints. A penalty is assigned based
on the number of positions by which each n-gram
differs between the input and the trained model;
n-grams which appear in only one of the two are
assigned the maximum penalty, equal to the size
of the fingerprints. The model with the lowest
penalty score is selected as the language of the in-
put.
For this work, the libtextcat source code
was modified to remove the hard-coded fingerprint
size of 400 n-grams. While adding the frequency
mapping, the code was discovered to also hard-
code the maximum distortion penalty at 400; this
was corrected to set the maximum penalty equal to
the maximum size of any loaded fingerprint.
1
Score mapping was implemented by dividing
each penalty value by the maximum penalty to
produce a proportion, applying the mapping func-
tion, and then multiplying the result by the maxi-
mum penalty and rounding to an integer (to avoid
other code changes). Because there are only a lim-
ited number of possible penalties, a lookup table is
pre-computed, eliminating the impact on speed.
4.3 mguesser
mguesser, version 0.4 (Barkov, 2008), is part of
the mnoGoSearch search engine. While its doc-
umentation indicates that it implements the Cav-
nar and Trenkle approach, its actual similarity
computation is very different. Each training and
test text is converted into a 4096-element hash ta-
ble by extracting byte n-grams of length 6 (trun-
cated at control characters and multiple consecu-
tive blanks), hashing each n-gram using CRC-32,
and incrementing the count for the corresponding
hash entry. The hash table entries are then nor-
malized to a mean of 0.0 and standard deviation
of 1.0, and the similarity is computed as the inner
(dot) product of the hash tables treated as vectors.
The trained model receiving the highest similarity
score against the input is declared the language of
the input.
Nonlinear mapping was added by inserting a
step just prior to the normalization of the hash ta-
ble. The counts in the table are converted to proba-
bilities by dividing by the sum of counts, the map-
ping is applied to that probability, and the result is
converted back into a count by multiplying by the
original sum of counts.
4.4 whatlang
whatlang, version 1.24 (Brown, 2014a), is
the stand-alone identification program from LA-
Strings (Brown, 2013). It performs identifica-
tion by computing the inner product of byte tri-
grams through k-grams (k=6 by default and in
this work) between the input and the trained mod-
els; for speed, the computation is performed in-
crementally, adding the length-weighted probabil-
1
The behavior observed by (Brown, 2013) of performance
rapidly degrading for fingerprints larger than 500 disappears
with this correction. It was an artifact of an increasing pro-
portion of n-grams present in the model receiving penalties
greater than n-grams absent from the model.
628
ity of each n-gram as it is encountered in the in-
put. Models are formed by finding the highest-
frequency n-grams of the configured lengths, with
some filtering as described in (Brown, 2012).
4.5 YALI
YALI (Yet Another Language Identifier) (Majlis,
2012) is an identifier written in Perl. It performs
minor text normalization by collapsing multiple
blanks into a single blank and removing leading
and trailing blanks from lines. Thereafter, it uses
a sliding window to generate byte n-grams of a
(configurable) fixed length, and sums the proba-
bilities for each n-gram in each trained model. As
with whatlang, this effectively computes the in-
ner products between the input and the models.
Mapping was added by applying the mapping
function to the model probabilities as they are
read in from disk. As with LangDetect, disam-
biguating digits were used to allow multiple mod-
els per language code.
5 Data
The data used for the experiments described in
this paper comes predominantly from Bible trans-
lations, Wikipedia, and the Europarl corpus of Eu-
ropean parliamentary proceedings (Koehn, 2005).
The 1459 files of the training corpus generate 1483
models in 1368 languages. A number of train-
ing files generate models in both UTF-8 and ISO
8859-1, numerous languages have multiple train-
ing files in different writing systems, and several
have multiple files for different regional variants
(e.g. European and Brazilian Portugese).
The text for a language is split into training,
test, and possibly a disjoint development set. The
amount of text per language varies, with quartiles
of 1.19/1.47/2.22 million bytes. In general, ev-
ery thirtieth line of text is reserved for the test set;
some smaller languages reserve a higher propor-
tion. If more than 3.2 million bytes remain af-
ter reserving the test set, every thirtieth line of
the remaining text is reserved as a development
set. There are development sets for 220 languages.
The unreserved test is used for model training.
The test data is word-wrapped to 65 characters
or less, and wrapped lines shorter than 25 bytes
are excluded. Up to the first 1000 lines of wrapped
text are used for testing. One language with fewer
than 50 test strings is excluded from the test set, as
is the constructed language Klingon due to heavy
pollution with English. In total, the test files con-
tain 1,090,571 lines of text in 1366 languages.
Wikipedia text and many of the Bible transla-
tions are redistributable under Creative Commons
licenses, and have been packaged into the LTI
LangID Corpus (Brown, 2014b). This smaller
corpus contains 781 languages, 119 of them with
development sets, and a total of 649,589 lines in
the test files. The languages are a strict subset
of those in the larger corpus, but numerous lan-
guages have had Wikipedia text substituted for
non-redistributable Bible translations.
6 Experiments
Using the data sets described in the previous sec-
tion, we ran a sweep of different gamma and tau
values for each language identifier to determine
their optimal values on both development and test
strings. Step sizes for ? were generally 0.1, while
those for ? were 1.0, with smaller steps near the
minima. Since it does not provide explicit con-
trol over model sizes, LangDetect was trained
on a maximum of 1,000,000 bytes per model, as
reported optimal in (Brown, 2013). The other pro-
grams were trained on a maximum of 2,500,000
bytes per model; libtextcat and whatlang
used default model sizes of 400 and 3500, respec-
tively, while mguesser was set to the previously-
reported 1500 n-grams per model. After some ex-
perimentation, YALI was set to use 5-grams, with
3500 n-grams per model to match whatlang.
7 Results
Tables 1 and 2 show the absolute performance and
relative percentage change in classification errors
for the five programs using the two mapping func-
tions, as well as the values of ? and ? at which the
fewest errors were made on the development set.
Overall, the smaller corpus performed worse due
to the greater percentage of Wikipedia texts, which
are polluted with words and phrases in other lan-
guages. In the test set, this occasionally causes
a correct identification as another language to be
scored as an error.
Figures 2 and 3 graph the classification error
rates (number of incorrectly-labeled strings di-
vided by total number of strings in the test set) in
percent for different values of ?. A gamma of 1.0
is the baseline condition. The dramatic improve-
ments in mguesser, whatlang and YALI are
quite evident, while the smaller but non-trivial im-
629
gamma mapping loglike mapping
Program Error% Error% ?% ? Error% ?% ?
LangDet. 3.233 2.767 -14.4 0.80 2.889 -10.6 1.0
libtextcat 6.787 6.514 -4.0 2.20 – – –
mguesser 15.704 4.330 -72.4 0.39 4.177 -73.4 3.8
whatlang 13.309 2.136 -83.9 0.27 2.146 -83.8 4.5
YALI 9.883 2.313 -76.6 0.20 2.313 -76.6 8.0
Table 1: Language-identification accuracy on the 1366-language corpus. ? and ? were tuned on the
220-language development set; only marginally better results can be achieved by tuning on the test set.
gamma mapping loglike mapping
Program Error% Error% ?% ? Error% ?% ?
LangDet. 3.603 3.093 -14.2 0.68 3.083 -14.4 2.3
libtextcat 6.693 6.521 -2.6 1.70 – – –
mguesser 14.200 4.936 -65.2 0.40 4.779 -66.3 3.7
whatlang 11.879 2.770 -76.7 0.14 2.772 -76.7 5.6
YALI 8.726 2.972 -65.9 0.09 2.989 -65.7 9.0
Table 2: Language-identification accuracy on the 781-language corpus. ? and ? were tuned on the 119-
language development set. libtextcat did not improve with the loglike mapping (see text).
provements in libtextcat are difficult to dis-
cern at this scale. Since libtextcat uses much
smaller models than the others by default, Figure
1 gives a closer look at its performance for larger
model sizes. As the models grow, the absolute
baseline performance improves, but the change
from gamma-correction decreases and the optimal
value of ? also decreases toward 1.0. This hints
that the implicit mapping of ranks either becomes
closer to optimal, or that gamma becomes less ef-
fective at correcting it. At a model size of 3000
n-grams, the baseline error rate is 2.465% while
the best performance is 2.457% at ? = 1.10.
That the best ? for libtextcat is greater
than 1.0 was not entirely unexpected. The power-
law distribution of n-gram frequencies implies
that the conversion from frequencies to ranks is
essentially logarithmic, and log n eventually be-
comes less than n
c
for any c > 0. The implication
of ? > 1 is simply that the conversion to ranks
is too strong a correction, which must be partially
undone by the gamma mapping.
Figures 4 and 5 graph the error rates for differ-
ent values of ? . On the graph, zero is the baseline
condition without mapping for comparison pur-
poses; the mapping function is not the identity for
? = 0. It can clearly be seen that libtextcat is
hurt by the loglike mapping, which never reduces
values, even with negative ? . Using the inverse of
2
3
4
5
6
7
8
9
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
n=400
n=500
n=2000
n=3000
Figure 1: libtextcat performance at different
fingerprint sizes. ? = 1 is the baseline.
the loglike mapping should improve performance,
but has not yet been tried. The other programs
show very similar behavior to their results with
gamma.
8 Conclusions and Future Work
Non-linear mapping is shown to be effective at
improving the accuracy of five different language
630
24
6
8
10
12
14
16
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 2: Performance of the identifiers on the
1366-language corpus using the gamma mapping.
2
4
6
8
10
12
14
16
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 3: Performance of the identifiers on the
781-language corpus using the gamma mapping.
identifier using four highly-divergent algorithms
for computing model scores from n-gram statis-
tics. Improvements range from small – 2.6% re-
duction in classification errors – to dramatic for
the three programs with the worst baselines – 65.2
to 76.7% reduction in errors on the smaller cor-
pus and 72.4 to 83.9% on the larger. While both
mappings have similar performance for four of the
programs, libtextcat only benefits from the
gamma mapping, as it can also reduce n-gram
scores, unlike the loglike mapping.
2
4
6
8
10
12
14
16
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Er
ro
r R
at
e 
(%
)
Tau
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 4: Performance of the identifiers on the
1366-language corpus using the loglike mapping.
2
4
6
8
10
12
14
16
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Er
ro
r R
at
e 
(%
)
Tau
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 5: Performance of the identifiers on the
781-language corpus using the loglike mapping.
Training data, source code, and supple-
mentary information may be downloaded from
http://www.cs.cmu.edu/?ralf/langid.html.
Future work includes modifying additional lan-
guage identifiers such as langid.py (Lui and
Baldwin, 2012) and VarClass (Zampieri and
Gebre, 2014), experimenting with other mapping
functions, and investigating the method’s efficacy
on pluricentric languages like those VarClass is
designed to identify.
631
References
Alexander Barkov. 2008. mguesser ver-
sion 0.4. http://www.mnogosearch.org/guesser/-
mguesser-0.4.tar.gz (accessed 2014-08-19).
Ralf D. Brown. 2012. Finding and Identifying Text in
900+ Languages. Digital Investigation, 9:S34–S43.
Ralf D. Brown. 2013. Selecting and Weighting N-
Grams to Identify 1100 Languages. In Proceedings
of Text, Speech, and Discourse 2013, September.
Ralf Brown. 2014a. Language-Aware String Extractor,
August. https://sourceforge.net/projects/la-strings/
(accessed 2014-08-19).
Ralf D. Brown. 2014b. LTI LangID Corpus, Release
1. http://www.cs.cmu.edu/?ralf/langid.html.
William B. Cavnar and John M. Trenkle. 1994. N-
Gram-Based Text Categorization. In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161–175.
UNLV Publications/Reprographics, April.
Ted Dunning. 1994. Statistical Identification of Lan-
guage. Technical Report MCCS 94-273, New Mex-
ico State University.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving Na-
tive Language Identification with TF-IDF Weight-
ing. In Proceedings of the 8th NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (BEA8).
Bernard Hugueney. 2011. libtextcat 2.2-
9: Faster Unicode-focused C++ reimplementation
of libtextcat. https://github.com/scientific-coder/-
libtextcat (accessed 2014-08-19).
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summix
X), pages 79–86.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2012),
pages 25–30, July.
Martin Majlis. 2012. Yet Another Language Identi-
fier. In Proceedings of the Student Research Work-
shop at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 46–54, Avignon, France, April. Association
for Computational Linguistics.
Charles Poynton. 1998. The Rehabilitation of
Gamma. In Human Vision and Electronic Imag-
ing III, Proceedings of SPIE/IS&T Conference
3299, January. http://www.poynton.com/PDFs/-
Rehabilitation of gamma.pdf.
Nakatani Shuyo. 2014. Language Detection Li-
brary for Java, March. http://code.google.com/p/-
language-detection/ (accessed 2014-08-19).
John Vogel and David Tresner-Kirsch. 2012. Robust
Language Identification in Short, Noisy Texts: Im-
provements to LIGA. In Proceedings of the Third
International Workshop on Mining Ubiquitous and
Social Environments (MUSE 2012), pages 43–50,
September.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2014. VarClass: An Open Source Language Iden-
tification Tool for Language Varieties. In Proceed-
ings of the Ninth International Language Resources
and Evaluation Conference (LREC 2014), Reyk-
javik, Iceland, May.
George Kingsley Zipf. 1935. The Psycho-biology of
Language: An Introduction to Dynamic Philology.
Houghton-Mifflin Co., Boston.
632
