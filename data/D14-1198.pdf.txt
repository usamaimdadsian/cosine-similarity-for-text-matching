Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846–1851,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Qi Li
†
Heng Ji
†
Yu Hong
†‡
Sujian Li
¶
†
Computer Science Department, Rensselaer Polytechnic Institute, USA
‡
School of Computer Science and Technology, Soochow University, China
¶
Key Laboratory of Computational Linguistics, Peking University, MOE, China
†
{liq7,hongy2,jih}@rpi.edu,
¶
lisujian@pku.edu.cn
Abstract
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
1 Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D¨olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in Tel Aviv
AttackPerson Weapon Geopolitical Entity
Place
InstrumentAttacker
Agent-Artifact
Physical
x1 x2 x3 x4 x5 x6 x7 x8x:
y:
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention “Asif Mohammed Hanif ” is an Attacker
of the Attack event triggered by “detonated”, and
the Weapon mention “explosives” is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between “Asif Mohammed
Hanif ” and “Tel Aviv”.
However, in practice many useful interactions
are missing during testing because of the data spar-
1846
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE’05
1
training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
2 Approach
We uniformly represent the IE output from each
sentence as an information network y = (V,E).
Each node v
i
? V is represented as a triple
?u
i
, v
i
, t
i
? of start index u
i
, end index v
i
, and node
type t
i
. A node can be an entity mention or an
event trigger. A particular type of node is ? (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc e
j
? E is represented as ?u
j
, v
j
, r
j
?,
where u
j
and v
j
are the end offsets of the nodes,
and r
j
is the arc type. For instance, in Fig-
ure 1, the event trigger “detonated” is represented
as ?4, 4, Attack?, the entity mention “Asif Mo-
hammed Hanif ” is represented as ?1, 3, Person?,
and their argument arc is ?4, 3, Attacker?. Our
goal is to extract the whole information network y
for a given sentence x.
2.1 Decoding Algorithm
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x
1
, ..., x
m
) be the input sentence. The decoder
performs two types of actions at each token x
i
from left to right:
• NODEACTION(i, j): appends a new node
?j, i, t? ending at the i-th token, where i? d
t
<
j ? i, and d
t
is the maximal length of type-t
nodes in training data.
• ARCACTION(i, j): for each j < i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ?i, j, r?.
After each action, the top-k hypotheses are se-
lected according to their features f(x, y
?
) and
1
http://www.itl.nist.gov/iad/mig//tests/ace
weights w:
best
k
y
?
?buffer
f(x, y
?
) ·w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d · b · m
2
), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
2.2 Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
y
?
?Y(x)
f(x, y
?
) ·w
and then updates the feature weights by using:
w
new
= w + f(x, y)? f(x, z)
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
min ?w
new
?w?
s.t. w
new
f(x, y)?w
new
f(x, z) ? L(y, z)
?z ? best
k
(x,w)
We employ the following three loss functions
for comparison:
1847
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He
(arg-1)
was escorted
(trigger)
into Iraq
(arg-2)
.
46 Physical Attack Target Place Many people
(arg-1)
were in the cafe
(arg-2)
during the blast
(trigger)
.
42 Agent-Artifact Attack Attacker Instrument Terrorists
(arg-1)
might use
(trigger)
the devices
(arg-2)
as weapons.
41 Physical Transport Artifact Origin The truck
(arg-1)
was carrying
(trigger)
Syrians fleeing the war in Iraq
(arg-2)
.
33 Physical Meet Entity Place They
(arg-1)
have reunited
(trigger)
with their friends in Norfolk
(arg-2)
.
32 Physical Die Victim Place Two Marines
(arg-1)
were killed
(trigger)
in the fighting in Kut
(arg-2)
.
28 Physical Attack Attacker Place Protesters
(arg-1)
have been clashing
(trigger)
with police in Tehran
(arg-2)
.
26 ORG-Affiliation End-Position Person Entity NBC
(arg-2)
is terminating
(trigger)
freelance reporter Peter Arnett
(arg-1)
.
Table 1: Frequent overlapping relation and event types in the training set.
• The first one is F
1
loss:
L
1
(y, z) = 1?
2 · |y ? z|
|y|+ |z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1, |y| = 6.
• The second one is 0-1 loss:
L
2
(y, z) =
{
1 y 6= z
0 y = z
It does not discriminate the extent to which z
deviates from y.
• The third loss function counts the difference be-
tween y and z:
L
3
(y, z) = |y|+ |z| ? 2 · |y ? z|
Similar to F
1
loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
2.3 Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D¨olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y
?
dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y
?
. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
Attacker Instrument
Agent-Artifact
Attacker Place
Physical
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
Table 2: Data set
0 20 40 60 80 100Number of instances0
2
4
6
8
10
12
14
Freq
uenc
y
Trigger WordsFrame IDs
Figure 2: Distribution of triggers and their frames.
2.4 Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: “Others were mutilated beyond recogni-
tion.” The Injure trigger “mutilated” does not oc-
cur in our training data. But there are some sim-
ilar words such as “stab” and “smash”. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame “Cause harm” is closely
related with Injure event and contains 68 lexical
units such as “stab”, “smash” and “mutilate”.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
1848
Methods
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
P R F
1
P R F
1
P R F
1
P R F
1
Pipelined Baseline
83.6 75.7 79.5
68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F
1
Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L
3
Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, “Cause harm” will be a feature for “mu-
tilated”. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
3 Experiments
3.1 Data and Evaluation
We use ACE’05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F
1
measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
• An entity mention is correct if its entity type (7
in total) and head offsets are correct.
• A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
• An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
• An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
3.2 Results
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p < 0.05 for entity mention extrac-
tion, and p < 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L
3
loss function achieved
better performance than F
1
loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
3.3 Discussions
3.3.1 Feature Study
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w
1
=“home” Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most significant
features about event triggers. The 3
rd
, 6
th
, and
9
th
rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
1849
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of “MIRA w/ L
3
loss”. We found
that FrameNet based features provided 0.8% and
2.2% F
1
gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F
1
gain for relation extraction.
3.3.2 Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
• “Williams picked up the child again and this
time, threw
Attack
her out the window.”
The word “threw” is used as an Attack event
trigger because the Victim argument is a “child”.
• “Ellison to spend $10.3 billion to get
Merge Org
his company.” The common word “get” is
tagged as a trigger of Merge Org, because its
object is “company”.
• “We believe that the likelihood of them
using
Attack
those weapons goes up.”
The word “using” is used as an Attack event
trigger because the Instrument argument is
“weapons”.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
• “we are paying great attention to their ability to
defend
Attack
on the ground.”,
our system fails to extract “defend” as an Attack
trigger. In the training data, “defend” appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
• “North Korea could do everything to defend it-
self. ”
“defend” is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
• “It’s important that people all over the world
know that we don’t believe in the war
Attack
.”,
• “Nobody questions whether this
Attack
is right
or not.”
“this” refers to “war” in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
4 Conclusions
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE’05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
References
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161–164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551–560.
1850
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735–3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111–118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.
Johannes D¨olling. 2011. Aspectual coercion and even-
tuality structure. pages 189–226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142–151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73–82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91–98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640–1649.
1851
