Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 736–746,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Automatically Determining a Proper Length for Multi-document
Summarization: A Bayesian Nonparametric Approach
Tengfei Ma and Hiroshi Nakagawa
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
{matf@r., nakagawa@}dl.itc.u-tokyo.ac.jp
Abstract
Document summarization is an important task
in the area of natural language processing,
which aims to extract the most important in-
formation from a single document or a clus-
ter of documents. In various summarization
tasks, the summary length is manually de-
fined. However, how to find the proper sum-
mary length is quite a problem; and keeping
all summaries restricted to the same length
is not always a good choice. It is obvi-
ously improper to generate summaries with
the same length for two clusters of docu-
ments which contain quite different quantity
of information. In this paper, we propose
a Bayesian nonparametric model for multi-
document summarization in order to automat-
ically determine the proper lengths of sum-
maries. Assuming that an original document
can be reconstructed from its summary, we
describe the ”reconstruction” by a Bayesian
framework which selects sentences to form
a good summary. Experimental results on
DUC2004 data sets and some expanded data
demonstrate the good quality of our sum-
maries and the rationality of the length deter-
mination.
1 Introduction
Text summarization is the process of generating a
short version of a given text to indicate its main top-
ics. As the number of documents on the web expo-
nentially increases, text summarization has attracted
increasing attention, because it can help people get
the most important information within a short time.
In most of the existing summarization systems,
people need to first define a constant length to re-
strict all the output summaries. However, in many
cases it is improper to require all summaries are of
the same length. Take the multi-document summa-
rization as an example, generating the summaries
of the same length for a 5-document cluster and a
50-document cluster is intuitively improper. More
specifically, consider two different clusters of doc-
uments: one cluster contains very similar articles
which all focus on the same event at the same time;
the other contains different steps of the event but
each step has its own topics. The former cluster may
need only one or two sentences to explain its infor-
mation, while the latter needs to include more.
Research on summary length dates back in the
late 90s. Goldstein et al. (1999) studied the char-
acteristics of a good summary (single-document
summarization for news) and showed an empiri-
cal distribution of summary length over document
size. However, the length problem has been grad-
ually ignored later, since researchers need to fix
the length so as to estimate different summarization
models conveniently. A typical instance is the Doc-
ument Understanding Conferences (DUC)1, which
provide authoritative evaluation for summarization
systems. The DUC conferences collect news arit-
cles as the input data and define various summariza-
tion tasks, such as generic multi-document summa-
rization, query-focused summarization and update
summarization. In all the DUC tasks, the output is
restricted within a length. Then human-generated
1After 2007, the DUC tasks are incorporated into the Text
Analysis Conference (TAC).
736
summaries are provided to evaluate the results of dif-
ferent summarization systems. Limiting the length
of summaries contributed a lot to the development
of summarization techniques, but as we discussed
before, in many cases keeping the summaries of the
same size is not a good choice.
Moreover, even in constant-length summariza-
tion, how to define a proper size of summaries for
the summarization tasks is quite a problem. Why
does DUC2007 main task require 250 words while
Update task require 100 words? Is it reasonable?
A short summary may sacrifice the coverage, while
a long summary may cause redundance. Automati-
cally determining the best size of summaries accord-
ing to the input documents is valuable, and it may
deepen our understanding of summarization.
In this work, we aim to find the proper length
for document summarization automatically and gen-
erate varying-length summaries based on the doc-
ument itself. The varying-length summarization is
more robust for unbalanced clusters. It can also
provide a recommended size as the predefined sum-
mary length for general constant-length summariza-
tion systems. We advance a Bayesian nonparametric
model of extractive multi-document summarization
to achieve this goal. As far as we are concerned, it is
the first model that can learn appropriate lengths of
summaries.
Bayesian nonparametric (BNP) methods are pow-
erful tools to determine the size of latent vari-
ables (Gershman and Blei, 2011). They let the data
”speak for itself” and allow the dimension of la-
tent variables to grow with the data. In order to
integrate the BNP methods into document summa-
rization, we follow the assumption that the original
documents should be recovered from the reconstruc-
tion of summaries (Ma and Wan, 2010; He et al.,
2012). We use the Beta process as a prior to gen-
erate binary vectors for selecting active sentences
that reconstruct the original documents. Then we
construct a Bayesian framework for summarization
and use the variational approximation for inference.
Experimental results on DUC2004 dataset demon-
strate the effectiveness of our model. Besides, we
reorganize the original documents to generate some
new datasets, and examine how the summary length
changes on the new data. The results prove that our
summary length determination is rational and neces-
sary on unbalanced data.
2 Related Work
2.1 Research on Summary Length
Summary length is an important aspect for gener-
ating and evaluating summaries. Early research on
summary length (Goldstein et al., 1999) focused on
discovering the properties of human-generated sum-
maries and analyzing the effect of compression ratio.
It demonstrated that an evaluation of summarization
systems must take into account both the compres-
sion ratios and the characteristics of the documents.
Radev and Fan (2000) compared the readability and
speedup in reading time of 10% summaries and 20%
summaries2 for topic sets with different number of
documents. Sweeney et al. (2008) developed an in-
cremental summary containing additional sentences
that provide context. Kaisser et al. (2008) studied
the impact of query types on summary length of
search results. Other than the content of original
documents, there are also some other factors affect-
ing summary length especially in specific applica-
tions. For example, Sweeney and Crestani (2006)
studied the relation between screen size and sum-
mary length on mobile platforms. The conclusion of
their work is the optimal summary size always falls
into the shorter one regardless of the screen size.
In sum, the previous works on summary length
mostly put their attention on the empirical study of
the phenomenon, factors and impacts of summary
length. None of them automatically find the best
length, which is our main task in this paper. Nev-
ertheless, they demonstrated the importance of sum-
mary length in summarization and the reasonability
of determining summary length based on content of
news documents (Goldstein et al., 1999) or search
results (Kaisser et al., 2008). As our model is mainly
applied for generic summarization of news articles,
we do not consider the factor of screen size in mo-
bile applications.
2.2 BNP Methods in Document Summarization
Bayesian nonparametric methods provide a
Bayesian framework for model selection and
adaptation using nonparametric models (Gershman
210% and 20% are the compression rates, and the documents
are from search results in information retrieval systems.
737
and Blei, 2011). A BNP model uses an infinite-
dimensional parameter space, but invokes only a
finite subset of the available parameters on any
given finite data set. This subset generally grows
with the data set. Thus BNP models address the
problem of choosing the number of mixture compo-
nents or latent factors. For example, the hierarchical
Dirichlet process (HDP) can be used to infer the
number of topics in topic models or the number of
states in the infinite Hidden Markov model (Teh et
al., 2006).
Recently, some BNP models are also involved in
document summarization approaches (Celikyilmaz
and Hakkani-Tu¨r, 2010; Chang et al., 2011; Darling
and Song, 2011). BNP priors such as the nested Chi-
nese restaurant process (nCRP) are associated with
topic analysis in these models. Then the topic dis-
tributions are used to get the sentence scores and
rank sentences. BNP here only impacts the number
and the structure of the latent topics, but the sum-
marization framework is still constant-length. Our
BNP summarization model differs from the previous
models. Besides using the HDP for topic analysis,
our approach further integrates the beta process into
sentence selection. The BNP method in our model
are directly used to determine the number of sum-
mary sentences but not latent topics.
3 BNP Summarization
In this section, we first introduce the BNP priors
which will be used in our model. Then we propose
our model called BNP summarization.
3.1 The Beta Process and the Bernoulli process
The beta process(BP) (Thibaux and Jordan, 2007;
Paisley and Carin, 2009) and the related Indian buf-
fet process(IBP) (Griffiths and Ghahramani, 2005)
are widely applied to factor/feature analysis. By
defining the infinite dimensional priors, these factor
analysis models need not to specify the number of
latent factors but automatically determine it.
Definition of BP (Paisley et al., 2010): Let B
0
be
a continuous measure on a space ? and B
0
(?) = ?.
If Bk is defined as follows,
Bk =
N
?
k=1
?k??
k
,
?k ? Beta(
??
N
,?(1?
?
N
))
?k ?
1
?
B
0
(1)
(where ??
k
is the atom at the location ?k; and ? is a
positive scalar), then as N ? ?, Bk ? B and B is
a beta process: B ? BP (?B
0
).
Finite Approximation: The beta process is de-
fined on an infinite parameter space, but sometimes
we can also use its finite approximation by sim-
ply setting N to a large number (Paisley and Carin,
2009).
Bernoulli Process: The beta process is conju-
gate to a class of Bernoulli processes, denoted by
X ? Bep(B). If B is discrete, of the form in
(1), then X =
?
k bk??k where the bk are indepen-
dent Bernoulli variables with the probability p(bk =
1) = ?k. Due to the conjugation between the
beta process priors and Bernoulli process, the pos-
terior of B given M samples X
1
, X
2
, ...XM where
Xi ? Bep(B)fori = 1, , ,M. is also a beta process
which has updated parameters:
B|X
1
, X
2
, ..., XM
? BP (?+M, ??+MB0 +
1
c+M
?
iXi) (2)
Application of BP: Furthermore, marginalizing
over the beta process measure B and taking ? =
1, provides a predictive distribution on indicators
known as the Indian buffet process (IBP) (Thibaux
and Jordan, 2007). The beta process or the IBP is
often used in a feature analysis model to generate
infinite vectors of binary indicator variables(Paisley
and Carin, 2009), which indicates whether a feature
is used to represent a sample. In this paper, we use
the beta process as the prior to select sentences.
3.2 Framework of BNP Summarization
Most existing approaches for generic extractive
summarization are based on sentence ranking. How-
ever, these methods suffer from a severe problem
that they cannot make a good trade-off between
the coverage and minimum redundancy (He et al.,
738
2012). Some global optimization algorithms are de-
veloped, instead of greedy search, to select the best
overall summaries (Nenkova and McKeown, 2012).
One approach to global optimization of summariza-
tion is to regard the summarization as a reconstruc-
tion process (Ma and Wan, 2010; He et al., 2012)
. Considering a good summary must catch most of
the important information in original documents, the
original documents are assumed able to be recov-
ered from summaries with some information loss.
Then the summarization problem is turned into find-
ing the sentences that cause the least reconstruction
error (or information loss). In this paper, we fol-
low the assumption and formulate summarization as
a Bayesian framework.
First we review the models of (Ma and Wan,
2010) and (He et al., 2012). Given a cluster of
M documents x
1
, x
2
, ..., xM and the sentence set
contained in the documents as S = [s
1
, s
2
, ..., sN ],
we denote all corresponding summary sentences as
V = [v
1
, ..., vn], where n is the number of summary
sentences and N is the number of all sentences in
the cluster. A document xi and a sentence vi or si
here are all represented by weighted term frequency
vectors in the space Rd, where d is the number of
total terms (words).
Following the reconstruction assumption, a can-
didate sentence vi can be approximated by the
linear combination of summary sentences: si 
?n
j=1 w
?
jvj , where w
?
j is the weight for summary
sentence vj . Thus the document can also be ap-
proximately represented by a linear combination of
summary sentences (because it is the sum of the sen-
tences).
xi 
n
?
j=1
wjvj . (3)
Then the work in (He et al., 2012) aims to find
the summary sentence set that can minimize the re-
construction error
?N
i=1 ||si ?
?n
j=1 w
?
jvj ||
2; while
the work in (Ma and Wan, 2010) defines the prob-
lem as finding the sentences that minimize the dis-
tortion between documents and its reconstruction
dis(xi,
?n
j=1 wjvj) where this distortion function
can also be a squared error function.
Now we consider the reconstruction for each doc-
ument, if we see the document xi as the dependent
variable, and the summary sentence set S as the
independent variable, the problem to minimize the
reconstruction error can be seen as a linear regres-
sion model. The model can be easily changed to a
Bayesian regression model by adding a zero-mean
Gaussian noise  (Bishop, 2006), as follows.
xi =
n
?
j=1
wjvj + i (4)
where the weights wj are also assigned a Gaussian
prior.
The next step is sentence selection. As our sys-
tem is an extractive summarization model, all the
summary sentences are from the original document
cluster. So we can use a binary vector zi =<
zi1, ..., ziN >
T to choose the active sentences V
(i.e. summary sentences) from the original sen-
tence set S. The Equation (4) is turned into xi =
?N
j=1 ?ij ?zijsj+i. Using a beta process as a prior
for the binary vector zi, we can automatically infer
the number of active component associated with zi.
As to the weights of the sentences, we use a random
vector ?i which has the multivariate normal distri-
bution because of the conjugacy. ?i ? RN is an
extension to the weights {w
1
, ...wn} in (4).
Integrating the linear reconstruction (4) and the
beta process3 (1), we get the complete process of
summary sentence selection as follows.
xi = S(?i ? zi) + i
S = [s
1
, s
2
, ..., sN ]
zij ? Bernoulli(?j)
?j ? Beta(
??
N
,?(1?
?
N
))
?i ? N (0, ?
2
?I)
i ? N (0, ?
2
 I) (5)
where N is the number of sentences in the whole
document cluster. The symbol ? represents the ele-
mentwise multiplication of two vectors.
One problem of the reconstruction model is that
the word vector representation of the sentences are
sparse, which dramatically increase the reconstruc-
tion error. So we bring in topic models to reduce the
3We use the finite approximation because the number of sen-
tences is large but finite
739
dimension of the data. We use a HDP-LDA (Teh et
al., 2006) to get topic distributions for each sentence,
and we represent the sentences and documents as
the topic weight vectors instead of word weight vec-
tors. Finally xi is a K-dimensional vector and S is
a K ?N matrix, where K is the number of topics in
topic models.
4 Variational Inference
In this section, we derive a variational Bayesian al-
gorithm for fast inference of our sentence selec-
tion model. Variational inference (Bishop, 2006)
is a framework for approximating the true posterior
with the best from a set of distributions Q : q? =
argminq?QKL(q(Z)|p(Z|X)). Suppose q(Z) can
be partitioned into disjoint groups denoted by Zj ,
and the q distribution factorizes with respect to these
groups: q(Z) =
?M
j=1 q(Zj). We can obtain a gen-
eral expression for the optimal solution q?j (Zj) given
by
ln q?j (Zj) = Ei =j [ln p(X,Z)] + const. (6)
where Ei =j [ln p(X,Z)] is the expectation of the log-
arithm of the joint probability of the data and latent
variables, taken over all variables not in the parti-
tion. We will therefore seek a consistent solution
by first initializing all of the factors qj(Zj) appro-
priately and then cycling through the factors and re-
placing each in turn with a revised estimate given by
(6) evaluated using the current estimates for all of
the other factors.
Update for Z
p(zij |?j , xi, S, ?i) ? p(xi|zij , sj , ?i)p(zij |?j)
We use q(zij) to approximate the posterior:
q(zij)
? exp{E[ln(p(xi|zij , z
?j
i , S, ?i)) + ln(p(zij |?))]}
? exp{E[ln(?j)]}?
exp{E[?
1
2?2
(
x
?j
i ? sjzij?ij
)T (
x
?j
i ? sjzij?ij
)
]}
? exp{ln(?j)}?
exp{?
(
?2ij ? z
2
ij ? s
T
j sj ? 2?ij ? zij ? sj
T
? x
?j
i
)
2?2
}
(7)
where x?ji = xi ? S
?j(?
?j
i ? z
?j
i ), and the symbol
¯ indicates the expectation value. The ?2ij can be
extended to this form:
?2ij = ?ij
2
+?
j
i (8)
where ?ji means the j
th diagonal element of ?i
which is defined by Equation 13.
As zi is a binary vector, we only calculate the
probability of zij = 1 and zij = 0.
q(zij = 1) ? exp{ln(?j)} ?
exp{?
1
2?2
(
?2ij ? s
T
j sj ? 2?ij ? sj
T
? x
?j
i
)
}
q(zij = 0) ? exp{ln(1? ?j)} (9)
The expectations can be calculated as
ln(?j) = ?(
??
N
+ nj)? ?(?+M) (10)
ln(1? ?j) = ?(?(1?
?
N
)+M ?nj)??(?+M)
(11)
where nj =
?M
i=1 zij .
Update for ?
p(?j |Z) ? p(?j |?, ?,N)p(Z|?j)
Because of the conjugacy of the beta to Bernoulli
distribution, the posterior of ? is still a beta distribu-
tion:
?j ? Beta(
??
N
+ nj , ?(1?
?
N
) +M ? nj) (12)
Update for ?
p(?i|xi, Z, S) ? p(xi|?i, Z, S)p(?i|?
2
?)
The posterior is also a normal distribution with mean
?i and covariance ?i.
?i =
(
1
?2
S˜i
T
S˜i +
1
?2?
I
)
?1
(13)
?i = ?i
(
1
?2
S˜i
T
xi
)
(14)
Here S˜i ? S ? z˜i and z˜i ? [zi, ..., zi]T is a K × N
matrix with the vector zi repeated K(the number of
the latent topics) times.
S˜i = S ? z˜i (15)
740
S˜i
T
S˜i = (S
TS) ? (zi ? zi
T +Bcovi) (16)
Bcovi = diag[zi1(1? zi1), ..., ziN (1? ziN )] (17)
Update for ?2
p(?2 |?, X, Z, S) ? p(X|?, Z, S, ?
2
 )p(?
2
 )
By using a conjugate prior, inverse gamma prior
InvGamma(u, v), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
u? = u+MK/2
v? = v +
1
2
M
?
i=1
(||xi ? S(zi ? ?i)||+ ?i)
(18)
where
?i =
?N
j=1(z
2
ij ? ?
2
ij ? s
T
j sj ? zij
2
? ?ij
2
? sTj sj)
+
?
j =l zij ? zil ??i,jl ? s
T
j sl
Update for ?2?
p(?2?|?) ? p(?|?
2
?)p(?
2
?)
By using a conjugate prior, inverse gamma prior
InvGamma(e, f), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
e? = e+MN/2
f ? = f +
1
2
M
?
i=1
(
(?)T?+ trace(??i)
)
(19)
5 Experiments
To test the capability of our BNP summarization sys-
tems, we design a series of experiments. The aim of
the experiments mainly includes three aspects:
1. To demonstrate the summaries extracted by our
model have good qualities and the summary
length determined by the model is reasonable.
2. To give examples where varying summary
length is necessary.
3. To observe the distribution of summary length.
We evaluate the performance on the dataset of
DUC2004 task2. The data contains 50 document
clusters, with 10 news articles in each cluster. Be-
sides, we construct three new datasets from the
DUC2004 dataset to further prove the advantage of
variable-length summarization. We separate each
cluster in the original dataset into two parts where
each has 5 documents, hence getting the Separate
Dataset; Then we randomly combine two origi-
nal clusters in the DUC2004 dataset, and get two
datasets called Combined1 and Combined2. Thus
each of the clusters in the combined datasets include
20 documents with two different themes.
5.1 Evaluation of Summary Qualities
First, we implement our BNP summarization model
on the DUC2004 dataset, with summary length not
limited. At the topic analysis step, we use the HDP
model and follow the inference in (Teh et al., 2006).
For the sentence selection step, we use the varia-
tional inference described in Section 4, where the
parameters in the beta process (5) are set as ? =
1, ? = 1. The summaries that we finally generate
have an average length of 164 words. We design sev-
eral popular unsupervised summarization systems
and compare them with our model.
• The Random model selects sentences randomly
for each document cluster.
• The MMR (Carbonell and Goldstein, 1998)
strives to reduce redundancy while maintaining
relevance. For generic summarization, we re-
place the query relevance with the relevance to
documents.
• The Lexrank model (Erkan and Radev, 2004) is
a graph-based method which choose sentences
based on the concept of eigenvector centrality.
• The Linear Representation model (Ma and
Wan, 2010) has the same assumption as ours
and it can be seen as an approximation of the
constant-length version of our model.
741











	


	



    
	




	

Figure 1: Rouge-1 values on DUC2004 dataset.













	


	




    
	




	

Figure 2: Rouge-2 values on DUC2004 dataset.











	


	



    
	




	

Figure 3: Rouge-L values on DUC2004 dataset.
All the compared systems are implemented at dif-
ferent predefined lengths from 50 to 300 words.
Then we evaluate the summaries with ROUGE4
tools (Lin and Hovy, 2003) in terms of the f-measure
4we use ROUGE1.5.5 in this work.
scores of Rouge-1 Rouge-2, and Rouge-L. The met-
ric of Rouge f-measure takes into consideration the
summary length in evaluation, so it is proper for
our experiments. From Fig.1, Fig.2 and Fig.3, we
can see that the result of BNP summarization (the
dashed line) gets the second best value among all
systems. It is only defeated by the Linear model
but the result is comparable to the best in Fig.1 and
Fig.3; while it exceeds other systems at all lengths.
This proves the good qualities of our BNP sum-
maries. The reason that the Linear system gets a
little better result may be its weights for linear com-
bination of summary sentences are guaranteed non-
negative while in our model the weights are zero-
mean Gaussian variables. This may lead to less re-
dundance in sentence selection for the Linear Rep-
resentation model.
Turn to the length determination. We take ad-
vantage of the Linear Representation model to ap-
proximate the constant-length version of our model.
Comparing the summaries generated at different
predefined lengths, Fig.4 shows the the model gets
the best performance (Rouge values) at the length
around 164 words, the length learned by our BNP
model. This result partly demonstrates our length
determination is rational and it can be used as the
recommended length for some constant-length sum-
marization systems, such as the Linear .


 



	
 !
 !


 !
 !"#


    
	


Figure 4: Rate-dist value V.S. summary word length.
742
5.2 A New Evaluation Metric
The Rouge evaluation requires golden standard sum-
maries as the base. However, in many cases we
cannot get the reference summaries. For example,
when we implement experiments on our expanded
datasets (the separate and combined clusters of doc-
uments), we do not have exact reference summaries.
Louis and Nenkova (2009) advanced an automatic
summary evaluation without human models. They
used the Jensen-Shannon divergence(JSD) between
the input documents and the summaries as a fea-
ture, and got high correlation with human evalua-
tions and the rouge metric. Unfortunately, it was
designed for comparison at a constant-length, which
cannot meet our needs. To extend the JSD evaluation
to compare varying-length summaries, we propose a
new measure based on information theory, the rate-
distortion (Cover and Thomas, 2006).
Rate-Distortion: The distortion function d(x, xˆ)
is a measure of the cost of representing the symbol
x to a new symbol xˆ; and the rate can indicate how
much compression can be achieved. The problem of
finding the minimum rate can be solved by minimiz-
ing the functional
F [p(xˆ|x)] = I(X; Xˆ) + ?E(d(x, xˆ)). (20)
where I(X; Xˆ) denotes the mutual information.
The rate-distortion theory is a fundamental the-
ory for lossy data compression. Recently, it has
also been successfully employed for text cluster-
ing (Slonim, 2002) and document summarization
(Ma and Wan, 2010). Slonim (2002) claims that
the mutual information I(X; Xˆ) measures the com-
pactness of the new representation. Thus the rate-
distortion function is a trade-off between the com-
pactness of new representation and the expected dis-
tortion. Specifically in summarization, the sum-
maries can be seen as the new representation Xˆ of
original documents X . A good summary balances
the compression ratio and the information loss, thus
minimizing the function (20). So we use the func-
tion (20)(we set ? = 1) to compare which summary
is a better compression. The JS-divergence (JSD),
which has been proved to have high correlation with
manual evaluation (Louis and Nenkova, 2009) for
constant-length summary evaluation, is utilized as
the distortion in the function. In the following sec-
tions, we simply call the values of the function (20)
rate-dist. In fact, the rate-dist values can be seen as
the JSD measure with length regularization.
To check the effectiveness of rate-dist measure,
we evaluate all summaries generated in Section 5.1
with the new measure (the lower the better). Fig. 5
shows that the results accord with the ones in Fig. 1
and Fig. 3. Moreover, in Fig. 4, the curve of rate-
dist values has a inverse tendency of Rouge mea-
sures (Rouge-1, Rouge-2, Rouge-L and Rouge-SU4
are all listed here), and the best performance also oc-
curs around the summary length of 164 words. This
even more clearly reveals that the BNP summariza-
tion achieves a perfect tradeoff between compact-
ness and informativeness. Due to the accordance
with rouge measures, it is promising to be regarded
as an alternative to the rouge measures in case we do
not have reference summaries.







	




    




	




	


	

Figure 5: Comparison of BNP Summarization with other
systems using rate-dist measure.
5.3 Necessity of Varying Summary Length
In this section, we discuss the necessity of length
determination and how summary length changes ac-
cording to the input data. As explained before,
we generate three new datasets from the original
DUC2004 dataset. Now we use them to indicate
varying summary length is necessary when the in-
put data varies a lot.
Table 1 shows the average summary length of dif-
ferent data sets. The results satisfy the intuitive ex-
pectation of summary length change. When we split
a 10-document cluster into two 5-document parts,
we expect the average summary length of the new
clusters to be a little smaller than the original clus-
ter but much larger than half of the original length,
743
because all the documents concentrate on the same
themes. When we combine two clusters into one, the
summary length should be smaller than the sum of
the summary lengths of two original clusters due to
some unavoidable common background information
but much larger than the summary length of original
clusters.
Original Separate Combined1 Combined2
164 115 250 231
Table 1: Average summary length (number of words) on
different datasets
We also run the Linear Representation system at
different lengths on the new datasets and evaluate
the qualities. As we do not have golden standard
for the new datasets, so we only use the rate-dist
measure here. Results in Table 2,3,4 show the sum-
maries which do not change the predefined length
5 perform significantly worse than the BNP sum-
marization. All the comparison is statistically sig-
nificant. So varying summary length is necessary
when the input changes a lot, and our model can just
give a good match to the new data. This characteris-
tic also can be used to give recommended summary
length for extractive summarization systems when
given unknown data.
Predefined Unchanged BNP
Length 665 bytes 164 words 115 words
Rate-dist 0.4130 0.4404 0.4007
Table 2: Comparison of summary lengths on Separate
Dataset.
Predefined Unchanged BNP
Length 665 bytes 164 words 250 words
Rate-dist 0.3768 0.3450 0.3238
Table 3: Comparison of summary lengths on Combined1
Dataset.
Then we observe the summary length distribu-
tions and compression ratios according to document
size(the length of the whole documents in a clus-
ter). The average summary length increases (Fig. 6),
5665 bytes is the DUC2004 requirement and 164 words is
the best length on original data
Predefined Unchanged BNP
Length 665 bytes 164 words 231 words
Rate-dist 0.3739 0.3464 0.3326
Table 4: Comparison of summary lengths on Combined2
Dataset.
while the compression ratios decreases (Fig. 7) as
document size grows. The rule of the compres-
sion ratio here agrees with the rule in (Goldstein
et al., 1999), although that work is done for single-
document summarization.













$!
	%





     












&'

&'

"(		
Figure 6: The distribution of summary word length.












$!
	%




     











&'

&'

"(	
Figure 7: Compression ratio versus document word
length.
744
6 Conclusion and Future Work
In this paper, we present a new problem of finding a
proper summary length for multi-document summa-
rization based on the document content. A Bayesian
nonparametric model is proposed to solve this prob-
lem. We use the beta process as the prior to construct
a Bayesian framework for summary sentence selec-
tion. Experimental results are shown on DUC2004
dataset, as well as some expanded datasets. We
demonstrate the summaries we extract have good
qualities and the length determination of our system
is rational.
However, there is still much work to do for
variable-length summarization. First, Our sys-
tem is extractive-base summarization, which cannot
achieve the perfect coherence and readability. A sys-
tem which can determine the best length even for
abstractive summarization will be better. Moreover,
in this work we only consider the aspect of data
compression and evaluate the performance using an
information-theoretic measure. In future we may
consider more human factors, and prove the sum-
mary length determined by our system agrees with
human preference. In addition, in the experiments,
we only use the imbalanced datasets as the example
that intuitively needs varying the summary length.
However, the data type is also important to impact
the summary length. In future, we may extend the
work by studying more cases that need varying sum-
mary length.
References
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. . Vol. 4. No. 4. New York: springer.
Jaime Carbonell, and Jade Goldstein. 1998. The Use
Of Mmr, Diversity-Based Reranking For Reordering
Documents And Producing Summaries. Proceedings
of the 21st annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval. ACM, 1998.
Asli Celikyilmaz and Dilek Hakkani-Tu¨r. 2010. A Hy-
brid Hierarchical Model for Multi-Document Summa-
rization. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
815-824.
Ying-Lan Chang, Jui-Jung Hung and Jen-Tzung Chien
2011. Bayesian Nonparametric Modeling Of Hier-
archical Topics And Sentences. IEEE International
Workshop on Machine Learning for Signal Processing,
September 18-21, 2011, Beijing, China.
Thomas M. Cover, and Joy A. Thomas. 2006. Elements
of information theory. Wiley-interscience, 2006.
William M. Darling and Fei Song. 2011. PathSum:
A Summarization Framework Based on Hierarchical
Topics. Canadian AI Workshop on Text Summariza-
tion, St. John’s, Newfoundland.
Samuel J. Gershman and David M. Blei. 2011. A Tuto-
rial On Bayesian Nonparametric Models. Journal of
Mathematical Psychology(2011).
Thomas L. Griffiths and Zoubin Ghahramani. 2005. Infi-
nite Latent Feature Models and the Indian Buffet Pro-
cess. Advances in Neural Information Processing Sys-
tems 18.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and
Jaime Carbonelly. 1999. Summarizing Text Doc-
uments: Sentence Selection and Evaluation Metrics.
Proceedings of SIGIR’99 , pages 121-128.
Zhanying He, Chun Chen, Jiajun Bu, CanWang, Lijun
Zhang, Deng Cai and Xiaofei He. 2012. Document
Summarization Based on Data Reconstruction. Pro-
ceedings of the Twenty-Sixth AAAI Conference on Ar-
tificial Intelligence.
Michael Kaisser, Marti A. Hearst, John B. Lowe. 2008.
Improving Search Results Quality by Customizing
Summary Lengths. Proceedings of ACL-08: HLT,
pages 701-709.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun
Nie. 2006. An Information-Theoretic Approach to
Automatic Evaluation of Summaries. Proceedings of
NAACL2006, pages 463-470.
Chin-Yew Lin, and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. Proceedings of NAACL2003.
Annie Louis and Ani Nenkova. 2009. Automatically
Evaluating Content Selection in Summarization with-
out Human Models. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 306-314. Singapore, 6-7 August 2009.
Tengfei Ma and Xiaojun Wan. 2010. Multi-
document Summarization Using Minimum Distortion.
IEEE 10th International Conference on Data Mining
(ICDM).
Ani Nenkova and Kathleen McKeown. 2012. A sur-
vey of text summarization techniques. Mining Text
Data, Chapter 3, Springer Science+Business Media,
LLC (2012).
John Paisley and Lawrence Carin. 2009. Nonparametric
Factor Analysis with Beta Process Priors. Proceed-
ings of the 26th International Conference on Machine
Learning, Montreal, Canada.
745
John Paisley, Aimee Zaas, Christopher W. Woods, Geof-
frey S. Ginsburg and Lawrence Carin. 2010. A Stick-
Breaking Construction of the Beta Process. Proceed-
ings of the 27 th International Confer- ence on Ma-
chine Learning, Haifa, Israel, 2010.
Dragomir R. Radev and Weiguo Fan. 2000. Effective
search results summary size and device screen size:
Is there a relationship. Proceedings of the ACL-2000
workshop on Recent advances in natural language
processing and information retrieval
Gu¨nes Erkan, and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search, 22 (2004) 457-479.
Noam Slonim. 2002. The Information Bottleneck: The-
ory and Applications. PHD Thesis of the Hebrew Uni-
versity .
Simon Sweeney and Fabio Crestani. 2006. Effective
search results summary size and device screen size: Is
there a relationship. Information Processing and Man-
agement 42 (2006) 1056-1074.
Simon Sweeney, Fabio Crestani and David E. Losada.
2008. ’Show me more’: Incremental length summari-
sation using novelty detection. Information Process-
ing and Management 44 (2008) 663-686.
Yee Whye Teh, Dilan Go¨ru¨r, and Zoubin Ghahramani.
2007. Stick-breaking Construction for the Indian Buf-
fet Process. Proceedings of the International Confer-
ence on Artificial Intelligence and Statistics.
Y.W. Teh, M.I. Jordan, M.J. Beal and D.M. Blei.
2006. Hierarchical Dirichlet Processes. JASA ,
101(476):1566-1581.
Romain Thibaux and Michael I. Jordan. 2009. Hierar-
chical Beta Processes and the Indian Buffet Process.
AISTATS2007.
746
