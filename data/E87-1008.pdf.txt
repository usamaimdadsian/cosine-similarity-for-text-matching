AUTOMATED REASONING ABOUT NATURAL LANGUAGE CORRECTNESS 
Wolfgang Menzel 
Zentral inst itut f~r Sprachwissenschaft 
Akademie der Wissenschaften der DDR 
Prenzlauer Promenade 149-152 
Berl in,  II00, DDR 
ABSTRACT 
Automated Reasoning techniques applied to 
the problem of natural language correct- 
ness allow the design of f lexible training 
aids for the teaching of foreign langua- 
ges. The approach involves important 
advantages for both the student and the 
teacher by detecting possible errors and 
pointing out their reasons. Explanations 
may be given on four dist inct levels, thus 
offering di f ferent ly instructive error 
messages according to the needs of the 
student. 
I. THE IDEA 
The application of techniques from the 
domain of Automated Reasoning to the 
problem of natural language correctness 
offers solutions to at least some of the 
deficiencies of tradit ional approaches to 
computer assisted language learning. By 
supplying a special ized inference mecha- 
nism with knowledge about what is correct 
within fragments of natural language 
utterances, a flexible training device can 
be designed. It prompts the student 
with e .g .  randomly generated sentence 
frames, where slots have to be fil led in. 
The system then accomplishes two main 
tasks: 
(I) It tries to diagnose possible errors 
in the students response in order to build 
up an internal model of the current 
capabil it ies of the student in terms of 
str ictly l inguistic categories. 
(2) It gives an explanation of the diag- 
nostic results to guide the student in his 
search for a correct solution. 
In contrast to other approaches (c.f. 
Barchan et al. 1985, Pulman 1984, Schwind 
1987) we concentrate our efforts more on 
the handling of fragmentory utterances, 
instead of trying to analyse the correct- 
ness of complete sentences. The enormous 
diff icult ies connected with the design of 
a universal error diagnosis for natural 
language sentences may only part ial ly be 
seen as a motivation for this restriction. 
Other, equally important justif ications 
could be mentioned as well: 
(I) The handl ing of only simple sen- 
tence fragments seems to be a more natural 
and transparent l imitation compared with 
an ad hoc exclusion of important parts of 
the grammar from the rule system. Promis- 
ing the student a universal sentence 
acceptor, the real capabil it ies of which 
are rather limited, may easi ly be mis- 
interpreted as a kind of bluff, since the 
consequences of such a cut will always 
remain a mysterious thing to the student. 
Severe restrictions on the grammatical 
knowledge are inevitable at the moment, 
but probably nobody will ever be able to 
explain the language competence of a 
training system to a learner of a second 
language without total ly confusing him. 
Hence, minimising the problem of grammati-  
cal coverage by accepting only fragments 
of sentences, drast ical ly improves the 
prospects of f inally achieving something 
like a "water-proof" solution. Nothing 
could be considered to be more harmful in 
a teaching environment than to blame a 
system's failure on the student. 
(2) The concentrat ion on small sub- 
fields of grammar makes the determination 
of very precise and detai led diagnost ic 
results possible. This, of course, is not 
so much important if seen only for the 
purpose of direct explanation: An explana- 
tion overloaded with details is likely to 
irritate the student. Nevertheless, a 
very precise diagnosis is a sound basis 
for building up a model of the current 
capabil it ies of the student, which advan- 
tageously may be used to guide the further 
course of interaction. 
(3) The approach allows a stepwise 
extension of the degree of sophist icat ion 
while preserving the same basic principles 
on all levels. This enables a rather 
smooth accomodation to dif ferent per- 
formance classes of hardware as well as an 
easy adaptation to dif ferent paedagogical 
objectives. Indeed, there are good reasons 
to expect the very simple examples (e.g. 
the insertion of a correct German deter- 
miner) to be well suited for practical 
46 
training purposes. 
(4) The focus on selected grammatical 
regularities facil itates a systematic 
training, which from a didactic viewpoint 
seems to be more promising than just the 
unspecif ied invitation: "Type in an arbi- 
trary sentence!" with the always present 
risk to catch the system out. Here we 
prefer to guide the student in a rather 
unconstrained way by prompting him with 
carefully selected sentence frames or 
questions. To hide the limitations of the 
dictionary, as usual, the domain context 
of a simple exercise environment (a room, 
a shop, an airport etc.) is used. 
In its diagnostic capabil it ies the 
presented approach shows a strong analogy 
to the basic concepts usually applied 
within a system of Automated Reasoning: a 
hypothesis is verif ied to be in accordance 
with a set of initial facts and a set of 
rules, which for our special purpose model 
the correctness conditions of a specific 
training exercise. The initial facts are 
given as a logical combination of syn- 
tactic and semantic features describing 
the grammatical properties of certain word 
forms in the system prompt. The hypothesis 
results from the the student's response 
where word forms are internally represen- 
ted by their associated features as well. 
II. KNOWLEDGE REPRESENTATION 
To formalize the correctness conditions 
of natural language constructs in a lin- 
guist ical ly adequate manner we adopted two 
basic operators from a dependency grammar 
• model (Kunze 1975): 
constraints of the kind: 
(*** <destination> <condition>) 
transmitters of the kind: 
(<source> <destination> <category>) 
Both of them operate on feature sets. A 
constraint reduces the feature set of a 
word form bound to the variable 
<destination> to its maximum subset which 
satisfies the given <condition>. Transmit- 
ters car ry  features belonging to a speci- 
fic <category> from a <source> to a 
<destination>, changing the feature set at 
the destination according to a predefined 
agreement relation. Typical categories are 
the ordinary ones: GENDER, NUMBER, CASE, 
PERSON etc., but semantic or very language 
specific features (like INFLECTIONAL 
DEGREE for German, cf. ROdiger 1975) may 
be used as well. Accordingly, by means of 
these operators the conditions for the 
morpho-syntactic correctness within a 
CAT=PREPOS I TION 
SELECT=DIRECTION 
CASE 
I,PREP-3 I 
CASE 
CAT=PREPOS ITION 
SELECT=LOCATION 
\ 
ARTICLE 
CAT=POSSESSIVE-PRONOUN 
DEMONSTRATIVE-PRONOUN 
CASE I 
NUMBER ~ 
I *NOUN I 
CASE 
CAT=NOUN GENDER 
INFLECTIONAL- ~ GREE 
CAT=ADJECT IVE 
Figure I: Correctness conditions for a special German preposit ional phrase 
47 
simple German prepos i t ional  phrase of the 
type (PREP DET ADJ NOUN) may be coded as 
shown in ~igure i. 
The " nodes in this graph denote 
variables, which have to be bound to 
s ingle word forms. Accord ing to their  
value ass ignment  mode two types of 
var iables  may be dist inguished.  Context  
var iab les  belong to the sentence frame and 
receive their  value (the feature set of a 
spec i f ic  word form) a l ready dur ing the 
sentence generat ion  process. The value of 
a slot variable, however, depends on the 
student's response and is estab l ished by a 
pattern match ing procedure based main ly  on 
word class information. The power of the 
pattern matcher  used determines a lmost  
complete ly  the f lex ib i l i ty  of the system: 
A rather s imple one, us ing ob l igatory  slot 
var iab les  only (hence, rest r ic t ing the 
slot to a f ixed length) wil l  be suf f ic ient  
under certa in c ircumstances.  The addit io-  
nal use of opt ional  s lot var iables  al lows 
the implementat ion  of more d ivers i f ied  
exercises. Somet imes even a s imple parser  
for sentence f ragments may be required. 
The t ransmit ters  obv ious ly  const i tute  
the part  of rules wi th in  the knowledge 
base. They can eas i ly  be interpreted as 
def in ing logical impl icat ions, semant ica l -  
ly extended by two ex istent ia l  quant i f iers  
for the var iab les  <source> and 
<destination>. In a certain sense trans-  
mit ters  correspond to the well known 
Constraints:  
(*** 
(*** 
(*** 
(*** 
(*** 
(*** 
(*** *ADJ 
*PREP-4 (CAT PREPOSIT ION))  
*PREP-4 (SELECT DIRECTION))  
*PREP-3 (CAT PREPOSIT ION))  
*PREP-3 (SELECT LOCATION))  
*NOUN (CAT NOMINAL))  
*DET (CAT ARTICLE 
POSSESS IVE-PRONOUN 
DEMONSTRATIVE-PRONOUN))  
(CAT ADJECTIVE))  
Transmitter:  
(*PREP-4 *NOUN CASE) 
(*PREP-3 *NOUN CASE) 
(*NOUN *DET CASE) 
(*NOUN *DET NUMBER) 
(*NOUN *DET GENDER) 
(*NOUN *ADJ CASE) 
(*NOUN *ADJ NUMBER) 
(*NOUN *ADJ GENDER) 
(*DET *ADJ INFLECTIONAL-DEGREE)  
f igure 2: Rule set for the example in 
f igure 1 
IF . . .THEN rules in a typical  expert  
system. 
The factual  knowledge, on the other 
side, consists  of constra ints  (which could 
be thought  of to be t ransmit ters  with a 
nowhere-source,  indicated by "***" in the 
rule set of f igure 2) together  with the 
feature combinat ions  in the d ic t ionary  
entries. Only  from the point  of v iew of 
exp lanat ion the factual  in format ion has a 
special  status: one cannot  ask for it by 
means of a why-quest ion.  
III. ERROR DIAGNOSIS  
Commonly  one tr ies to d i s t ingu ish  the 
f ield of Automated Reasoning from the 
deve lopment  of expert  systems by compar ing 
a mean size of the knowledge base as well 
as the length of a typica l  in ference 
chain. Normally, a system of Automated 
Reason ing is expected to have a rather 
l imited number of rules but the ab i l i ty  to 
handle  extremely  long chains whereas the 
character i s t i cs  of an expert  system 
include p lenty  of rules but very  short 
inferences. In this respect, a system for 
foreign language t ra in ing  belongs to a 
third category, s ince both, the size of 
the knowledge base as well  as the mean 
length of an inference path are com- 
parat ive ly  small. Unfortunately,  this 
s impl ic i ty  doesn ' t  result  in a very s imple 
des ign for the inference engine as well. 
D i f f i cu l t ies  ar ise from a pecu l ia r i ty  of 
the language t ra in ing task: On the one 
hand, facts and rules are g iven to de- 
scr ibe the c o r r e c t n e s s of 
natural  language constructs.  On the other 
hand, exp lanat ions  are required about the 
d e f i c i e n c i e s of a students 
solution. P robab ly  the system is never  
asked to point  out the reasons why a 
spec i f ic  inference can be drawn, but it is 
expected to expla in the reasons why a 
correctness  proof  can n o t be 
establ ished. This, of course, requires a 
special  d iagnos is  procedure  which in the 
case of an error in the student's response 
searches for p laus ib le  a l ternat ives  which 
might  have been leading to a correct  
solution. 
The d iagnos is  is carr ied out in two 
steps (f igure 3). Us ing a c lass ica l  non- 
determin is t i c  forward chain ing a lgor i thm 
the f irst step tr ies to show the correct -  
ness by success ive ly  apply ing constra ints  
and t ransmit ters  on all the feature sets 
prev ious ly  bound to variables.  A t ransmit -  
ter can be applied, if its source doesn ' t  
appear to be a dest inat ion  in any other 
48 
t ransmit ter  wait ing for appl icat ion yet. 
This implies that cycles of t ransmit ters  
are not a l lowed with in the knowledge base, 
a conf igurat ion  which actual ly  doesn' t  
occur in a natural  language sentence, 
anyhow. 
The appl icat ion of a constra int  or a 
t ransmit ter  fails, if it results in an 
empty feature set at the dest inat ion.  
Fai lures due to the miss ing of facts in 
the knowledge base may indicate an error 
in the students response, and all the 
categories, var iables and values concerned 
are stored as fa i lure points to be 
analysed in detai l  later. A sentence frame 
can be cons idered to be correct ly  
completed by the student, if all the 
re levant constra ints  and t ransmit ters  have 
been appl ied successful ly.  If such a 
solut ion cannot be found (that is, a 
mistake of the student has been 
encountered),  the second step resumes the 
analysis  by invest igat ing the consequences 
of assuming in each case just the 
complementory  feature set at the fa i lure 
point. By doing this, the d iagnosis  
p rocedure  in fact tr ies to s imulate the 
ignoring of the corresponding rule by the 
student and aims at f inding out all the 
result ing consequences. 
To de l iver  the informat ion needed by 
the second step of the d iagnosis  procedure 
requires to extend the capabi l i t ies  of the 
basic rout ine for feature set compar ison 
beyond the usual un i f i cat ion  operations. 
In addit ion to the normal intersection 
between the re levant features at the 
<source> and the <dest inat ion> the 
procedure determines the complement  of the 
feature set at the <dest inat ion> (see 
f igure 4). To achieve the des i red high 
resolut ion of the d iagnos is  un i f i cat ion  is 
always carr ied out for a s ingle category. 
All the other features are left unchanged. 
Given the case of an error in the 
students response the invest igat ion of 
both alternatives,  the intersect ion as 
well as the complement  becomes necessary. 
That is, the d iagnos is  is confronted with 
an enormous number  of analysis  paths. 
Strong heur is t ic  cr i ter ia  are needed to 
restr ict  the size of the search space 
effectively. So far, an a lgor i thm 
cons ider ing only paths with a min imum 
number of fa i lure points has turned out to 
be suf f ic ient  in most cases. 
IV. EXPLANATION COMPONENT 
Usually, due to the often numerous 
morpho-syntact ic  readings of a word form 
the d iagnosis  component  comes out with a 
couple of poss ib le  error interpretations, 
all of them can by no means be expla ined 
to a student wi thout  tota l ly  confus ing 
him. Again, heur is t ic  cr i ter ia  are needed 
to reduce the number of interpretat ions in 
a sens ib le  way. 
Step I: CORRECTNESS PROOF 
Hypothes is  
init ial facts 
Step II: INVESTIGATION OF INFERENCE 
FAILURES 
Hypothes is 
I i 11/T2" + 
ILr  gG 
init ial facts 
c= 
successful  t ransmit ter  appl icat ion 
fa i lure point  
complementary  transmitter appl icat ion 
poss ib le  error explanat ion 
F igure 3: Two step d iagnos is  
49 
\[NOM1 
CASE : IGENI 
L Acc\] 
l unif ied with I 
\[NOM\] 
CASE = |DAT| \[ACC\] 
I results in 
: 1 CASE LAce\] 
CASE = \[DAT\] 
(source) 
(destination) 
(intersection) 
(complement) 
Figure 4: Example for the extended feature 
set unif ication 
To select an appropriate (that is, 
helpful from the students point of view) 
error descript ion the diagnostic results 
have to be ordered by an estimated 
explanatory power. So far, the fol lowing 
criteria have been taken into 
consideration: 
(I) A category preference, which 
chooses a certain transmitter function 
(e.g. GENDER) as a more probable one. This 
is a simple but obviously crude and 
unrel iable criterion. 
(2) The distance between the complemen- 
tary transmitter application and the hypo- 
thesis, whereby errors "higher up" in a 
sentence structure are preferred. For 
example, it is more likely that the case 
governed by a preposit ion has been mis- 
taken than that the agreement within the 
preposit ional phrase is violated. 
(3) In a multiple error diagnosis a 
category common to most of the alterna~ 
rives could be taken for the explanation. 
Given the very frequent error combination 
(CASE and GENDER) or (NUMBER and GENDER) 
missing gender agreement should be a 
reasonable explanation. 
A good heuristics certainly has to 
include the structure of the dict ionary 
entries and the rule set in its investiga- 
tion of possible alternatives. If there is 
indeed a second reading with respect to 
one of the hypothesised error reasons then 
probably the student overlooked this 
possibility. Here further investigations 
are necessary. 
From a paedagogical point of view it 
would be desirable to explain the diagnos- 
tic results (detected errors and their 
possible reasons) on di f ferent ly instruc- 
t ive levels, selecting the right one 
according to previous results or current 
desires of the student. The fol lowing four 
levels seem to be appropriate and theore- 
t ical ly motivated: 
(I) r ight/wrong answer without further 
explanation 
(2) explanation on the level of rules 
(e.g. "missing gender agreement between 
xxx and yyy") 
(3 )  explanation on the level of facts 
(e.g. "xxx is a feminine noun, hence you 
should take a feminine determiner") 
(4) explanation on the level of 
examples using the inverted dict ionary as 
a data base to retrieve appropriate word 
forms by means of the inferred feature 
sets. 
The verbal izat ion of an explanation is 
done on the basis of sentence schemata, 
which have to be defined together with the 
correctness conditions. On demand, the 
actual categories, values or examples are 
inserted and minor surface smoothing 
operations are carried out. 
V. DIALOG CONTROL & USER MODELLING 
By careful ly investigating a series of 
responses a model of the current capabil i-  
ties of the student can be build up. Based 
on this model the system autonomously may 
vary dif ferent aspects of the dialog 
behaviour. The most simple example is the 
selection of one of the explanation 
levels. The system switches over to a 
deeper level of explanation if the student 
either repeatedly fails to find the 
correct solution or signals his inabil ity 
for understanding the previous error 
message. It goes back to a higher level if 
consecutive successes of the student 
justify this. 
A series of responses may contain hints 
about where the weaknesses of the student 
actually lie. Thus, in addition to the 
criteria of section IV another heuristics 
for the selection of diagnost ic results is 
available: Continued repetit ion of one and 
50 
the same error type will cause the 
explanation to focus on this category. 
Furthermore, the collected information can 
be used to guide the training strategy. 
Exercise generation may be controlled to 
just concentrate on the weak points of the 
student or even to alter the degree of 
exercise difficulty. 
VI. EXPERIMENTATION 
To study some selected problems (espe- 
cially the exploitation of heuristic rules 
within the diagnosis and explanation 
components) in greater detail, a first 
prototype has been implemented. Currently 
the system includes a random sentence 
generator to supply the system prompts, a 
simple pattern matcher for obl igatory slot 
variables, the two step diagnosis 
described above and an explanation 
component up to the level of facts. 
The training examples studied so far 
have mainly been taken from the area of 
German noun phrase inflection (indeed an 
intricate subject from the foreigne{s 
point of view). The experiments confirmed 
that simple versions of training exercises 
may run already on very cheap type of  
hardware (i.e. 8-bit micros). 
the explanation mostly points out the 
location of the error rather precisely. 
(4) A model of the student% capabil i-  
ties is built up and the teacher is 
supplied with a statistics in terms of 
l inguistic categories even in the case of 
very complex or mixed exercises. 
(5) Instead of explicit ly listing them, 
exercises can be generated automatically, 
thus achieving a variety which almost 
excludes repetition even in the case of 
extremely long or repeated training 
sessions. 
Limitations for the application domain 
mostly result from the feature based 
approach to knowledge representation. It 
first of all predestines the solution for 
the training of morpho-syntactic reg- 
ularities (esp. agreement relations). To 
handle problems of e.g. usage or style in 
a suff ic iently general manner seems to be 
far beyond the current possibil ities. 
REFERENCES 
VII. DISCUSSION 
The design of foreign language training 
systems based on fundamental techniques of 
Automated Reasoning exhibits several 
important advantages as compared with an 
immediate implementation of the almost 
trivial scheme a Pattern Drill Book is 
based upon: 
(I) Automated Reasoning allows more 
flexibility. Not the one correct solution 
is asked for. The student may choose 
h i s solution within the limitations of 
the dict ionary (expressed by the exercise 
environment). Dialog situations may easily 
be simulated. Experimentation becomes 
possible. 
(2) In addition to the right/wrong 
diagnosis further three levels of explana- 
tion are available. A correct solution can 
be generated just for the part icular word 
samples chosen by the student. 
(3) It becomes possible to include 
rather complex regularit ies between con- 
text and slot variables. Nevertheless, 
Barchan, J.; Woodmansee, B. and Yazdani, 
M. (1985) Computer Assisted Instruction 
using a French Grammar Analyser. 
Research Report 128, Department of 
Computer Science, University of Exeter. 
Kunze, J. (1975) Abh~ngigkeitsgrammatik. 
studia grammatica XII, Akademie-Verlag, 
Berlin. 
Pulman, S.G. (1984) Limited Domain 
System for Language Teaching. 
Proceedings Coling 84, Stanford: 84-87. 
RGdiger, B. (1975) Flexivische und Wort- 
bi ldungsanalyse des Deutschen. 
Linguist ische Studien, Reihe A, Sonder- 
heft 1975, Berlin. 
Schwind, C.B. (1987) Prototyp eines 
Sprachtutorensystems fGr Deutsch als 
Fremdsprache, KI-Rundbrief 44, Januar 
1987: 42 
Wos, L.; Overbeek, R.; Lusk, E. and Boyle, 
J .(1984) Automated Reasoning. Prentice 
Hall, Englewood Cliffs. 
51 
