Integrating cohesion and coherence for Automatic Summarization
Laura Alonso i Alemany	 Maria Fuentes Fort
GRIAL	 Departament d'Informatica i Matematica Aplicada
Departament de Lingilistica General 	 Universitat de Girona
Universitat de Barcelona 	 maria.fuentes@udg.es
lalonso@lingua.fil.ub.es
Abstract
This paper presents the integration of
cohesive properties of text with co-
herence relations, to obtain an ade-
quate representation of text for auto-
matic summarization. A summarizer
based on Lexical Chains is enchanced
with rhetorical and argumentative struc-
ture obtained via Discourse Markers.
When evaluated with newspaper corpus,
this integration yields only slight im-
provement in the resulting summaries
and cannot beat a dummy baseline con-
sisting of the first sentence in the doc-
ument. Nevertheless, we argue that
this approach relies on basic linguis-
tic mechanisms and is therefore genre-
independent.
1 Motivation
Text Summarization (TS) can be decomposed into
three phases: analysing the input text to obtain
text representation, transforming it into a sum-
mary representation, and synthesizing an appro-
priate output form to generate the summary text.
Much of the early work in summarization has
been concerned with detecting relevant elements
of text and presenting them in the "shortest possi-
ble form". More recently, an increasing attention
has been devoted to the adequacy of the resulting
texts to a human user. Well-formedness, cohesion
and coherence are currently under inspection, not
only because they improve the quality of a sum-
mary as a text, but also because they can reduce
the final summary by reducing the reading time
and cost that is needed to process it.
TS systems that performed best in last DUC
contest (DUC, 2002) apply template-driven sum-
marization, by information-extraction procedures
in the line of (Schank and Abelson, 1977). This
approach yields very good results in assessing rel-
evance and keeping well-formedness, but it is de-
pendent on a clearly defined representation of the
information need to be fulfilled and, in most cases,
also on some regularities of the kind of texts to be
summarized.
In more generic TS, genre-dependent regular-
ities are not always found, and template-driven
analysis cannot capture the variety of texts. In ad-
dition, the information need is usually very fuzzy.
In these circumstances, the most reliable source
of information on relevance and coherence prop-
erties of a text is the source text itself. An ad-
equate representation of that text should account
not only for relevant elements, but also for the re-
lations holding between them, in the diverse tex-
tual levels. Exploiting the discursive properties of
text seems to accomplish both these requirements,
since they have language-wide validity can be suc-
cessfully combined with information at superficial
or semantic level.
In this paper, we present an integration of two
kinds of discursive information, cohesion and co-
herence, to obtain an adequate representation of
text for the task of TS. Our starting point is an
extractive informative summarization system that
exploits the cohesive properties of text by building
and ranking lexical chains (see Section 3). This
system is enhanced with discourse coherence in-
formation (Section 5.3). Experiments were carried
out on the combination of these two kinds of infor-
mation, and results were evaluated on a Spanish
news agency corpus (Section 5).
1
2 Previous Work on Combining 	 3 Summarizing with Lexical Chains
Cohesion and Coherence
Traditionally, two main components have been
distinguished in the discursive structure of a text:
cohesion and coherence. As defined by (Halliday
and Hasan, 1976), cohesion tries to account for
relationships among the elements of a text. Four
broad categories of cohesion are identified: refer-
ence, ellipsis, conjunction, and lexical cohesion.
On the other hand, coherence is represented in
terms of relations between text segments, such as
elaboration, cause or explanation. (Mani, 2001)
argues that an integration of these two kinds of
discursive information would yield significant im-
provements in the task of text summarization.
(Corston-Oliver and Dolan, 1999) showed that
eliminating discursive satellites as defined by the
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), yields an improvement in the
task of Information Retrieval. Precision is im-
proved because only words in discursively relevant
text locations are taken into account as indexing
terms, while traditional methods treat texts as un-
structured bags of words.
Some analogous experiments have been carried
out in the area of TS. (Brunn et al., 2001; Alonso
and Fuentes, 2002) claim that the performance of
summarizers based on lexical chains can be im-
proved by ignoring possible chain members if they
occur in irrelevant locations such as subordinate
clauses, and therefore only consider chain candi-
dates in main clauses. However, syntactical sub-
ordination does not always map discursive rele-
vance. For example, in clauses expressing finality
or dominated by a verb of cognition, like Y said
that X, the syntactically subordinate clause X is
discursively nuclear, while the main clause is less
relevant (Verhagen, 2001).
In (Alonso and Fuentes, 2002), we showed that
identifying and removing discursively motivated
satellites yields an improvement in the task of text
summarization. Nevertheless, we will show that
a more adequate representation of the source text
can be obtained by ranking chain members in ac-
cordance to their position in the discourse struc-
ture, instead of simply eliminating them.
The lexical chain summarizer follows the work of
(Morris and Hirst, 1991) and (Barzilay, 1997).
As can be seen in Figure 1 (left) the text is first
segmented, at different granularity levels (para-
graph, sentence, clause) depending on the appli-
cation. To detect chain candidates, the text is mor-
phologically analysed, and the lemma and POS of
each word are obtained. Then, Named Entities are
identified and classified in a gazzetteer. For Span-
ish, a simplified version of (Palomar et al., 2001)
extracts co-referenece links for some types of pro-
nouns, dropping off the constraints and rules in-
volving syntactic information.
Semantic tagging of common nouns is been per-
formed with is-a relations by attaching Euro Word-
Net (Vossen, 1998) synsets to them. Named Enti-
ties are been semantically tagged with instance re-
lations by a set of trigger words, like former pres-
ident, queen, etc., associated to each of them in a
gazzetteer. Semantic relations between common
nouns and Named Entities can be established via
the EWN synset of the trigger words associated to
a each entity.
Chain candidates are common nouns, Named
Entities, definite noun phrases and pronouns, with
no word sense disambiguation. For each chain
candidate, three kinds of relations are considered,
as defined by (Barzilay, 1997):
• Extra-strong between repetitions of a word.
• Strong between two words connected by a
direct EuroWordNet relation.
• Medium-strong if the path length between
the EuroWordNet synsets of the words is
longer than one.
Being based on general resources and princi-
ples, the system is highly parametrisable. It has a
relative independence because it may obtain sum-
maries for texts in any language for which there is
a version of WordNet an tools for POS tagging and
Named Entity recognition and classification. It
can also be parametrised for obtaining summaries
of various lengths and at granularity levels.
As for relevance assessment, some constraints
can be set on chain building, like determining the
maximum distance between WN synsets of chain
2
candidates for building medium-strong chains, or
the type of chain merging when using gazetteer
information. Once lexical chains are built, they
are scored according to a number of heuristics that
consider characteristics such as their length, the
kind of relation between their words and the point
of text where they start. Textual Units (TUs) are
ranked according to the number and type of chains
crossing them, and the TUs which are ranked high-
est are extracted as a summary. This ranking of
TUs can be parametrised so that a TU can be as-
signed a different relative scoring if it is crossed
by a strong chain, by a Named Entity Chain or by
a co-reference chain. For a better adaptation to
textual genres, heuristics schemata can be applied.
However, linguistic structure is not taken into
account for scoring the relevance lexical chains
or TUs, since the relevance of chain elements is
calculated irrespective of other discourse informa-
tion. Consequently, the strength of lexical chains
is exclusively based on lexic. This partial repre-
sentation can be even misleading to discover the
relevant elements of a text. For example, a Named
Entity that is nominally conveying a piece of news
in a document can present a very tight pattern of
occurrence, without being actually relevant to the
aim of the text. The same applies to other linguis-
tic structures, such as recurring parallelisms, ex-
amples or adjuncts. Nevertheless, the relative rel-
evance of these elements is usually marked struc-
turally, either by sentential or discursive syntax.
4 Incorporating Rhetorical and
Argumentative Relations
The lexical chain summarizer was enhanced with
discourse structural information as can be seen in
Figure 1 (right).
Following the approach of (Marcu, 1997), a par-
tial representation of discourse structre was ob-
tained by means of the information associated to
a Discourse Marker (DM) lexicon. DMs are de-
scribed in four dimensions:
• matter: following (Asher and Lascarides,
2002), three different kinds of subject-matter
meaning are distinguished, namely causality,
parallelism and context.
•
argumentation: in the line of (Anscom-
bre and Ducrot, 1983), three argumentative
moves are distinguished: progression, elabo-
ration and revision.
• structure: following the notion of right fron-
tier (Polanyi, 1988), symmetric and asymmet-
ric relations are distinguished.
• syntax: describes the relation of the DM with
the rest of the elements at the discourse level,
in the line of (Forbes et al., 2003), mainly
used for discourse segmentation.
The information stored in this DM lexicon was
used for identifying inter- and intra-sentential dis-
course segments (Alonso and Caste116n, 2001)
and the discursive relations holding between them.
Discourse segments were taken as Textual Units
by the Lexical Chain summarizer, thus allowing a
finer granularity level than sentences.
Two combinations of DM descriptive features
were used, in order to account for the interaction
of different structural information with the lexical
information of lexical chains. On the one hand,
nucleus-satellite relations were identified by the
combination of matter and structure dimensions of
DMs. This rhetorical information yielded a hier-
archical structure of text, so that satellites are sub-
ordinate to nucleus and they are accordingly con-
sidered less relevant. On the other hand, the ar-
gumentative line of text was traced via the argu-
mentation and also structure DM dimensions, so
that segments were tagged with their contribution
to the progression of the argumentation.
These two kinds of structural analyses are com-
plementary. Rhetorical information is mainly
effective at discovering local coherence struc-
tures, but it is unreliable when analyzing macro-
structure. As (Knott et al., 2001) argue, a differ-
ent kind of analysis is needed to track coherence
throughout a whole text; in their case the alter-
native information used is focus, we have opted
for argumentative orientation. Argumentative in-
formation accounts for a higher-level structure, al-
though it doesn't provide much detail about it.
This lexicon has been developed for Spanish
(Alonso et al., 2002a). Nevertheless, the struc-
ture of the DM lexicon and the discourse parsing
tools based on it is highly portable, and versions
3
Textual Unit
segmentation
morphological
analysis
Lexical Unit
segmentation
co—reference
resolution
semantic
Lagging
PRE—PROCESSED
TEXT
LEXICAL CHAINER
—1Parameters RANKING &SELECTION
.1;
I SENTENCE
COMPRESSION
Lexical Chain
Summary
I,excal Chain and
S4ntence Compression
' ----Sirmmary -        
cleaning up                                            
;;
morphosyntactical
analysis
;
;
PN rules discourse
segmentation
discourse
markers
rhetorical relation
interpretation
co—reference
rules
trigger—words
TEXT
heuristics
EuroWN
;
;
RIIETORICAL !
;
I INFORMATION
;
;;
;
textual
units
CHAINS
OUTPUT
Figure 1: Integration of discursive information: lexical chains (left) and discourse structural (right)
for English and Catalan are being developed by
bootstraping techniques (Alonso et al., 2002b).
5 Experiments
A number of experiments were carried out in or-
der to test whether taking into account the struc-
tural status of the textual unit where a chain mem-
ber occurs can improve the relevance assessment
of lexical chains (see Figure 2). Since the DM
lexicon and the evaluation corpus were available
only for Spanish, the experiments were limited to
that language. Linguistic pre-processing was per-
formed with the CLiC-TALP system (Carmona et
al., 1998; Arevalo et al., 2002).
For the evaluation of the different experiments,
the evaluation software MEADeval (MEA, 2002)
was used, to compare the obtained summaries with
a golden standard (see Section 5.1). From this
package, the usual precision and recall measures
were selected, as well as the simple cosine. Sim-
ple cosine (simply cosine from now on) was cho-
sen because it provides a measure of similarity be-
tween the golden standard and the obtained ex-
tracts, overcoming the limitations of measures de-
pending on concrete textual units.
5.1 Golden Standard
The corpus used for evaluation was created within
Hermes projea l , to evaluate automatic summariz-
'Information about this	 project available in
http://terral.ieec.uned.es/hermes/
4
Rhetoric & Arcrrmcn laiic Lexical Chains
to
ToLexical (Mail ] 	 -.1+ Cr
iNONaoreJEohii
Lexical Chains
prellS.1
rcuull
coninus
LRemoving Satellites
Lexical ChainoRhetorical Information
Figure 2: Experiments to assess the impact of discourse
structure on lexical chain members
ers for Spanish, by comparison to human summa-
rizers. It consists of 120 2 news agency stories of
various topics, ranging from 2 to 28 sentences and
from 28 to 734 words in length, with an average
length of 275 words per story.
To avoid the variability of human generated ab-
stracts, human summarizers built an extract-based
golden standard. Paragraphs were chosen as the
basic textual unit because they are self-contained
meaning units. In most of the cases, paragraphs
contained a single sentence. Every paragraph in
a story was ranked from 0 to 2, according to its
relevance. 31 human judges summarized the cor-
pus, so that at least 5 different evaluations were
obtained for each story.
Golden standards were obtained coming as
close as possible to the 10% of the length of the
original text (19% compression average).
The two main shortcomings of this corpus are
its small size and the fact that it belongs to the
journalistic genre. However, we know of no other
corpus for summary evaluation in Spanish.
5.2 Performance of the Lexical Chain System
The performance of the Lexical Chain System
with no discourse structural information was taken
as the base to improve. (Fuentes and Rodriguez,
2002) report on a number of experiments to evalu-
ate the effect of different parameters on the results
of lexical chains. To keep comparability with the
golden standard, and to adequately calculate pre-
cision and recall measures, paragraph-sized TUs
were extracted at 10% compression rate.
Some parameters were left unaltered for the
whole of the experiment set: only strong or extra-
2For the experiments reported here, one-paragraph news
were dropped, resulting in a final set of Ill news stories.
Precision	 Recall	 Cosine
Lead .95 .85 .90
SweSum .90 .81 .87
HEURISTIC 1
Lex. Chains .82 .81 .85
Lex. Chains
+ PN Chains
.85 .85 .88
Lex. Chains
+ PN Chains
+ coRef Chains
.83 .83 .87
Lex. Chains
+ PN Chains
+ coRef Chains
+ 1st TU
.88 .88 .90
HEURISTIC 2
Lex. Chains .71 .72 .79
Lex. Chains
+ PN Chains
.73 .74 .81
Lex. Chains
+ PN Chains
+ coRef Chains
.70 .71 .78
Lex. Chains
+ PN Chains
+ coRef Chains
+ 1st TU
.82 .82 .86
Table 1: Performance of the lexical chain Summarizer
strong chains were built, no information from de-
fined noun phrases or trigger words could be used
and only short co-reference chains were built. Re-
sults are presented in Table I.
The first column in the table shows the main
parameters governing each trial: simple lexi-
cal chains, lexical chains successively augmented
with proper noun and co-Reference chains, and fi-
nally giving special weighting to the 1st TU be-
cause of global document structure appliable to the
journalistic genre.
Two heuristics schemata were experimented:
heuristic 1 ranks as most relevant the first TU
crossed by a strong chain, while heuristic 2 ranks
highest the TU crossed by the maximum of strong
chains. An evaluation of SweSum (SweSum,
2002), a summarization system available for Span-
ish, is also provided as a comparison ground. Tri-
als with SweSum were carried out with the default
parameters of the system. In addition, the first
paragraph of every text, the so-called lead sum-
mary, was taken as a dummy baseline.
As can be seen in Table 1, the lead achieves the
best results, with almost the best possible score.
This is due to the pyramidal organisation of the
journalistic genre, that causes most relevant infor-
mation to be placed at the beginning of the text.
Consequently, any heuristic assigning more rele-
vance to the beginning of the text will achieve bet-
5
ter results in this kind of genre. This is the case for
the default parameters of SweSum and heuristic 1.
However, it must be noted that lexical chain
summarizer produces results with high cosine and
low precision, while SweSum yields high pre-
cision and low cosine. This means that, while
the textual units extracted by the summarizer are
not identical to the ones in the golden standard,
their content is not dissimilar. This seems to
indicate that the summarizer successfully cap-
tures content-based relevance, which is genre-
independent. Consequently, the lexical chain sum-
marizer should be able to capture relevance when
applied to non-journalistic texts. This seems to be
supported by the fact that heuristic 2 improves co-
sine over precision four points higher than heuris-
tic 1, which seems more genre-dependent.
Unexpectedly, co-reference chains cause a de-
crease in the performance of the system. This may
be due to their limited length, and also to the fact
that both full forms and pronouns are given the
same score, which does not capture the difference
in relevance signalled by the difference in form.
5.3 Results of the Integration of
Heterogenous Discursive Informations
Structural discursive information was integrated
with only those parameters of the lexical chain
summarizer that exploited general discursive in-
formation. Heuristic I was not considered because
it is too genre-dependent. No co-reference infor-
mation was taken into account, since it does not
seem to yield any improvement.
The results of integrating lexical chains with
discourse structural information can be seen in Ta-
ble 2. Following the design sketched in Figure
5, the performance of the lexical chains summa-
rizer was first evaluated on a text where satellites
had been removed. As stated by (Brunn et al.,
2001; Alonso and Fuentes, 2002), removing satel-
lites slightly improves the relevance assessment of
the lexical chainer (by one point).
Secondly, discourse coherence information was
incorporated. Rhetorical and argumentative infor-
mations were distinguished, since the first iden-
tifies mainly unimportant parts of text and the
second identifies both important and unimportant.
Identifying satellites instead of removing them
1	 Precision	 I	 Recall	 1	 Cosine
Sentence Compression
+ Lexical Chains
Sentence Compression
+ Lexical Chains
+ PN Chains
.74 .75 .70
Sentence Compression
+ Lexical Chains
+ PN Chains
+ 1st TU
.86 .85 .76
Rhetoecal Information
+ Lexical Chains
Rhetorical Information
+ Lex. Chains
+ PN Chains
.74 .76 .82
Rhetorical Information
+ Lex. Chains
+ PN Chains
+ 1st TU
.83 .84 .86
Rhetorical
+ Argumentative
+ Lexical Chains
Rhetorical Information
+ Argumentative
+ Lex. Chains
+ PN Chains
.79 .80 .84
Rhetorical Information
+ Argumentative
+ Lex. Chains
+ PN Chains
+ 1st TU
.84 .85 .87
Table 2: Results of the integration of lexical chains and
discourse structural information
yields only a slight improvement on recall (from
.75 to .76), but significantly improves cosine (from
.70 to .82).
When argumentative information is provided,
an improvement of .5 in performance is observed
in all three metrics in comparison to removing
satellites. As can be expected, ranking the first
TU higher results in better measures, because of
the nature of the genre. When this parameter is
set, removing satellites outperforms the results ob-
tained by taking into account discourse structural
information in precision. However, this can also
be due to the fact that when the text is compressed,
TUs are shorter, and a higher number of them can
be extracted within the fixed compression rate. It
must be noted, though, that recall does not drop
for these summaries.
Lastly, intra-sentential and sentential satellites
of the best summary obtained by lexical chains
were removed, increasing compression of the re-
sulting summaries from an average 18.84% for
lexical chain summaries to a 14.43% for sum-
maries which were sentence-compressed. More-
over, since sentences were shortened, readability
was increased, which can be considered as a fur-
6
ther factor of compression. However, these sum-
maries have not been evaluated with the MEADe-
val package because no golden standard was avail-
able for textual units smaller than paragraphs. Pre-
cision and recall measures could not be calcu-
lated for summaries that removed satellites, be-
cause they could not be compared with the golden
standard, consisting only full sentences.
5.4 Discussion
The presented evaluation successfully shows the
improvements of integrating cohesion and coher-
ence, but it has two weak points. First, the small
size of the corpus and the fact that it represents a
single genre, which does not allow for safe gener-
alisations. Second, the fact that evaluation metrics
fall short in assessing the improvements yielded
by the combination of these two discursive infor-
mations, since they cannot account for quantitative
improvements at granularity levels different from
the unit used in the golden standard, and therefore
a full evaluation of summaries involving sentence
compression is precluded. Moreover, qualitative
improvements on general text coherence cannot be
captured, nor their impact on summary readability.
As stated by (Goldstein et al., 1999), "one of the
unresolved problems in summarization evaluation
is how to penalize extraneous non-useful informa-
tion contained in a summary". We have tried to
address this problem by identifying text segments
which carry non-useful information, but the pre-
sented metrics do not capture this improvement.
6 Conclusions and Future Work
We have shown that the collaborative integration
of heterogeneous discursive information yields an
improvement on the reperesentation of source text,
as can be seen by improvements in resulting sum-
maries. Although this enriched representation
does not outperform a dummy baseline consisting
of taking the first paragraph of the text, we have
argued that the resulting representation of text is
genre-independent and succeeds in capturing con-
tent relevance, as shown by cosine measures.
Since the properties exploited by the presented
system are text-bound and follow general princi-
ples of text organization, they can be considered
to have language-wide  validity. This means that
the system is domain-independent, though it can
be easily tuned to different genres.
Moreover, the system presents portability to a
variety of languages, as long as it has the knowl-
edge sources required, basically, shallow tools for
morpho-syntactical analysis, a version of WordNet
for building and ranking lexical chains, and a lex-
icon of discourse markers for obtaining a certain
discourse structure.
Future work concerning the lexical chain sum-
marizer will be focussed in building longer lexical
chains, exploiting other relations in EWN, merg-
ing chains and even merging heterogeneous infor-
mation. Improvements in the analysis of struc-
tural discursive information include enhancing the
scope to paragraph and global document level,
integrating heterogeneous discursive information
and proving language-wide validity of Discourse
Marker information.
To provide an adequate assessment of the
achieved improvements, the evaluation procedure
is currently being changed. Given the enormous
cost of building a comprehensive corpus for sum-
mary evaluation, the system has been partially
adapted to English, so that it can be evaluated with
the data and procedures of (DUC, 2002).
Nevertheless, our future efforts will also be di-
rected to gathering a corpus of Spanish texts with
abstracts from which to automatically obtain a cor-
pus of extracts with their corresponding texts, as
proposed by (Marcu, 1999). Concerning quali-
tative evaluation, we will try to apply evaluation
metrics that are able to capture content and coher-
ence aspects of summaries, such as more complex
content similarity or readability measures.
7 Acknowledgements
This research has been conducted thanks to a grant asso-
ciated to the X-TRACT project, PB98-1226 of the Span-
ish Research Department. It has also been partially
funded by projects HERMES (TIC2000-0335-0O3-02), PE-
TRA (TIC2000-1735-0O2-02), and by CLiC (Centre de Ll-
lengutatge i ComputaciO).
References
Laura Alonso and Irene CastellOn. 2001. Towards a delimita-
tion of discursive segment for natural language processing
applications. In First International Workshop on Seman-
tics, Pragmatics and Rhetoric, Donostia - San Sebastian,
November.
7
Laura Alonso and Maria Fuentes. 2002. Collaborating dis-
course for text summarisation. In Proceedings of the Sev-
enth ESSLLI Student Session.
Laura Alonso, Irene CastellOn, and Lluis Padre,. 2002a. De-
sign and implementation of a spanish discourse marker
lexicon. In SEPLIV, Valladolid.
Laura Alonso, Irene CastellOn, and Lillis PadrO. 2002b.
X-tractor: A tool for extracting discourse markers. In
LREC 2002 workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated Lan-
guage Data, Las Palmas.
J. C. Anscombre and 0. Ducrot. 1983. L'argumentation clans
la langue. Mardaga.
Montse Arevalo, Xavi Carreras, Lids Marquez, M.AntOnia
Marti, Liu& Padre', and M.Jose Simon. 2002. A pro-
posal for wide-coverage spanish named entity recognition.
Procesamiento del Len guaje Natural, 1(3).
Nicholas Asher and Alex Lascarides. 2002. The Logic of
Conversation. Cambridge University Press.
Regina Barzilay. 1997. Lexical Chains for Summarization.
Ph.D. thesis, Ben-Gurion University of the Negev.
Meru Brunn, Yllias Chali, and Christopher J. Pinchak. 2001.
Text Summarization using lexical chains. In Workshop on
Text Summarization in conjunction with the ACM SIGIR
Conference 2001, New Orleans, Louisiana.
Josep Carmona, Sergi Cervell, Lluis Marquez, M. AntOnia
Mart, Lluis PadrO, Roberto Placer, Horacio Rodriguez,
Mariona Taule, and Jordi Turmo. 1998. An environ-
ment for morphosyntactic processing of unrestricted span-
ish text. In First International Conference on Language
Resources and Evaluation (LREC'98), Granada, Spain.
Simon H. Corston-Oliver and W. Dolan. 1999. Less is more:
Eliminating index terms from subordinate clauses. In 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL'99), pages 348 - 356.
DUC. 2002. DUC-document understanding conference.
http://duc.nist.gov/.
K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar, A. Joshi,
and B. Webber. 2003. D-LTAG system - discourse pars-
ing with a lexicalized tree-adjoining grammar. Journal of
Language, Logic and Information. to appear.
Maria Fuentes and Horacio Rodriguez. 2002. Using cohe-
sive properties of text for automatic summarization. In
JOTRI'02.
Jade Goldstein, Vibhu Mittal, Mark Kantrowitz, and Jaime
Carbonell. 1999. Summarizing text documents: Sentence
selection and evaluation metrics. In SIGIR-99.
M. A. K. Halliday and R. Hasan. 1976. Cohesion in English.
English Language Series. Longman Group Ltd.
Alistair Knott, Jon Oberlander, Mick O'Donnell, and Chris
Mellish. 2001. Beyond elaboration: The interaction of
relations and focus in coherent text. In Ted Sanders, Joust
Schilperoord, and Wilbert Spooren, editors, Text represen-
tation: linguistic and psycholinguistic aspects, pages 181-
196. Benjamins.
Inderjeet Mani. 2001. Automatic Summarization. Nautral
Language Processing. John Benjamins Publishing Com-
pany.
William C. Mann and Sandra A. Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of text
organisation. Text, 3(8):234-281.
Daniel Marcu. 1997. The Rhetorical Parsing, Summariza-
tion and Generation of Natural Language Texts. Ph.D.
thesis, Department of Computer Science, University of
Toronto, Toronto, Canada.
Daniel Marcu. 1999. The automatic construction of large-
scale corpora for summarization research. In SIGIR-99.
2002. MEADeval. http://perun.si.umich.edu/clair/meadeval/.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion, the
thesaurus, and the structure of text. Computational lin-
guistics, 17(I):21-48.
M. Palomar, A. Ferrandez, L. Moreno, P. Martinez-Barco,
J. Peral, M. Saiz-Noeda, and R. Mu noz. 2001. An algo-
rithm for anaphora resolution in spanish texts. Computa-
tional Linguistics, 27(4).
Livia Polanyi. 1988. A formal model of the structure of
discourse. Journal of Pragmatics, 12:601-638.
R. Schank and R. Abelson. 1977. Scripts, Plans, Goals, and
Understanding. Lawrence Erlbaum, Hillsdale, NJ.
SweSum. 2002. http://www.nada.kth.set  xmartin/
swesum/index-eng.html.
Arie Verhagen. 2001. Subordination and discourse segmen-
tation revisited, or: Why matrix clauses may be more
dependent than complements. In Ted Sanders, Joost
Schilperoord, and Wilbert Spooren, editors, Text Rep-
resentation. Linguistic and psychological aspects, pages
337-357. John Benjamins.
Piek Vossen, editor. 1998. Euro WordNet: a multilingual
database with lexical semantic networks. Kluwer Aca-
demic Publishers.
8
