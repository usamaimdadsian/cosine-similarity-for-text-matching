Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1367–1372,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Not All Contexts Are Created Equal:
Better Word Representations with Variable Attention
Wang Ling Lin Chu-Cheng Yulia Tsvetkov Silvio Amir
Ram
´
on Fernandez Astudillo Chris Dyer Alan W Black Isabel Trancoso
L
2
F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior T´ecnico, Lisbon, Portugal
{lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu
{ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt
Abstract
We introduce an extension to the bag-of-
words model for learning words represen-
tations that take into account both syn-
tactic and semantic properties within lan-
guage. This is done by employing an at-
tention model that finds within the con-
textual words, the words that are relevant
for each prediction. The general intuition
of our model is that some words are only
relevant for predicting local context (e.g.
function words), while other words are
more suited for determining global con-
text, such as the topic of the document.
Experiments performed on both semanti-
cally and syntactically oriented tasks show
gains using our model over the existing
bag of words model. Furthermore, com-
pared to other more sophisticated models,
our model scales better as we increase the
size of the context of the model.
1 Introduction
Learning word representations using raw text data
have been shown to improve many NLP tasks,
such as part-of-speech tagging (Collobert et al.,
2011), dependency parsing (Chen and Manning,
2014; Kong et al., 2014) and machine transla-
tion (Liu et al., 2014; Kalchbrenner and Blunsom,
2013; Devlin et al., 2014; Sutskever et al., 2014).
These embeddings are generally learnt by defining
an objective function, which predicts words con-
ditioned on the context surrounding those words.
Once trained, these can be used as features (Turian
et al., 2010), as initializations of other neural net-
works (Hinton and Salakhutdinov, 2012; Erhan et
al., 2010; Guo et al., 2014).
The continuous bag-of-words (Mikolov et al.,
2013) is one of the many models that learns word
representations from raw textual data. While these
models are adequate for learning semantic fea-
tures, one of the problems of this model is the
lack of sensitivity for word order, which limits
their ability of learn syntactically motivated em-
beddings (Ling et al., 2015a; Bansal et al., 2014).
While models have been proposed to address this
problem, the complexity of these models (“Struc-
tured skip-n-gram” and “CWindow”) grows lin-
early as size of the window of words considered
increases, as a new set of parameters is created
for each relative position. On the other hand, the
continuous bag-of-words model requires no addi-
tional parameters as it builds the context repre-
sentation by summing over the embeddings in the
window and its performance is an order of magni-
tude higher than of other models.
In this work, we propose an extension to the
continuous bag-of-words model, which adds an at-
tention model that considers contextual words dif-
ferently depending on the word type and its rela-
tive position to the predicted word (distance to the
left/right). The main intuition behind our model
is that the prediction of a word is only dependent
on certain words within the context. For instance,
in the sentence We won the game! Nicely played!,
the prediction of the word played, depends on both
the syntactic relation from nicely, which narrows
down the list of candidates to verbs, and on the
semantic relation from game, which further nar-
rows down the list of candidates to verbs related
to games. On the other hand, the words we and
the add very little to this particular prediction. On
the other hand, the word the is important for pre-
dicting the word game, since it is generally fol-
lowed by nouns. Thus, we observe that the same
1367
word can be informative in some contexts and not
in others. In this case, distance is a key factor, as
the word the is informative to predict the immedi-
ate neighboring words, but not distance ones.
2 Attention-Based Continuous
Bag-of-words
2.1 Continuous Bag-Of-Words (CBOW)
The work in (Mikolov et al., 2013) is frequently
used to learn word embeddings. It defines pro-
jection matrix W ? <
d×|V |
where d is the em-
bedding dimension with the vocabulary V . These
parameters are optimized by by maximizing the
likelihood that words are predicted from their con-
text. Two models were defined, the skip-gram
model and the continuous bag-of-words model.
In this work, we focus on the continuous bag-
of-words model. The CBOW model predicts the
center word w
0
given a representation of the sur-
rounding words w
?b
, . . . ,w
?1
,w
1
,w
b
, where b
is a hyperparameter defining the window of con-
text words. The context vector is obtained by
averaging the embeddings of each word c =
1
2b
?
i?[?b,b]?{0}
w
i
and the prediction of the cen-
ter word w
0
is obtained by performing a softmax
over all the vocabulary V . More formally, define
the output matrix O ? <
|V |×d
w
, which maps the
context vector c into a |V |-dimensional vector rep-
resenting the predicted word, and maximizes the
following probability:
p(v
0
| w
[?b,b]?{0}
) =
expv
>
0
Oc
?
v?V
expv
>
Oc
(1)
where Oc corresponds to the projection of the
context vector c onto the vocabulary V and v
is a one-hot representation. For larger vocabu-
laries it is inefficient to compute the normalizer
?
v?V
expv
>
Oc. Solutions for problem are us-
ing the hierarchical softmax objective function
(Mikolov et al., 2013) or resorting to negative
sampling to approximate the normalizer (Gold-
berg and Levy, 2014).
The continuous bag-of-words model differs
from other proposed models in the sense that its
complexity does not rise substantially as we in-
crease the window b, since it only requires two
extra additions to compute c, which correspond to
d
w
operations each. On the other hand, the skip-
n-gram model requires two extra predictions cor-
responding to d
w
× V operations each, which is
an order of magnitude more expensive even when
subsampling V . However, the drawback the bag-
of-words model is that it does not learn embed-
dings that are prone for learning syntactically ori-
ented tasks, mainly due to lack of sensitivity to
word order, since the context is defined by a sum
of surrounding words. Extensions are proposed
in (Ling et al., 2015a), where the sum if replaced
by the concatenation of the word embeddings in
the order these occur. However, this model does
not scale well as b increases as it requires V × d
w
more parameters for each new word in the win-
dow.
Finally, setting a good value for b is difficult as
larger values may introduce a degenerative behav-
ior in the model, as more effort is spent predict-
ing words that are conditioned on unrelated words,
while smaller values of b may lead to cases where
the window size is not large enough include words
that are semantically related. For syntactic tasks,
it has been shown that increasing the window size
can adversely impact in the quality of the embed-
dings (Bansal et al., 2014; Lin et al., 2015).
2.2 CBOW with Attention
We present a solution to these problems while
maintaining the efficiency underlying the bag-of-
words model, and allowing it to consider contex-
tual words within the window in a non-uniform
way. We first rewrite the context window c as:
c =
?
i?[?b,b]?{0}
a
i
(w
i
)w
i
(2)
where we replace the average of the word embed-
dings with a weighted sum of the individual word
embeddings within the context. That is, each word
is w
i
at relative position i is attributed an attention
level representing how much the attention model
believes this it is important to look at in order to
predict the center word. The attention a
i
(w) given
to word w ? V at the relative position i is com-
puted as:
a
i
(w) =
exp k
w,i
+ s
i
?
j?[?b,b]?{0}
exp k
w,j
+ s
j
(3)
where K ? <
|V |×2b
(with elements k
i,j
) is a set
of parameters that which determines the impor-
tance of each word type in each (relative) position,
s ? <
2b
is a bias, which is conditioned only on
the relative position. As this is essentially a soft-
max over context words, the default bag-of-words
model can be seen as a special case of this model
1368
where all parameters K and s are fixed at zero.
Computing the attention of all words in the input
requires 2b operations, as it simply requires re-
trieving one value from the lookup matrix K for
each word and one value from the bias s for each
word in the window. Considering that these mod-
els must be trainable on billions of tokens, effi-
ciency is paramount. Although more sophisticated
attentional models are certainly imaginable (Bah-
danau et al., 2014), ours is a good balance of com-
putational efficiency and modeling expressivity.
2.3 Parameter Learning
Gradients of the loss function with respect to
the parameters (W,O,K, s) are computed with
backpropagation, and parameters are updated after
each training instance using a fixed learning rate.
3 Experiments
3.1 Word Vectors
We used a subsample from an English Wikipedia
dump
1
containing 10 million documents, contain-
ing a total of 530 million tokens. We built word
embeddings using the original CBOW and our
proposed attentional model on this dataset.
In both cases, word vectors were constructed us-
ing window size b = 20, which enables us to cap-
ture longer-range dependencies between words.
We set the embedding size d
w
= 50 and used a
negative sampling rate of 10. Finally, the vocabu-
lary was reduced to words with more than 40 oc-
currences. In terms of computational speed, the
original bag-of-words implementation was able to
compute approximately 220k words per second,
while our model computes approximately 100k
words per second. The slowdown is tied to the
fact that we are computing the gradients, the atten-
tion model parameters, as well as the word embed-
dings. On the other hand, the skip-n-gram model
process words at only 10k words per second, as it
must predict every word in the window b.
Figure 1 illustrates the attention model for the
prediction of the word south in the sentence an-
tartica has little rainfall with the south pole mak-
ing it a continental desert. Darker cell indicate
higher attention values from a(i, w). We can ob-
serve that function words (has, the and a) tend to
be attributed very low attentions, as these are gen-
erally less predictive power. On the other hand,
1
Collected in September of 2014
antartica has little rainfall with the
south
pole making it a continental desert
Figure 1: Illustration of the inferred attention pa-
rameters for a sentence from our training data
when predicting the word south; darker cells in-
dicate higher weights.
content words, such as antartica, rainfall, conti-
nental and desert are attributed higher weights as
these words provide hints that the predicted word
is likely to be related to these words. Finally, the
word pole is assigned the highest attention as it
close to the predicted word, and there is a very
likely chance that south will precede pole.
3.2 Syntax Evaluation
For syntax, we evaluate our embeddings in the
domain of part-of-speech tagging in both su-
pervised (Ling et al., 2015b) and unsupervised
tasks (Lin et al., 2015). This later task is newly
proposed, but we argue that success in it is a com-
pelling demonstration of separation of words into
syntactically coherent clusters.
Part-of-speech induction. The work in (Lin et
al., 2015) attempts to infer POS tags with a
standard bigram hmm, which uses word embed-
dings to infer POS tags without supervision. We
use the same dataset, obtained from the ConLL
2007 shared task (Nivre et al., 2007) Scoring is
performed using the V-measure (Rosenberg and
Hirschberg, 2007), which is used to predict syn-
tactic classes at the word level. It has been shown
in (Lin et al., 2015) that word embeddings learnt
from structured skip-ngrams tend to work better
at this task, mainly because it is less sensitive to
larger window sizes. These results are consistent
with our observations found in Table 1, in rows
“Skip-ngram” and “SSkip-ngram”. We can ob-
serve that our attention based CBOW model (row
“CBOW Attention”) improves over these results
for both tasks and also the original CBOW model
(row “CBOW”).
1369
POS Induction POS Tagging Sentiment Analysis
CBOW 50.40 97.03 71.99
Skip-ngram 33.86 97.19 72.10
SSkip-ngram 47.64 97.40 69.96
CBOW Attention 54.00 97.39 71.39
Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one
per column) using different types of embeddings (one per row).
Part-of-speech tagging. The evaluation is per-
formed on the English PTB, with the standard
train (Sections 0-18), dev (Sections 19-21) and test
(Sections 22-24) splits. The model is trained with
the Bidirectional LSTM model presented in (Ling
et al., 2015b) using the same hyper-parameters.
Results on the POS accuracy on the test set are
reported on Table 1. We can observe our model
can obtain similar results compared to the struc-
tured skip-ngram model on this task, while train-
ing the model is significantly faster. The gap be-
tween the usage of different embeddings is not as
large as in POS induction, as this is a supervised
task, where pre-training generally leads to smaller
improvements.
3.3 Semantic Evaluation
To evaluate the quality of our vectors in terms
of semantics, we use the sentiment analysis task
(Senti) (Socher et al., 2013), which is a binary
classification task for movie reviews. We sim-
ply use the mean of the word vectors of words
in a sentence, and use them as features in an `
2
-
regularized logistic regression classifier. We use
the standard training/dev/test split and report ac-
curacy on the test set in table 1.
We can see that in this task, our models do
not perform as well as the CBOW and Skipngram
model, which hints that our model is learning em-
beddings that learn more towards syntax. This is
expected as it is generally uncommon for embed-
dings to outperform existing models on both syn-
tactic and semantic tasks simultaneously, as em-
beddings tend to be either more semantically or
syntactically oriented. It is clear that the skipn-
gram model learns embeddings that are more se-
mantically oriented as it performs badly on all
syntactic tasks. The structured skip-ngram model
on the other hand performs badly on the syntactic
tasks, but we observe a large drop on this semanti-
cally oriented task. Our attention-based model, on
the other hand, out performs all other models on
syntax-based tasks, while maintaining a compet-
itive score on semantic tasks. This is an encour-
aging result that shows that it is possible to learn
representations that can perform well on both se-
mantic and syntactic tasks.
4 Related Work
Many methods have been proposed for learning
word representations. Earlier work learns em-
beddings using a recurrent language model (Col-
lobert et al., 2011), while several simpler and
more lightweight adaptations have been pro-
posed (Huang et al., 2012; Mikolov et al., 2013).
While most of the learnt vectors are semantically
oriented, work has been done in order to ex-
tend the model to learn syntactically oriented em-
beddings (Ling et al., 2015a). Attention models
are common in vision related tasks (Tang et al.,
2014), where models learn to pay attention to cer-
tain parts of a image in order to make accurate
predictions. This idea has been recently intro-
duced in many NLP tasks, such as machine trans-
lation (Bahdanau et al., 2014). In the area of word
representation learning, no prior work that uses at-
tention models exists to our knowledge.
5 Conclusions
In this work, we presented an extension to the
CBOW model by introducing an attention model
to select relevant words within the context to make
more accurate predictions. As consequence, the
model learns representations that are both syntac-
tic and semantically motivated that do not degrade
with large window sizes, compared to the orig-
inal CBOW and skip-ngram models. Efficiency
is maintained by learning a position-based atten-
tion model, which can compute the attention of
surrounding words with a relatively small number
of operations. Finally, we show improvements on
syntactically oriented tasks, without degrading re-
sults significantly on semantically oriented tasks.
1370
Acknowledgements
The PhD thesis of Wang Ling is supported by
FCT grant SFRH/BD/51157/2010. This research
was supported in part by the U.S. Army Re-
search Laboratory, the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533
and NSF IIS-1054319 and FCT through the pluri-
anual contract UID/CEC/50021/2013 and grant
number SFRH/BPD/68428/2010.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, Baltimore, MD, USA, June.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of
EMNLP.
Geoffrey E Hinton and Ruslan Salakhutdinov. 2012.
A better way to pretrain deep boltzmann machines.
In Advances in Neural Information Processing Sys-
tems, pages 2447–2455.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP, pages
1700–1709.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proc. of EMNLP, pages 1001–1012,
Doha, Qatar, October.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2015. Unsupervised POS induction with
word embeddings. In Proceedings of NAACL.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015a. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics.
Wang Ling, Tiago Lu´?is, Lu´?is Marujo, R´amon Fer-
nandez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015b. Finding func-
tion in form: Compositional character models for
open vocabulary word representation. EMNLP.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL, pages
1491–1500.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning(EMNLP-CoNLL), pages 410–
420.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
1371
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Yichuan Tang, Nitish Srivastava, and Ruslan R
Salakhutdinov. 2014. Learning generative models
with visual attention. In Advances in Neural Infor-
mation Processing Systems, pages 1808–1816.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
1372
