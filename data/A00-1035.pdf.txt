Spell ing and Grammar  Correct ion for Danish in SCARRIE  
Pat r i z ia  Pagg io  
Center for Sprogteknologi  
Copenhagen (DK) 
patrizia@cst, ku. dk 
Abst rac t  
This paper reports on work carried out to de- 
velop a spelling and grammar corrector for Dan- 
ish, addressing in particular the issue of how a 
form of shallow parsing is combined with er- 
ror detection and correction for the treatment 
of context-dependent spelling errors. The syn- 
tactic grammar for Danish used by the system 
has been developed with the aim of dealing with 
the most frequent error types found in a parallel 
corpus of unedited and proofread texts specif- 
ically collected by the project's end users. By 
focussing on certain grammatical constructions 
and certain error types, it has been possible 
to exploit the linguistic 'intelligence' provided 
by syntactic parsing and yet keep the system 
robust and efficient. The system described is 
thus superior to other existing spelling checkers 
for Danish in its ability to deal with context- 
dependent errors. 
1 In t roduct ion  
In her much-quoted and still relevant review of  
technologies for automatic word correction (Ku- 
kich, 1992), Kukich observes that "research in 
context-dependent spelling correction is in its 
infancy" (p. 429), and that the task of treating 
context-dependent rrors is still an elusive one 
due to the complexity of the linguistic knowl- 
edge often necessary to analyse the context in 
sufficient depth to find and correct such er- 
rors. But progress in parsing technology and 
the growing speed of computers eem to have 
made the task less of a chimera. The '90s 
have in fact seen a renewed interest in gram- 
mar checking, and proposals have been made 
for systems covering English (Bernth, 1997) and 
other languages uch as Italian (Bolioli et al., 
1992), Spanish and Greek (Bustamante and 
Ldon, 1996), Czech (Holan et al., 1997) and 
Swedish (Hein, 1998). 
This paper describes the prototype of a 
spelling and grammar corrector for Danish 
which combines traditional spelling checking 
functionalities with the ability to carry out com- 
pound analysis and to detect and correct cer- 
tain types of context-dependent spelling errors 
(hereafter simply "grammar errors"). Gram- 
mar correction is carried out by parsing the 
text, making use of feature overriding and error 
weights to accommodate he errors. Although 
a full parse of each sentence is attempted, the 
grammar has been developed with the aim of 
dealing only with the most frequent error types 
found in a parallel corpus of unedited and proof- 
read texts specifically collected by the project's 
end users. By focussing on certain grammati- 
cal constructions and certain error types, it has 
been possible to exploit the linguistic 'intelli- 
gence' provided by syntactic parsing and yet 
keep the system robust and efficient. The sys- 
tem described is thus superior to other existing 
spelling checkers for Danish in its ability to deal 
with certain types of grammar errors. 
We begin by giving an overview of the sys- 
tem's components in Section 2. In Section 3 we 
describe the error types we want to deal with: 
Section 4 gives an overview of the grammar: 
in particular, the methods adopted for treating 
feature mismatches and structural errors are ex- 
plained. Finally, in Section 5 evaluation results 
are presented and a conclusion is drawn. 
2 The  proto type  
The prototype is a system for high-quality 
proofreading for Danish which has been de- 
veloped in the context of a collaborative EU- 
project 1. Together with the Danish prototype, 
1Main contractors in the consortium were: 
WordFinder Software AB (Sweden), Center for 
255 
the project has also produced similar systems 
for Swedish and Norwegian, all of them tailored 
to meet the specific needs of the Scandinavian 
publishing industry. They all provide writing 
support in the form of word and grammar check- 
ing. 
The Danish version of the system 2 constitutes 
a further development of the CORRie prototype 
(Vosse, 1992) (Vosse, 1994), adapted to deal 
with the Danish language, and to the needs of 
the project's end users. The system processes 
text in batch mode and produces an annotated 
output text where errors are flagged and re- 
placements suggested where possible. Text cor- 
rection is performed in two steps: first the sys- 
tem deals with spelling errors and typos result- 
ing in invalid words, and then with grammar 
errors. 
Invalid words are identified on the basis of 
dictionary lookup. The dictionary presently 
consists of 251,000 domain-relevant word forms 
extracted from a collection of 68,000 newspa- 
per articles. A separate idiom list allowing for 
the identification of multi-word expressions is 
also available. Among the words not found in 
the dictionary or the idiom list, those occurring 
most frequently in the text (where frequency is 
assessed relative to the length of the text) are 
taken to be new words or proper names 3. The 
remaining unknown words are passed on to the 
compound analysis grammar, which is a set of 
regular expressions covering the most common 
types of compound nominals in Danish. This is 
an important feature, as in Danish compound- 
ing is very productive, and compounds are writ- 
ten as single words. 
Words still unknown at this point are taken 
to be spelling errors. The System flags them as 
Sprogteknologi (Denmark), Department of Linguistics 
at Uppsala University (Sweden), Institutt for lingvistikk 
og litteraturvitenskab at the University of Bergen (Nor- 
way), and Svenska Dagbladet (Sweden). A number of 
subcontractors also contributed to the project. Subcon- 
tractors in Denmark were: Munksgaard International 
Publishers, Berlingske Tidende, Det Danske Sprog- og 
Litteraturselskab, and Institut for Almen og Anvendt 
Sprogvidenskab t the University of Copenhagen. 
2In addition to the author of the present paper, 
tlle Danish SCARRIE team at CST consisted of Claus 
Povlsen, Bart Kongejan and Bradley Music. 
3The system also checks whether a closely matching 
alternative can be found in the dictionary, to avoid mis- 
taking a consistently misspelt word for a new word. 
such and tries to suggest a replacement. The 
algorithm used is based on trigram and tri- 
phone analysis (van Berkel and Smedt, 1988), 
and takes into account the orthographic strings 
corresponding to the invalid word under con- 
sideration and its possible replacement, as well 
as the phonetic representations of the same two 
words. Phonetic representations are generated 
by a set of grapheme-to-phoneme rules (Hansen,  
1999) the aim of which is to assign phonetically 
motivated misspellings and their correct coun- 
terparts identical or similar phonetic represen- 
tations. 
Then the system tries to identify context- 
dependent spelling errors. This is done by pars- 
ing the text. Parsing results are passed on to 
a corrector to find replacements for the errors 
found. The parser is an implementation of the 
Tomita algorithm with a component for error 
recognition whose job is to keep track of error 
weights and feature mismatches as described in 
(Vosse, 1991). Each input sentence is assigned 
the analysis with the lowest error weight. If the 
error is due to a feature mismatch, the offending 
feature is overridden, and if a dictionary entry 
satisfying the grammar constraints expressed by 
the context is found in the dictionary, it is of- 
fered as a replacement. If the structure is in- 
complete, on the other hand, an error message 
is generated. Finally, if the system identifies an 
error as a split-up or a run-on, it will suggest 
either a possible concatenation, or a sequence 
of valid words into which the misspelt word can 
be split up. 
3 The  er rors  
To ensure the coverage of relevant error types, 
a set of parallel unedited and proofread texts 
provided by the Danish end users has been col- 
lected. This text collection consists of newspa- 
per and magazine articles published in 1997 for 
a total of 270,805 running words. The articles 
have been collected in their raw version, as well 
as in the edited version provided by the pub- 
lisher's own proofreaders. Although not very 
large in number of words, th@ corpus consists 
of excerpts from 450 different articles to en- 
sure a good spread of lexical domains and error 
types. The corpus has been used to construct 
test suites for progress evaluation, and also to 
guide grammar development. The aim set for 
256 
Error type No. 
Context independent errors 386 
Context dependent errors 308 
Punctuation problems 212 
Style problems 89 
Graphical problems 24 
Total 1019 
% 
38 
30 
21 
9 
2 
100 
Figure 1: Error distribution in the Danish cor- 
pus 
grammar development was then to enable the 
system to identify and analyse the grammati- 
cal constructions in which errors typically occur, 
whilst to some extent disregarding the remain- 
der of the text. 
The errors occurring in the corlbus have been 
analysed according to the taxonomy in (Ram- 
bell, 1997). Figure 1 shows the distribution of 
the various error types into the five top-level 
categories of the taxonomy. As can be seen, 
grammar errors account for 30~0 of the errors. 
Of these, 70% fall into one of the following cat- 
egories (Povlsen, 1998): 
• Too many finite verbal forms or missing fi- 
nite verb 
• Errors in nominal phrases: 
- agreement errors, 
- wrong determination, 
- genitive errors, 
- errors concerning pronouns; 
• Split-ups and run-ons. 
Another way of grouping the errors is by the 
kind of parsing failure they generate: they can 
then be viewed as either feature mismatches, 
or as structural errors. Agreement errors are 
typical examples of feature mismatches. In the 
following nominal phrase, for example: 
(1) de *interessant projekter 
(the interesting projects) 
_the error can be formalised as a mismatch be- 
tween the definiteness of the determiner de (the) 
and the indefiniteness of the adjective interes- 
sant (interesting). Adjectives have in fact both 
an indefinite and a definite form in Danish. 
The sentence below, on the other hand, is an 
example of structural error. 
(2) i sin tid *skabet han skulpturer over 
atomkraften 
(during his time wardrobe/created he 
sculptures about nuclear power) 
Since the finite verb skabte (created) has been 
misspelt as skabet (the wardrobe), the syntactic 
structure corresponding to the sentence is miss- 
ing a verbal head. 
Run-ons and split-ups are structural errors of 
a particular kind, having to do with leaves in the 
syntactic tree. In some cases they can only be 
detected on the basis of the context, because the 
misspelt word has the wrong category or bears 
some other grammatical feature that is incorrect 
in the context. Examples are given in (3) and 
(4) below, which like the preceding examples are 
taken from the project's corpus. In both cases, 
the error would be a valid word in a different 
context. More specifically, rigtignok (indeed) is 
an adverb, whilst rigtig nok (actually correct) is 
a modified adjective; and inden .for (inside) is a 
preposition, whilst indenfor (indoors) is an ad- 
verb. In both examples the correct alternative 
is indicated in parentheses. 
(3) ... studerede rain gruppe *rigtig nok 
(rigtignok) under temaoverskrifter 
(studied my group indeed on the basis 
of topic headings) 
(4) *indenfor (inden for) de gule mute 
(inside the yellow walls) 
Although the system has a facility for identi- 
fying and correcting split-ups and run-ons based 
on a complex interaction between the dictio- 
nary, the idiom list, the compound grammar 
and the syntactic grammar, this facility has not 
been fully developed yet, and will therefore not 
be described any further here. More details can 
be found in (Paggio, 1999). 
4 The  grammar  
The grammar is an augmented context-free 
grammar consisting of rewrite rules where sym- 
bols are associated with features. Error weights 
and error messages can also be attached to ei- 
ther rules or single features. The rules are ap- 
plied by unification, but in cases where one or 
more features do not unify, the offending fea- 
tures will be overridden. 
257 
In the current version of the grammar~ only 
the structures relevant to the error types we 
want the system to deal with - in other words 
nominal phrases and verbal groups - are ac- 
counted for in detail. The analysis produced is 
thus a kind of shallow syntactic analysis where 
the various sentence constituents are attached 
under the topmost S node as fragments. 
For example, adjective phrases can be anal- 
ysed as fragments, as shown in the following 
rule: 
Fragment -> 
AP "?Fragment AP rule":2 
To indicate that the fragment analysis is not 
optimal, it is associated with an error weight, 
as well as an error message to be used for de- 
bugging purposes (the message is not visible to 
the end user). The weight penalises parse trees 
built by applying the rule. The rule is used e.g. 
to analyse an AP following a copula verb as in: 
(5) De projekter er ikke interessante. 
(Those projects are not interesting) 
The main motivation for implementing a 
grammar based on the idea of fragments was 
efficiency. Furthermore, the fragment strategy 
could be implemented very quickly. However, 
as will be clarified in Section 5, this strategy is 
sometimes responsible for bad flags. 
4.1 Feature  mismatches  
As an alternative to the fragment analysis, APs 
can be attached as daughters in NPs. This is of 
course necessary for the treatment of agreement 
in NPs, one of the error types targeted in our 
application. This is shown in the following rule: 
NP(def Gender PersNumber) -> 
Det (def Gender PersNumber) 
AP(def _ _) 
N(indef Gender:9- PersNumber) 
The rule will parse a correct definite NP such 
as :  
(6) 
but also 
(7) 
(S) 
de interessante projekter 
(the interesting projects) 
de *interessant projekter 
de interessante *projekterne 
The  feature overriding mechanism makes it 
possible for the system to suggest interessante 
as the correct replacement in (7), and projekter 
in (8). Let us see how this is done in more de- 
tail for example (7). The parser tries to apply 
the NP rule to the input string. The rule states 
that the adjective phrase must be definite (AP 
(def _ _)). But the dictionary entry correspond- 
ing to interessant bears the feature 'indef'. The 
parser will override this feature and build an 
NP according to the constraints expressed by 
the rule. At this point, a new dictionary lookup 
is performed, and the definite form of the ad- 
jective can be suggested as a replacement. 
Weights are used to control rule interaction 
as well as to establish priorities among features 
that may have to be overridden. For example 
in our NP rule, a weight has been attached to 
the Gender feature in the N node. The weight 
expresses the fact that it costs more to over- 
ride gender on the head noun than on the de- 
terminer or adjective. The rationale behind this 
is the fact that if there is a gender mismatch, 
the parser should not try to find an alternative 
• form of the noun (which does not exist), but if 
necessary override the gender feature either on 
the adjective or the determiner. 
4.2. Captur ing  s t ruc tura l  e r ro rs  in 
grammar  ru les  
To capture structural errors, the formalism al- 
lows the grammar writer to write so-called error 
rules. The syntax of error rules is very similar 
to that used in 'normal' rules, the only differ- 
ence being that an error rule must have an er- 
• ror weight and an error message attached to it. 
The purpose of the weight is to ensure that er-  
ror rules are applied only if 'normal' rules are 
not applicable. The error message can serve two 
purposes. Depending on whether it is stated as 
an implicit or an explicit message (i.e. whether 
it is preceded by a question mark or not), it will 
appear in the log file where it can be used for 
debugging purposes, or in the output text as a 
message to the end user. 
The following is an error rule example. 
VGroup(_ finite Tense) -> 
V(_ finite:4 Tense) 
V(_ finite:4 _) 
"Sequence of two finite verbs":4 
, ' j r -  O 258
Error type Total Flagged Hits Misses No suggs 
Errors in single words 81 69 (85.2%) 35 (50.8%) 11 (15.9%) 23 (33.3%) 
Errors in compounds 42 28 (66.7%) 8 (28.6%) 8 (28.6%) 12 (42.8%) 
NP agreement errors 18 15 (83.3%) 15 (100.0%) 0 (0.0%) 0 (0.0%) 
VP errors 13 8 (61.6%) 8 (100.0%) 0 (0 .0%)0  (0.0%) 
Total 154 120 (78.0%)66 (55.0%) 19 (15.8%) 35 (29.2%) 
Figure 2: Error coverage and suggestion adequacy evaluated on test suites 
A weight of 4 is attached to the rule as a 
whole, but there are also weights attached to the 
'finiteness' feature on the daughters: their func- 
tion is to make it costly for the system to apply 
the rule to non-finite forms. In other words, the 
feature specification 'finite' is made difficult to 
override to ensure that it is indeed a sequence of 
finite verbal forms the rule applies to and flags. 
The rule will for example parse the verbal se- 
quence in the following sentence: 
(9) Jeg vil *bevarer (berate) rain frihed. 
(*I want keep my freedom) 
As a result of parsing, the system in this case 
will not attempt to correct the wrong verbal 
form, but issue the error message "Sequence of 
two finite verbs". 
Error rules can thus be used to explicitly 
describe an error and to issue error messages. 
However, so far we have made very limited use 
of them, as controlling their interaction with 
'normal' rules and with the feature overriding 
mechanism is not entirely easy. In fact, they are 
consistently used only to identify incorrect se- 
quences of finite verbal forms or sentences miss- 
ing a finite verb. To this sparse use of error rules 
corresponds, on the other hand, an extensive x- 
ploitation of the feature overriding mechanism. 
This strategy allows us to keep the number of 
rules in the  grammar elatively low, but relies 
on a careful manual adjustment of the weights 
attached to the various features in the rules. 
5 Eva luat ion  and  Conc lus ion  
The project's access to a set of parallel unedited 
and proofread texts has made it possible to au- 
tomate the evaluation of the system's linguis- 
tic functionality. A tool has been implemented 
to compare the results obtained by the sys- 
tem with the corrections suggested by the pub- 
lisher's human proofreaders in order to derive 
measures telling us how well the system per- 
formed on recall (lexical coverage as well as cov- 
erage of errors), precision (percentage of cor- 
rect flaggings), as well as suggestion adequacy 
(hits, misses and no suggestions offered). The 
reader is referred to (Paggio and Music, 1998) 
for more details on the evaluation methodology. 
The automatic procedure was used to evaluate 
the system during development, and in connec- 
tion with the user validation. Testing was done 
on constructed test suites displaying examples 
of the errors targeted in the project and with 
text excerpts from the parallel corpora~ 
Figure 2 shows error recall and suggestion ad- 
equacy figures for the various error types repre- 
sented in the test suites. These figures are very 
positive, especially with regard to the treatment 
of grammar errors. To make a comparison with 
a commercial product, the Danish version of the 
spelling and grammar checker provided by Mi- 
crosoft Word does not flag any of the grammar 
errors. 
Figure 3 shows how the system performed on 
one of the test corpora. The corpus was assem- 
bled by mixing short excerpts containing rele- 
vant grammar errors and randomly chosen text. 
Since unlike test suites, the corpus also contains 
correct ext, the figure this time also shows lex- 
ical coverage and precision figures. The corpus 
consists of 278 sentences, with an average length 
of 15.18 words per sentence. It may be sur- 
prising to see that it contains a limited number 
of errors, but it must be remembered that the 
texts targeted in the project are written by ex- 
perienced journalists. 
The corpus was processed in 58 cpu-seconds 
on an HP 9000/B160. As expected, the system 
performs less well than on the test suites, and in 
general precision is clearly too low. However, we 
still consider these results encouraging given the 
relatively small resources the project has been 
able to spend on grammar development, and we 
259 
Recall 
4220 total words 
4157 valid words 
3996 (96.1%) valid words accepted 
161 (3.9%) valid words rejected 
63 invalid words (real errors) 
36 (57.1%) real errors potted (good flags) 
27 (42.9%) real errors missed 
Precision 
175 flaggings 
36 (20.6%) good flags 
139 (79.4%) bad flags (false positives) 
Suggestion adequacy 
36 good flags 
15 (41.7%) hits (initial suggestion correct ) 
8 (22.2%) misses (suggestions offered, none correct) 
13 (36.1%) with no suggestions offered 
Figure 3: Test corpus evaluation 
believe they can be improved. 
We regard error coverage as quite satisfac- 
tory for a research prototype. In a comparative 
test made on a similar (slightly smaller) corpus, 
SCARR/E obtained 58.1% error coverage, and 
Word 53.5%. To quote a figure from another 
recently published test (Martins et al., 1998), 
the ReGra system is reported to miss 48.1% 
real errors. It is worth noting that ReGra has 
much more extensive linguistic resources avail- 
able than SCARRIE, i.e. a dictionary of 1.5 
million words and a grammar of 600 production 
rules. Most of the errors not found by SCAR- 
RIE in the test have to do with punctuation 
and other stylistic matters not treated in the 
project. There are also, however, agreement er- 
rors which go unnoticed. These failures are due 
to one of two reasons: either that no parse has 
been produced for the sentence in question, or 
that the grammar has produced a wrong analy- 
sis. 
The precision obtained is at least at first sight 
much too low. On the same test corpus, how- 
ever, Word only reached 15.9% precision. On 
closer inspection, 72 of the bad flags produced 
by SCARRIE turned out to be due to unrecog- 
nised proper names. Disregarding those, preci- 
sion goes up to 34.9%. As was mentioned early, 
SCARRIE has a facility for guessing unknown 
proper names on the basis of their frequency 
of occurrence in the text. But since the test 
corpus consists of Short unrelated excerpts, a 
large number of proper names only occur once 
or twice. To get an impression of how the sys- 
tem would perform in a situation where the 
same proper names and unknown words had a 
higher frequency of occurrence, we doubled the 
test corpus by simply repeating the same text 
twice. As expected, precision increased. The 
system produced 178 flags, 60 of which were 
correct (39.7%). This compares well with the 
40% precision reported for instance for ReGra. 
In addition to the problem of unkown proper 
names, false flags are related to unrecognised 
acronyms and compounds (typically forms con- 
taining acronyms or dashes), and a not very pre- 
cise treatment of capitalisation. Only 13 false 
flags are due to wrong grammar analyses caused 
either by the fragment approach or by the gram- 
mar's limited coverage. In particular, genitive 
phrases, which are not treated at the moment, 
are responsible for most of these false alarms. 
In conclusion, we consider the results ob- 
tained so far promising, and the problems re- 
vealed by the evaluation tractable within the 
current system design. In particular, future 
development should focus on treating stylistic 
matters uch as capitalisation and punctuation 
which have not been in focus in the current 
prototype. The coverage of the grammar, in 
particular the treatment of genitive phrases, 
should also be further developed. The data pro- 
260 
vided by the evaluation reported on in this pa- 
per, however, are much too limited to base fur- 
ther development on. Therefore, more exten- 
sive testing and debugging should also be car- 
ried out. 
In addition, two aspects of the system that 
have only be touched on in this paper would 
be worth further attention: one is the mecha- 
nism for the treatment of split-ups and run-ons, 
which as mentioned earlier is not well-integrated 
at the moment; the other is the weight adjust- 
ment process, which is done manually at the 
moment, and for which the adoption of a semi- 
automatic tool could be considered. 
Re ferences  
A. Bernth. 1997. EasyEnglish: a tool for im- 
proving document quality. In Proceedings of 
• the Fifth Conference on Applied Natural Lan- 
guage Processing. 
A. Bolioli, L. Dini, and G. Malnati. 1992. 
JDII: Parsing Italian with a robust constraint 
grammar. In Proceedings of COLING:92, 
pages 1003-1007. 
Flora Ram~rez Bustamante and Fer- 
nando S£nchez L@on. 1996. GramCheck: A 
grammar and style checker. In Proceedings 
of COLING-96, pages 175-181, Copenhagen, 
Denmark. 
Peter Molb~ek Hansen. 1999. Grapheme-to- 
phoneme rules for the Danish component 
of the SCARRIE project. In Hanne E. 
Thomsen and Sabine'Kirchmeier-Andersen, 
editors, Datalingvistisk Forenings drsmcde 
1998 i Kcbehavn, Proceedings, number 25 in 
LAMBDA, pages 79-91. Institut for datal- 
ingvistik, Handelshcjskolen i Kcbenhaven. 
Anna S£gvall Hein. 1998. A chart-based frame- 
work for grammar checking: Initial studies. 
In Proceedings of Nodalida-98. 
Tom~ Holan, Vladislav Kubofi, and Mar- 
tin Pl£tek. 1997. A prototype of a gram- 
mar checker for Czech. In Proceedings of 
ANLP'97. 
Karen Kukich. 1992. Techniques for automati- 
cally correcting words in text. A CM Comput- 
_ ing Surveys, 24(4):377-439. 
Ronaldo Teixeira Martins, Ricardo Hasegawa, 
Maria Volpe Nunes, Gisele Monthila, and Os- 
valdo Novais De Oliveira Jr. 1998. Linguistic 
issues in the development ofReGra: a gram- 
mar Checker for Brazilian Portuguese. Natu- 
ral Language Engineering, 4(4):287-307, De- 
cember. 
Patrizia' Paggio and Bradley Music. 1998. Eval- 
uation in the SCARRIE project. In Pro- 
ceedings of the First International Conference 
on Language Resources ~ Evaluation, pages 
277-282. Granada, Spain. 
Patrizia Paggio. 1999. Treatment of grammat- 
ical errors and evaluation in SCARRIE. In 
Hanne E. Thomsen and Sabine Kirchmeier- 
Andersen, editors, Datalingvistisk Forenings 
drsmCde 1998 i KCbehavn, Proceedings, num- 
ber 25 in LAMBDA, pages 65-78. Insti- 
tut for datalingvistik, Handelshcjskolen i 
Kcbenhaven. 
Claus Povlsen. 1998. Three types of gram- 
matical errors in Danish. Technical report, 
Copenhagen: Center for Sprogteknologi. 
Olga Rambell. 1997. Error typology for auto- 
matic proof-reading purposes. Technical re- 
port, Uppsala: Uppsala University. : 
Brigitte van Berkel and Koenra~d De Smedt. 
1988. Triphone analysis: a combined method 
for the correction of orthographical nd ty- 
pographical errors. In Proceedings of the 2nd 
conference on Applied Natural Language Pro- 
cessing, pages 77-83. ACL, Austin. 
Theo  Vosse. 1991. Detection and correction 
of morpho-syntactic errors in shift-reduce 
parsing. In R. Heemels, A. Nijholt, and 
K. Sikkel, editors, Tomita's Algorithm: Ex- 
tensions and Applications, number 91-68 in 
Memoranda Informatica, pages 69-78. Uni- 
versity of Twente/ 
Theo Vosse. 1992. Detecting and correcting 
morpho-syntactic errors in real texts. In Pro- 
ceedings of the Third Conference on Applied 
Natural Language Processing, pages 111-118, 
Trento, Italy. 
Theo G. Vosse. 1994. The Word Connection - 
Grammar-based Spelling Error Correction in 
Dutch. Ph.D. thesis, Rijksuniversiteit a Lei- 
den: the Netherlands. ISBN 90-75296-01-0. 
261 
