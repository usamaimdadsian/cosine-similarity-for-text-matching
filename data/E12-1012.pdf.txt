Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109–119,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics
Adaptation of Statistical Machine Translation Model for Cross-Lingual
Information Retrieval in a Service Context
Vassilina Nikoulina
Xerox Research Center Europe
vassilina.nikoulina@xrce.xerox.com
Bogomil Kovachev
Informatics Institute
University of Amsterdam
B.K.Kovachev@uva.nl
Nikolaos Lagos
Xerox Research Center Europe
nikolaos.lagos@xrce.xerox.com
Christof Monz
Informatics Institute
University of Amsterdam
C.Monz@uva.nl
Abstract
This work proposes to adapt an existing
general SMT model for the task of translat-
ing queries that are subsequently going to
be used to retrieve information from a tar-
get language collection. In the scenario that
we focus on access to the document collec-
tion itself is not available and changes to
the IR model are not possible. We propose
two ways to achieve the adaptation effect
and both of them are aimed at tuning pa-
rameter weights on a set of parallel queries.
The first approach is via a standard tuning
procedure optimizing for BLEU score and
the second one is via a reranking approach
optimizing for MAP score. We also extend
the second approach by using syntax-based
features. Our experiments show improve-
ments of 1-2.5 in terms of MAP score over
the retrieval with the non-adapted transla-
tion. We show that these improvements are
due both to the integration of the adapta-
tion and syntax-features for the query trans-
lation task.
1 Introduction
Cross Lingual Information Retrieval (CLIR) is an
important feature for any digital content provider
in today’s multilingual environment. However,
many of the content providers are not willing to
change existing well-established document index-
ing and search tools, nor to provide access to
their document collection by a third-party exter-
nal service. The work presented in this paper as-
sumes such a context of use, where a query trans-
lation service allows translating queries posed to
the search engine of a content provider into sev-
eral target languages, without requiring changes
to the undelying IR system used and without ac-
cessing, at translation time, the content provider’s
document set. Keeping in mind these constraints,
we present two approaches on query translation
optimisation.
One of the important observations done dur-
ing the CLEF 2009 campaign (Ferro and Peters,
2009) related to CLIR was that the usage of Sta-
tistical Machine Translation (SMT) systems (eg.
Google Translate) for query translation led to
important improvements in the cross-lingual re-
trieval performance (the best CLIR performance
increased from ˜55% of the monolingual baseline
in 2008 to more than 90% in 2009 for French
and German target languages). However, general-
purpose SMT systems are not necessarily adapted
for query translation. That is because SMT sys-
tems trained on a corpus of standard parallel
phrases take into account the phrase structure im-
plicitly. The structure of queries is very differ-
ent from the standard phrase structure: queries are
very short and the word order might be different
than the typical full phrase one. This problem can
be seen as a problem of genre adaptation for SMT,
where the genre is “query”.
To our knowledge, no suitable corpora of par-
allel queries is available to train an adapted SMT
system. Small corpora of parallel queries1 how-
ever can be obtained (eg. CLEF tracks) or man-
ually created. We suggest to use such corpora
in order to adapt the SMT model parameters for
query translation. In our approach the parameters
of the SMT models are optimized on the basis of
the parallel queries set. This is achieved either di-
rectly in the SMT system using the MERT (Mini-
mum Error Rate Training) algorithm and optimiz-
1Insufficient for a full SMT system training (˜500 entries)
109
ing according to the BLEU2(Papineni et al., 2001)
score, or via reranking the Nbest translation can-
didates generated by a baseline system based on
new parameters (and possibly new features) that
aim to optimize a retrieval metric.
It is important to note that both of the pro-
posed approaches allow keeping the MT system
independent of the document collection and in-
dexing, and thus suitable for a query translation
service. These two approaches can also be com-
bined by using the model produced with the first
approach as a baseline that produces the Nbest list
of translations that is then given to the reranking
approach.
The remainder of this paper is organized as fol-
lows. We first present related work addressing the
problem of query translation. We then describe
two approaches towards adapting an SMT system
to the query-genre: tuning the SMT system on a
parallel set of queries (Section 3.1) and adapting
machine translation via the reranking framework
(Section 3.2). We then present our experimental
settings and results (Section 4) and conclude in
section 5.
2 Related work
We may distinguish two main groups of ap-
proaches to CLIR: document translation and
query translation. We concentrate on the second
group which is more relevant to our settings. The
standard query translation methods use different
translation resources such as bilingual dictionar-
ies, parallel corpora and/or machine translation.
The aspect of disambiguation is important for the
first two techniques.
Different methods were proposed to deal with
disambiguation issues, often relying on the docu-
ment collection or embedding the translation step
directly into the retrieval model (Hiemstra and
Jong, 1999; Berger et al., 1999; Kraaij et al.,
2003). Other methods rely on external resources
like query logs (Gao et al., 2010), Wikipedia (Ja-
didinejad and Mahmoudi, 2009) or the web (Nie
and Chen, 2002; Hu et al., 2008). (Gao et al.,
2006) proposes syntax-based translation models
to deal with the disambiguation issues (NP-based,
dependency-based). The candidate translations
proposed by these models are then reranked with
the model learned to minimize the translation er-
2Standard MT evaluation metric
ror on the training data.
To our knowledge, existing work that use MT-
based techniques for query translation use an out-
of-the-box MT system, without adapting it for
query translation in particular (Jones et al., 1999;
Wu et al., 2008) (although some query expan-
sion techniques might be applied to the produced
translation afterwards (Wu and He, 2010)).
There is a number of works done for do-
main adaptation in Statistical Machine Transla-
tion. However, we want to distinguish between
genre and domain adaptation in this work. Gen-
erally, genre can be seen as a sub-problem of do-
main. Thus, we consider genre to be the general
style of the text e.g. conversation, news, blog,
query (responsible mostly for the text structure)
while the domain reflects more what the text is
about – eg. social science, healthcare, history, so
domain adaptation involves lexical disambigua-
tion and extra lexical coverage problems. To our
knowledge, there is not much work addressing ex-
plicitly the problem of genre adaptation for SMT.
Some work done on domain adaptation could be
applied to genre adaptation, such as incorporating
available in-domain corpora in the SMT model:
either monolingual (Bertoldi and Federico, 2009;
Wu et al., 2008; Zhao et al., 2004; Koehn and
Schroeder, 2007), or small parallel data used for
tuning the SMT parameters (Zheng et al., 2010;
Pecina et al., 2011).
3 Our approach
This work is based on the hypothesis that the
general-purpose SMT system needs to be adapted
for query translation. Although in (Ferro and
Peters, 2009) it has been mentioned that using
Google translate (general-purpose MT) for query
translation allowed to CLEF participants to obtain
the best CLIR performance, there is still 10% gap
between monolingual and cross-lingual IR. We
believe that, as in (Clinchant and Renders, 2007),
more adapted query translation, possibly further
combined with query expansion techniques, can
lead to improved retrieval.
The problem of the SMT adaptation for query-
genre translation has different quality aspects.
On the one hand, we want our model to pro-
duce a “good” translation (well-formed and trans-
mitting the information contained in the source
query) of an input query. On the other hand, we
want to obtain good retrieval performance using
110
the proposed translation. These two aspects are
not necessarily correlated: a bag-of-word transla-
tion can lead to good retrieval performance, even
though it won’t be syntactically well-formed; at
the same time a well-formed translation can lead
to worse retrieval if the wrong lexical choice is
done. Moreover, often the retrieval demands some
linguistic preprocessing (eg. lemmatisation, PoS
tagging) which in interaction with badly-formed
translations might bring some noise.
A couple of works studied the correlation be-
tween the standard MT evaluation metrics and
the retrieval precision. Thus, (Fujii et al., 2009)
showed a good correlation of the BLEU scores
with the MAP scores for Cross-Lingual Patent
Retrieval. However, the topics in patent search
(long and well structured) are very different from
standard queries. (Kettunen, 2009) also found a
pretty high correlation ( 0.8 ? 0.9) between stan-
dard MT evaluation metrics (METEOR(Banerjee
and Lavie, 2005), BLEU, NIST(Doddington,
2002)) and retrieval precision for long queries.
However, the same work shows that the correla-
tion decreases ( 0.6? 0.7) for short queries.
In this paper we propose two approaches to
SMT adaptation for queries. The first one op-
timizes BLEU, while the second one optimizes
Mean Average Precision (MAP), a standard met-
ric in information retrieval. We’ll address the is-
sue of the correlation between BLEU and MAP in
Section 4.
Both of the proposed approaches rely on the
phrase-based SMT (PBMT) model (Koehn et al.,
2003) implemented in the Open Source SMT
toolkit MOSES (Koehn et al., 2007).
3.1 Tuning for genre adaptation
First, we propose to adapt the PBMT model by
tuning the model’s weights on a parallel set of
queries. This approach addresses the first as-
pect of the problem, which is producing a “good”
translation. The PBMT model combines differ-
ent types of features via a log-linear model. The
standard features include (Koehn, 2010, Chapter
5): language model, word penalty, distortion, dif-
ferent translation models, etc. The weights of
these features are learned during the tuning step
with the MERT (Och, 2003) algorithm. Roughly
the MERT algorithm tunes feature weights one by
one and optimizes them according to the BLEU
score obtained.
Our hypothesis is that the impact of different
features should be different depending on whether
we translate a full sentence, or a query-genre en-
try. Thus, one would expect that in the case
of query-genre the language model or the distor-
tion features should get less importance than in
the case of the full-sentence translation. MERT
tuning on a genre-adapted parallel corpus should
leverage this information from the data, adapting
the SMT model to the query-genre. We would
also like to note that the tuning approach (pro-
posed for domain adaptation by (Zheng et al.,
2010)) seems to be more appropriate for genre
adaptation than for domain adaptation where the
problem of lexical ambiguity is encoded in the
translation model and re-weighting the main fea-
tures might not be sufficient.
We use the MERT implementation provided
with the Moses toolkit with default settings. Our
assumption is that this procedure although not ex-
plicitly aimed at improving retrieval performance
will nevertheless lead to “better” query transla-
tions when compared to the baseline. The results
of this apporach allow us also to observe whether
and to what extent changes in BLEU scores are
correlated to changes in MAP scores.
3.2 Reranking framework for query
translation
The second approach addresses the retrieval qual-
ity problem. An SMT system is usually trained to
optimize the quality of the translation (eg. BLEU
score for SMT), which is not necessarily corre-
lated with the retrieval quality (especially for the
short queries). Thus, for example, the word or-
der which is crucial for translation quality (and is
taken into account by most MT evaluation met-
rics) is often ignored by IR models. Our second
approach follows (Nie, 2010, pp.106) argument
that “the translation problem is an integral part
of the whole CLIR problem, and unified CLIR
models integrating translation should be defined”.
We propose integrating the IR metric (MAP) into
the translation model optimisation step via the
reranking framework.
Previous attempts to apply the reranking ap-
proach to SMT did not show significant improve-
ments in terms of MT evaluation metrics (Och
et al., 2003; Nikoulina and Dymetman, 2008).
One of the reasons being the poor diversity of the
Nbest list of the translations. However, we be-
111
lieve that this approach has more potential in the
context of query translation.
First of all the average query length is ˜5 words,
which means that the Nbest list of the translations
is more diverse than in the case of general phrase
translation (average length 25-30 words).
Moreover, the retrieval precision is more natu-
rally integrated into the reranking framework than
standard MT evaluation metrics such as BLEU.
The main reason is that the notion of Average Re-
trieval Precision is well defined for a single query
translation, while BLEU is defined on the corpus
level and correlates poorly with human quality
judgements for the individual translations (Specia
et al., 2009; Callison-Burch et al., 2009).
Finally, the reranking framework allows a lot
of flexibility. Thus, it allows enriching the base-
line translation model with new complex features
which might be difficult to introduce into the
translation model directly.
Other works applied the reranking framework
to different NLP tasks such as Named Entities
Extraction (Collins, 2001), parsing (Collins and
Roark, 2004), and language modelling (Roark et
al., 2004). Most of these works used the reranking
framework to combine generative and discrimina-
tive methods when both approaches aim at solv-
ing the same problem: the generative model pro-
duces a set of hypotheses, and the best hypoth-
esis is chosen afterwards via the discriminative
reranking model, which allows to enrich the base-
line model with the new complex and heteroge-
neous features. We suggest using the reranking
framework to combine two different tasks: Ma-
chine Translation and Cross-lingual Information
Retrieval. In this context the reranking framework
doesn’t only allow enriching the baseline transla-
tion model but also performing training using a
more appropriate evaluation metric.
3.2.1 Reranking training
Generally, the reranking framework can be re-
sumed in the following steps :
1. The baseline (generic-purpose) MT system
generates a list of candidate translations
GEN(q) for each query q;
2. A vector of features F (t) is assigned to each
translation t ? GEN(q);
3. The best translation tˆ is chosen as the one
maximizing the translation score, which is
defined as a weighted linear combination of
features: tˆ(?) = argmaxt?GEN(q) ?·F (t)
As shown above the best translation is selected ac-
cording to features’ weights ?. In order to learn
the weights ? maximizing the retrieval perfor-
mance, an appropriate annotated training set has
to be created. We use the CLEF tracks to create
the training set. The retrieval scores annotations
are based on the document relevance annotations
performed by human annotators during the CLEF
campaign.
The annotated training set is created out of
queries {q1, ..., qK} with an Nbest list of trans-
lations GEN(qi) of each query qi, i ? {1..K} as
follows:
• A list of N (we take N = 1000) translations
(GEN(qi)) is produced by the baseline MT
model for each query qi, i = 1..K.
• Each translation t ? GEN(qi) is used
to perform a retrieval from a target docu-
ment collection, and an Average Precision
score (AP (t)) is computed for each t ?
GEN(qi) by comparing its retrieval to the
relevance annotations done during the CLEF
campaign.
The weights ? are learned with the objective of
maximizing MAP for all the queries of the train-
ing set, and, therefore, are optimized for retrieval
quality.
The weights optimization is done with
the Margin Infused Relaxed Algorithm
(MIRA)(Crammer and Singer, 2003), which
was applied to SMT by (Watanabe et al., 2007;
Chiang et al., 2008). MIRA is an online learning
algorithm where each weights update is done to
keep the new weights as close as possible to the
old weights (first term), and score oracle trans-
lation (the translation giving the best retrieval
score : t?i = argmaxtAP (t)) higher than each
non-oracle translation (tij) by a margin at least as
wide as the loss lij (second term):
? = min??
1
2??
?
? ??2 +
C
?K
i=1 maxj=1..N
(
lij ? ?
?
· (F (t?i )? F (tij)
)
The loss lij is defined as the difference in the re-
trieval average precision between the oracle and
non-oracle translations: lij = AP (t?i )?AP (tij).
C is the regularization parameter which is chosen
via 5-fold cross-validation.
112
3.2.2 Features
One of the advantages of the reranking frame-
work is that new complex features can be easily
integrated. We suggest to enrich the reranking
model with different syntax-based features, such
as:
• features relying on dependency structures:
called therein coupling features (proposed by
(Nikoulina and Dymetman, 2008));
• features relying on Part of Speech Tagging:
called therein PoS mapping features.
By integrating the syntax-based features we
have a double goal: showing the potential of
the reranking framework with more complex fea-
tures, and examining whether the integration of
syntactic information could be useful for query
translation.
Coupling features. The goal of the coupling
features is to measure the similarity between
source and target dependency structures. The ini-
tial hypothesis is that a better translation should
have a dependency structure closer to the one of
the source query.
In this work we experiment with two dif-
ferent coupling variants proposed in (Nikoulina
and Dymetman, 2008), namely, Lexicalised and
Label-specific coupling features.
The generic coupling features are based on
the notion of “rectangles” that are of the follow-
ing type : ((s1, ds12, s2), (t1, dt12, t2)), where
ds12 is an edge between source words s1 and s2,
dt12 is an edge between target words t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2.
Lexicalised features take into account the qual-
ity of lexical alignment, by weighting each rect-
angle (s1, s2, t1, t2) by a probability of align-
ing s1 to t1 and s2 to t2 (eg. p(s1|t1)p(s2|t2) or
p(t1|s1)p(t2|s2)).
The Label-Specific features take into account
the nature of the aligned dependencies. Thus, the
rectangles of the form ((s1, subj, s2), (t1, subj,
t2)) will get more weight than a rectangle ((s1,
subj, s2), (t1, nmod, t2)). The importance of
each “rectangle” is learned on the parallel anno-
tated corpus by introducing a collection of Label-
Specific coupling features, each for a specific pair
of source label and target label.
PoS mapping features. The goal of the PoS
mapping features is to control the correspondence
of Part Of Speech Tags between an input query
and its translation. As the coupling features, the
PoS mapping features rely on the word align-
ments between the source sentence and its trans-
lation3. A vector of sparse features is introduced
where each component corresponds to a pair of
PoS tags aligned in the training data. We intro-
duce a generic PoS map variant, which counts a
number of occurrences of a specific pair of PoS
tags, and lexical PoS map variant, which weights
down these pairs by a lexical alignment score
(p(s|t) or p(t|s)).
4 Experiments
4.1 Experimental basis
4.1.1 Data
To simulate parallel query data we used trans-
lation equivalent CLEF topics. The data set used
for the first approach consists of the CLEF topic
data from the following years and tasks: AdHoc-
main track from 2000 to 2008; CLEF AdHoc-
TEL track 2008; Domain Specific tracks from
2000 to 2008; CLEF robust tracks 2007 and 2008;
GeoCLEf tracks 2005-2007. To avoid the issue of
overlapping topics we removed duplicates. The
created parallel queries set contained 500 ? 700
parallel entries (depending on the language pair,
Table 1) and was used for Moses parameters tun-
ing.
In order to create the training set for the rerank-
ing approach, we need to have access to the rele-
vance judgements. We didn’t have access to all
relevance judgements of the previously desribed
tracks. Thus we used only a subset of the previ-
ously extracted parallel set, which includes CLEF
2000-2008 topics from the AdHoc-main, AdHoc-
TEL and GeoCLEF tracks.
The number of queries obtained altogether is
shown in (Table 1).
4.1.2 Baseline
We tested our approaches on the CLEF AdHoc-
TEL 2009 task (50 topics). This task dealt
with monolingual and cross-lingual search in a
library catalog. The monolingual retrieval is
3This alignment can be either produced by a toolkit like
GIZA++(Och and Ney, 2003) or obtained directly by a sys-
tem that produced the Nbest list of the translations (Moses).
113
Language pair Number of queries
Total queries
En - Fr, Fr - En 470
En - De, De - En 714
Annotated queries
En - Fr, Fr - En 400
En - De, De - En 350
Table 1: Top: total number of parallel queries gathered
from all the CLEF tasks (size of the tuning set). Bot-
tom: number of queries extracted from the tasks for
which the human relevance judgements were availble
(size of the reranking training set).
performed with the lemur4 toolkit (Ogilvie and
Callan, 2001). The preprocessing includes lem-
matisation (with the Xerox Incremental Parser-
XIP (A?¨t-Mokhtar et al., 2002)) and filtering out
the function words (based on XIP PoS tagging).
Table 2 shows the performance of the monolin-
gual retrieval model for each collection. The
monolingual retrieval results are comparable to
the CLEF AdHoc-TEL 2009 participants (Ferro
and Peters, 2009). Let us note here that it is not
the case for our CLIR results since we didn’t ex-
ploit the fact that each of the collections could ac-
tually contain the entries in a language other than
the official language of the collection.
The cross-lingual retrieval is performed as fol-
lows :
• the input query (eg. in English) is first trans-
lated into the language of the collection (eg.
German);
• this translation is used to search the target
collection (eg. Austrian National Library for
German ) .
The baseline translation is produced with
Moses trained on Europarl. Table 2 reports the
baseline performance both in terms of MT evalu-
ation metrics (BLEU) and Information Retrieval
evaluation metric MAP (Mean Average Preci-
sion).
The 1best MAP score corresponds to the case
when the single translation is proposed for the
retrieval by the query translation model. 5best
MAP score corresponds to the case when the 5
top translations proposed by the translation ser-
vice are concatenated and used for the retrieval.
4http://www.lemurproject.org/
The 5best retrieval can be seen as a sort of query
expansion, without accessing the document col-
lection or any external resources.
Given that the query length is shorter than for a
standard sentence, the 4-gramm BLEU (used for
standart MT evaluation) might not be able to cap-
ture the difference between the translations (eg.
English-German 4-gramm BLEU is equal to 0 for
our task). For that reason we report both 3- and
4-gramm BLEU scores.
Note, that the French-English baseline retrieval
quality is much better than the German-English.
This is probably due to the fact that our German-
English translation system doesn’t use any de-
coumpounding, which results into many non-
translated words.
4.2 Results
We performed the query-genre adaptation ex-
periments for English-French, French-English,
German-English and English-German language
pairs.
Ideally, we would have liked to combine the
two approaches we proposed: use the query-
genre-tuned model to produce the Nbest list
which is then reranked to optimize the MAP
score. However, it was not possible in our exper-
imental settings due to the small amount of train-
ing data available. We thus simply compare these
two approaches to a baseline approach and com-
ment on their respective performance.
4.2.1 Query-genre tuning approach
For the CLEF-tuning experiments we used the
same translation model and language model as for
the baseline (Europarl-based). The weights were
then tuned on the CLEF topics described in sec-
tion 4.1.1. We then tested the system obtained on
50 parallel queries from the CLEF AdHoc-TEL
2009 task.
Table 3 describes the results of the evalua-
tion. We observe consistent 1-best MAP improve-
ments, but unstable BLEU (3-gramm) (improve-
ments for English-German, and degradation for
other language pairs), although one would have
expected BLEU to be improved in this experi-
mental setting given that BLEU was the objective
function for MERT. These results, on one side,
confirm the remark of (Kettunen, 2009) that there
is a correlation (although low) between BLEU
and MAP scores. The unstable BLEU scores
114
MAP
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Monolingual IR Bilingual IR
English 0.3159
French-English 0.1828 0.2186 0.1199 0.1568
German-English 0.0941 0.0942 0.2351 0.2923
French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423
German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218
Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Fr-En 0.1954 0.2229 0.1062 0.1489
De-En 0.1018 0.1078 0.2240 0.2486
En-Fr 0.1611 0.1516 0.2072 0.2908
En-De 0.1062 0.1132 0.0000 0.1924
Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.
might also be explained by the small size of the
test set (compared to a standard test set of 1000
full-sentences).
Secondly, we looked at the weights of the fea-
tures both in the baseline model (Europarl-tuned)
and in the adapted model (CLEF-tuned), shown in
Table 4. We are unsure how suitable the sizes of
the CLEF tuning sets are, especially for the pairs
involving English and French. Nevertheless we
do observe and comment on some patterns.
For the pairs involving English and German
the distortion weight is much higher when tuning
with CLEF data compared to tuning with Europarl
data. The picture is reversed when looking at the
two pairs involving English and French. This is
to be expected if we interpret a high distortion
weight as follows: “it is not encouraged to place
source words that are near to each other far away
from each other in the translation”. Indeed, the lo-
cal reorderings are much more frequent between
English and French (e.g. white house = maison
blanche), while the long-distance reorderings are
more typcal between English and German.
The word penalty is consistenly higher over all
pairs when tuning with CLEF data compared to
tuning with Europarl data. We could see an ex-
planation for this pattern in the smaller size of
the CLEF sentences if we interpret higher word
penalty as a preference for shorter translations.
This can be explained both with the smaller aver-
age size of the queries and with the specific query
structure: mostly content words and fewer func-
tion words when compared to the full sentence.
The language model weight is consistently
though not drastically smaller when tuning with
CLEF data. We suppose that this is due to the
fact that a Europarl-base language model is not
the best choice for translating query data.
4.2.2 Reranking approach
The reranking experiments include different
features combinations. First, we experiment with
the Moses features only in order to make this ap-
proach comparable with the first one. Secondly,
we compare different syntax-based features com-
binations, as described in section 3.2.2. Thus, we
compare the following reranking models (defined
by the feature set): moses, lex (lexical coupling
+ moses features), lab (label-specific coupling +
moses features), posmaplex (lexical PoS mapping
+ moses features ), lab-lex (label-specific cou-
pling + lexical coupling + moses features), lab-
lex-posmap (label-specific coupling + lexical cou-
pling features + generic PoS mapping). To reduce
the size of feature-functions vectors we take only
the 20 most frequent features in the training data
for Label-specific coupling and PoS mapping fea-
tures. The computation of the syntax features is
based on the rule-based XIP parser, where some
heuristics specific to query processing have been
integrated into English and French (but not Ger-
man) grammars (Brun et al., 2012).
The results of these experiments are illustrated
115
Lng pair Tune set DW LM ?(f |e) lex(f |e) ?(e|f) lex(e|f) PP WP
Fr-En
Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975
CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707
De-En
Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822
CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434
En-Fr
Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002
CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972
En-De
Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233
CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214
Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW - distortion weight, LM - language
model weight, PP - phrase penalty, WP - word penalty, ?-phrase translation probability, lex-lexical weighting.
Query Example MAP bleu1
Src1 Weibliche Ma¨rtyrer
Ref Female Martyrs
T1 female martyrs 0.07 1
T2 Women martyr 0.4 0
Src 2 Genmanipulation am
Menschen
Ref Human Gene Manipula-
tion
T1 On the genetic manipula-
tion of people
0.044 0.167
T2 genetic manipulation of
the human being
0.069 0.286
Src 3 Arbeitsrecht in der Eu-
ropa¨ischen Union
Ref European Union Labour
Laws
T1 Labour law in the Euro-
pean Union
0.015 0.5
T2 labour legislation in the
European Union
0.036 0.5
Table 5: Some examples of queries translations (T1:
baseline, T2: after reranking with lab-lex), MAP and
1-gramm BLEU scores for German-English.
in Figure 1. To keep the figure more readable,
we report only on 3-gramm BLEU scores. When
computing the 5best MAP score, the order in the
Nbest list is defined by a corresponding reranking
model. Each reranking model is illustrated by a
single horizontal red bar. We compare the rerank-
ing results to the baseline model (vertical line) and
also to the results of the first approach (yellow bar
labelled MERT:moses) on the same figure.
First, we remark that the adapted models
(query-genre tuning and reranking) outperform
the baseline in terms of MAP (1best and 5 best)
for French-English and German-English transla-
tions for most of the models. The only exception
is posmaplex model (based on PoS tagging) for
German which can be explained by the fact that
the German grammar used for query processing
was not adapted for queries as opposed to English
and French grammars. However, we do not ob-
serve the same tendency for BLEU score, where
only a few of the adapted models outperform the
baseline, which confirms the hypothesis of the
low correlation between BLEU and MAP scores
in these settings. Table 5 gives some examples of
the queries translations before (T1) and after (T2)
reranking. These examples also illustrate differ-
ent types of disagreement between MAP and 1-
gramm BLEU5 score.
The results for English-German and English-
French look more confusing. This can be partly
due to the more rich morphology of the target lan-
guages which may create more noise in the syn-
tax structure. Reranking however improves over
the 1-best MAP baseline for English-German, and
5-best MAP is also improved excluding the mod-
els involving PoS tagging for German (posmap,
posmaplex, lab-lex-posmap). The results for
English-French are more difficult to interpret. To
find out the reason of such a behavior, we looked
at the translations. We observed the following to-
kenization problem for French: the apostrophe is
systematically separated, e.g. “d ’ aujourd ’ hui”.
This leads to both noisy pre-retrieval preprocess-
ing (eg. d is tagged as a NOUN) and noisy syntax-
based feature values, which might explain the un-
stable results.
Finally, we can see that the syntax-based fea-
tures can be beneficial for the final retrieval qual-
ity: the models with syntax features can outper-
form the model basd on the moses features only.
The syntax-based features leading to the most sta-
5The higher order BLEU scores are equal to 0 for most
of the individual translations.
116
Figure 1: Reranking results. The vertical line corresponds to the baseline scores. The lowest bar (MERT:moses,
in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.
ble results seem to be lab-lex (combination of lex-
ical and label-specific coupling): it leads to the
best gains over 1-best and 5-best MAP for all lan-
guage pairs excluding English-French. This is a
surprising result given the fact that the underlying
IR model doesn’t take syntax into account in any
way. In our opinion, this is probably due to the
interaction between the pre-retrieval preprocess-
ing (lemmatisation, PoS tagging) done with the
linguistic tools which might produce noisy results
when applied to the SMT outputs. The rerank-
ing with syntax-based features allows to choose
a better-formed query for which the PoS tagging
and lemmatisation tools produce less noise which
leads to a better retrieval.
5 Conclusion
In this work we proposed two methods for query-
genre adaptation of an SMT model: the first
method addressing the translation quality aspect
and the second one the retrieval precision aspect.
We have shown that CLIR performance in terms
of MAP is improved between 1-2.5 points. We
believe that the combination of these two meth-
ods would be the most beneficial setting, although
we were not able to prove this experimentally
(due to the lack of training data). None of these
methods require access to the document collec-
tion at test time, and can be used in the context
of a query translation service. The combination
of our adapted SMT model with other state-of-the
art CLIR techniques (eg. query expansion with
PRF) will be explored in future work.
Acknowledgements
This research was supported by the European
Union’s ICT Policy Support Programme as part of
the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (Project GALATEAS).
References
Salah A?¨t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
117
cremental deep parsing. Natural Language Engi-
neering, 8:121–144, June.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Adam Berger, John Lafferty, and John La Erty. 1999.
The weaver system for document retrieval. In In
Proceedings of the Eighth Text REtrieval Confer-
ence (TREC-8, pages 163–174.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 182–189. Association for Computa-
tional Linguistics.
Caroline Brun, Vassilina Nikoulina, and Nikolaos La-
gos. 2012. Linguistically-adapted structural query
annotation for digital libraries in the social sciences.
In Proceedings of the 6th EACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, Avignon, France, April.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 224–233. Association
for Computational Linguistics.
Ste´phane Clinchant and Jean-Michel Renders. 2007.
Query translation through dictionary adaptation. In
CLEF’07, pages 182–187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Michael Collins. 2001. Ranking algorithms for
named-entity extraction: boosting and the voted
perceptron. In ACL’02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 489–496, Philadelphia, Pennsyl-
vania. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
George Doddington. 2002. Automatic evaluation
of Machine Translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, pages 138–145, San Diego,
California. Morgan Kaufmann Publishers Inc.
Nicola Ferro and Carol Peters. 2009. CLEF 2009
ad hoc track overview: TEL and persian tasks.
In Working Notes for the CLEF 2009 Workshop,
Corfu, Greece.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating effects of ma-
chine translation accuracy on cross-lingual patent
retrieval. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ’09, pages
674–675.
Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006.
Statistical query translation models for cross-
language information retrieval. 5:323–359, Decem-
ber.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-
Fai Wong, and Hsiao-Wuen Hon. 2010. Exploit-
ing query logs for cross-lingual query suggestions.
ACM Trans. Inf. Syst., 28(2).
Djoerd Hiemstra and Franciska de Jong. 1999. Dis-
ambiguation strategies for cross-language informa-
tion retrieval. In Proceedings of the Third European
Conference on Research and Advanced Technology
for Digital Libraries, pages 274–293.
Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,
Zheng Chen, and Qiang Yang. 2008. Web query
translation via web log mining. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’08, pages 749–750. ACM.
Amir Hossein Jadidinejad and Fariborz Mahmoudi.
2009. Cross-language information retrieval us-
ing meta-language index construction and structural
queries. In Proceedings of the 10th cross-language
evaluation forum conference on Multilingual in-
formation access evaluation: text retrieval experi-
ments, CLEF’09, pages 70–77, Berlin, Heidelberg.
Springer-Verlag.
Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-
mano, and Kazuo Sumita. 1999. Exploring the
use of machine translation resources for english-
japanese cross-language information retrieval. In In
Proceedings of MT Summit VII Workshop on Ma-
chine Translation for Cross Language Information
Retrieval, pages 181–188.
Kimmo Kettunen. 2009. Choosing the best mt pro-
grams for clir purposes — can mt metrics be help-
ful? In Proceedings of the 31th European Confer-
ence on IR Research on Advances in Information
Retrieval, ECIR ’09, pages 706–712, Berlin, Hei-
delberg. Springer-Verlag.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
118
’07, pages 224–227. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In ACL ’07: Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177–180. As-
sociation for Computational Linguistics.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Wessel Kraaij, Jian-Yun Nie, and Michel Simard.
2003. Embedding web-based statistical trans-
lation models in cross-language information re-
trieval. Computational Linguistiques, 29:381–419,
September.
Jian-yun Nie and Jiang Chen. 2002. Exploiting the
web as parallel corpora for cross-language informa-
tion retrieval. Web Intelligence, pages 218–239.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan & Claypool Publishers.
Vassilina Nikoulina and Marc Dymetman. 2008. Ex-
periments in discriminating phrase-based transla-
tions on the basis of syntactic coupling features. In
Proceedings of the ACL-08: HLT Second Workshop
on Syntax and Structure in Statistical Translation
(SSST-2), pages 55–60. Association for Computa-
tional Linguistics, June.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003.
Syntax for Statistical Machine Translation: Final
report of John Hopkins 2003 Summer Workshop.
Technical report, John Hopkins University.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Ogilvie and James P. Callan. 2001. Experiments
using the lemur toolkit. In TREC.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation.
Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-
pavassiliou, Prokopis Prokopidis, and Maria Gi-
agkou. 2011. Towards using web-crawled data for
domain adaptation in statistical machine translation.
In Proceedings of the 15th Annual Conference of
the European Associtation for Machine Translation,
pages 297–304, Leuven, Belgium. European Asso-
ciation for Machine Translation.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’04), July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Confer-
ence of the EAMT, page 28–35, Barcelona, Spain.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764–773, Prague, Czech Republic.
Association for Computational Linguistics.
Dan Wu and Daqing He. 2010. A study of query
translation using google machine translation sys-
tem. Computational Intelligence and Software En-
gineering (CiSE).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Col-
ing2008), pages 993–100.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ’04. Associ-
ation for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and
Hao Yu. 2010. Domain adaptation for statisti-
cal machine translation in development corpus se-
lection. In Universal Communication Symposium
(IUCS), 2010 4th International, pages 2–7. IEEE.
119
