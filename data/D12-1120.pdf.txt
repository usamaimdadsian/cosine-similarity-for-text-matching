Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1313–1323, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Biased Representation Learning for Domain Adaptation
Fei Huang , Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fhuang,yates}@temple.edu
Abstract
Representation learning is a promising tech-
nique for discovering features that allow su-
pervised classifiers to generalize from a source
domain dataset to arbitrary new domains. We
present a novel, formal statement of the rep-
resentation learning task. We argue that be-
cause the task is computationally intractable
in general, it is important for a representa-
tion learner to be able to incorporate expert
knowledge during its search for helpful fea-
tures. Leveraging the Posterior Regularization
framework, we develop an architecture for in-
corporating biases into representation learn-
ing. We investigate three types of biases, and
experiments on two domain adaptation tasks
show that our biased learners identify signif-
icantly better sets of features than unbiased
learners, resulting in a relative reduction in er-
ror of more than 16% for both tasks, with re-
spect to existing state-of-the-art representation
learning techniques.
1 Introduction
Supervised natural language processing (NLP) sys-
tems have been widely used and have achieved im-
pressive performance on many NLP tasks. Howev-
er, they exhibit a significant drop-off in performance
when tested on domains that differ from their train-
ing domains. (Gildea, 2001; Sekine, 1997; Pradhan
et al., 2007) One major cause for poor performance
on out of-domain texts is the traditional representa-
tion used by supervised NLP systems (Ben-David et
al., 2007). Most systems depend on lexical features,
which can differ greatly between domains, so that
important words in the test data may never be seen
in the training data. The connection between word-
s and labels may also change across domains. For
instance, “signaling” appears only as a present par-
ticiple (VBG) in WSJ text (as in, “signaling that...”),
but predominantly as a noun (as in “signaling path-
way”) in biomedical text.
Recently, several authors have found that learning
new features based on distributional similarity can
significantly improve domain adaptation (Blitzer et
al., 2006; Huang and Yates, 2009; Turian et al.,
2010; Dhillon et al., 2011). This framework is at-
tractive for several reasons: experimentally, learned
features can yield significant improvements over s-
tandard supervised models on out-of-domain test-
s. Moreover, since the representation-learning tech-
niques are unsupervised, they can easily be applied
to arbitrary new domains. There is no need to supply
additional labeled examples for each new domain.
Traditional representations still hold one signif-
icant advantage over representation-learning, how-
ever: because features are hand-crafted, these rep-
resentations can readily incorporate the linguistic
or domain expert knowledge that leads to state-of-
the-art in-domain performance. In contrast, the on-
ly guide for existing representation-learning tech-
niques is a corpus of unlabeled text.
To address this shortcoming, we introduce
representation-learning techniques that incorporate
a domain expert’s preferences over the learned fea-
tures. For example, out of the set of all possi-
ble distributional-similarity features, we might pre-
fer those that help predict the labels in a labeled
training data set. To capture this preference, we
might bias a representation-learning algorithm to-
wards features with low joint entropy with the labels
in the training data. This particular biased form of
1313
representation learning is a type of semi-supervised
learning that allows our system to learn task-specific
representations from a source domain’s training da-
ta, rather than the single representation for all tasks
produced by current, unsupervised representation-
learning techniques.
We present a novel formal statement of represen-
tation learning, and demonstrate that it is computa-
tionally intractable in general. It is therefore criti-
cal for representation learning to be flexible enough
to incorporate the intuitions and knowledge of hu-
man experts, to guide the search for representations
efficiently and effectively. Leveraging the Posteri-
or Regularization framework (Ganchev et al., 2010),
we present an architecture for learning representa-
tions for sequence-labeling tasks that allows for bi-
ases. In addition to a bias towards task-specific rep-
resentations, we investigate a bias towards repre-
sentations that have similar features across domain-
s, to improve domain-independence; and a bias to-
wards multi-dimensional representations, where d-
ifferent dimensions are independent of one another.
In this paper, we focus on incorporating the bias-
es with HMM-type representations (Hidden Markov
Model). However, this technique can also be ap-
plied to other graphical model-based representations
with little modification. Our experiments show that
on two different domain-adaptation tasks, our biased
representations improve significantly over unbiased
ones. In a part-of-speech tagging experiment, our
best model provides a 25% relative reduction in er-
ror over a state-of-the-art Chinese POS tagger, and
a 19% relative reduction in error over an unbiased
representation from previous work.
The next section describes background and previ-
ous work. Section 3 introduces our framework for
learning biased representations. Section 4 describes
how we estimate parameters for the biased objective
functions efficiently. Section 5 details our experi-
ments and results, and section 6 concludes and out-
lines directions for future work.
2 Background and Previous Work
2.1 Terminology and Notation
A representation is a set of features that describe da-
ta points. Formally, given an instance set X , it is a
functionR : X ? Y for some suitable space Y (of-
ten Rd), which is then used as the input space for a
classifier. For instance, a traditional representation
for POS tagging over vocabulary V would include
(in part) |V | dimensions, and would map a word to a
binary vector with a 1 in only one of the dimensions.
By a structured representation, we mean a function
R that incorporates some form of joint inference. In
this paper, we use Viterbi decoding of variants of
Hidden Markov Models (HMMs) for our structured
representations, although our techniques are appli-
cable to arbitrary (Dynamic) Bayes Nets. A domain
is a probability distribution D over the instance set
X ; R(D) denotes the induced distribution over Y .
In domain adaptation tasks, a learner is given sam-
ples from a source domain DS , and is evaluated on
samples from a target domain DT .
2.2 Theoretical Background
Ben-David et al. (2010) give a theoretical analysis
of domain adaptation which shows that the choice
of representation is crucial. A good choice is one
that minimizes error on the training data, but equally
important is that the representation must make data
from the two domains look similar. Ben-David et al.
show that for every hypothesis h, we can provably
bound the error of h on the target domain by its error
on the source domain plus a measure of the distance
between DS and DT :
Ex?DTL(x,R, f, h) ? Ex?DSL(x,R, f, h)
+ d1(R(DS), R(DT ))
where L is a loss function, f is the target function,
and the variation divergence d1 is given by
d1(D,D
?) = 2 sup
B?B
|PrD[B]? PrD? [B]| (1)
where B is the set of measurable sets under D,D?.
2.3 Problem Formulation
Ben-David et al.’s theory provides learning bound-
s for domain adaptation under a fixed R. We now
reformulate this theory to define the task of repre-
sentation learning for domain adaptation as the fol-
lowing optimization problem: given a set of unla-
beled instances US drawn from the source domain
and unlabeled instances UT from the target domain,
as well as a set of labeled instances LS drawn from
1314
the source domain, identify a function R? from the
space of possible representationsR:
R? = argmin
R?R
{min
h?H
(Ex?DSL(x,R, f, h))
+ d1(R(DS), R(DT ))}
(2)
Unlike most learning problems, where the repre-
sentation R is fixed, this problem formulation in-
volves a search over the space of representation-
s and hypotheses. The equation also highlights an
important underlying tension: the best representa-
tion for the source domain would naturally include
domain-specific features, and allow a hypothesis to
learn domain-specific patterns. We are aiming, how-
ever, for the best general classifier, that happens to
be trained on training data from one or a few do-
mains. Domain-specific features would contribute
to distance between domains, and to classifier errors
on data taken from unseen domains. By optimizing
for this combined objective function, we allow the
optimization method to trade off between features
that are best for classifying source-domain data and
features that allow generalization to new domains.
Naturally, the objective function in Equation 2 is
completely intractable. Just finding the optimal hy-
pothesis for a fixed representation of the training da-
ta is intractable for many hypothesis classes. And
the d1 metric is intractable to compute from samples
of a distribution, although Ben-David et al. propose
some tractable bounds (2007; 2010). We view Equa-
tion 2 as a high-level goal rather than a computable
objective. We leverage prior knowledge to bias the
representation learner towards attractive regions of
the representations space R, and we develop effi-
cient, greedy optimization techniques for learning
effective representations.
2.4 Previous Work
There is a long tradition of research on representa-
tions for NLP, mostly falling into one of three cat-
egories: 1) vector space models and dimensionality
reduction techniques (Salton and McGill, 1983; Tur-
ney and Pantel, 2010; Sahlgren, 2005; Deerwester et
al., 1990; Honkela, 1997) 2) using structured repre-
sentations to identify clusters based on distributional
similarity, and using those clusters as features (Lin
and Wu, 2009; Candito and Crabbe´, 2009; Huang
and Yates, 2009; Ahuja and Downey, 2010; Turi-
an et al., 2010; Huang et al., 2011); 3) and struc-
tured representations that induce multi-dimensional
real-valued features (Dhillon et al., 2011; Emami et
al., 2003; Morin and Bengio, 2005). Our work fall-
s into the second category, but builds on the pre-
vious work by demonstrating how to improve the
distributional-similarity clusters with prior knowl-
edge. To our knowledge, we are the first to apply
semi-supervised representation learning techniques
for structured NLP tasks.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume´
III, 2007; Jiang and Zhai, 2007; Daume´ III et al.,
2010). Learning bounds are known (Blitzer et al.,
2007; Mansour et al., 2009). A few authors have
considered domain adaptation with no labeled data
from the target domain (Blitzer et al., 2006; Huang
et al., 2011) by using features based on distributional
similarity. We demonstrate empirically that incorpo-
rating biases into this type of representation-learning
process can significantly improve results.
3 Biased Representation Learning
As before, let US and UT be unlabeled data, and LS
be labeled data from the source domain only. Pre-
vious work on representation learning with Hidden
Markov Models (HMMs) (Huang and Yates, 2009)
has estimated parameters ? for the HMM from un-
labeled data alone, and then determined the Viterbi-
optimal latent states for training and test data to pro-
duce new features for a supervised classifier. The
objective function for HMM learning in this case is
marginal log-likelihood, optimized using the Baum-
Welch algorithm:
L(?) =
?
x?US?UT
log
?
y
p(x,Y = y|?) (3)
where x is a sentence, Y is the sequence of latent
random variables for the sentence, and y is an in-
stance of the latent sequence. The joint distribution
in an HMM factors into observation and transition
distributions, typically mixtures of multinomials:
p(x,y|?) = P (y1)P (x1|y1)
?
i?2
P (yi|yi?1)P (xi|yi)
1315
Innocent
bystanders are often the
JJ NNS RB VBP DT
y1
y2 y3 y4 y5
...
victims
y6
NNS
Innocent
bystanders are often the victims
...
E?
entropy
(Y,z)
P(Y)
p
1
p
2
p
3
p
m
p
n
KL(p
m
|| p
n
)
Monday, March 26, 12
Figure 1: Illustration of how the entropy bias is incor-
porated into HMM learning. The dotted oval shows the
space of desired distributions in the hidden space, which
have small or zero entropy with the real labels. The learn-
ing algorithm aims to maximize the log-likelihood of the
unlabeled data, and to minimize the KL divergence be-
tween the real distribution, pm, and the closest desired
distribution, pn.
Intuitively, this form of representation learning i-
dentifies clusters of distributionally-similar words:
those words with the same Viterbi-optimal latent s-
tate. The Viterbi-optimal latent states are then used
as features for the supervised classifier. Our previ-
ous work (2009) has shown that the features from
the learned HMM significantly improve the accura-
cy of POS taggers and chunkers on benchmark do-
main adaptation datasets.
We use the HMM model from our previous work
(2009) as our baseline. Our techniques follow the
same general setup, as it provides an efficient and
empirically-proven starting point for exploring (one
part of) the space of possible representations. Note,
however, that the HMM on its own does not provide
even an approximate solution to the objective func-
tion in our problem formulation (Eqn. 2), since it
makes no attempt to find the representation that min-
imizes loss on labeled data. To address this and other
concerns, we modify the objective function for HM-
M training. Specifically, we encode biases for rep-
resentation learning by defining a set of properties ?
that we believe a good representation function would
minimize. One possible bias is that the HMM states
should be predictive of the labels in labeled training
data. We can encode this as a property that computes
the entropy between the HMM states and the label-
s. For example, in Figure 1, we want to learn the
best HMM distribution for the sentence “Innocen-
t bystanders are often the victims” for POS tagging
task. The hidden sequence y1, y2, y3, y4, y5, y6 can
have any distribution p1, p2, p3, ..., pm, ..., pn from
the latent space Y . Since we are doing POS tagging,
we want the distribution to learn the information en-
coded in the original POS labels “JJ NNS RB VBP
DT NNS”. Therefore, by calculating the entropy be-
tween the hidden sequence and real labels, we can
identify a subset of desired distributions that have
low entropy, shown in the dotted oval. By minimiz-
ing the KL divergence between the learned distribu-
tion and the set of desired distributions, we can find
the best distribution which is the closest to our de-
sire.
The following subsections describe the specific
properties we investigate; here we show how to in-
corporate them into the objective function. Let z
be the sequence of labels in LS , and let ?(x,y, z)
be a property of the completed data that we wish
the learned representation to minimize, based on our
prior beliefs. Let Q be the subspace of the possible
distributions over Y that have a small expected val-
ue for ?: Q = {q(Y)|EY?q[?(x,Y, z)] ? ?}, for
some constant ?. We then add penalty terms to the
objective function (3) for the divergence between the
HMM distribution p and the “good” distributions q,
as well as for ?:
L(?)?min
q,?
[KL(q(Y)||p(Y|x, ?)) + ?|?|] (4)
s.t. EY?q[?(x,Y, z)] ? ? (5)
where KL is the Kullback-Leibler divergence, and
? is a free parameter indicating how important the
bias is compared with the marginal log likelihood.
To incorporate multiple biases, we define a vec-
tor of properties ?, and we constrain each property
?i ? ?i. Everything else remains the same, except
that in the penalty term ?|?|, the absolute value is
replaced with a suitable norm: ? ???. To allow our-
selves to place weights on the relative importance
of the different biases, we use a norm of the form
?x?A =
?
(xtAx), where A is a diagonal matrix
whose diagonal entries Aii are free parameters that
provide weights on the different properties. For our
1316
experiments, we set the free parameters ? and Aii
using a grid search over development data, as de-
scribed in Section 5.1
3.1 A Bias for Task-specific Representations
Current representation learning techniques are unsu-
pervised, so they will generate the exact same repre-
sentation for different tasks. Yet it is exceedingly
rare that two state-of-the-art NLP systems for differ-
ent tasks share the same feature set, even if they do
tend to share some core set of lexical features.
Traditional non-learned (i.e., manually-
engineered) representations essentially always
include task-specific features. In response, we
propose to bias our representation learning such
that the learned representations are optimized for a
specific task. In particular, we propose a property
that measures how difficult it is to predict the labels
in training data, given the learned latent states.
Our entropy property uses conditional entropy of
the labels given the latent state as the measure of
unpredictability:
?entropy(y, z) = ?
?
i
P˜ (yi, zi) log P˜ (zi|yi) (6)
where P˜ is the empirical probability and i indicates
the ith position in the data. We can plug this feature
into Equation 5 to obtain a new version of Equation
4 as an objective function for task-specific represen-
tations. We refer to this model as HMM+E. Un-
like previous formulations for supervised and semi-
supervised dimensionality reduction (Zhang et al.,
2007; Yang et al., 2006), our framework works effi-
ciently for structured representations.
3.2 A Bias for Domain-Independent Features
Following the theory in Section 2.2, we devise a bi-
ased objective to provide an explicit mechanism for
minimizing the distance between the source and tar-
get domain. As before, we construct a property of
the completed data:
?distance(y) = d1(P˜S , P˜T )
where P˜S(Y ) is the empirical distribution over la-
tent state values estimated from source-domain la-
tent states, and similarly for P˜T (Y ). Essentially,
1Note that ?, unlike A and ?, is not a free parameter. It is
explicitly minimized in the modified objective function.
minimizing this property will bias the the represen-
tation towards features that appear approximately as
often in the source domain as the target domain. We
refer to the model trained with a bias of minimiz-
ing ?distance as HMM+D, and the model with both
?distance and ?entropy biases as HMM+D+E.
3.3 A Bias for Multi-Dimensional
Representations
Words are multidimensional objects. In English,
words can be nouns or verbs, singular or plural,
count or mass, just to name a few dimensions along
which they may vary. Factorial HMMs (FHMM-
s) (Ghahramani and Jordan, 1997) can learn multi-
dimensional models, but inference and learning are
complex and computationally expensive even in su-
pervised settings. Our previous work (2010) creat-
ed a multi-dimensional representation called an “I-
HMM” by training several HMM layers indepen-
dently; we showed that by finding several latent cat-
egories for each word, this representation can pro-
vide useful and domain-independent features for su-
pervised learners. In this work, we also learn a sim-
ilar multi-dimensional model (I-HMM+D+E), but
within each layer we add in the two biases described
above. While more efficient than FHMMs, the draw-
back of these I-HMM-based models is that there
is no mechanism to encourage the different HMM
models to learn different things. As a result, the lay-
ers may produce similar or equivalent features de-
scribing the dominant aspect of distributional sim-
ilarity in the data, but miss features that are less
strong, but still important, in the data.
To encourage learning a truly multi-dimensional
representation, we add a bias towards I-HMM mod-
els in which each layer is different from all previ-
ous layers. We define an entropy-based predictabili-
ty property that measures how predictable each pre-
vious layer is, given the current one. Formally, let
yli denote the hidden state at the ith position in lay-
er l of the model. For a given layer l, this proper-
ty measures the conditional entropy of ym given yl,
summed over layers m < l, and subtracts this from
the maximum possible entropy:
?predictl (y) = MAX+
?
i;m<l
P˜ (yli, y
m
i ) log P˜ (y
m
i |y
l
i)
The entropy between layer l and the previous layer-
1317
s m measures how unpredictable the previous lay-
ers are, given layer l. By biasing the model such
that MAX minus the entropy approaches zero, we
encourage layer l towards completely different fea-
tures from previous layers. We call the model with
this bias P-HMM+D+E.
4 Efficient Parameter Estimation
Several machine learning paradigms have been de-
veloped recently for incorporating biases and con-
straints into parameter estimation (Liang et al.,
2009; Chang et al., 2007; Mann and McCallum,
2007). We leverage the Posterior Regularization
(PR) framework for our problem because of its flex-
ibility in handling different kinds of biases; we pro-
vide a brief overview of the technique here, but see
(Ganchev et al., 2010) for full details.
4.1 Overview of PR
PR introduces a modified EM algorithm to handle
constrained objectives, like Equation 4. The modi-
fied E-step estimates a distribution q(Y) that is close
to the current estimate of p(Y|x, ?), but also close
to the ideal set of distributions that (in expectation)
have ? = 0 for each property ?. The M step re-
mains the same, except that it re-estimates parame-
ters with respect to expected latent states computed
with q rather than p.
E step:
qt+1 = argmin
q
min
?
KL(q(Y)||p(Y|x, ?t)) + ? ???
s.t. Eq[?(x,Y, z)] ? ?
M step:
?t+1 = argmax
?
Eqt+1 [log p(x,Y|?
t))]
To make the optimization task in the E-step more
tractable, PR transforms it to a dual problem:
max
??0,??????
? log
?
Y
p(Y|x, ?) exp{??·?(x,Y, z)}
where ?·?? is the dual norm of ?·?. The gradient of
this dual objective is ?Eq[?(x,Y, z)]. A projected
subgradient descent algorithm is used to perform the
optimization.
4.2 Modifying ? for Tractability
In unstructured settings, this optimization problem
is relatively straightforward. However, for struc-
tured representations, we need to ensure that the
dynamic programming algorithms needed for infer-
ence remain tractable for the biased objectives. For
efficient PR over structured models, the properties ?
need to be decomposed as a sum over the cliques in
the structured model. Unfortunately, the properties
we mention above do not decompose so nicely, so
we must resort to approximations.
In order to efficiently compute the expected val-
ue of the entropy property with respect to Y ? q,
we need to be able to compute each componen-
t EYi?q[?
entropy(Yi, zi)] separately. Yet P˜ depends
on the setting of other latent states Yj in the corpus.
To avoid this problem, we pre-compute the expected
empirical distributions over the completed data. For
each specific value y and z:
P˜q(y, z) =
1
|LS |
?
x
|x|?
i=1
1[zi = z]q(Yi = y)
P˜q(y) =
1
|LS |
?
x
|x|?
i=1
q(Yi = y)
These expected empirical distributions P˜q can be
computed efficiently using standard inference algo-
rithms, such as the forward algorithm for HMMs.
Note that P˜q depends on q, but unlike the original
P˜ from Equation 6, they do not depend on the data
completions y. Thus we can compute P˜q once for
each qt, and then substitute it for P˜ for all values
of Y in the computation of EY?q?entropy(Y, z),
making this computation tractable. For the entropy-
based predictability properties, the calculation is
similar, but instead of using the label z, we use the
decoded states yli from previous layers.
For the distance property, Ben-David et al.’s anal-
ysis depends on a particular notion of distance (E-
qn. 1) that is computationally intractable. They also
propose more tractable lower bounds, but these are
again incompatible with the PR framework. Since
no computationally feasible exact algorithm exists
for this distance feature, we resort to a crude but ef-
ficient approximation of this measure: for each pos-
1318
sible value y of the latent states, we define:
?disty (y) =
?
i|xi?US
1[yi = y]q(Yi = y)
|US |
?
?
i|xi?UT
1[yi = y]q(Yi = y)
|UT |
Each of these individual properties is tractable for
structured models. Combining these properties us-
ing the ?·?A norm results in a Euclidean distance
(weighted byA) between the frequencies of features
in each domain, rather than d1 distance.
5 Experiments
We tested the structured representations with biases
on two NLP tasks: Chinese POS tagging and En-
glish NER. In both cases, we use a domain adapta-
tion setting where no labeled data is available for the
target domain — a particularly difficult setting, but
one that provides a strong test for an NLP system’s
ability to generalize . In our work (Huang and Yates,
2009), we used a plain HMM for domain adaptation
tasks in which there is labeled source data and un-
labeled source and target data, but no labeled target
data for training. Therefore, here, we use the HMM
technique as a baseline, and build on it by including
biases.
5.1 Chinese POS tagging
We use the UCLA Corpus of Written Chinese,
which is a part of The Lancaster Corpus of Man-
darin Chinese (LCMC). The UCLA Corpus consists
of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres. We use gold-standard
word segmentation labels during training and test-
ing. The LCMC tagset consists of 50 Chinese POS
tags. Each genre averages 5284 word tokens, for a
total of 68,695 tokens among all genres. We use the
‘news’ genre as our source domain and randomly se-
lect 20% of every other genre as labeled test data. To
train our representation models, we use the ‘news’
text, plus the remaining 80% of the texts from the
other genres. We use 90% of the labeled news text
for training, and 10% for development. We replace
hapax legomena in the unlabeled data with the spe-
cial symbol *UNKNOWN*, and also do the same
for word types in the labeled test sets that never ap-
pear in our unlabeled training texts.
0.888
0.893
0.898
0.903
0.908
0.913
0.918
0.923
0.928
0.1 1 10 100 1000
Ac
cu
rac
y 
? (log scale) 
News Domain (development data) 
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 2: Grid search for parameters on news text
Following our previous HMM setup in (Huang
and Yates, 2009) for consistency, we use an HMM
with 80 latent states. For our multi-layer models,
we use 7 layers of HMMs. We tuned the free pa-
rameters ? and A on development data. We varied
? from 0.1 to 1000. To tune A, we start by setting
the diagonal entry for ?entropy to 1, without loss of
generality. We then tie all the entries in A for ?disty
to a single parameter ?, and tie all of the entries for
?predicty to a parameter ?. We vary ? and ? over the
set {0.01,0.1,1,10,100}. Figure 2 shows our results
for ? and ? on news development data. A setting
of ? = 0.01 and ? = 100 performs best, with all
? = 100 doing reasonably well. Results for each
of these models on the general fiction test text con-
firm the general trends seen on development data —
a comforting sign, since this indicates we can opti-
mize the free parameters on in-domain development
data, rather than requiring labeled data from the tar-
get domain. Our models tended to perform better
with increasing ? on development data, though with
diminishing returns. We pick the largest setting test-
ed, ? = 100, for our final models.
We use a linear-chain Conditional Random Field
(CRF) for our supervised classifier. To incorporate
the learned representations, we use the Viterbi Algo-
rithm to find the optimal latent state sequence from
each HMM-based model and then use the optimal
states as features in the CRF. Table 1 presents the
full list of features in the CRF. To handle Chinese,
we add in two features introduced in previous work
(Wang et al., 2009): radical features and repeated
characters. A radical is a portion of a Chinese char-
acter that consists of a small number of pen or brush
strokes in a regular pattern.
1319
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
0.92
0.1 1 10 100 1000
Ac
cu
rac
y 
?ORJVFDOH 
General Fiction Domain (test data)  
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 3: Validating parameter settings on fiction text
CRF Feature Set
Transition
?z1[zj = z]
?z,z?1[zj = z and zj?1 = z?]
Word
?w,z1[xj = w and zj = z]
Radical
?z,r1[?c?xjradical(c) = r and zj = z]
Repeated Words
?A,B,z1[xj = AABB and zj = z]
?A,z1[(xj = AAor xj = AA/) and zj = z]
?A,B,z1[xj = ABAB and zj = z]
Features from Representation Learning
?y,l,z1[ylj = y and zj = z]
Table 1: Features used in our Chinese POS tagging
CRF systems. c represents a character within a word.
Table 2 shows our results. We compare against
the Baseline CRF without any additional representa-
tions and the unbiased HMM, a state-of-the-art do-
main adaptation technique from previous work, over
all 13 domains (source and target). We also com-
pare against a state-of-the-art Chinese POS tagger
for in-domain text, the CRF-based Stanford tagger
(Tseng et al., 2005), retrained for this corpus. H-
MM+D+E outperforms the Stanford tagger on 10
out of 12 target domains and the unbiased HMM on
all domains, while the P-HMM+D+E outperform-
s the Stanford tagger (2.6% average improvement)
and HMM (1.7%) on all 12 target domains. The I-
HMM+D+E is slightly better than the HMM+D+E
(.3%), but incorporating the multi-dimensional bias
(P-HMM+D+E) adds an additional 0.6% improve-
ment.
Our interpretation for the success of I-
HMM+D+E and P-HMM+D+E is that the increase
in the state space of the models yields improved
performance. Because P-HMM+D+E biases against
redundant states found in I-HMM+D+E, it effective-
ly increases the state space beyond I-HMM+D+E.
Ahuja and Downey (2010) and our own work with
HMMs as representations (2010) have previously
shown that increasing the state space of the HMM
can significantly improve the representation, but
memory constraints eventually prevent further
progress this way. The I-HMM+D+E and P-
HMM+D+E models can provide similar benefits,
but because they split parameters across multiple
HMMs, they can accommodate much greater state
spaces in the same amount of memory.
We also tested the entropy and distance biases
separately. Figure 4 shows the result of the distance-
biased HMM+D on the general-fiction test text, as
we vary ? over the set {0.1,1,10,100,1000} (we ob-
served similar results for other domains). For all val-
ues of ?, the biased representation outperforms the
unbiased HMM. There is also a strong negative cor-
relation between the expected value of ??distance?
and the resulting accuracy, as expected from Ben-
David et al.’s theoretical analysis. The HMM+E
model outperforms the HMM on the (source) news
domain by 0.3%, but actually performs worse for
most target domains. We suspect that the entropy
feature, which is learned only from labeled source-
domain data, makes the representation biased to-
wards features that are important in the source do-
main only. However, after we add in the distance
bias and a parameter to balance the weights from
both biases, the representation is able to capture the
label information as well as the target domain fea-
tures. Thus, the representation won’t solely depend
on source data. HMM+D+E, which combines both
biases, outperforms HMM+D, suggesting that task-
specific features for domain adaptation can be help-
ful, but only if there is some control for the domain-
independence of the features.
5.2 English Named Entity Recognition
To evaluate on a second task, we turn to Named En-
tity Recognition. We use the training data from the
1320
news (source) lore reli humour gen-fic essay mystery romance sci-fi skill science adv-fic report avg
words 9774 5428 3248 3326 4913 5214 5774 5489 3070 5464 5262 5071 6662 5284
CRF w/o HMM 93.8 85.0 80.0 85.4 85.0 83.8 84.7 86.0 82.8 78.2 82.2 77.1 85.3 84.5
HMM+E 97.1 88.2 83.1 87.5 87.4 89.2 89.5 87.1 86.7 82.1 87.2 79.4 91.7 88.3
Stanford 98.8 88.4 83.5 89.0 87.5 88.4 87.4 87.5 88.6 82.7 86.0 82.1 91.7 88.7
HMM 96.9 89.7 85.2 89.6 89.4 89.0 90.1 89.0 87.0 84.9 87.8 80.0 91.4 89.2
HMM+D 97.4 89.9 85.4 89.4 89.6 89.9 90.1 88.6 87.9 85.3 87.9 80.0 92.0 89.5
HMM+D+E 97.7 90.1 86.1 89.8 90.9 89.7 90.3 89.8 88.4 85.6 87.9 81.2 92.0 89.9
I-HMM+D+E 97.8 90.5 87.0 89.1 91.1 90.2 90.0 90.5 89.8 86.0 87.1 82.2 92.1 90.2
P-HMM+D+E 98.2 91.5 87.7 89.0 91.8 91.0 89.9 91.4 90.4 87.0 87.7 83.4 92.4 90.8
Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the
Stanford tagger on all target domains. The ‘avg’ column includes source-domain development data results. Differ-
ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11
out of 12 target domain. We used the two-tailed Chi-square test with Yates’ correction.
0.894
0.895
0.896
0.897
0.898
0.899
0.9
0.901
0.902
0.903
0.904
4.55E -05 4.60E-05 4.65E -05 4.70E-05 4.75E -05 4.80E-05
Ac
cu
rac
y 
?E q (?distance)? 
HMM+D on General Fiction Test  
?=1000  
?=100 
?=1  ?=0.1  
?=10  
Unconstrained HMM  
Figure 4: Greater distance between domains correlates
with worse target-domain tagging accuracy.
CoNLL 2003 shared task for our labeled training set,
consisting of 204k tokens from the newswire do-
main. We tested the system on the MUC7 formal
run test data, consisting of 59k tokens of stories on
the telecommunications and aerospace industries.
To train our representations, we use the CoNL-
L training data and the MUC7 training data without
labels. We again use a CRF, with features introduced
by Zhang and Johnson (2003) for our baseline. We
use the same setting of free parameters from our
POS tagging experiments.
Results are shown in Table 3. Our best biased
representation P-HMM+D+E outperformed the un-
biased HMM representation by 3.6%, and beats the
I-HMM+D+E by 1.6%. The domain-distance and
multi-dimensional biases help most, while the task-
specific bias helps somewhat, but only when the
domain-distance bias is included. The best sys-
System F1
CRF without HMM 66.15
HMM+E 74.25
HMM 75.06
HMM+D 75.75
HMM+D+E 76.03
I-HMM+D+E 77.04
P-HMM+D+E 78.62
Table 3: English Named Entity recognition results
tem tested on this dataset achieved a slightly bet-
ter F1 score (78.84) (Turian et al., 2010), but used
a much larger training corpus (they use RCV1 cor-
pus which contains approximately 63 million token-
s). Other studies (Turian et al., 2010; Huang et
al., 2011) have performed a detailed comparison be-
tween these types of systems, so we concentrate on
comparisons between biased and unbiased represen-
tations here.
5.3 Does the task-specific bias actually help?
In this section, we test whether the task-specific
bias (entropy bias) actually learns something task-
specific. We learn the entropy-biased representa-
tions for two tasks on the same set of sentences,
labeled differently for the two tasks: English POS
tagging and Named Entity Recognition. Then we
switch the representations to see whether they will
help or hurt the performance on the other task. We
randomly picked 500 sentences from WSJ section
1321
Representation/Task POS Accuracy NER F1
HMM 88.5 66.3
HMM+E(POS labels) 89.7 64.5
HMM+E(NER labels) 86.5 68.0
Table 4: Results of POS tagging and Named Entity
recognition tasks with different representations. With the
entropy-biased representation, the system has better per-
formance on the task which the bias is trained for, but
worse performance on the other task.
0-18 as our labeled training data and 500 sentences
from WSJ section 20-23 as testing data. Because
WSJ data does not have gold standard NER tags,
we manually labeled these sentences with NER tags.
For simplicity, we only use three types of NER tags:
person, organization and location. The result is
shown in Table 4. When the entropy bias uses la-
bels from the same task as the classifier, the perfor-
mance is improved: about 1.2% in accuracy on POS
tagging and 1.7% in F1 score on NER. Switching
the representations for the tasks actually hurts the
performance compared with the unbiased represen-
tation. The results suggest that the entropy bias does
indeed yield a task-specific representation.
6 Conclusion and Future Work
We introduce three types of biases into represen-
tation learning for sequence labeling using the PR
framework. Our experiments on POS tagging and
NER indicate domain-independent biases and multi-
dimensional biases significantly improve the repre-
sentations, while the task-specific bias improves per-
formance on out-of-domain data if it is combined
with the domain-independent bias. Our results indi-
cate the power of representation learning in building
domain-agnostic classifiers, but also the complexi-
ty of the task and the limitations of current tech-
niques, as even the best models still fall significantly
short of in-domain performance. Important consid-
erations for future work include identifying further
effective and tractable biases, and extending beyond
sequence-labeling to other types of NLP tasks.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397.
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems 20, Cambridge, MA. MIT
Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151–175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
M. Candito and B. Crabbe´. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138–141.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Pro-
ceedings of the ACL.
Hal Daume´ III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume´ III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Neural Information Processing Systems (NIP-
S).
A. Emami, P. Xu, and F. Jelinek. 2003. Using a con-
nectionist model in a syntactical based language mod-
el. In Proceedings of the International Conference on
Spoken Language Processing, pages 372–375.
Kuzman Ganchev, Joa˜o Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:10–49.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245–273.
1322
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In In Pro-
ceedings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language models as representation-
s for weakly supervised nlp tasks. In Conference on
Natural Language Learning (CoNLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030–1038.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In In Proc. ICML.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
International Workshop on Artificial Intelligence and
Statistics, pages 246–252.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In Pro-
ceedings of NAACL-HLT, pages 556–563.
M. Sahlgren. 2005. An introduction to random indexing.
In In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering (TKE).
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96–102.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384–394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
Lijie Wang, Wanxiang Che, and Ting Liu. 2009. An
svmtool-based chinese pos tagger. In Journal of Chi-
nese Information Processing.
X. Yang, H. Fu, H. Zha, and J. Barlow. 2006. Semi-
supervised nonlinear dimensionality reduction. In
Proceedings of the 23rd International Conference on
Machine Learning.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
D. Zhang, Z.H. Zhou, and S. Chen. 2007. Semi-
supervised dimensionality reduction. In Proceedings
of the 7th SIAM International Conference on Data
Mining.
1323
