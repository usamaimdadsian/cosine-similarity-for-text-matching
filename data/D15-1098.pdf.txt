Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 829–834,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Component-Enhanced Chinese Character Embeddings
Yanran Li1, Wenjie Li1, Fei Sun2, and Sujian Li3
1Department of Computing, The Hong Kong Polytechnic University, Hong Kong
2Institute of Computing Technology, Chinese Academy of Sciences, China
3Key Laboratory of Computational Linguistics, Peking University, MOE, China
{csyli, cswjli}@comp.polyu.edu.hk, ofey.sunfei@gmail.com,
lisujian@pku.edu.cn
Abstract
Distributed word representations are very
useful for capturing semantic information
and have been successfully applied in a
variety of NLP tasks, especially on En-
glish. In this work, we innovatively de-
velop two component-enhanced Chinese
character embedding models and their bi-
gram extensions. Distinguished from En-
glish word embeddings, our models ex-
plore the compositions of Chinese char-
acters, which often serve as semantic in-
dictors inherently. The evaluations on
both word similarity and text classification
demonstrate the effectiveness of our mod-
els.
1 Introduction
Due to its advantage over traditional one-hot rep-
resentation, distributed word representation has
demonstrated its benefit for semantic representa-
tion in various NLP tasks. Among the existing ap-
proaches (Huang et al, 2012; Levy and Goldberg,
2014; Yang and Eisenstein, 2015), the continu-
ous bag-of-wordsmodel (CBOW) and the continu-
ous skip-gram model (SkipGram) remain the most
popular ones that one can use to build word embed-
dings efficiently (Mikolov et al, 2013a; Mikolov
et al, 2013b). These two models learn the dis-
tributed representation of a word based on its con-
text. The context defined by the window of sur-
rounding words may unavoidably include certain
less semantically-relevant words and/or miss the
wordswith important and relevantmeanings (Levy
and Goldberg, 2014).
To overcome this shortcoming, a line of research
deploys the order information of the words in the
contexts by either deriving the contexts using de-
pendency relations where the target word partici-
pates (Levy and Goldberg, 2014; Yu and Dredze,
2014; Bansal et al, 2014) or directly keeping the
order features (Ling et al, 2015). As to another
line, Luong et al (2013) captures morphological
composition by using neural networks and Qiu
et al (2014) introduces the morphological knowl-
edge as both additional input representation and
auxiliary supervision to the neural network frame-
work. While most previous work focuses on En-
glish, there is a little work on Chinese. Zhang et
al (2013) extracts the syntactical morphemes and
Cheng et al (2014) incorporates the POS tags and
dependency relations. Basically, the work in Chi-
nese follows the same ideas as in English.
Distinguished from English, Chinese characters
are logograms, of which over 80% are phono-
semantic compounds, with a semantic component
giving a broad category of meaning and a phonetic
component suggesting the sound1. For example,
the semantic component ? (human) of the Chi-
nese character? (he) provides the meaning con-
nected with human. In fact, the components of
most Chinese characters inherently bring with cer-
tain levels of semantics regardless of the contexts.
Being aware that the components of Chinese char-
acters are finer grained semantic units, then an im-
portant question arises before slipping to the appli-
cations of word embeddings—would it be better to
learn the semantic representations from the charac-
ter components in Chinese?
We approach this question from both the prac-
tical and the cognitive points of view. In prac-
tice, we expect the representations to be optimized
for good generalization. As analyzed before, the
components are more generic unit inside Chinese
characters that provides semantics. Such inher-
ent information somehow alleviates the shortcom-
ing of the external contexts. From the cognitive
point of view, it has been found that the knowl-
edge of semantic components significantly corre-
1http://en.wikipedia.org/wiki/Radical_
(Chinese_characters)
829
late to Chinese word reading and sentence compre-
hension (Ho et al, 2003).
These evidences inspire us to explore novel
Chinese character embedding models. Different
from word embeddings, character embeddings re-
late Chinese characters that occur in similar con-
texts with their component information. Chinese
characters convey the meanings from their compo-
nents, and beyond that, the meanings of most Chi-
nese words also take roots in their composite char-
acters. For example, the meaning of the Chinese
word?? (cradle) can be interpreted in terms of
its composite characters? (sway) and? (basket).
Considering this, we further extend character em-
beddings from uni-gram models to bi-gram mod-
els.
At the core of our work is the exploration of
Chinese semantic representations from a novel
character-based perspective. Our proposed Chi-
nese character embeddings incorporate the finer-
grained semantics from the components of char-
acters and in turn enrich the representations inher-
ently in addition to utilizing the external contexts.
The evaluations on both intrinsic word similarity
and extrinsic text classification demonstrate the ef-
fectiveness and potentials of the new models.
2 Component-Enhanced Character
Embeddings
Chinese characters are often composed of smaller
and primitive components called radicals or
radical-like components, which serve as the most
basic units for building character meanings. Dat-
ing back to the 2nd century AD, the Han dynasty
scholar Shen XU organizes his etymological dic-
tionary shu? wén ji? zì (word and expression) by
selecting 540 recurring graphic components that
he called bù (means “categories”). Bù is nearly
the same as what we call radicals today2. Most
radicals are common semantic components. Over
time, some original radicals evolve into radical-
like components. Nowadays, a Chinese character
often contains exactly one radical (rarely has two)
and several other radical-like components. In what
follows, we refer to as components both radicals
and radical-like components.
Distinguished from English, these composite
components are unique and inherent features in-
side Chinese characters. A lot of times, they allow
2http://en.wikipedia.org/wiki/Radical_
(Chinese_characters)
us to assumingly understand or infer the meanings
of characters without any context. In other words,
the component-level features inherently bring with
additional information that benefits semantic rep-
resentations of characters. For example, we know
that the characters? (you),? (he),? (compan-
ion),? (companion), and? (people) all have the
meanings related to human because of their shared
component ? (human), a variant of the Chinese
character? (human). This kind of component in-
formation is intrinsically different from the con-
texts deriving by dependency relations and POS
tags. It motivates us to investigate the component-
enhanced Chinese character embedding models.
While Sun et al (2014) utilizes radical information
in a supervised fashion, we build our models in a
holistic unsupervised and bottom-up way.
It is important to note the variation of a radical
inside a character. There are two types of varia-
tions. The main type is position-related. For ex-
ample, the radical of the Chinese character? (wa-
ter) is itself, but it becomes ? as the radical of
? (pool). The original radicals are stretched or
squeezed so that they can fit into the general Chi-
nese character shape of a square. The second vari-
ation type emerges along with the history of char-
acter simplification when traditional characters are
converted into simplified characters. For instance,
? (eat) is written as ? when it forms as a part
of some traditional characters, but is written as?
in simplified characters. To cope with these vari-
ations and recover the semantics, we match all the
radical variants back into their original forms. We
extract all the components to build a component
list for each Chinese character. With the assump-
tion that a character’s radical often bring more im-
portant semantics than the rest3, we regard the rad-
ical of a character as the first component in its com-
ponent list.
Let a sequence of charactersD = {z
1
, . . . , z
N
}
denotes a corpus of N characters over the char-
acter vocabulary V . And z, c, e,K, T,M, |V | de-
note the Chinese character, the context charac-
ter, the component list, the corresponding em-
bedding dimension, the context window size, the
number of components taken into account for
each character, and the vocabulary size, respec-
tively. We develop two component-enhanced
character embedding models, namely charCBOW
3Inside a character, its radical often serves as the semantic-
component while its other radical-like components may be
phonetics.
830
and charSkipGram.
charCBOW follows the original continuous
bag-of-words model (CBOW) proposed by
(Mikolov et al, 2013a). We predict the central
character z
i
conditioned on a 2(M+1)TK-
dimensional vector that is the concatena-
tion of the remaining character-level contexts
(c
i?T
, . . . , c
i?1
, c
i+1
, . . . , c
i+T
) and the compo-
nents in their component lists. More formally,
we wish to maximize the log likelihood of all the
characters as follows,
L =
?
z
n
i
?D
log p(z
i
|h
i
),
h
i
= cat(c
i?T
, e
i?T
, . . . , c
i+T
, e
i+T
)
where h
i
denotes the concatenation of the
component-enhanced contexts. We make predic-
tion using a 2KT (M+1)|V |-dimensional matrix
O. Different from the original CBOW model,
the extra parameter introduced in the matrix O
allows us to maintain the relative order of the
components and treat the radical differently from
the rest components.
The development of charSkipGram is straight-
forward. We derive the component-enhanced con-
texts as (?c
i?T
, e
i?T
?, . . . , ?c
i+T
, e
i+T
?) based on
the central character z
i
. The sum of log probabili-
ties given z
i
is maximized:
L =
?
z
i
?D
T
?
j=?T
j ?=0
(
log p(c
j+i
|z
i
) + log p(e
j+i
|z
i
)
)
Figure 1 illustrates the two component-
enhanced character embedding models. It is easy
to extend charCBOW and charSkipGram to their
corresponding bi-character extensions. Denote
the z
i
, c
i
and e
i
in charCBOW and charSkipGram
as uni-character z
ui
, c
ui
and e
ui
, the bi-character
extensions are the models fed by bi-character
formed z
bi
, c
bi
and e
bi
.
3 Evaluations
We examine the quality of the proposed two Chi-
nese character embedding models as well as their
corresponding extensions on both intrinsic word
similarity evaluation and extrinsic text classifica-
tion evaluation.
Word Similarity. As the widely used public
word similarity datasets like WS-353 (Finkelstein
et al, 2001), RG-65 (Rubsenstein and Goode-
nough, 1965) are built for English embeddings,
.
z
i
...
c
i?T
c
i?T+1
c
i+T?1
c
i+T
c
i?T
e
i?T
c
i?T+1
e
i?T+1
c
i+T?1
e
i+T?1
c
i+T
e
i+T
O
OUTPUTPROJECTIONINPUT
(a) charCBOW
.
z
i
c
i?T+1
e
i?T+1
c
i?T
e
i?T
c
i+T?1
e
i+T?1
c
i+T
e
i+T
OUTPUTPROJECTIONINPUT
(b) charSkipGram
Figure 1: Illustrations of two component-
enhanced character embedding models.
we start from developing appropriate Chinese syn-
onym sets. Two candidate choices are Chinese
dictionaries HowNet (Dong and Dong, 2006) and
HIT-CIR’s Extended Tongyici Cilin (denoted as
E-TC)4. As HowNet contains less modern words,
such as ?? (Google), we select E-TC as our
benchmark for word similarity evaluation.
Text Classification. We use Tencent news ti-
tles as our text classification dataset5. A total of
8,826 titles of four categories (society, entertain-
ment, healthcare, and military) are extracted. The
lengths of titles range from 10 to 20 words. We
train ?
2
-regularized logistic regression classifiers
using the LIBLINEAR package (Fan et al, 2008)
with the learned embeddings.
To build the component-enhanced character em-
beddings, we employ the GB2312 character set
4http://ir.hit.edu.cn/demo/ltp/Sharing_
Plan.htm
5http://www.datatang.com/data/44341
831
Table 1: Word Similarity Results of Embedding Models
Model Spearman’s rank correlation (%)
A B C D E F G H I J K L
CBOW 33.2 25.2 32.2 27.8 36.5 37.6 43.2 40.2 37.3 39.5 44.2 40.4
SkipGram 35.9 26.7 33.8 29.9 36.6 40.2 45.3 44.3 39.0 41.2 46.9 43.0
charCBOW 34.0 23.2 34.1 26.7 37.8 49.2 48.1 44.5 40.2 42.0 48.0 43.2
charSkipGram 33.8 22.6 33.1 25.2 37.2 47.5 48.0 43.0 38.8 40.9 46.5 41.8
CBOW-bi 37.0 27.8 34.2 29.2 38.1 43.2 50.3 48.2 43.5 46.3 50.9 45.2
SkipGram-bi 38.2 29.0 34.0 29.4 38.9 44.9 50.2 49.3 45.6 48.4 51.3 47.4
charCBOW-bi 36.0 25.3 36.8 31.2 40.2 54.3 55.7 49.7 45.3 48.9 53.2 47.7
charSkipGram-bi 35.7 24.6 33.4 30.5 39.7 53.3 53.9 48.2 33.2 47.1 52.0 45.7
Table 2: Text Classification Results of Embedding Models
Model Society Entertainment Healthcare MilitaryP R F P R F P R F P R F
CBOW-bi 43.0 28.0 33.9 48.2 32.7 39.0 47.6 29.5 36.4 57.6 40.8 47.8
SkipGram-bi 47.2 31.1 37.5 49.8 34.0 40.4 48.4 32.7 39.0 58.8 42.3 49.2
charCBOW-bi 57.4 37.4 45.2 62.2 42.0 50.1 59.2 45.3 51.3 70.3 51.0 59.1
charSkipGram-bi 50.3 34.6 41.0 57.6 34.5 43.2 57.3 42.5 48.8 67.8 48.3 56.4
CBOW-combine 46.2 29.0 35.6 50.3 35.0 41.3 51.0 33.6 40.5 62.2 45.7 52.7
SkipGram-combine 50.9 34.6 41.2 51.4 37.2 43.2 52.1 35.6 42.3 62.1 49.0 54.8
charCBOW-combine 62.2 39.8 48.5 66.7 46.6 54.9 62.2 50.2 55.6 74.4 53.8 62.4
charSkipGram-combine 54.4 38.2 44.9 59.2 36.5 45.2 62.0 47.9 54.0 73.4 53.5 61.9
and extract all their component lists. It is easy
to obtain the first components (i.e., the radicals),
as they are readily available in the online Xinhua
Dictionary6. For the rest radical-like components,
we extract them by matching the patterns like “?
(from)+X” in the Xinhua dictionary. Such a pat-
tern indicates that a character has a component of
X. We also enrich the component lists by matching
the pattern “X is only seen” in Hong Kong Com-
puter Chinese Basic Component Reference7 .
It is observed that nearly 65% Chinese charac-
ters have only one component (their radicals), and
95% Chinese characters have two components (in-
cluding their radicals). Thus, we decide to main-
tain up to two extracted components to build the
character embeddings according to the frequency
of their occurrences. To cope with the radical vari-
ation problem, we transform 24 radical variants to
their origins, such as ? to ? (human), ? to ?
(hand), ? to? (water) and? to? (foot). The
complete list of the transformations is provided in
6http://xh.5156edu.com/
7http://www.ogcio.gov.hk/tc/business/tech_
promotion/ccli/cliac/glyphs_guidelines.htm
Appendix for easy reference.
We adopt Chinese Wikipedia Dump8 to train
our models as well as the original CBOW and
SkipGram, implemented in the Word2Vec tool9 for
comparison. The corpus in total contains 232,894
articles. In preprocessing, we remove pure digit
words and non-Chinese characters, and ignore the
words less than 10 occurrences during training.
We set the context window size T as 2 and use 5
negative samples experimentally. All the embed-
ding dimensions K are set to 50.
In the word similarity evaluation, we compute
the Spearman’s rank correlation (Myers and Well,
1995) between the similarity scores based on the
learned embedding models and the E-TC similar-
ity scores computed by following Tian and Zhao
(2010). The bi-character embeddings are concate-
nation of the composite character embeddings. For
the text classification evaluation, we average the
composite single character embeddings for each
bi-gram. And each bi-gram overlaps with the pre-
vious one. The titles are represented by averaging
8http://download.wikipedia.com/zhwiki/
9https://code.google.com/p/word2vec/
832
the embeddings of their composite grams10.
Table 1 presents the word similarity evaluation
results of the eight embedding models mentioned
above, where A–L denote the twelve categories in
E-TC. The first four rows are the results with the
uni-character inputs, and the last four rows corre-
spond to the bi-character embeddings results.
We can see that both CBOW and CBOW-bi per-
form worse than the corresponding SkipGram and
SkipGram-bi. This result is consistent with the
finding in the previous work (Pennington et al,
2014; Levy and Goldberg, 2014; Levy et al, 2015).
To some extent, CBOW and its extension CBOW-
bi are the most different among the eight (the first
four models in Table 1 and the first four models in
Table 2). They tie together the characters in each
context window by representing the context vector
as the sum of their characters’ vectors. Although
they have a potential of deriving better representa-
tions (Levy et al, 2015), they lose some particular
information from each unit of input in the average
operations.
Although the performance on twelve differ-
ent categories varies, in overall charCBOW,
charSkipGram and their extensions consistently
better correlate to E-TC. It provides the evidence
that the component information in Chinese char-
acters is of significance. Clearly, the bi-character
models achieve higher rank correlations. These re-
sults are not surprised. As amatter of fact, a major-
ity of Chinesewords are compounds of two charac-
ters. Thus, in many cases two characters together
is equivalent to a Chinese word. Considering the
superiority of the bi-character models, we only ap-
ply them in the text classification evaluations.
The results shown in the first four rows of Ta-
ble 2 are similar to those in the word similarity
evaluation. Please notice the significant improve-
ment of charCBOW and charCBOW-bi. We con-
jecture this as a hint of the importance of the or-
der information, which is introduced by the extra
parameter in the output matrixes. Their better per-
formances verify our assumption that the radicals
are more important than non-radicals. This is also
attributed to the benefit from the order of the char-
acters in the contexts.
Actually, we also conduct an additional experi-
ment to combine the uni-gram and the bi-gram em-
beddings for text classification and notice in aver-
10We do not compare the uni-formed characters with bi-
formed compound characters. The word pairs that cannot be
found in the vocabulary are removed.
age about 8.4% of gain over the bi-gram embed-
dings alone. The detailed results are presented in
the last four rows of Table 2.
4 Conclusions and Future Work
In this paper, we propose two component-
enhanced Chinese character embedding models
and their extensions to explore both the internal
compositions and the external contexts of Chinese
characters. Experimental results demonstrate their
benefits in learning rich semantic representations.
For the future work, we plan to devise embed-
ding models based together on the composition of
component-character and of character-word. The
two types of compositions will serve in a coordi-
nate fashion for the distributional representations.
Acknowledgements
The work described in this paper was supported
by the grants from the Research Grants Coun-
cil of Hong Kong (PolyU 5202/12E and PolyU
152094/14E), the grants from the National Natu-
ral Science Foundation of China (61272291 and
61273278) and a PolyU internal grant (4-BCB5).
Appendix
As mentioned in Section 3, we present the com-
plete list of transformations of the variant and orig-
inal forms of 24 radicals. The meaning columns
provide the corresponding meanings of the com-
ponents in the left.
transform meaning transform meaning
? ? ? grass ? ? ? hand
? ? ? human ? ? ? water
? ? ? knife ? ? ? vehicle
? ? ? dog ? ? ? hit
? ? ? fire ? ? ? silk
? ? ? gold ? ? ? old
? ? ? wheat ? ? ? cattle
? ? ? eat ? ? ? eat
? ? ? memory ? ? ? heart
? ? ? nest ? ? ? jade
? ? ? speak ? ? ? cloth
? ? ? body ? ? ? walk
833
References
Mohit Bansal, KevinGimpel, andKaren Livescu. 2014.
Tailoring continuous word representations for de-
pendency parsing. In Proc. of ACL.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
et al. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research,
3: 1137-1155.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP, pages 740–750.
Fei Cheng, Kevin Duh, Yuji Matsumoto. 2014. Pars-
ing Chinese Synthetic Words with a Character-based
Dependency Model. LREC.
Ronan Collobert, Jason Weston, L´eon Bottou, et al.
2011. Natural language processing (almost) from
scratch. JMLR, 12.
Zhendong Dong and Qiang Dong. 2006. HowNet and
the Computation of Meaning. World Scientific Pub-
lishing Co. Pte. Ltd., Singapore.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al.
2008. LIBLINEAR: A library for large linear classi-
fication. The Journal ofMachine Learning Research,
9: 1871-1874.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, et
al. 2001. Placing search in context: the concept re-
visited. In Proc. of WWW.
Connie Suk-Han Ho, Ting-Ting Ng, and Wing-Kin
Ng. 2003. A “radical” approach to reading develop-
ment in Chinese: The role of semantic radicals and
phonetic radicals. In Journal of Literacy Research,
35(3), 849-878.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proc. of ACL.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, et al. 2014. A dependency parser
for tweets. In Proc. of EMNLP, pages 1001–1012,
Doha, Qatar, October.
Remi Lebret, Jo ´el Legrand, and Ronan Collobert.
2013. Is deep learning really necessary for word em-
beddings? In Proc. of NIPS.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. of ACL.
Omer Levy, Yoav Goldberg, And Ido Dagan. 2015.
Improving Distributional Similarity with Lessons
Learned from Word Embeddings. In Proc. of TACL.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proc. of ACL, pages 1491–
1500.
Minh-Thang Luong, Richard Socher, and Christopher
D. Manning. 2013. Better Word Representations
with Recursive Neural Networks for Morphology. In
Proc. of CoNLL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
S. Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
composition-ality. In Advances in Neural Informa-
tion Processing Systems. pages 3111-3119.
Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge.
Jeffrey Pennington, Richard Socher, and Christopher
D. Manning. Glove: Global Vectors for Word Rep-
resentation. In Proc. of EMNLP.
Siyu Qiu, Qing Cui, Jiang Bian, and et al. 2014. Co-
learning of Word Representations and Morpheme
Representations. In Proc. of COLING.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Yaming Sun, Lei Lin, Duyu Tang, et al. 2014. Radical-
Enhanced Chinese Character Embedding.CoRR abs/
1404.4714.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.
Duyu Tang, Furu Wei, Nan Yang, et al. 2014. Learning
sentiment-specific word embedding for twitter sen-
timent classification. In Proc. of ACL.
Jiu-le Tian and Wei Zhao. 2010. Words Similarity Al-
gorithm Based on Tongyici Cilin in Semantic Web
Adaptive Learning System.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proc. of NAACL, Denver,
CO.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proc. of Computation and Lan-
guage.
Yi Yang and Jacob Eisenstein. 2015. Unsupervised
multi-domain adaptation with feature embeddings.
In Proc. of NAACL-HIT.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proc. of ACL.
Meishan Zhang, Yue Zhang, Wan Xiang Che, and et al.
2013. Chinese parsing exploiting characters. InProc.
of ACL.
834
