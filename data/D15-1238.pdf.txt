Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Personalized Machine Translation: Predicting Translational Preferences
Shachar Mirkin
?
IBM Research - Haifa
Mount Carmel, Haifa
31905, Israel
shacharm@il.ibm.com
Jean-Luc Meunier
Xerox Research Centre Europe
6 chemin de Maupertuis
38240 Meylan, France
jean-luc.meunier@xrce.xerox.com
Abstract
Machine Translation (MT) has advanced in re-
cent years to produce better translations for
clients’ specific domains, and sophisticated
tools allow professional translators to obtain
translations according to their prior edits. We
suggest that MT should be further personal-
ized to the end-user level – the receiver or the
author of the text – as done in other applica-
tions. As a step in that direction, we propose a
method based on a recommender systems ap-
proach where the user’s preferred translation
is predicted based on preferences of similar
users. In our experiments, this method outper-
forms a set of non-personalized methods, sug-
gesting that user preference information can be
employed to provide better-suited translations
for each user.
1 Introduction
Technologies are increasingly personalized, accommo-
dating their behavior for each user. Such personaliza-
tion is done through user modeling where the goal is to
“get to know” the user. To that end, personalization is
based on users’ attributes, such as demographics (gen-
der, age etc.), personalities, and preferences. For ex-
ample, in Information Retrieval, results are customized
according to the user’s information and search his-
tory (Speretta and Gauch, 2005), performance of Auto-
matic Speech Recognition substantially improves when
adapted to a specific speaker (Neumeyer et al., 1995),
and Targeted Advertising makes use of the user’s loca-
tion and prior purchases (K¨olmel and Alexakis, 2002).
Personalization in machine translation has a some-
what different nature. Providers of MT tools and ser-
vices offer means to “customize” or “personalize” the
translation engine for each client, mostly through do-
main adaptation techniques, and a great deal of effort
is made to make the human-involved translation pro-
cess more efficient (see Section 2.2). Most of the focus,
though, goes to customization for companies or profes-
sional translators. We argue that Personalized Machine
Translation (PMT below) should and can take the next
step and directly address individual end-users.
?
This work was done while the first author was at Xerox
Research Centre Europe.
The difficulty to objectively determine whether one
(automatic) translation is better than another has been
repeatedly revealed in the MT literature. Our con-
jecture is that one reason is individual preferences, to
which we refer as Translational Preferences (TP). TP
come into play both when the alternative translations
are all correct, and when each of them is wrong in a
different way. In the former case, a preference may be
a stylistic choice, and in the latter, a matter of com-
prehension or a selection of the least intolerable error
in one’s opinion. For instance, one user may prefer
shorter sentences than others; she may favor a more
formal style, while another would rather have it casual.
A user could be fine with some reordering errors but be
more picky concerning punctuations. One user will not
be bothered if some words are left untranslated (per-
haps because the source language belongs to the same
language family as the target language that he speaks),
while another will find it utterly displeasing. Such dif-
ferences may be the result of the type of translation sys-
tem being employed (e.g. syntax- vs. phrased-based),
the specific training data or many other factors. On the
user’s side, a preference may be attributed, for exam-
ple, to her mother tongue, her age or her personality.
Two aspects of end-user PMT may be considered:
(i) Personalized translation of texts written by a spe-
cific user, and (ii) PMT to provide better translations
for a specific reader. In this work we address the sec-
ond task, aiming to identify translations each user is
more likely to prefer.
1
Specifically, we consider a set-
ting where at least two MT systems are available, and
the goal is to predict which of the translation systems
the user would choose, assuming we have no knowl-
edge about her preference between them. Benchmark-
ing the systems in advance with respect to a reference
set, or estimating the quality of the translations (Specia
et al., 2009) are viable alternatives for translation selec-
tion; these, however, are not personalized to the target
user. Instead, we employ a user-user Collaborative Fil-
tering approach, common in Recommender Systems,
which we map to the TP prediction task.
We assess this approach using a collection of user
rankings of MT systems from a shared translation task
1
In (Mirkin et al., 2015) we investigate the first task, as-
sessing whether the author’s demographic and personality
traits are preserved over machine translation.
2019
(see Section 3). Our results show that the personalized
method modestly, but consistently, outperforms several
other approaches that rank the systems in general, dis-
regarding the specific user. We consider this as an in-
dication that user feedback can be employed towards a
more personalized approach to machine translation.
2 Background
2.1 Collaborative filtering
Collaborative filtering (CF) is a common approach
employed by recommender systems for suggesting to
users items, such as books or movies. A recommender
system may simply suggest to all users the most pop-
ular items; often, however, the recommendations are
personalized for each individual user to fit her taste or
preferences. User-user CF relies on community pref-
erences. The idea is to recommend to the user items
that are liked by users similar to her, as manifested, for
example, by high rating. Similar users are those that
agree with the current user on previously-rated items.
In k-nearest-neighbors CF, a user is typically repre-
sented by a vector of her preferences, where each entry
of the vector is, e.g., a rating of a movie. k similar users
are then identified by measuring the similarity between
the users’ vectors. Cosine similarity is a popular func-
tion for that purpose, and we also use it in our work
(Resnick et al., 1994; Sarwar et al., 2001; Ricci et al.,
2011). An alternative to cosine, Pearson’s correlation
coefficient (Pearson, 1895), allows addressing different
rating patterns across users. In comparison to cosine,
here vector entries are normalized with respect to the
user’s average rating. In our case, such normalization
is not very meaningful since the entries of the users
vectors represent comparisons rather than absolute rat-
ings, as will be made clear in Section 4. Nevertheless,
we have experimented with Pearson correlation as well,
and found no advantage in using it instead of cosine.
2.2 Customization, personalization and
adaptation in MT
Various means of customization and personalization
are available, in both academic and commercial MT.
Many of them target the company, rather than the in-
dividual user, and much of the effort is invested in de-
signing tools for professional translators, aiming to im-
prove their productivity, through intelligent Computer
Aided Translation (CAT).
Domain adaptation methods are commonly used to
adapt to the topic, the genre and even the style of the
translated material. Using the company’s own corpora
is one of the simplest techniques to do so, but many
more approaches have been proposed, including data-
selection (Axelrod et al., 2011; Gasc´o et al., 2012;
Mirkin and Besacier, 2014), mixture models (Foster
and Kuhn, 2007) and table fill-up (Bisazza et al., 2011).
Clients can utilize their own glossaries (Federico et
al., 2014), corpora (parallel or monolingual) and trans-
lation memories (TM), either shared or private ones
(Caskey and Maskey, 2014; Federico et al., 2014).
Through Adaptive and Interactive MT (Nepveu et al.,
2004), the system learns from the translator’s edits, in
order to avoid repeating errors that have already been
corrected. Post-editions can continuously be added to
the translator’s TM or be used as additional training
material, for tighter adaptation to the domain of inter-
est, through batch or incremental training.
2.3 User preferences in MT
Many tasks that require annotation by humans are af-
fected by the annotator and not only by the item be-
ing judged. Metrics for inter-rater reliability or inter-
annotator agreement, such as Cohen’s Kappa (Cohen,
1960), help measuring the extent to which annotators
disagree. Disagreement may be due to untrained or
inattentive annotators, a result of a task that is not well
defined, or when there is no obvious “truth”. Such is
the case with the evaluation of translation quality – it
is not always straightforward to tell whether one trans-
lation is better than another. A single sentence can be
translated in multiple correct ways. The decision be-
comes even harder when the translations are automat-
ically produced and are imperfect: Is one error worse
than another? The answer is in the eye of the beholder.
MT papers regularly report rather low Kappa levels,
even when measured on simpler tasks, such as short
segments (Mach´a?cek and Bojar, 2015).
Turchi et al. (2013) refer to the issue of “subjectiv-
ity” of human annotators. They address the task of
binary classification of “good” vs. “bad” translations,
and show that relying on human annotation for training
a binary quality estimator is less effective than using
automatically-generated labels. This subjectivity is ex-
actly what we are after. We treat it as a preference,
trying to identify the systems or specific translations
which the user subjectively prefers.
Kichhoff et al. (2012) analyze user preferences with
respect to MT errors. They show that some types, e.g.
word order errors, are the most dis-preferred by users,
and that this is a more important factor than the number
of errors. While very relevant for our research, their
analysis is aggregated over all users participating in the
study, and is not focusing on individuals’ preferences.
3 Data
In this work we used the data provided for the MT
Shared Task in the 2013 Workshop on Statistical Ma-
chine Translation (WMT) (Bojar et al., 2013).
2
This
data was of a particularly large scale, with crowd-
sourced human judges, either volunteer researchers or
paid Amazon Turkers. For each source sentence, a
judge was presented with the source sentence itself,
a reference translation, and the outputs of five ma-
chine translation systems. The five systems were ran-
domly selected from the pool of participating systems,
2
http://www.statmt.org/wmt13/
translation-task.html
2020
and were anonymized and randomly-ordered when pre-
sented to the judge. The judge had to rank the transla-
tions, with ties allowed (i.e. two system can receive the
same ranking). Hence, each annotation point provided
with 10 pairwise rankings between systems. Transla-
tions of 10 language pairs were assessed, with 11 to 19
systems for each pair. In total, over 900K non-tied pair-
wise rankings were collected. The Turkers’ annotation
included a control task for quality assurance, rejecting
Turkers failing more than 50% of the control points.
The inter-annotator score showed on average a fair to
moderate level of agreement.
4 Translational preferences with
collaborative filtering
Our method, denoted CTP (Collaborative Translational
Preferences), is based on a k-nearest-neighbors ap-
proach for user-user CF. That is, we predict the trans-
lational preferences of a user based on those of similar
users. In our setting, a user preference is the choice be-
tween two translation systems – which system’s trans-
lations does the user prefer. Given two systems (or
models of the same system) we wish to predict which
one the user would prefer, without assuming the user
has ever expressed her preference between these two
specific systems. It is important to emphasize that
the method presented here considers the users’ overall
preferences of systems, and does not regard the spe-
cific sentence that is being translated. In future work
we intend to make use of this information as well.
4.1 Representation
As mentioned in Section 3, each annotation consists
of a ranking of five systems. From that, we extract
pairwise rankings for every pair of systems that were
ranked for a given language pair. For each user u ? U
(where U are all users who annotated the language
pair), we create a user-preference vector, p
u
, that con-
tains an entry for each pair of translation systems. De-
noting the set of systems with S, we have
|S|·(|S|?1)
2
system pairs. E.g., for Czech-English, with 11 partic-
ipating systems, the user vector size is 55. Each entry
(i, j) of the vector is assigned the following value:
p
u
(i,j)
=
w
(i,j)
u
? l
(i,j)
u
w
(i,j)
u
+ l
(i,j)
u
(1)
where w
(i,j)
u
and l
(i,j)
u
are the number of wins and loses
of system s
i
vs. system s
j
as judged by user u.
3
With this representation, a user vector contains val-
ues between ?1 (if s
i
always lost to s
j
) and 1 (if s
i
al-
ways won). If the user always ranked the two systems
identically, the value is 0, and if she has never evalu-
ated the pair, the entry is regarded as a missing value
(NA). Altogether, we have a matrix of users by system
pairs, as depicted in Figure 1.
3
We have also considered including ties in the denomina-
tor of the equation; discarding them was found superior.
?
(?, ?)
??(?,?)
? × ?
?
??
Figure 1: The user-preferences matrix.
4.2 Finding similar users
Given a user preference to predict for a pair of sys-
tems (s
i
, s
j
), we compute the similarity between p
u
and each one of p
u
?
for all other u
?
? U . In our exper-
iments we used cosine as the similarity measure. The
k most-similar-users (MSU ) are then selected. To be
included in MSU (u), we require that u and u
?
have
judged at least 2 common system pairs.
4.3 Preference prediction
Given the similarity scores, to predict the user’s prefer-
ence for the target system pair, we compute a weighted
average of the predictions of the users in MSU (u).
We include in the average only users with similar-
ity scores above a certain positive threshold (0.05). We
then require that a minimum number of users meet the
above criteria of common annotations and minimum
similarity (we used 5). If not enough such similar
users are found, we turn to a fallback, where we use
the non-weighted average preference across all users
(AVPF presented in Section 5).
4
The prediction is then
the sign of the weighted average. A positive value
means s
i
is the preferred system; a negative one means
it is s
j
, and a zero is a draw. In our evaluation we com-
pare this prediction to the sign of the actual preference
of the user, p
u
(i,j)
. Formally, CTP computes the fol-
lowing prediction function f for a given user u and a
system pair (s
i
, s
j
):
f
CTP
(u)
(i,j)
= sign(
?
u
?
p
u
?
(i,j)
· sim(u, u
?
)
?
u
?
sim(u, u
?
)
) (2)
where u
?
? MSU (u) are the most similar users (the
nearest neighbors) of u; p
u
?
(i,j)
are the preferences
of user u
?
for (s
i
, s
j
) and sim(u, u
?
) is the similarity
score between the two users.
5
5 Experiments and results
5.1 Evaluation methodology
In our experiments we try to predict which one of two
translation systems would be preferred by a given user.
4
The fallback was used 0.1% of the times.
5
The denominator is not required as long as we predict
only the sign since all used similarity scores are positive. We
keep it in order to obtain a normalized score that can be used
for other decisions, e.g. ranking multiple systems.
2021
We evaluate our method, as well as several other pre-
diction functions, when compared with the user’s pair-
wise system preference according to the annotation –
p
u
(i,j)
, shown in Equation 1. For each user this is an
aggregated figure over all her pairwise rankings for the
pair, determining the preferred system as the one cho-
sen by the user (i.e. ranked higher) more times.
We conduct a leave-one-out experiment. For each
language pair, we iterate over all non-NA entries in the
user-preferences matrix, remove the entry and try to
predict it. User similarity scores are re-computed for
each evaluation point, to ensure they do not consider
the target pair. The “gold” preference is positive when
the user prefers s
i
, negative when she prefers s
j
and
0 when she has no preference between them. Hence,
each of the assessed methods is measured by the accu-
racy of predicting the sign of the preference.
5.2 Non-personalized methods
We compare CTP to the following prediction methods:
Always i (ALI) This is a na¨?ve baseline showing the
score when always predicting that system i wins. Note
that the baseline is not simply 50% due to ties.
Average rank (RANK) Here, two systems are com-
pared by the average of their rankings across all anno-
tations (r ? {1, 2, 3, 4, 5}):
f
RANK
(u)
(i,j)
= sign(r
j
? r
i
) (3)
r
j
and r
i
are the average ranks of s
j
and s
i
respec-
tively. Since a smaller value of r corresponds to a
higher rank, we subtract the rank of s
i
from s
j
and
not the other way around. This way, if for instance,
s
i
is ranked on averaged higher than s
j
, the prediction
would be positive, as desired.
Expected (EXPT) This metric, proposed by
Koehn (2012) and used by Bojar et al. (2013) in
order to rank the participating systems in the WMT
benchmark, compares the expected wins of the two
systems. Its intuition is explained as follows: “If
the system is compared against a randomly picked
opposing system, on a randomly picked sentence, by a
randomly picked judge, what is the probability that its
translation is ranked higher?” The expected wins of s
i
,
e(s
i
), is the probability of s
i
to win when compared
to another system, estimated as the total number of
wins of s
i
relative to the total number of comparisons
involving it, excluding ties, and normalized by the total
number of systems excluding s
i
, |{s
k
}|:
e(i) =
1
|{s
k
}|
?
k 6=i
w
(i,k)
w
(i,k)
+ l
(i,k)
(4)
where w
(i,k)
and l
(i,k)
are summed over all users.
The preference prediction is therefore:
f
EXPT
(u)
(i,j)
= sign(e(i)? e(j)) (5)
RANK and EXPT predict preferences based on a sys-
tem’s performance in general, when compared to all
other systems. We propose an additional prediction
function for comparison which uses only the informa-
tion concerning the system pair under consideration.
Average user preference (AVPF) This method takes
into account only the specific system pair and averages
the user preferences for the pair. Formally:
f
AVPF
(u)
(i,j)
= sign(
?
u
?
p
(i,j)
u
?
|{u
?
}|
) (6)
where u
?
6= u, and {u
?
} are all users except u.
This method can be viewed as a non-personalized
version of CTP, with two differences:
(1) It considers all users, and not only similar ones.
(2) It does not weight the preferences of the other
users by their similarity to the target user.
5.3 Results
Table 1 shows the results of an experiment comparing
the performance of the various methods in terms of pre-
diction accuracy. Figure 2 shows the micro-average
scores, when giving each of the 97,412 test points an
equal weight in the average. CTP outperforms all others
for 9 out of 10 language pairs, and in the overall micro-
averaged results. The difference between CTP and each
of the other metrics was found statistically significance
with p < 5 · 10
?6
at worse, as measured with a paired
Wilcoxon signed rank test (Wilcoxon, 1945) on the pre-
dictions of the two methods. The significance test cap-
tures in this case the fact that the methods disagreed in
many more cases than is visible by the score difference.
Our method was found superior to all others also
when computing macro-average, taking the average of
the scores of each language pair, as well as when the
ties are included in the computation of p
u
.
The parameters with which the above results were
obtained are found within the method’s description in
Section 4. Yet, in our experiments, CTP turned out to
be rather insensitive to their values. In this experiment
we used a global set of parameters and did not tune
them for each language pair separately. It is reasonable
to assume that such tuning would improve results. For
instance, choosing k, the number of users to include in
the average, depends on the total number of users. E.g.,
for en-es, where there are only 57 users in total, reduc-
ing k’s value from 50 to 25, improves results of CTP
from 62.6% to 63.2%, higher than all other methods
(whose scores are not affected).
Specifically in comparison to AVPF, weighting by
the similarity scores was found to be a more significant
factor than selecting a small subset of the users. This
may not come as a surprise, since less similar users that
are added to MSU (u) have a smaller impact on the fi-
nal decision since their weight in the average is smaller.
2022
Lang. f Acc.
cs-en
ALI 31.6
RANK 62.9
EXPT 63.5
AVPF 63.5
CTP 64.4
en-cs
ALI 36.2
RANK 67.8
EXPT 67.9
AVPF 67.4
CTP 68.2
Lang. f Acc.
de-en
ALI 41.7
RANK 62.6
EXPT 62.6
AVPF 62.6
CTP 63.5
en-de
ALI 42.0
RANK 67.2
EXPT 66.9
AVPF 66.5
CTP 67.6
Lang. f Acc.
es-en
ALI 35.5
RANK 61.0
EXPT 61.2
AVPF 61.4
CTP 63.0
en-es
ALI 35.9
RANK 62.3
EXPT 63.0
AVPF 61.5
CTP 62.6
Lang f Acc.
fr-en
ALI 35.0
RANK 61.3
EXPT 61.2
AVPF 61.2
CTP 61.8
en-fr
ALI 35.0
RANK 65.0
EXPT 65.1
AVPF 64.4
CTP 65.3
Lang. f Acc.
ru-en
ALI 43.5
RANK 57.6
EXPT 57.8
AVPF 56.6
CTP 58.2
en-ru
ALI 44.6
RANK 70.2
EXPT 72.1
AVPF 71.4
CTP 72.4
Table 1: Results in accuracy percentage for the 10 language pairs, including the languages: Czech (cs), English
(en), German (de), Spanish (es), French (fr) and Russian (ru). The best results is in bold. The difference between
CTP and each of the other methods is highly statistically significant. Figure 2 shows a micro-average of these
results.
One weakness of CTP, as well as of other methods,
is that it poorly predict ties. In the above experiment,
approximately 13.5% of the preferences were 0, none
of them was correctly identified. Our analysis showed
that numerical accuracy is not the main cause; setting
any prediction that is smaller than some values of |?|
to 0 was not found helpful. Arguably, ties need not be
predicted, since if the user has no preference between
two systems, any choice is just as good. Still, we be-
lieve that better ties prediction could lead to general
improvement of our method and we wish to address it
in future work.
39.3
61.7 61.8 61.4 62.6
30
40
50
60
ALI RANK EXPT AVPF CTP
Pred
ictio
n ac
cura
cy (%
)
MethodsALI
RANK
EXPT
AVPF
CTP
Figure 2: Micro-average over all 97,412 test points.
6 Discussion
We addressed the task of predicting user preference
with respect to MT output via a collaborative filtering
approach whose prediction is based on preferences of
similar users. This method predicts TP better than a
set of non-personalized methods. The gain is modest
in absolute numbers, but the results are highly statisti-
cally significant and stable over parameter values.
We consider this work as a step towards more per-
sonalized MT. This line of research can be extended
in multiple ways. First and foremost, as mentioned,
we did not consider the actual content of the sentences,
but rather identified a general preference for one system
over another. It is plausible, however, that one system is
better – from the user’s perspective – at translating one
type of text, while another is preferred for other texts.
Taking the actual texts into account seems therefore es-
sential. Content-based methods for recommender sys-
tems may be useful for this purpose. Another factor
that may be affecting preferences is translation quality:
when compared translations are all poor, preferences
play a less significant role. Hence, it may be informa-
tive to assess TP prediction separately across different
levels of translation quality.
Large parallel corpora are typically required for
training reasonable statistical translation models. Yet,
parallel corpora, and even more so in-domain ones,
are hard to gather. It is virtually impossible to find a
user-specific parallel corpus, and methods for mono-
lingual domain adaptation are easier to envisage if one
wishes to address author-aware PMT (the first PMT
task mentioned in Section 1). Collecting user feed-
back is another challenge, especially since most end-
users do not speak the source language. For that and
other reasons, it currently seems more feasible to col-
lect preference information from professional transla-
tors, explicitly or implicitly.Yet, in this research we
aim at end-users rather than translators whose prefer-
ences are often driven by the ease of correction more
than anything else. We believe that one way to tackle
this issue is to exploit other kinds of feedback, from
which we can infer user preferences and similarity.
Online MT providers are recently collecting end-user
feedback for their proposed translations which may be
useful for TP prediction. For instance, in early 2015
Facebook introduced a feature letting users rate (Bing)
translations, and Google Translate asks for suggested
improvements. We are hopeful that such data becomes
publicly available. Nevertheless, it remains unlikely
to obtain feedback from each and every user. A po-
tential direction for both corpora and feedback col-
lection is personalizing models and identifying prefer-
ences for groups of users based on socio-demographic
traits, such as gender, age or mother tongue, or based
on (e.g. Big 5) personality traits. These can even be
inferred by automatically analyzing user texts.
Acknowledgments
We wish to thank Herv´e D´ejean and the EMNLP re-
viewers for their valuable feedback on this work.
2023
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 355–362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT), San Francisco, California, USA.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation (WMT), pages 1–44, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Sasha P. Caskey and Sameer Maskey. 2014. Trans-
lation cache prediction, August 12. US Patent
8,805,672.
Jacob Cohen. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement, 20(1):37.
Marcello Federico, Nicola Bertoldi, Mauro Cettolo,
Matteo Negri, Marco Turchi, Marco Trombetti,
Alessandro Cattelan, Antonio Farina, Domenico
Lupinetti, Andrea Martines, Alberto Massidda, Hol-
ger Schwenk, Lo¨?c Barrault, Frederic Blain, Philipp
Koehn, Christian Buck, and Ulrich Germann. 2014.
The matecat tool. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 129–
132, Dublin, Ireland, August. Dublin City University
and Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
SecondWorkshop on Statistical Machine Translation
(WMT), Prague, Czech Republic, June.
Guillem Gasc´o, Martha-Alicia Rocha, Germ´an
Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Fran-
cisco Casacuberta. 2012. Does more data always
yield better translations? In Proceedings of the
13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
152–161, Avignon, France, April. Association for
Computational Linguistics.
Katrin Kirchhoff, Daniel Capurro, and Anne Turner.
2012. Evaluating user preferences in machine
translation using conjoint analysis. Proceedings of
the European Association of Machine Translation,
16:119–126.
Philipp Koehn. 2012. Simulating human judgment
in machine translation evaluation campaigns. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 179–184,
Hong Kong.
Bernhard K¨olmel and Spiros Alexakis. 2002. Location
based advertising. In Proceedings of the First In-
ternational Conference on Mobile Business, Athens,
Greece.
Matou?s Mach´a?cek and Ond?rej Bojar. 2015. Evaluat-
ing machine translation quality using short segments
annotations. The Prague Bulletin of Mathematical
Linguistics, No. 103:85–110, April.
Shachar Mirkin and Laurant Besacier. 2014. Data se-
lection for compact adapted SMT models. In Pro-
ceedings of the eleventh biennial conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2014), Vancouver, Canada, Oct.
Shachar Mirkin, Scott Nowson, Caroline Brun, and
Julien Perez. 2015. Motivating personality-aware
machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Lisbon, Portugal. Association
for Computational Linguistics.
Laurent Nepveu, P Langlais, G Lapalme, and George
Foster. 2004. Adaptive language and translation
models for interactive machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 190–
197, Barcelona, Spain.
Leonardo Neumeyer, Ananth Sankar, and Vassilios Di-
galakis. 1995. A comparative study of speaker
adaptation techniques. In Fourth European Con-
ference on Speech Communication and Technology,
EUROSPEECH 1995, Madrid, Spain, September
18-21, 1995.
Karl Pearson. 1895. Note on regression and inheri-
tance in the case of two parents. Proceedings of the
Royal Society of London, 58(347-352):240–242.
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-
ter Bergstrom, and John Riedl. 1994. Grouplens:
an open architecture for collaborative filtering of
netnews. In Proceedings of the 1994 ACM con-
ference on Computer supported cooperative work,
pages 175–186. ACM.
Francesco Ricci, Lior Rokach, and Bracha Shapira.
2011. Introduction to recommender systems hand-
book.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the
10th international conference on World Wide Web,
pages 285–295. ACM.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28–37.
2024
Mirco Speretta and Susan Gauch. 2005. Per-
sonalized search based on user search histories.
In Web Intelligence, 2005. Proceedings. The 2005
IEEE/WIC/ACM International Conference on, pages
622–628. IEEE.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the subjectivity of human judge-
ments in MT quality estimation. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation (WMT), pages 240–251, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80–83,
December.
2025
