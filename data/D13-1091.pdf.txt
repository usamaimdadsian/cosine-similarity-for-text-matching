Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897–902,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Is Twitter A Better Corpus for Measuring Sentiment Similarity?
Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong3
1Northeastern University, Shenyang, China
2University of International Relations, Beijing, China
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com
{byli,kfwong}@se.cuhk.edu.hk
Abstract
Extensive experiments have validated the ef-
fectiveness of the corpus-based method for
classifying the word’s sentiment polarity.
However, no work is done for comparing d-
ifferent corpora in the polarity classification
task. Nowadays, Twitter has aggregated huge
amount of data that are full of people’s senti-
ments. In this paper, we empirically evaluate
the performance of different corpora in sen-
timent similarity measurement, which is the
fundamental task for word polarity classifica-
tion. Experiment results show that the Twitter
data can achieve a much better performance
than the Google, Web1T and Wikipedia based
methods.
1 Introduction
Measuring semantic similarity for words and short
texts has long been a fundamental problem for many
applications such as word sense disambiguation,
query expansion, search advertising and so on.
Determining the word’s polarity plays a critical
role in opinion mining and sentiment analysis task.
Usually we can detect the word’s polarity by mea-
suring it’s semantic similarity with a positive seed
word sep and a negative seed word sen respectively,
as shown in Formula (1):
SO(w) = sim(w, sep)? sim(w, sen) (1)
where sim(wi, wj) is the semantic similarity mea-
surement method for the given word wi and wj . A
lot of papers have been published for designing ap-
propriate similarity measurements. One direction is
to learn similarity from the knowledge base or con-
cept taxonomy (Lin, 1998; Resnik, 1999). Anoth-
er direction is to learn semantic similarity with the
help of large corpus such as Web or Wikipedia da-
ta (Sahami and Heilman, 2006; Yih and Meek, 2007;
Bollegala et al., 2011; Gabrilovich and Markovitch,
2007). The basic assumption of this kind of methods
is that the word with similar semantic meanings of-
ten co-occur in the given corpus. Extensive experi-
ments have validated the effectiveness of the corpus-
based method in polarity classification task (Turney,
2002; Kaji and Kitsuregawa, 2007; Velikovich et al.,
2010). For example, PMI is a well-known similari-
ty measurement (Turney, 2002), which makes use of
the whole Web as the corpus, and utilizes the search
engine hits number to estimate the co-occurrence
probability of the give word pairs. The PMI based
method has achieved promising results. However,
according to Kanayama’s investigation, only 60%
co-occurrences in the same window in Web pages
reflect the same sentiment orientation (Kanayama
and Nasukawa, 2006). Therefore, we may ask the
question whether the choosing of corpus can change
the performance of sim and is there any better cor-
pus than the Web page data for measuring the senti-
ment similarity?
Everyday, enormous numbers of tweets that con-
tain people’s rich sentiments are published in Twit-
ter. The Twitter may be a good source for measuring
the sentiment similarity. Compared with the Web
page data, the tweets have a higher rate of subjective
text posts. The length limitation can guarantee the
polarity consistency of each tweet. Moreover, the
tweets contain graphical emoticons, which can be
897
considered as natural sentiment labels for the corre-
sponding tweets in Twitter. In this paper, we attempt
to empirically evaluate the performance of differen-
t corpora in sentiment similarity measurement task.
As far as we know, no work is done on this topic.
2 The Characteristics of Twitter Data
As the world’s second largest SNS website, at the
end of 2012 Twitter had aggregated more than 500
million registered users, among which 200 million
were active users . More than 400 million tweets are
posted every day.
Several examples of typical posts from Twitter are
shown below.
(1) She had a headache and feeling light headed
with no energy. :(
(2) @username Nice work! Looks like you had a
fun day. I’m headed there Sat or Sun. :)
(3) I seen the movie on Direc Tv. I ordered it and
I really liked it. I can’t wait to get it for blu ray!
Excellent work Rob!
We observe that comparing with the other corpus,
the Twitter data has several advantages in measuring
the sentiment similarity.
Large. Users like to record their personal feelings
and talk about the trend topics in Twitter (Java et al.,
2007; Kwak et al., 2010). So there are huge amount
of subjective texts with various topics generated in
the millions of tweets everyday. Further more, the
flexible Twitter API makes these data easy to access
and collect.
Length Limitation. Twitter has a length limita-
tion of 140 characters. Users have limited space to
express their feelings. So the sentiments in tweet-
s are usually concise, straightforward and polarity
consistent.
Emoticons. Users tend to utilize emoticons to
emphasize their sentiment feelings. According to
the statistics, about 8.1% tweets contain at least one
emoticon (Yang and Leskovec, 2011). Since the
tweets have the length limitation, the sentiments ex-
pressed in these short texts are usually consistent
with the embedded emoticons, such as the word fun
and headache in above examples.
In addition to the above advantages, there are al-
so some disadvantages for measuring sentiment sim-
ilarity using Twitter data. The spam tweets that
caused by advertisements may add noise and bias
during the similarity measurement. The short length
may also bring in lower co-occurrence probability
of words. Some words may not co-occur with each
other when the corpus is small. These disadvantages
set obstacles for measuring sentiment similarity by
using Twitter data as corpus. In the experiment sec-
tion, we will see if we can overcome these draw-
backs and get benefit from the advantages of Twitter
data.
3 The Corpus-based Sentiment Similarity
Measurements
The intuition behind the corpus-based semantic sim-
ilarity measuring method is that the words with sim-
ilar meanings tend to co-occur in the corpus. Given
the word wi, wj , we use the notation P (wi) to de-
note the occurrence counts of word wi in the corpus
C. P (wi, wj) denotes the co-occurrence counts of
word wi and wj in C. In this paper we employ the
corpus-based version of the three well-known simi-
larity measurements: Jaccard, Dice and PMI.
CorpusJaccard(wi, wj)
= P (wi,wj)P (wi)+P (wj)?P (wi,wj)
(2)
CorpusDice(wi, wj) =
2× P (wi, wj)
P (wi) + P (wj)
(3)
CorpusPMI(wi, wj) = log2(
P (wi,wj)
N
P (wi)
N
P (wj)
N
) (4)
In Formula (4), N is the number of documents in
the corpus C. The above similarity measurements
may have their own strengths and weaknesses. In
this paper, we utilize these classical measurements
to evaluate the quality of the corpus in polarity clas-
sification task.
Google is the world’s largest search engine, which
has indexed a huge number of Web pages. Us-
ing the extreme large indexed Web pages as cor-
pus, Cilibrasi and Vitanyi (2007) presented a method
for measuring similarity between words and phras-
es based on information distance and Kolmogorov
complexity. The search result page counts of Google
were utilized to estimate the occurrence frequencies
of the words in the corpus. Suppose wi, wj rep-
resent the candidate words, the Normalized Google
898
Distance is defined as:
NGD(wi, wj) =
max{logP (wi),logP (wj)}?logP (wi,wj)
logN?min{logP (wi),logP (wj)}
(5)
where P (wi) denotes page counts returned by
Google using wi as keyword; P (wi, wj) denotes the
page counts by using wi and wj as joint keywords;
N is the number of Web pages indexed by Google.
Cilibrasi and Vitanyi have validated the effective-
ness of Google distance in measuring the semantic
similarity between concept words.
Based on the above formulas, we compare the
Twitter data with the Web and Wikipedia data as the
similarity measurement corpus. Given a candidate
word w, we firstly measure its sentiment similar-
ity with a positive seed word and a negative seed
word respectively in Formula (1), and the difference
of sim is used to further detect the polarity of w.
The above four similarity measurements serve as
sim with Web, Wikipedia and Twitter data as cor-
pus. Turney (2002) chose excellent and poor as
seed words. However, using isolated seed word-
s may cause the bias problem. Therefore, we fur-
ther select two groups of seed words that are lack
of sensitivity to context and form a positive seed set
PS and a negative seed set NS (Turney, 2003). The
Formula (1) can be rewritten as:
SO(w) =
?
sep?PS
sim(w, sep)?
?
sen?NS
sim(w, sen) (6)
Based on the Formula(6) and the sentiment seed
words, we can measure the sentiment polarity of the
given candidate words.
4 Experiment
4.1 Experiment Setup
Corpus Preparing. The Twitter corpus corre-
sponds to the 476 million Twitter tweets (Yang and
Leskovec, 2011), which includes over 476 million
Twitter posts from 20 million users, covering a 7
month period from June 1, 2009 to December 31,
2009. We filter out the non-English tweets and the
spam tweets that have only few words with URLs.
The tweets that contain three or more trending topics
are also removed. Finally, we construct the Twitter
corpus that consists of 266.8 million English tweets.
For calculating page counts in Web data, the candi-
date words were launched to Google from February
2013 to April 2013. We also conduct the experi-
ments on the Google Web 1T data that consists of
Google n-gram counts (frequency of occurrence of
each n-gram) for 1 ? n ? 5 (Brants and Franz,
2006). The Web 1T data provides a nice approxi-
mation to the word co-occurrence statistics in Web
pages in a predefined window size (1 ? n ? 5).
For example, the 5 gram Web1T data means the co-
occurrence window size is 5. The English Wikipedia
dump 1 we used was extracted at the end of March
2013, which contained more than 13 million articles.
We extracted the plain texts of the Wikipedia data as
the training corpus for the Formula (6).
EvaluationMethod. Two well-know sentiment lex-
icons are utilized as gold standard for polarity clas-
sification task. The statistics of Liu’s sentiment lex-
icon (Liu et al., 2005) and MPQA subjectivity lexi-
con (Wilson et al., 2005) are shown in Table 1. For
each word w in the lexicons, we employ the Formu-
la (6) to calculate the word’s polarity using different
corpora. If SO(w) > 0, the word w is classified in-
to the positive category. Otherwise if SO(w) < 0, it
is classified into the negative category. The accura-
cy of the classification result is used to measure the
quality of the corpus.
Positive# Negative#
Liu 2,006 4,783
MPQA 2,304 4,153
Table 1: Lexicon size
4.2 Experiment Results
Firstly, we chose the seed words excellent and poor
as Turney’s (2002) settings. The polarity classifica-
tion accuracies are shown in Table 2.
In Table 2, Google, Web1T, Wikipedia, Twitter
represent the corpora that used in the experiment;
CJ, CD, CP, GD represent the Formula (2) to For-
mula (5) respectively. We can see from the Table 2
that the Twitter based method can achieve the best
performance. The rich sentiment information and
1http://en.wikipedia.org/
899
Lexicon Corpus CJ CD CP GD
Liu
Google 0.5116 0.5117 0.5064 0.5076
Web1T-5gram 0.3903 0.3903 0.3897 0.3864
Web1T-4gram 0.3771 0.3771 0.3772 0.3227
Wikipedia 0.5280 0.5280 0.5350 0.5412
Twitter 0.5567 0.5567 0.5635 0.5635
MPQA
Google 0.4897 0.4890 0.4891 0.4864
Web1T-5gram 0.3843 0.3843 0.3837 0.3783
Web1T-4gram 0.3729 0.3729 0.3714 0.3225
Wikipedia 0.5181 0.5181 0.5380 0.5344
Twitter 0.5421 0.5421 0.5493 0.5494
Table 2: Polarity classification accuracies using excellent
and poor as seed words
natural window size (140 characters) have a posi-
tive impact on determining the word’s polarity. The
Google based method gets a lower accuracy, this
may be due to the length of Web documents which
can not usually guarantee the semantic consistency
in the returned data. Even though two words appear
in one page (returned by Google), they might not be
semantically related. Furthermore, the Google based
method is time-consuming, because we have to peri-
odically send queries in order to avoid being blocked
by Google. The Web1T based method gets a much
worse accuracy. After detailed analysis, we find that
although the small window size (4 or 5) can guar-
antee the semantic consistency, the short length also
brings in lower co-occurrence probability. Statistics
show that about 38% SO values are zero when using
Web1T corpus. Due to the short length, the Twitter
data also suffers from the low co-occurrence prob-
lem.
To tackle the low co-occurrence problem, the seed
word sets are selected as Turney’s (2003) settings.
The positive word set PS={good, nice, excellent,
positive, fortunate, correct, superior} and negative
word set NS = {bad, nasty, poor, negative, unfortu-
nate, wrong, inferior} for the Formula (6). These
seed words have been verified to be effective in Tur-
ney’s paper for polarity classification. The experi-
ment results are shown in Table 3.
Table 3 shows that the performance of Twitter cor-
pus is much improved since the multiple seed words
alleviate the problem of low co-occurrence probabil-
ity in tweets. Generally, when using the seed word
groups the Twitter can achieve a much better per-
formance than all the other corpora. The improve-
ments are statistically significant (p-value < 0.05).
Lexicon Corpus CJ CD CP GD
Liu
Google 0.4859 0.4936 0.4884 0.5060
Web1T-5gram 0.5785 0.5785 0.3963 0.5782
Web1T-4gram 0.5766 0.5766 0.3872 0.5775
Wikipedia 0.6226 0.6225 0.5957 0.6145
Twitter 0.6678 0.6678 0.6917 0.6457
Twitter+ 0.6921 0.6921 0.7273 0.6599
MPQA
Google 0.5108 0.5225 0.5735 0.5763
Web1T-5gram 0.5737 0.5737 0.4225 0.5718
Web1T-4gram 0.5749 0.5749 0.3329 0.4797
Wikipedia 0.6086 0.6085 0.5773 0.5985
Twitter 0.6431 0.6431 0.6671 0.6253
Twitter+ 0.6665 0.6665 0.7001 0.6383
Table 3: Polarity classification accuracies using the seed
word groups
We further add the emoticons ‘:)’ and ‘:(’ into the
seed word groups, denoted by Twitter+ in Table 3.
The emoticons are natural sentiment labels. We can
see that the performances are further improved by
considering emoticons as seed words. The above
experiment results have validated the effectiveness
of Twitter data as a better corpus for measuring the
sentiment similarity. The results also reveal the po-
tential usefulness of Twitter corpus in semantic sim-
ilarity measurement.
5 Related Work
Detecting the polarity of words is the fundamental
problem for most of sentiment analysis tasks (Hatzi-
vassiloglou and McKeown, 1997; Pang and Lee,
2007; Feldman, 2013).
Many methods have been proposed to measure
the words’ or short texts similarity based on large
corpus (Sahami and Heilman, 2006; Yih and Meek,
2007; Gabrilovich and Markovitch, 2007). Bolle-
gala et al. (2011) submitted the word to the search
engine, and the related result pages were employed
to represent the meaning of the original word. Mi-
halcea et al. (2006) proposed a method to measure
the semantic similarity of words or short texts, con-
sidering both corpus-based and knowledge-based in-
formation. Although the previous algorithms have
achieved promising results, there are no work done
on evaluating the quality of different corpora.
Mohtarami et al. (2012; 2013a; 2013b) intro-
duced the concept of sentiment similarity, which
was considered as different from the traditional se-
mantic similarity, and more focused on revealing the
underlying sentiment relations between words. Mo-
900
htarami et al. (2013b) proposed a hidden emotion-
al model to calculating the sentiment similarity of
word pairs. However, the impact of the different cor-
pora is not considered for this task.
Mohammad et al. (2013) generated word-
sentiment association lexicons from Tweets with the
help of hashtags and emoticons. Pak and Paroubek
(2010) collected tweets with happy and sad emoti-
cons as training corpus, and built sentiment classi-
fier based on traditional machine learning methods.
Brody and Diakopoulos (2011) showed that length-
ening was strongly associated with subjectivity and
sentiment in tweets. Davidov et al. (2010) treated 50
Twitter tags and 15 smileys as sentiment labels and
a supervised sentiment classification framework was
proposed to classify the tweets. The previous litera-
tures have showed that the emoticons can be treated
as natural sentiment labels of the tweets.
6 Conclusion and Future Work
The quality of corpus may affect the performance
of sentiment similarity measurement. In this pa-
per, we compare the Twitter data with the Google,
Web1T and Wikipedia data in polarity classification
task. The experiment results validate that when us-
ing the seed word groups the Twitter can achieve a
much better performance than the other corpora and
adding emoticons as seed words can further improve
the performance. It is observed that the twitter cor-
pus is a potential good source for measuring senti-
ment similarity between words. In future work, we
intend to design new similarity measurements that
can make best of the advantages of Twitter data.
Acknowledgments
This research is partially supported by Gener-
al Research Fund of Hong Kong (No. 417112)
and Shenzhen Fundamental Research Program (J-
CYJ20130401172046450). This research is al-
so supported by the State Key Development Pro-
gram for Basic Research of China (Grant No.
2011CB302200-G), State Key Program of Nation-
al Natural Science of China (Grant No. 61033007),
National Natural Science Foundation of China
(Grant No. 61370074, 61100026), and the Funda-
mental Research Funds for the Central Universities
(N120404007).
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
562–570, Edinburgh, UK, ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2011. A Web Search Engine-Based Approach to
Measure Semantic Similarity between Words. IEEE
Transactions on Knowledge and Data Engineering,
23(7): 977–990.
Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Simi-
larity Distance. IEEE Transactions on Knowledge and
Data Engineering, 19(3): 370–383.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 241–
249, Beijing, China, ACL.
Ronen Feldman. 2013. Techniques and Applications for
Sentiment Analysis. Communications of the ACM,
56(4):82–89.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606–1611, Hyderabad, India.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
174–181, Madrid, Spain, ACL.
Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-
seng. 2007. Why We Twitter: An Analysis of a Mi-
croblogging Community. In Proceedings of the 9th
International Workshop on Knowledge Discovery on
the Web and 1st International Workshop on Social Net-
works Analysis, pages 118–138, San Jose, CA, USA,
Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. Building Lex-
icon for Sentiment Analysis from Massive Collec-
tion of HTML Documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pp. 1075–1083, Prague, Czech
Republic, ACL.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Ful-
ly Automatic Lexicon Expansion for Domain-oriented
901
Sentiment Analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 355–363, Sydney, Australia, ACL.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
B. Moon. 2010. What is Twitter, a Social Network or a
News Media? In Proceedings of the 19th Internation-
al Conference on World Wide Web, pages 591–600,
Raleigh, North Carolina, USA, ACM.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 768–774, Montreal, Quebec, Canada, ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342–351,
Chiba, Japan, ACM.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence and the
18th Innovative Applications of Artificial Intelligence
Conference, pages 775–780, Boston, Massachusetts,
USA, AAAI Press.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense Sentimen-
t Similarity: An Analysis. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 1706–1712, Toronto, Ontario, Canada,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a.
From Semantic to Emotional Space in Probabilistic
Sense Sentiment Analysis. In Proceedings of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 711–717, Bellevue, Washington, USA,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b.
Probabilistic Sense Sentiment Similarity through Hid-
den Emotions. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 983–992, Sofia, Bulgaria, ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic E-
valuation Exercises, Atlanta, Georgia, USA, ACL.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Proceedings of the 2010 International Conference
on Language Resources and Evaluation, pages 1320–
1326, Valletta, Malta, ELRA.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy:
An Information based Measure and Its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence Research, 11:95–130.
Mehran Sahami and Timothy D. Heilman. 2006. A Web-
based Kernel Function for Measuring the Similarity of
Short Text Snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, pages 377–
386, Edinburgh, Scotland, UK, ACM.
Bo Pang and Lillian Lee. 2007. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1–135.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 417–424, Philadelphia, PA, USA, ACL.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transaction Information Sys-
tem, 21(4): 315–346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan T. McDonald. The Viability of Web-
derived Polarity Lexicons. In Proceedings of the 2010
North American Chapter of the Association of Compu-
tational Linguistics, pp. 777–785, Los Angeles, Cali-
fornia, USA, ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman-
n. 2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347–354, Vancouver, British Columbi-
a, Canada, ACL.
Jaewon Yang and Jure Leskovec. 2011. Patterns of Tem-
poral Variation in Online Media. In Proceedings of
the Forth International Conference on Web Search and
Web Data Mining, pages 177–186, Hong Kong, Chi-
na, ACM.
Wen-tau Yih and Christopher Meek. 2007. Improving
Similarity Measures for Short Segments of Text. In
Proceedings of the 22nd AAAI Conference on Artificial
Intelligence, pages 1489–1494, Vancouver, British
Columbia, Canada, AAAI Press.
902
