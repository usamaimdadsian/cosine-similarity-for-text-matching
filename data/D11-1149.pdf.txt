Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
A Probabilistic Forest-to-String Model for Language Generation from
Typed Lambda Calculus Expressions
Wei Lu and Hwee Tou Ng
Department of Computer Science
School of Computing
National University of Singapore
{luwei,nght}@comp.nus.edu.sg
Abstract
This paper describes a novel probabilistic ap-
proach for generating natural language sen-
tences from their underlying semantics in the
form of typed lambda calculus. The approach
is built on top of a novel reduction-based
weighted synchronous context free grammar
formalism, which facilitates the transforma-
tion process from typed lambda calculus into
natural language sentences. Sentences can
then be generated based on such grammar
rules with a log-linear model. To acquire such
grammar rules automatically in an unsuper-
vised manner, we also propose a novel ap-
proach with a generative model, which maps
from sub-expressions of logical forms to word
sequences in natural language sentences. Ex-
periments on benchmark datasets for both En-
glish and Chinese generation tasks yield sig-
nificant improvements over results obtained
by two state-of-the-art machine translation
models, in terms of both automatic metrics
and human evaluation.
1 Introduction
This work focuses on the task of generating natu-
ral language sentences from their underlying mean-
ing representations in the form of formal logical ex-
pressions (typed lambda calculus). Many early ap-
proaches to generation from logical forms make use
of rule-based methods (Wang, 1980; Shieber et al.,
1990), which concern surface realization (ordering
and inflecting of words) but largely ignore lexical ac-
quisition. Recent approaches start to employ corpus-
based probabilistic methods, but many of them as-
sume the underlying meaning representations are of
specific forms such as variable-free tree-structured
representations (Wong and Mooney, 2007a; Lu et
al., 2009) or database entries (Angeli et al., 2010).
While these algorithms usually work well on spe-
cific semantic formalisms, it is unclear how well
they could be applied to a different semantic formal-
ism. In this work, we propose a general probabilis-
tic model that performs generation from underlying
formal semantics in the form of typed lambda calcu-
lus expressions (we refer to them as ?-expressions
throughout this paper), where both lexical acquisi-
tion and surface realization are integrated in a single
framework.
One natural proposal is to adopt a state-of-the-art
statistical machine translation approach. However,
unlike text to text translation, which has been ex-
tensively studied in the machine translation commu-
nity, translating from logical forms into text presents
additional challenges. Specifically, logical forms
such as ?-expressions may have complex internal
structures and variable dependencies across sub-
expressions. Problems arise when performing auto-
matic acquisition of a translation lexicon, as well as
performing lexical selection and surface realization
during generation.
In this work, we tackle these challenges by mak-
ing the following contributions:
• A novel forest-to-string generation algorithm:
Inspired by the work of Chiang (2007), we in-
troduce a novel reduction-based weighted bi-
nary synchronous context-free grammar for-
malism for generation from logical forms (?-
expressions), which can then be integrated with
a probabilistic forest-to-string generation algo-
1611
rithm.
• A novel grammar induction algorithm: To au-
tomatically induce such synchronous grammar
rules, we propose a novel generative model
that establishes phrasal correspondences be-
tween logical sub-expressions and natural lan-
guage word sequences, by extending a previ-
ous model proposed for parsing natural language
into meaning representations (Lu et al., 2008).
To our best knowledge, this is the first probabilis-
tic model for generating sentences from the lambda
calculus encodings of their underlying formal mean-
ing representations, that concerns both surface real-
ization and lexical acquisition. We demonstrate the
effectiveness of our model in Section 5.
2 Related Work
The task of language generation from logical forms
has a long history. Many early works do not rely on
probabilistic approaches. Wang (1980) presented an
approach for generation from an extended predicate
logic formalism using hand-written rules. Shieber
et al. (1990) presented a semantic head-driven ap-
proach for generation from logical forms based on
rules written in Prolog. Shemtov (1996) presented a
system for generation of multiple paraphrases from
ambiguous logical forms. Langkilde (2000) pre-
sented a probabilistic model for generation from a
packed forest meaning representation, without con-
cerning lexical acquisition. Specifically, we are not
aware of any prior work that handles both automatic
unsupervised lexical acquisition and surface realiza-
tion for generation from logical forms in a single
framework.
Another line of research efforts focused on the
task of language generation from other meaning rep-
resentation formalisms. Wong and Mooney (2007a)
as well as Chen and Mooney (2008) made use
of synchronous grammars to transform a variable-
free tree-structured meaning representation into sen-
tences. Lu et al. (2009) presented a language gener-
ation model using the same meaning representation
based on tree conditional random fields. Angeli et
al. (2010) presented a domain-independent proba-
bilistic approach for generation from database en-
tries. All these models are probabilistic models.
Recently there are also substantial research efforts
on the task of mapping natural language to meaning
representations in various formalisms – the inverse
task of language generation called semantic parsing.
Examples include Zettlemoyer and Collins (2005;
2007; 2009), Kate and Mooney (2006), Wong and
Mooney (2007b), Lu et al. (2008), Ge and Mooney
(2009), as well as Kwiatkowski et al. (2010).
Of particular interest is our prior work Lu et al.
(2008), in which we presented a joint generative pro-
cess that produces a hybrid tree structure containing
words, syntactic structures, and meaning represen-
tations, where the meaning representations are in a
variable-free tree-structured form. One important
property of the model in our prior work is that it
induces a hybrid tree structure automatically in an
unsupervised manner, which reveals the correspon-
dences between natural language word sequences
and semantic elements. We extend our prior model
in the next section, so as to support ?-expressions.
The model in turn serves as the basis for inducing
the synchronous grammar rules later.
3 ?-Hybrid Tree
In Lu et al. (2008), a generative model was pre-
sented to model the process that jointly generates
both natural language sentences and their underly-
ing meaning representations of a variable-free tree-
structured form. The model was defined over a
hybrid tree, which consists of meaning representa-
tion tokens as internal nodes and natural language
words as leaves. One limitation of the hybrid tree
model is that it assumes a single fixed tree struc-
ture for the meaning representation. However, ?-
expressions exhibit complex structures and variable
dependencies, and thus it is not obvious how to rep-
resent them in a single tree structure.
In this section, we present a novel ?-hybrid tree
model that provides the following extensions over
the model of Lu et al. (2008):
1. The internal nodes of a meaning representation
tree involve ?-expressions which are not neces-
sarily of variable-free form;
2. The meaning representation has a packed forest
representation, rather than a single determinis-
tic tree structure.
3.1 Packed ?-Meaning Forest
We represent a ?-expression with a packed forest of
meaning representation trees (called ?-meaning for-
1612
est). Multiple different meaning representation trees
(called ?-meaning trees) can be extracted from the
same ?-meaning forest, but they all convey equiva-
lent semantics via reductions, as discussed next.
Constructing a ?-meaning forest for a given ?-
expression requires decomposition of a complete ?-
expression into semantically complete and syntacti-
cally correct sub-expressions in a principled man-
ner. This can be achieved with a process called
higher order unification (Huet, 1975). The process
was known to be very complex and was shown to be
undecidable in unrestricted form (Huet, 1973). Re-
cently a restricted form of higher order unification
was applied to a semantic parsing task (Kwiatkowski
et al., 2010). In this work, we employ a similar tech-
nique for building the ?-meaning forest.
For a given ?-expression e, our algorithm finds ei-
ther two expressions h and f such that (h f) ? e, or
three expressions h, f , and g such that ((h f) g) ?
e, where the symbol? is interpreted as ?-equivalent
after reductions1 (Barendregt, 1985). We then build
the ?-meaning forest based on the expressions h, f ,
and g. In practice, we develop a BUILDFOREST(e)
procedure which recursively builds ?-forests by ap-
plying restricted higher-order unification rules on
top of the ?-expression e. Each node of the ?-forest
is called a ?-production, to which we will give more
details in Section 3.2. For example, once a candi-
date triple (h, f, g) as in ((h f) g) ? e has been
identified, the procedure creates a ?-forest with the
root node being a ?-production involving h, and two
sets of child ?-forests given by BUILDFOREST(f)
and BUILDFOREST(g) respectively. For restricted
higher-order unification, besides the similar assump-
tions made by Kwiatkowski et al. (2010), we also
impose one additional assumption: limited free vari-
able, which states that the expression hmust contain
no more than one free variable. Note that this pro-
cess provides a semantically equivalent packed for-
est representation of the original ?-expression, with-
out altering its semantics in any way.
For better readability, we introduce the symbol
 as an alternative notation for functional appli-
cation. In other words, h  f refers to (h f) or
h(f), and h  f  g refers to ((h f) g). For ex-
1In this work, for reductions, we consider ?-conversions
(changing bound variables) and ?-conversions (applying func-
tors to their arguments).
ample, the expression ?x.state(x)? loc(boston, x)
can be represented as the functional application form
of [?f.?x.f(x) ? loc(boston, x)] ?x.state(x).2
Such a packed forest representation contains ex-
ponentially many tree structures which all convey
the same semantics. We believe such a semantic
representation is more advantageous than the sin-
gle fixed tree-structured representation. In fact, one
could intuitively regard a different decomposition
path as a different way of interpreting the same se-
mantics. Thus, such a representation could poten-
tially accommodate a wider range of natural lan-
guage expressions, which all share the same seman-
tics but with very different word choices, phrase or-
derings, and syntactic structures (like paraphrases).
It may also alleviate the non-isomorphism issue that
was commonly faced by researchers when mapping
meaning representations and sentences (Wong and
Mooney, 2007b). We will validate our belief later
through experiments.
3.2 The Joint Generative Process
. . .
?a : pia  ?b  ?c
w1 ?b : pib  ?d
. . . w4
w2 ?c : pic
w5
w3
Figure 1: The joint generative process of both ?-meaning tree
and its corresponding natural language sentence, which results
in a ?-hybrid tree.
The generative process for a sentence together
with its corresponding ?-meaning tree is illustrated
in Figure 1, which results in a ?-hybrid tree. Internal
nodes of a ?-hybrid tree are called ?-productions,
which are building blocks of a ?-forest. Each
?-production in turn has at most two child ?-
productions. A ?-production has the form ?a : pia 
?b, where ?a is the expected type3 after type evalu-
ation of the terms to its right, pia is a ?-expression
(serves as the functor), and ?b are types of the child
?-productions (as the arguments). The leave nodes
2Throughout this paper, we abuse this notation a bit by al-
lowing the arguments to be types rather than actual expressions,
such as ?y.?x.loc(y, x))  e, which indicates that the functor
?y.?x.loc(y, x) expects an expression of type e to serve as its
argument.
3This work considers basic types: e (entities) and t (truth
values). It also allows function types, e.g., ?e, t? is the type
assigned to functions that map from entities to truth values.
1613
r : ?e, t? 1
?e, t? 1 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 2 : ?f.?g.?x.?y.g(y) ? (f(x) y) ?e, ?e, t?? 1  ?e, t? 2
?e, t? 2 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 1 : ?y.?x.loc(y, x) e 1
runs throughe 1 : miss r
the mississippi
that
?e, t? 2 : ?x.state(x)
states
?e, ?e, t?? 1 : ?y.?x.next to(x, y)
bordering
?e, t? 1 : ?x.state(x)
the states
give me
Figure 2: One example ?-hybrid tree for the sentence “give me the states bordering states that the mississippi runs through” together
with its logical form “?x0.state(x0) ? ?x1.[loc(miss r, x1) ? state(x1) ? next to(x1, x0)]”.
w are contiguous word sequences. The model re-
peatedly generates ?-hybrid sequences, which con-
sist of words intermixed with ?-productions, from
each ?-production at different levels.
Consider part of the example ?-hybrid tree in Fig-
ure 2. The probability associated with generation of
the subtree that spans the sub-sentence “that the mis-
sissippi runs through” can be written as:
P
(
?x.loc(miss r, x), that the mississippi runs through
)
= ?(m? wYw|p1)× ?(that e 1 runs through|p1)
×?(p2|p1, arg1)× ?(m? w|p2)× ?(the mississippi|p2)
where p1 = ?e, t? : ?y.?x.loc(y, x) e 1 , and p2 =
e : miss r.
Following the work of Lu et al. (2008), the gener-
ative process involves three types of parameters ?¯ =
{?, ?, ?}: 1) pattern parameters ?, which model in
what way the words and child ?-productions are in-
termixed; 2) emission parameters ?, which model
the generation process of words from ?-productions,
where either a unigram or a bigram assumption can
be made (Lu et al., 2008); and 3) meaning repre-
sentation (MR) model parameters ?, which model
the generation process from one ?-production to its
child ?-productions. An analogous inside-outside
algorithm (Baker, 1979) used there is employed
here. Since we allow a packed ?-meaning forest rep-
resentation rather than a fixed tree structure, the MR
model parameters ? in this work should be estimated
with the inside-outside algorithm as well, rather than
being estimated directly from the training data by
simple counting, as was done in Lu et al. (2008).
4 The Language Generation Algorithm
Now we present the algorithm for language gener-
ation. We introduce the grammar first, followed by
the features we use. Next, we present the method for
grammar induction, and then discuss the decoder.
4.1 The Grammar
We use a weighted synchronous context free gram-
mar (SCFG) (Aho and Ullman, 1969), which was
previously used in Chiang (2007) for hierarchical
phrase-based machine translation. The grammar is
defined as follows:
? ? ?p? , hw,?? (1)
where ? is the type associated with the ?-production
p?4, and hw is a sequence consisting of natural lan-
guage words intermixed with types. The symbol
? denotes the one-to-one correspondence between
nonterminal occurrences (i.e., in this case types of
?-expressions) in both p? and hw.
We allow a maximum of two nonterminal sym-
bols in each synchronous rule, as was also assumed
in Chiang (2007), which makes the grammar a bi-
nary SCFG. Two example rules are:
?e, t? ?
?
?y.?x.loc(y, x) e 1 , that e 1 runs through
?
e ?
?
miss r, the mississippi
?
where the boxed indices give the correspondences
between nonterminals.
A derivation with the above two synchronous
rules results in the following ?-expression paired
with its natural language counterpart:
4Since type is already indicated by ? , we avoid redundancy
by omitting it when writing p?, without loss of information.
1614
Type 1: ?e, ?e, t?? ?
?
?y.?x.next to(x, y) , bordering
?
?e, t? ?
?
?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2 , ?e, t? 2 ?e, t? 1
?
Type 2: ?e, t? ?
?
?x.loc(miss r, x) ? state(x) , states that the mississippi runs through
?
?e, t? ?
?
?x.loc(miss r, x) , that the mississippi runs through
?
Type 3: ?e, t? ?
?
?f.?x.state(x) ? ?y.[f(y) ? next to(y, x)] ?e, t? 1 , the states bordering ?e, t? 1
?
?e, t? ?
?
?y.?x.loc(y, x) ? state(x) e 1 , states that e 1 runs through
?
Figure 3: Example synchronous rules that can be extracted from the ?-hybrid tree of Figure 2.
?e, t??
?
?x.loc(miss r, x) , that the mississippi runs through
?
where the source side ?-expression is constructed
from the application ?y.?x.loc(y, x) miss r fol-
lowed by a reduction (?-conversion). Assuming the
?-expression to be translated is ?x.loc(miss r, x),
the above rule in fact gives one candidate translation
“that the mississippi runs through”.
4.2 Features
Following the work of Chiang (2007), we assign
scores to derivations with a log-linear model, which
are essentially weighted products of feature values.
For generality, we only consider the following
four simple features in this work:
1. p˜(hw|p?): the relative frequency estimate of a
hybrid sequence hw given the ?-production p?;
2. p˜(p?|hw, ?): the relative frequency estimate of
a ?-production p? given the phrase hw and the
type ? ;
3. exp(?wc(hw)): the number of words gener-
ated, where wc(hw) refers to the number of
words in hw (i.e., word penalty); and
4. pLM (sˆ): the language model score of the gen-
erated sentence sˆ.
The first three features, which are also widely
used in state-of-the-art machine translation models
(Koehn et al., 2003; Chiang, 2007), are rule-specific
and thus can be computed before decoding. The last
feature is computed during the decoding phase in
combination with the sibling rules used.
We score a derivation D with a log-linear model:
w(D) =
(?
r?D
?
i
fi(r)wi
)
× pLM (sˆ)wLM (2)
where r ? D refers to a rule r that appears in
the derivation D, sˆ is the target side (sentence) as-
sociated with the derivation D, and fi is a rule-
specific feature (one of features 1–3 above) which
is weighted with wi. The language model feature is
weighted with wLM .
Once the feature values are computed, our goal is
to find the optimal weight vector w¯? that maximizes
a certain evaluation metric when used for decoding,
as we will discuss in Section 4.4.
Following popular approaches to learning feature
weights in the machine translation community (Och
and Ney, 2004; Chiang, 2005), we use the minimum
error rate training (MERT) (Och, 2003) algorithm to
learn the feature weights that directly optimize cer-
tain automatic evaluation metric. Specifically, the
Z-MERT (Zaidan, 2009) implementation of the al-
gorithm is used in this work.
4.3 Grammar Induction
Automatic induction of the grammar rules as de-
scribed above from training data (which consists
of pairs of ?-expressions and natural language sen-
tences) is a challenging task. Current state-of-the-
art string-based translation systems (Koehn et al.,
2003; Chiang, 2005; Galley and Manning, 2010)
typically begin with a word-aligned corpus to con-
struct phrasal correspondences. Word-alignment in-
formation can be estimated from alignment models,
such as the IBM alignment models (Brown et al.,
1993) and HMM-based alignment models (Vogel et
al., 1996; Liang et al., 2006). However, unlike texts,
logical forms have complex internal structures and
variable dependencies across sub-expressions. It is
not obvious how to establish alignments between
logical terms and texts with such alignment models.
Fortunately, the generative model for ?-hybrid
tree introduced in Section 3 explicitly models the
mappings from ?-sub-expressions to (possibly dis-
contiguous) word sequences with a joint genera-
tive process. This motivates us to extract grammar
rules from the ?-hybrid trees. Thus, we first find
the Viterbi ?-hybrid trees for all training instances,
1615
Tree fragment :
?e, t? 2 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 1 : ?y.?x.loc(y, x) e 1
runs throughe 1 : . . .that
?e, t? 2 : ?x.state(x)
states
Source : (substitution) ?y?.
[
?g.?f.?x.g(x) ? f(x) [?y.?x.loc(y, x) y?] ?x.state(x)
]
 e 1
(two ?-conversions)? ?y?.[?f.?x.loc(y?, x) ? f(x) ?x.state(x)] e 1
(?-conversion)? ?y?.?x.loc(y?, x) ? state(x) e 1
(?-conversion)? ?y.?x.loc(y, x) ? state(x) e 1
Target : “states that e 1 runs through”
Rule : ?e, t? ?
?
?y.?x.loc(y, x) ? state(x) e 1 , states that e 1 runs through
?
Figure 4: Construction of a two-level ?-hybrid sequence rule via substitution and reductions from a tree fragment. Note that the
subtree rooted by e 1 : miss r gets “abstracted” by its type e. The auxiliary variable y? of type e is thus introduced to facilitate the
construction process.
based on the learned parameters of the generative ?-
hybrid tree model.
Next, we extract grammar rules on top of these
?-hybrid trees. Specifically, we extract the follow-
ing three types of synchronous grammar rules, with
examples given in Figure 3:
1. ?-hybrid sequence rules: They are the conven-
tional rules constructed from one ?-production
and its corresponding ?-hybrid sequence.
2. Subtree rules: These rules are constructed from
a complete subtree of the ?-hybrid tree. Each
rule provides a mapping between a complete
sub-expression and a contiguous sub-sentence.
3. Two-level ?-hybrid sequence rules: These rules
are constructed from a tree fragment with one
of its grandchild subtrees (the subtree rooted by
one of its grandchild nodes) being abstracted
with its type only. These rules are constructed
via substitution and reductions.
Figure 4 gives an example based on a tree frag-
ment of the ?-hybrid tree in Figure 2. Note that
the first step makes use of the auxiliary vari-
able y? of type e to represent the grandchild
subtree. ?y? is introduced so as to allow any
?-expression of type e serving as this expres-
sion’s argument to replace y?. In fact, if the
semantics conveyed by the grandchild subtree
serves as its argument, we will obtain the exact
complete semantics of the current subtree. As
we can see, the resulting rule is more general,
and is able to capture longer structural depen-
dencies. Such rules are thus potentially more
useful.
The overall algorithm for learning the grammar
rules is sketched in Figure 5.
4.4 Decoding
Our goal in decoding is to find the most probable
sentence sˆ for a given ?-expression e:
sˆ = s
(
arg max
D s.t. e(D)?e
w(D)
)
(3)
where e(D) refers to the source side (?-expression)
of the derivation D, and s(D) refers to the target
side (natural language sentence) of D.
A conventional CKY-style decoder as used by
Chiang (2007) is not applicable to this work since
the source side does not exhibit a linear structure.
As discussed in Section 3.1, ?-expressions are rep-
resented as packed ?-meaning forests. Thus, in
this work, we make use of a bottom-up dynamic
programming chart-parsing algorithm that works di-
rectly on translating forest nodes into target natural
language words. The algorithm is similar to that of
Langkilde (2000) for generation from an underly-
ing packed semantic forest. Language models are
incorporated when scoring the n-best candidates at
each forest node, where the cube-pruning algorithm
of Chiang (2007) is used. In order to accommodate
type 2 and type 3 rules as discussed in Section 4.3,
whose source side ?-productions are not present in
the nodes of the original ?-meaning forest, new ?-
productions are created (via substitution and reduc-
tions) and attached to the original ?-meaning forest.
1616
Procedures
• f ? BUILDFOREST(e)
It takes in a ?-expression e and outputs its ?-
meaning forest f . (Sec. 3.1)
• ?¯ ? TRAINGENMODEL(f, s)
It takes in ?-meaning forest-sentence pairs (f, s),
performs EM training of the generative model, and
outputs the parameters ?¯. (Sec. 3.2)
• h? FINDHYBRIDTREE(f, s, ?¯)
It finds the most probable ?-hybrid tree h contain-
ing the given f -s pair, under the generative model
parameters ?¯. (Sec. 4.3)
• ?h ? EXTRACTRULES(h)
It takes in a ?-hybrid tree h, and extracts a set of
grammar rules ?h out of it. (Sec. 4.3)
Algorithm
1. Inputs and initializations:
• A training set (e, s), an empty rule set ? = ?
2. Learn the grammar:
• For each ei ? e, find its ?-meaning forest:
fi = BUILDFOREST(ei). This gives the set
(f, s).
• Learn the generative model parameter :
?¯? = TRAINGENMODEL(f, s).
• For each (fi, si) ? (f, s), find the most proba-
ble ?-hybrid tree hi, and then extract the gram-
mar rules from it:
hi = FINDHYBRIDTREE(fi, si, ?¯?)
? = ? ? EXTRACTRULES(hi)
3. Output the learned grammar rule set ?.
Figure 5: The algorithm for learning the grammar rules
5 Experiments
For experiments, we evaluated on the GEOQUERY
dataset, which consists of 880 queries on U.S. geog-
raphy. The dataset was manually labeled with ?-
expressions as their semantics in Zettlemoyer and
Collins (2005). It was used in many previous re-
search efforts on semantic parsing (Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Kwiatkowski et al., 2010).
The original dataset was annotated with English sen-
tences only. In order to assess the generation per-
formance across different languages, in our work
the entire dataset was also manually annotated with
Chinese by a native Chinese speaker with linguistics
background5.
For all the experiments we present in this sec-
tion, we use the same split as that of Kwiatkowski
5The annotator created annotations with both ?-expressions
and corresponding English sentences available as references.
et al. (2010), where 280 instances are used for test-
ing, and the remaining instances are used for learn-
ing. We further split the learning set into two por-
tions, where 500 instances are used for training the
models, which includes induction of grammar rules,
training a language model, and computing feature
values, and the remaining 100 instances are used for
tuning the feature weights.
As we have mentioned earlier, we are not aware
of any previous work that performs generation from
formal logical forms that concerns both lexical ac-
quisition and surface realization. The recent work
by Angeli et al. (2010) presented a generation sys-
tem from database records with an additional focus
on content selection (selection of records and their
subfields for generation). It is not obvious how to
adopt their algorithm in our context where content
selection is not required but the more complex log-
ical semantic representation is used as input. Other
earlier approaches such as the work of Wang (1980)
and Shieber et al. (1990) made use of rule-based
approaches without automatic lexical acquisition.
We thus compare our system against two state-
of-the-art machine translation systems: a phrase-
based translation system, implemented in the Moses
toolkit (Koehn et al., 2007)6, and a hierarchical
phrase-based translation system, implemented in the
Joshua toolkit (Li et al., 2009), which is a reim-
plementation of the original Hiero system (Chiang,
2005; Chiang, 2007). The state-of-the-art unsuper-
vised Berkeley aligner (Liang et al., 2006) with de-
fault setting is used to construct word alignments.
We train a trigram language model with modified
Kneser-Ney smoothing (Chen and Goodman, 1996)
from the training dataset using the SRILM toolkit
(Stolcke, 2002), and use the same language model
for all three systems. We use an n-best list of size
100 for all three systems when performing MERT.
5.1 Automatic Evaluation
For automatic evaluation, we measure the original
IBM BLEU score (Papineni et al., 2002) (4-gram
precision with brevity penalty) and the TER score
(Snover et al., 2006) (the amount of edits required
to change a system output into the reference)7. Note
that TER measures the translation error rate, thus a
6We used the default settings, and enabled the default lexi-
calized reordering model, which yielded better performance.
7We used tercom version 0.7.25 with the default settings.
1617
smaller score indicates a better result. For clarity,
we report 1?TER scores. Following the tuning pro-
cedure as conducted in Galley and Manning (2010),
we perform MERT using BLEU as the metric.
We compare our model against state-of-the-art
statistical machine translation systems. As a base-
line, we first conduct an experiment with the fol-
lowing naive approach: we treat the ?-expressions
as plain texts. All the bound variables (e.g., x
in ?x.state(x)) which do not convey semantics
are removed, but free variables (e.g., state in
?x.state(x)) which might convey semantics are left
intact. Quantifiers and logical connectives are also
left intact. While this naive approach might not ap-
pear very sensible, we merely want to treat it as our
simplest baseline.
Alternatively, analogous to the work of Wong
and Mooney (2007a), we could first parse the ?-
expressions into binary tree structures with a deter-
ministic procedure, and then linearize the tree struc-
ture as a sequence. Since there exists different ways
to linearize a binary tree, we consider preorder, in-
order, and postorder traversal of the trees, and lin-
earize them in these three different ways.
As for our system, during the grammar learning
phase, we initialize the generative model parame-
ters with output from the IBM alignment model 1
(Brown et al., 1993)8, and run the ?-hybrid tree gen-
erative model with the unigram emission assumption
for 10 iterations, followed by another 10 iterations
with the bigram assumption. Grammar rules are then
extracted based on the ?-hybrid trees obtained from
such learned generative model parameters.
Since MERT is prone to search errors, we run each
experiment 5 times with randomly initialized fea-
ture weights, and report the averaged scores. Ex-
perimental results for both English and Chinese are
presented in Table 1. As we can observe, the way
that a meaning representation tree is linearized has
a significant impact on the translation performance.
Interestingly, for both Moses and Joshua, the pre-
order setting yields the best performance for En-
glish, whereas it is inorder that yields the best per-
formance for Chinese. This is perhaps due to the
fact that Chinese presents a very different syntactic
structure and word ordering from English.
8We assume word unigrams are generated from free vari-
ables, quantifiers, and logical connectives in IBM model 1.
Our system, on the other hand, employs a packed
forest representation for ?-expressions. Therefore,
it eliminates the ordering constraint by encompass-
ing exponentially many possible tree structures dur-
ing both the alignment and decoding stage. As a
result, our system obtains significant improvements
in both BLEU and 1?TER using the significance
test under the paired bootstrap resampling method
of Koehn (2004). We obtain p < 0.01 for all cases,
except when comparing against Joshua-preorder for
English, where we obtain p < 0.05 for both metrics.
English Chinese
BLEU 1?TER BLEU 1?TER
Moses
text 48.93 61.08 43.23 51.71
preorder 51.13 63.73 42.08 50.43
inorder 46.72 57.59 48.03 55.29
postorder 44.30 55.05 46.36 54.59
Joshua
text 37.40 48.97 36.60 46.20
preorder 51.40 64.69 40.05 49.70
inorder 40.31 50.47 48.32 54.64
postorder 31.10 42.44 41.31 49.71
This work (t) 54.58 67.65 55.11 63.77
(t) w/o type 2 rules 53.77 66.43 54.30 62.49
(t) w/o type 3 rules 53.68 66.17 50.96 60.13
Table 1: Performance on generating English and Chinese from
?-expressions with automatic evaluation metrics (we report per-
centage scores).
5.2 Human Evaluation
We also conducted human evaluation with 5 eval-
uators each on English and Chinese. We randomly
selected about 50% (139) test instances and obtained
output sentences from the three systems. Moses and
Joshua were run with the top-performing settings in
terms of automatic metrics (i.e., preorder for En-
glish and inorder for Chinese). Following Angeli
et al. (2010), evaluators are instructed to give scores
based on language fluency and semantic correctness,
on the following scale:
Score Language Fluency Semantic Correctness
5 Flawless Perfect
4 Good Near Perfect
3 Non-native Minor Errors
2 Disfluent Major Errors
1 Gibberish Completely Wrong
For each test instance, we first randomly shuffled
the output sentences of the three systems, and pre-
sented them together with the correct reference to
the evaluators. The evaluators were then asked to
score all the output sentences at once. This eval-
uation process not only ensures that the annotators
have no access to which system generated the out-
1618
English Judge E1 Judge E2 Judge E3 Judge E4 Judge E5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEM
Moses preorder 4.56 4.57 4.58 4.54 4.52 4.52 4.48 4.14 4.28 4.22 4.48 ± 0.12 4.40 ± 0.20
Joshua preorder 4.50 4.43 4.49 4.29 4.44 4.36 4.46 4.04 4.12 4.06 4.40 ± 0.16 4.24 ± 0.18
This work 4.76 4.73 4.73 4.70 4.68 4.60 4.64 4.37 4.49 4.44 4.66 ± 0.10 4.57 ± 0.16
Chinese Judge C1 Judge C2 Judge C3 Judge C4 Judge C5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEM
Moses inorder 4.38 4.22 3.95 3.99 4.01 3.80 4.27 4.19 4.09 4.01 4.14 ± 0.18 4.04 ± 0.17
Joshua inorder 4.32 4.04 3.74 3.91 3.76 3.55 4.21 4.04 3.96 3.97 4.00 ± 0.26 3.90 ± 0.21
This work 4.61 4.47 4.53 4.43 4.50 4.31 4.71 4.55 4.57 4.32 4.59 ± 0.08 4.42 ± 0.10
Table 2: Human evaluation results on English and Chinese generation. FLU: language fluency; SEM: semantic correctness.
put, but also minimizes bias associated with scor-
ing different outputs for the same input. The de-
tailed and averaged results (with one standard devi-
ation) for human evaluation are presented in Table 2
for English and Chinese respectively. For both lan-
guages, our system achieves a significant improve-
ment over Moses and Joshua (p < 0.01 with paired
t-tests), in terms of both language fluency and se-
mantic correctness. This set of results is important,
as it demonstrates that our system produces more
fluent texts with more accurate semantics when per-
ceived by real humans.
5.3 Additional Experiments
We also performed the following additional experi-
ments. First, we attempted to increase the number
of EM iterations (to 100) when training the model
with the bigram assumption, so as to assess the ef-
fect of the number of EM iterations on the final gen-
eration performance. We observed similar perfor-
mance. Second, in order to assess the importance of
the two types of novel rules – subtree rules (type 2)
and two-level ?-hybrid sequence rules (type 3), we
also conducted experiments without these rules for
generation. Experiments show that these two types
of rules are important. Specifically, type 3 rules,
which are able to capture longer structural depen-
dencies, are of particular importance for generating
Chinese. Detailed results for these additional exper-
iments are presented in Table 1.
5.4 Experiments on Variable-free Meaning
Representations
Finally, we also assess the effectiveness of our
model on an alternative meaning representation for-
malism in the form of variable-free tree structures.
Specifically, we tested on the ROBOCUP dataset
(Kuhlmann et al., 2004), which consists of 300
English instructions for coaching robots for soc-
cer games, and a variable-free version of the GEO-
QUERY dataset. These are the standard datasets
used in the generation tasks of Wong and Mooney
(2007a) and Lu et al. (2009). Similar to the tech-
nique introduced in Kwiatkowski et al. (2010), our
proposed algorithm could still be applied to such
datasets by writing the tree-structured representa-
tions as function-arguments forms. The higher order
unification-based decomposition algorithm could be
applied on top of such forms accordingly. For exam-
ple, midfield(opp) ? ?x.midfield(x)  opp. See
Kwiatkowski et al. (2010) for more details. How-
ever, since such forms present monotonous struc-
tures, and thus give less alternative options in the
higher-order unification-based decomposition pro-
cess, it prevents the algorithm from creating many
disjunctive nodes in the packed forest. It is thus hy-
pothesized that the advantages of the packed forest
representation could not be fully exploited with such
a meaning representation formalism.
Following previous works, we performed 4 runs
of 10-fold cross validation based on the same split
as that of Wong and Mooney (2007a) and Lu et
al. (2009), and measured standard BLEU percent-
age and NIST (Doddington, 2002) scores. For ex-
perimentation on each fold, we trained a trigram
language model on the training data of that fold,
and randomly selected 70% of the training data
for grammar induction, with the remaining 30%
for learning of the feature weights using MERT.
Next, we performed grammar induction with the
complete training data of that fold, and used the
learned feature weights for decoding of the test in-
stances. The averaged results are shown in Ta-
ble 3. Our approach outperforms the previous sys-
tem WASP?1++ (Wong and Mooney, 2007a) sig-
nificantly, and achieves comparable or slightly bet-
ter performance as compared to Lu et al. (2009).
This set of results is particularly striking. We note
1619
Variable-present dataset
?-expression : argmax(x, river(x) ? ?y.[state(y) ? next to(y, india s) ? loc(x, y)], len(x))
Reference : what is the longest river that flows through a state that borders indiana
Moses : what is the states that border long indiana
Joshua : what is the longest river surrounding states border indiana
This work : what is the longest river in the states that border indiana
?-expression : density(?x.loc(argmax(y, loc(y, usa co) ? river(y), size(y)), x) ? state(x))
Reference : which is the density of the state that the largest river in the united states runs through
Moses : what is the population density in lie on the state with the smallest state in the us
Joshua : what is the population density of states lie on the smallest state in the us
This work : what is the population density of the state with the largest river in the us
Variable-free datasets
?-expression : population(largest one density(state all))
Reference : what is the population of the state with the highest population density
This work : how many people live in the state with the largest population density
?-expression : rule(and(bpos(from goal line(our, jnum(n0.0, n32.0))), not(bpos(left(penalty area(our))))),-
dont(player our(n3), intercept))
Reference : player 3 should not intercept the ball if the ball is within 32 meters of our goal line and not in our left penalty area
This work : if the ball is within 32 meters from our goal line and not on the left side of our penalty area then player 3 should not
intercept it
Figure 6: Sample English outputs for various datasets. For the variable-present dataset, we also show outputs from Moses and
Joshua.
that the algorithm of Lu et al. (2009) is capable
of modeling dependencies over phrases, which gives
global optimization over the sentence generated, and
works by building conditional random fields (Laf-
ferty et al., 2001) over trees. But the algorithm of
Lu et al. (2009) is also limited to handling tree-
structured meaning representation, and is therefore
unable to accept inputs such as the variable ver-
sion of ?-expressions. Our algorithm works well
by introducing additional new types of synchronous
rules that are able to capture longer range depen-
dencies. WASP?1++, on the other hand, also makes
use of a synchronous parsing-based statistical ma-
chine translation approach. Their system, however,
requires linearization of the tree structure for both
alignment and translation. In contrast, our model
directly performs alignment and translation from a
packed forest representation to a sentence. As a
result, though WASP?1++ made use of additional
features (lexical weights), our system yielded bet-
ter performance. Sample English output sentences
are given in Figure 6.
Robocup Geoquery
BLEU NIST BLEU NIST
WASP?1++ 60.22 6.8976 53.70 6.4808
Lu et al. (2009) 62.20 6.9845 57.33 6.7459
This work 62.45 7.0011 57.62 6.6867
Table 3: Performance on variable-free representations
6 Conclusions and Future Work
In this work, we presented a novel algorithm for gen-
erating natural language sentences from their under-
lying semantics in the form of typed lambda calcu-
lus. We tackled the problem by introducing a novel
reduction-based weighted synchronous context-free
grammar formalism, which allows sentence genera-
tion with a log-linear model. In addition, we pro-
posed a novel generative model that jointly gener-
ates lambda calculus expressions and natural lan-
guage sentences. The model is then used for auto-
matic grammar induction. Empirical results show
that our model outperforms state-of-the-art machine
translation models, for both English and Chinese,
in terms of both automatic and human evaluation.
Furthermore, we have demonstrated that the model
can also effectively handle inputs with a variable-
free version of meaning representation.
We believe the algorithm used for inducing the
reduction-based synchronous grammar rules may
find applications in other research problems, such
as statistical machine translation and phrasal syn-
chronous grammar induction. We are interested in
exploring further along such directions in the future.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore. We would like to thank Tom
Kwiatkowski and Luke Zettlemoyer for sharing their
dataset, and Omar F. Zaidan for his help with Z-
MERT.
1620
References
A.V. Aho and J.D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3(1):37–56.
G. Angeli, P. Liang, and D. Klein. 2010. A simple
domain-independent probabilistic approach to gener-
ation. In Proc. EMNLP, pages 502–512.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. The Journal of the Acoustical Society of Amer-
ica, 65:S132.
H. P. Barendregt. 1985. The Lambda Calculus, Its Syntax
and Semantics (Studies in Logic and the Foundations
of Mathematics, Volume 103). Revised Edition. North-
Holland.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19:263–311.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
ACL, pages 310–318.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: a test of grounded language acquisition. In
Proc. ICML, pages 128–135.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL, pages
263–270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201–228.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. HLT, pages 138–145.
M. Galley and C. D. Manning. 2010. Accurate
non-hierarchical phrase-based translation. In Proc.
HLT/NAACL, pages 966–974.
R. Ge and R. J. Mooney. 2009. Learning a compositional
semantic parser using an existing syntactic parser. In
Proc. ACL/IJCNLP, pages 611–619.
G. P. Huet. 1973. The undecidability of unification in
third order logic. Information and Control, 22(3):257–
267.
G. P. Huet. 1975. A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1(1):27–57.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. COLING/ACL,
pages 913–920.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. NAACL/HLT, pages
48–54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. ACL (Demon-
stration Sessions), pages 177–180.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. EMNLP, pages 388–
395.
G. Kuhlmann, P. Stone, R. Mooney, and J. Shavlik. 2004.
Guiding a reinforcement learner with natural language
advice: Initial results in RoboCup soccer. In Proc.
AAAI Workshop on Supervisory Control of Learning
and Adaptive Systems.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Proc. EMNLP, pages 1223–1233.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML, pages 282–289.
I. Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proc. NAACL, pages 170–177.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W. N. G. Thornton,
J. Weese, and O. F. Zaidan. 2009. Joshua: an open
source toolkit for parsing-based machine translation.
In Proc. WMT, pages 135–139.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. HLT/NAACL, pages 104–111.
W. Lu, H. T. Ng, W. S. Lee, and L. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proc. EMNLP, pages
783–792.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Proc. EMNLP, pages 400–409.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417–449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311–318.
H. Shemtov. 1996. Generation of paraphrases from am-
biguous logical forms. In Proc. COLING, pages 919–
924.
S. M. Shieber, G. van Noord, F. C. N. Pereira, and
R. C. Moore. 1990. Semantic-head-driven generation.
Computational Linguistics, 16(1):30–42.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223–231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Proc. ICSLP, pages 901–904.
1621
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc.
COLING, pages 836–841.
J. Wang. 1980. On computational sentence generation
from logical form. In Proc. COLING, pages 405–411.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
Proc. HLT/NAACL, pages 439–446.
Y. W. Wong and R. J. Mooney. 2007a. Generation by in-
verting a semantic parser that uses statistical machine
translation. In Proc. NAACL/HLT, pages 172–179.
Y. W. Wong and R. J. Mooney. 2007b. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proc. ACL, pages 960–967.
O. F. Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79–88.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
UAI, pages 658–666.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
Proc. EMNLP-CoNLL, pages 678–687.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In Proc. ACL/IJCNLP, pages 976–984.
1622
