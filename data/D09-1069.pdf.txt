Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 658–667,
Singapore, 6-7 August 2009. c©2009 ACL and AFNLP
Can Chinese Phonemes Improve Machine Transliteration?:
A Comparative Study of English-to-Chinese Transliteration Models
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
Inspired by the success of English
grapheme-to-phoneme research in speech
synthesis, many researchers have pro-
posed phoneme-based English-to-Chinese
transliteration models. However, such ap-
proaches have severely suffered from the
errors in Chinese phoneme-to-grapheme
conversion. To address this issue,
we propose a new English-to-Chinese
transliteration model and make system-
atic comparisons with the conventional
models. Our proposed model relies on
the joint use of Chinese phonemes and
their corresponding English graphemes
and phonemes. Experiments showed that
Chinese phonemes in our proposed model
can contribute to the performance im-
provement in English-to-Chinese translit-
eration.
1 Introduction
1.1 Motivation
Transliteration, i.e., phonetic translation, is com-
monly used to translate proper names and techni-
cal terms across languages. A variety of English-
to-Chinese machine transliteration models has
been proposed in the last decade (Meng et al.,
2001; Gao et al., 2004; Jiang et al., 2007; Lee
and Chang, 2003; Li et al., 2004; Li et al., 2007;
Wan and Verspoor, 1998; Virga and Khudanpur,
2003). They can be categorized into those based
on Chinese phonemes (Meng et al., 2001; Gao
et al., 2004; Jiang et al., 2007; Lee and Chang,
2003; Wan and Verspoor, 1998; Virga and Khu-
danpur, 2003) and those that don’t rely on Chinese
phonemes (Li et al., 2004; Li et al., 2007).
Inspired by the success of English grapheme-to-
phoneme research in speech synthesis, many re-
searchers have proposed phoneme-based English-
to-Chinese transliteration models. In these ap-
proaches, Chinese phonemes are generated from
English graphemes or phonemes, and then the
Chinese phonemes are converted into Chinese
graphemes (or characters), where Chinese Pinyin
strings1 are used for representing a syllable-level
Chinese phoneme sequence. Despite its high ac-
curacy in generating Chinese phonemes from En-
glish, this approach has severely suffered from er-
rors in Chinese phoneme-to-grapheme conversion,
mainly caused by Chinese homophone confusion
– one Chinese Pinyin string can correspond to sev-
eral Chinese characters (Li et al., 2004). For ex-
ample, the Pinyin string “LI” corresponds to such
different Chinese characters as,, and. For
this reason, it has been reported that English-to-
Chinese transliteration without Chinese phonemes
outperforms that with Chinese phonemes (Li et al.,
2004).
Then “Can Chinese phonemes improve
English-to-Chinese transliteration, if we can re-
duce the errors in Chinese phoneme-to-grapheme
conversion?” Our research starts from this
question.
1.2 Our Approach
Previous approaches using Chinese phonemes
have relied only on Chinese phonemes in Chi-
nese phoneme-to-grapheme conversion. However,
the simple use of Chinese phonemes doesn’t al-
ways provide a good clue to reduce the ambi-
guity in Chinese phoneme-to-grapheme conver-
sion. Let us explain with an example, the Chinese
transliteration of Greeley in Table 1, where Chi-
nese phonemes are represented in terms of Chi-
nese Pinyin strings and English phonemes are rep-
resented by ARPAbet symbols2.
In Table 1, Chinese Pinyin string “LI” corre-
sponds to two different Chinese characters, and
1Pinyin, the most commonly used Romanization sys-
tem for Chinese characters, faithfully represents Chinese
658
Table 1: Chinese Pinyin string “LI” and its corre-
sponding Chinese characters in Chinese transliter-
ation of Greeley
English grapheme g ree ley
English phoneme G R IY L IY
Chinese Pinyin GE LI LI
Chinese character   
. It seems difficult to find evidence for select-
ing the correct Chinese character corresponding to
each Chinese Pinyin string “LI” by just looking
at the sequence of Chinese Pinyin strings “GE LI
LI.” However, English graphemes (ree and ley) or
phonemes (“R IY” and “L IY”) corresponding to
Chinese Pinyin string “LI”, especially their conso-
nant parts (r and l in the English graphemes and
“R” and “L” in the English phonemes), provide
strong evidence to resolve the ambiguity. Thus,
we can easily find rules for the conversion from
Chinese Pinyin string “LI” to and as follows:
• ? “R IY”, LI ? ?
• ? “L IY”, LI ? ?
Based on the observation, we propose an
English-to-Chinese transliteration model based on
the joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes. We
define a set of English-to-Chinese transliteration
models and categorize them into the following
three classes:
• M
I
: Models Independent of Chinese
phonemes
• M
S
: Models based on Simple use of Chinese
phonemes
• M
J
: Models based on Joint use of Chi-
nese phonemes and English graphemes and
phonemes that correspond to our proposed
model.
Our comparison among the three types of translit-
eration models can be summarized as follows.
• The M
I
models relying on either English
graphemes or phonemes could not outper-
form those based on both English graphemes
and phonemes.
phonemes and syllables (Yin and Felley, 1990).
2http://www.cs.cmu.edu/
˜
laura/pages/
arpabet.ps
• The M
S
models always showed the worst
performance due to the severe error rate in
Chinese phoneme-to-grapheme conversion.
• The M
J
models significantly reduced er-
rors in Chinese phoneme-to-grapheme con-
version; thus they achieved the best perfor-
mance.
The rest of this paper is organized as follows.
Section 2 introduces the notations used through-
out this paper. Section 3 describes the translitera-
tion models we compared. Section 4 describes our
tests and results. Section 5 concludes the paper
with a summary.
2 Preliminaries
Let E
G
be an English word composed of n English
graphemes, and let E
P
be a sequence of English
phonemes that represents the pronunciation of E
G
.
Let C
G
be a sequence of Chinese graphemes cor-
responding to the Chinese transliteration of E
G
,
and let C
P
be a sequence of Chinese phonemes
that represents the pronunciation of C
G
.
C
P
corresponds to a sequence of the Chinese
Pinyin strings of C
G
. Because a Chinese Pinyin
string represents the pronunciation of a sylla-
ble consisting of consonants and vowels, we di-
vide a Chinese Pinyin string into consonant and
vowel parts like “L+I”, “L+I+N”, and “SH+A.”
In this paper, we define a Chinese phoneme
as the vowel and consonant parts in a Chinese
Pinyin string (e.g., “L”, “SH”, and “I”). A Chi-
nese character usually corresponds to multiple
English graphemes, English phonemes, and Chi-
nese phonemes (i.e.,  corresponds to English
graphemes ree, English phonemes “R IY”, and
Chinese phonemes “L I” in Table 1). To repre-
sent these many-to-one correspondences, we use
the well-known BIO labeling scheme to represent
a Chinese character, where B and I represent the
beginning and inside/end of the Chinese charac-
ters, respectively, and O is not used. Each Chi-
nese phoneme corresponds to a Chinese character
with B and I labels. For example, Chinese charac-
ter “” in Table 1 can be represented as “:B”
and “:I”, where “:B” and “:I” correspond
to Chinese phonemes “L” and “I”, respectively. In
this paper, we define a Chinese grapheme as a Chi-
nese character represented with a BIO label, e.g.,
“:B” and “:I.”
659
Table 2: eg
i
and its corresponding ep
i
, cp
i
, and cg
i
in Greeley and its corresponding Chinese translit-
eration “”
i 1 2 3 4 5 6 7
E
G
g r e e l e y
E
P
G R IY ? L IY ?
C
P
GE L I ? L I ?
GE LI ? LI ?
C
G
:B :B :I ? :B :I ?
  ?  ?
Then E
P
, C
P
, and C
G
can be segmented into a
series of sub-strings, each of which corresponds to
an English grapheme in E
G
. We can thus write
• E
G
= eg
1
, · · · , eg
n
= eg
n
1
• E
P
= ep
1
, · · · , ep
n
= ep
n
1
• C
P
= cp
1
, · · · , cp
n
= cp
n
1
• C
G
= cg
1
, · · · , cg
n
= cg
n
1
where eg
i
, ep
i
, cp
i
, and cg
i
represent the ith
English grapheme, English phonemes, Chinese
phonemes, and Chinese graphemes corresponding
to eg
i
, respectively.
Based on the definition, we model English-
to-Chinese transliteration so that each English
grapheme is tagged with its corresponding En-
glish phonemes, Chinese phonemes, and Chinese
graphemes. Table 2 illustrates eg
i
, ep
i
, cp
i
, and
cg
i
with the same example listed in Table 1 (En-
glish word Greeley and its corresponding Chinese
transliteration “”)3, where ? represents an
empty string.
3 Transliteration Model
We defined eighteen transliteration models to be
compared. These transliteration models are clas-
sified into three classes, M
I
, M
S
, andM
J
as de-
scribed in Section 1.2; each class has three basic
transliteration models and three hybrid ones. In
this section, we first describe the basic translit-
eration models in each class by focusing on the
main difference among the three classes and then
describe the hybrid transliteration models.
3We performed alignment between E
G
and E
P
and be-
tween E
P
and C
P
in a similar manner presented in Li et al.
(2004). Then the two alignment results were merged using
E
P
as a pivot. Finally, we made a correspondence relation
among eg
i
, ep
i
, cp
i
, and cg
i
using the merged alignment re-
sult and the Pinyin table.
3.1 Basic Transliteration Models
The basic transliteration models in each class are
denoted as M(x, y).
• (x, y) ? X × Y
• x ? X = {E
G
, E
P
, E
GP
}
• y ? Y = {?, C
P
, JC
P
}
x is an English-side parameter representing En-
glish grapheme (E
G
), English phoneme (E
P
), and
the joint use of English grapheme and phoneme
(E
GP
= ?E
G
, E
P
?) that contributes to generat-
ing Chinese phonemes or Chinese graphemes in
a transliteration model. y is a Chinese-phoneme
parameter that represents a way of using Chinese
phonemes to generate Chinese graphemes in a
transliteration model. Since M(x, ?) represents
a transliteration model that does not rely on Chi-
nese phonemes, it falls intoM
I
, while M(x, C
P
)
corresponds to a transliteration model in M
S
that
only uses Chinese phonemes in Chinese phoneme-
to-grapheme conversion. M(x, JC
P
) is a translit-
eration model in theM
J
class that generates Chi-
nese transliterations based on joint use of x and
Chinese phoneme C
P
, where x ? X . Thus,
M(x, JC
P
) can be rewritten as M(x, ?x, C
P
?),
where the joint representation of x and C
P
,
?x, C
P
?, is used in Chinese phoneme-to-grapheme
conversion. The three basic models inM
J
can be
interpreted as follows:
• M(E
G
, JC
P
) = M(E
G
, ?E
G
, C
P
?)
• M(E
P
, JC
P
) = M(E
P
, ?E
P
, C
P
?)
• M(E
GP
, JC
P
) = M(E
GP
, ?E
GP
, C
P
?)
M(E
G
, JC
P
) directly converts English
graphemes into Chinese phonemes without
the help of English phonemes and then gener-
ates Chinese transliterations based on the joint
representation of English graphemes and Chi-
nese phonemes. The main difference between
M(E
P
, JC
P
) and M(E
GP
, JC
P
) lies in the
use of English graphemes to generate Chinese
phonemes and graphemes. English graphemes
are only used in English grapheme-to-phoneme
conversion, and English phonemes play a crucial
role for generating Chinese transliteration in
M(E
P
, JC
P
). Chinese phoneme-to-grapheme
conversion that relies on the joint use of English
graphemes, English phonemes, and Chinese
660
PM(E
G
,JC
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)× P (C
G
|E
G
, C
P
) (1)
P
M(E
P
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)× P (C
P
|E
P
)× P (C
G
|E
P
, C
P
) (2)
P
M(E
GP
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)× P (C
P
|E
G
, E
P
)× P (C
G
|E
G
, E
P
, C
P
) (3)
P
M(E
G
,C
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)× P (C
G
|C
P
) (4)
P
M(E
P
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)× P (C
P
|E
P
)× P (C
G
|C
P
) (5)
P
M(E
GP
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)× P (C
P
|E
G
, E
P
)× P (C
G
|C
P
) (6)
phonemes is the key feature of M(E
GP
, JC
P
).
Because M(x, JC
P
) can be interpreted as
M(x, ?x, C
P
?), English-side parameter x de-
termines the English graphemes and phonemes,
or both jointly used with Chinese phonemes in
Chinese phoneme-to-grapheme conversion. Then
we can represent the three basic transliteration
models as in Eqs. (1)–(3), where P (C
G
|E
G
, C
P
),
P (C
G
|E
P
, C
P
), and P (C
G
|E
G
, E
P
, C
P
) are the
key points in our proposed models,M
J
.
The three basic transliteration models in M
S
– M(E
G
, C
P
), M(E
P
, C
P
), and M(E
GP
, C
P
) –
are formulated as Eqs. (4)–(6). Chinese phoneme-
based transliteration models in the literature fall
into either M(E
G
, C
P
) or M(E
P
, C
P
) (Meng et
al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee
and Chang, 2003; Wan and Verspoor, 1998; Virga
and Khudanpur, 2003). The three basic transliter-
ation models inM
S
are identical as those inM
J
,
except for the Chinese phoneme-to-grapheme con-
version method. They only depend on Chinese
phonemes in Chinese phoneme-to-grapheme con-
version represented as P (C
G
|C
P
) in Eqs. (4)–(6).
P
M(E
G
,?)
(C
G
|E
G
) = P (C
G
|E
G
) (7)
P
M(E
P
,?)
(C
G
|E
G
) (8)
=
?
?E
P
P (E
P
|E
G
)× P (C
G
|E
P
)
P
M(E
GP
,?)
(C
G
|E
G
) (9)
=
?
?E
P
P (E
P
|E
G
)× P (C
G
|E
G
, E
P
)
The three basic transliteration models in M
I
are
represented in Eqs. (7)–(9). Because theM
I
mod-
els are independent of Chinese phonemes, they are
the same as the transliteration models in the lit-
erature used for machine transliteration from En-
glish to other languages without relying on target-
language phonemes (Karimi et al., 2007; Malik,
2006; Oh et al., 2006; Sherif and Kondrak, 2007;
Yoon et al., 2007). Note that M(E
G
, ?) is the
same transliteration model as the one proposed by
Li et al. (2004).
3.2 Hybrid Transliteration Models
The hybrid transliteration models in each class
are defined by discrete mixture between the prob-
ability distribution of the two basic transliter-
ation models, as in Eq. (10) (Al-Onaizan and
Knight, 2002; Oh et al., 2006), where 0 < ? <
1. We denote a hybrid transliteration model be-
tween two basic transliteration models M(x
1
, y)
and M(x
2
, y) as M(x
1
+ x
2
, y, ?), where y ?
Y = {?, C
P
, JC
P
}, x
1
6= x
2
, and x
1
, x
2
?
X = {E
G
, E
P
, E
GP
}. In this paper, we define
three types of hybrid transliteration models in each
class: M(E
G
+ E
P
, y, ?), M(E
G
+ E
GP
, y, ?),
and M(E
P
+ E
GP
, y, ?).
P
M(x
1
+x
2
,y,?)
(C
G
|E
G
) (10)
= ? × P
M(x
1
,y)
(C
G
|E
G
)
+ (1? ?) × P
M(x
2
,y)
(C
G
|E
G
)
3.3 Probability Estimation
Because Eqs. (1)–(9) can be estimated in a similar
way, we limit our focus to Eq. (3) in this section.
Assuming that P (E
P
|E
G
), P (C
P
|E
G
, E
P
), and
P (C
G
|E
G
, E
P
, C
P
) in Eq. (3) depend on the size
of the context window, k (k = 3 in this paper),
661
Table 3: Feature functions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
) with an example in Table 2, where i = 2
f
1
gram
3
(eg
i
) eg
i+2
i
= “ree” cg
i
= “:B”
f
2
pair
11
(cp
i?1
, cg
i?1
) cp
i?1
= “G”, cg
i?1
= “:B” cg
i
= “:B”
f
3
pair
12
(cg
i?1
, cp
i?1
) cp
i
i?1
= “GE L”, cg
i?1
= “:B” cg
i
= “:B”
f
4
pair
22
(cp
i?1
, cg
i?2
) eg
i
i?1
= “gr”, epi
i?1
= “G R” cg
i
= “:B”
f
5
triple
1
(eg
i
, cp
i
, cg
i?1
) eg
i
= “r”, cp
i?1
= “GE”, cg
i?1
= “:B” cg
i
= “:B”
f
6
triple
2
(eg
i?1
, cg
i?1
, cp
i?1
) eg
i?1
= “g”, cpi
i?1
= “GE L”, cg
i?1
= “:B” cg
i
= “:B”
they can be simplified into a series of products in
Eqs. (11)–(13).
The maximum entropy model is used to esti-
mate the probabilities in Eqs. (11)–(13) (Berger
et al., 1996). Generally, a conditional maxi-
mum entropy model is an exponential model that
gives the conditional probability, as described in
Eq. (14), where ?
i
is the parameter to be estimated
and f
i
(a, b) is a feature function corresponding to
?
i
(Berger et al., 1996; Ratnaparkhi, 1997):
P (E
P
|E
G
) ?
?
i
P (ep
i
|ep
i?1
i?k
, eg
i+k
i?k
) (11)
P (C
P
|E
G
, E
P
) (12)
?
?
i
P (cp
i
|cp
i?1
i?k
, ?eg, ep?
i+k
i?k
)
P (C
G
|E
G
, E
P
, C
P
) (13)
?
?
i
P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
)
P (b|a) =
exp(
?
i
?
i
f
i
(a, b))
?
b
?
exp(
?
i
?
i
f
i
(a, b
?
))
(14)
f
i
(a, b) is a binary function returning TRUE
or FALSE based on context a and output b.
If f
i
(a, b)=1, its corresponding model parame-
ter ?
i
contributes toward conditional probability
P (b|a) (Berger et al., 1996; Ratnaparkhi, 1997).
The feature functions used here are defined in
terms of context predicates — a function return-
ing TRUE or FALSE that depends on the presence
of the information in the current context (Ratna-
parkhi, 1997). Context predicates and their de-
scriptions used are given in Table 4.
N-GRAM includes gram
1
(u
j
), gram
2
(u
j
), and
gram
3
(u
j
) corresponding to a unigram, a bigram,
and a trigram, respectively. PAIR includes a pair of
unigrams (pair
11
), unigram and bigram (pair
12
),
and bigrams (pair
22
). TRIPLE includes a triple of
three unigrams (triple
1
) and a triple of two uni-
grams and one bigram (triple
2
). Note that if dif-
ferent context predicates represent the same con-
text, we accept one of them and ignore the others
Table 4: Context predicates and their descriptions
Category Context predicates Description
N-GRAM gram
1
(u
j
) u
j
gram
2
(u
j
) uj+1
j
gram
3
(u
j
) uj+2
j
PAIR pair
11
(u
j
, v
k
) u
j
, v
k
pair
12
(u
j
, v
k
) u
j
, v
k+1
k
pair
22
(u
j
, v
k
) u
j+1
j
, v
k+1
k
TRIPLE triple
1
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l
triple
2
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l+1
l
(e.g., pair
12
(u
j
, u
j+1
) = trigram(u
j
) = u
j+2
j
).
Table 3 represents the examples of feature func-
tions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
).
We used the “Maximum Entropy Modeling
Toolkit”4 to estimate the probabilities and the
LBFGS algorithm to find ?
i
in Eq. (14). For
each transliteration model, we produced n-best
transliterations using a stack decoder (Schwartz
and Chow, 1990).
3.4 Summary
In this paper, we defined eighteen transliteration
models to be compared. There are six translitera-
tion models, three basic and three hybrid ones, in
each class, M
I
, M
S
, and M
J
. We compared the
transliteration models from the viewpoint of Chi-
nese phonemes or the class of transliteration mod-
els in our experiments.
4 Testing and Results
We used the same test set used in Li et al. (2004)
for our testing5. It contains 37,694 pairs of English
words and their official Chinese transliterations
4Available at http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html
5This test set was also used in “NEWS09 machine translit-
eration shared task” for English-to-Chinese transliteration (Li
et al., 2009)
662
extracted from the “Chinese Transliteration of For-
eign Personal Names” (Xinhua News Agency,
1992), which includes names in English, French,
German, and many other foreign languages (Li et
al., 2004). We used the same test data as in Li et
al. (2004). But we randomly selected 90% of the
training data used in Li et al. (2004) as our training
data and the remainder as the development data, as
shown in Table 5.
Table 5: Number of English-Chinese translitera-
tion pairs in each data set
Ours Li et al. (2004)
Training data 31,299 34,777
Development data 3,478 N/A
Blind test data 2,896 2,896
We used the training data for training the
transliteration models. For each model, we tuned
the parameters including the number of iterations
for training the maximum entropy model and a
Gaussian prior for smoothing the maximum en-
tropy model using the development data. Further,
the development data was used to select param-
eter ? of the hybrid transliteration models. We
varied parameter ? from 0 to 1 in 0.1 intervals
(i.e., ?=0, 0.1, 0.2, · · · ,1) and tested the perfor-
mance of the hybrid models with the development
data. Then we chose ? that showed the best per-
formance in each hybrid model. The blind test
data was used for evaluating the performance of
each transliteration model. The CMU Pronounc-
ing Dictionary6, which contains about 120,000
English words and their pronunciations, was used
for estimating P (E
P
|E
G
).
We conducted two experiments. First, we com-
pared the overall performance of the translitera-
tion models. Second, we investigated the effect
of training data size on the performance of each
transliteration model.
The evaluation was done for word accuracy
in top-1 (ACC), Chinese pronunciation accuracy
(CPA) and a mean reciprocal rank (MRR) met-
ric (Kantor and Voorhees, 2000; Li et al., 2009;
Chang et al., 2009). ACC measures how many
correct transliterations appeared in the top-1 re-
sult of each system. CPA measures the Chinese
pronunciation accuracy in the top-1 of the n-best
Chinese pronunciation. We used CPA for com-
6Available at http://www.speech.cs.cmu.edu/
cgi-bin/cmudict
paring the performance between systems based on
Chinese phonemes. MRR, mean reciprocal ranks
of n-best results of each system over the test en-
tries, is an evaluation measure for n-best translit-
erations. If a transliteration generated by a system
matches a reference transliteration7 at the rth posi-
tion of the n-best results, its reciprocal rank equals
1/r; otherwise its reciprocal rank equals 0, where
1 ? r ? n. We produced 10-best Chinese translit-
erations for each English word in our experiments.
4.1 Comparison of the Overall Performance
Table 6 represents the overall performance of one
system in a previous work (Li et al., 2004) and
eighteen systems based on the transliteration mod-
els defined in this paper. ACC, MRR, and CPA
represent the evaluation results for each model
trained by our training data. To test transliteration
models without the errors introduced by incorrect
Chinese phonemes, we carried out the experiments
with the correct Chinese pronunciation (or the
correct Chinese phoneme sequence) in Chinese
phoneme-to-grapheme conversion. In the exper-
iment, we put the correct Chinese pronunciation
into the top-1 of the n-best Chinese pronunciation
with the highest probability, say P (C
P
|E
G
)=1;
thus CPA was assumed to be 100%. The ACC
of the transliteration models under this condition
is denoted as ACC’ in Table 6. TRAIN represents
the evaluation results of the transliteration mod-
els trained by our training data. To compare Li
et al. (2004) and transliteration models defined in
this paper under the same condition, we also car-
ried out experiments with the same training data
in Li et al. (2004). Since the training data used
in Li et al. (2004) is identical as the union of
our training and development data, we denoted it
as TRAIN+DEV in Table 6. In both TRAIN and
TRAIN+DEV, we used the same parameter setting
that was obtained by using the development data.
LI04 represents a system in Li et al. (2004),
and its ACC’ in TRAIN+DEV is taken from the
literature. The systems based on the translitera-
tion models defined in our paper are represented
from the second row in Table 6. The phoneme-
based transliteration models in the literature cor-
respond to either M(E
G
, C
P
) (Wan and Verspoor,
1998; Lee and Chang, 2003; Jiang et al., 2007) or
M(E
P
, C
P
) (Meng et al., 2001; Gao et al., 2004;
7In our test set, an English word corresponds to one refer-
ence Chinese transliteration.
663
Table 6: Comparison of the overall performance
Class Model TRAIN TRAIN+DEV
ACC MRR CPA ACC’ ACC MRR CPA ACC’
LI04 N/A N/A N/A N/A 70.1 N/A N/A N/A
M(E
G
, JC
P
) 71.9 80.4 72.3 88.2 72.3 80.7 73.1 88.9
M(E
P
, JC
P
) 61.1 70.3 62.4 82.8 61.1 70.6 63.1 83.8
M
J
M(E
GP
, JC
P
) 72.3 80.9 73.2 89.6 73.5 81.5 73.9 90.4
M(E
G
+E
P
, JC
P
, 0.7) 72.8 80.7 73.8 89.7 73.2 81.0 74.7 90.5
M(E
G
+E
GP
, JC
P
, 0.6) 73.5 81.7 74.2 90.6 73.7 81.8 74.8 91.2
M(E
P
+E
GP
, JC
P
, 0.1) 71.6 80.3 73.3 89.8 72.5 80.8 73.8 90.1
M(E
G
, ?) 70.0 78.5 N/A N/A 70.6 79.0 N/A N/A
M(E
P
, ?) 58.5 69.3 N/A N/A 59.4 70.1 N/A N/A
M
I
M(E
GP
, ?) 71.2 79.9 N/A N/A 72.3 80.7 N/A N/A
M(E
G
+E
P
, ?, 0.7) 70.7 79.1 N/A N/A 72.0 80.0 N/A N/A
M(E
G
+E
GP
, ?, 0.4) 72.0 80.3 N/A N/A 72.8 80.9 N/A N/A
M(E
P
+E
GP
, ?, 0.1) 71.0 79.6 N/A N/A 72.0 80.4 N/A N/A
M(E
G
, C
P
) 58.9 70.2 72.3 78.4 59.1 70.4 73.1 78.4
M(E
P
, C
P
) 50.2 62.3 62.4 78.4 50.4 62.6 63.1 78.5
M
S
M(E
GP
, C
P
) 59.1 70.4 73.2 78.4 59.3 70.5 73.9 78.5
M(E
G
+E
P
, C
P
, 0.8) 59.7 71.3 73.8 79.0 60.3 71.7 74.7 79.0
M(E
G
+E
GP
, C
P
, 0.6) 59.8 71.7 74.2 78.9 60.6 72.1 74.8 78.9
M(E
P
+E
GP
, C
P
, 0.1) 58.8 70.4 73.3 78.9 59.4 70.7 73.8 78.8
Virga and Khudanpur, 2003).
A comparison between the basic and hybrid
transliteration models showed that the hybrid
ones usually performed better (the exception was
M(E
P
+E
GP
, y, ?) but the performance still com-
parable to the basic ones in each class). Es-
pecially, the hybrid ones based on the best two
basic transliteration models, M(E
G
+E
GP
, y, ?),
showed the best performance.
A comparison among the M
I
, M
S
, and
M
J
models showed that Chinese phonemes did
contribute to the performance improvement of
English-to-Chinese transliteration when Chinese
phonemes were used together with their corre-
sponding English graphemes and phonemes in
Chinese phoneme-to-grapheme conversion. A
one-tail paired t-test between the M
I
and M
J
models showed that the results of the M
J
mod-
els were always significantly better than those
of the M
I
models if the M
I
and M
J
models
shared the same English-side parameter, x ?
{E
G
, E
P
, E
GP
} (level of significance = 0.001).
In the results obtained by the M
S
and M
J
mod-
els, the figures in CPA are the same when theM
S
and our M
J
models share the same English-side
parameter. Moreover, the difference between the
figures in ACC and CPA can be interpreted as
the error rate of Chinese phoneme-to-grapheme
conversion. Our proposed M
J
models gener-
ated Chinese transliterations with a very low er-
ror rate in Chinese phoneme-to-grapheme conver-
sion, while theM
S
models suffered from a signif-
icant error rate in Chinese phoneme-to-grapheme
conversion. ACC’ showed that the M
J
models
still outperformed the M
S
models even without
errors in generating Chinese pronunciation from
the English words. These results indicate that the
joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes sig-
nificantly improved the performance in Chinese
phoneme-to-grapheme conversion and English-to-
Chinese transliteration.
Table 7 shows the Chinese transliterations gen-
erated by M(E
G
, ?), M(E
GP
, ?), M(E
G
, JC
P
),
and M(E
GP
, JC
P
) where English or Chinese
phonemes contributed to the correct translitera-
tion. In this table, the first column show the
English words and their English phonemes, and
the second and third columns represent the Chi-
nese transliterations and their phonemes. Note
that the Chinese phonemes in the second and third
columns of theM
I
models are not used in translit-
eration. They are shown in the table to indicate
the difference in the Chinese phonemes of Chinese
664
Table 7: Top-1 results of M(E
G
, ?), M(E
GP
, ?),
M(E
G
, JC
P
), and M(E
GP
, JC
P
), where * rep-
resents incorrect transliterations
M(EGP,JCP)M(EG,JCP)MJ models
????*
(LAI YIN HA TE)
????*
(LAI YIN HA TE)
Reinhardt
(R AI N HH AA R T)
??
(AI WEI)
??*
(YI WEI)
Ivy
(AY V IY)
???*
(AI MI LI)
???*
(AI MI LI)
Emily
(EH M IH L IY)
????
LAI YIN HA TE
????
LAI YIN HA TE
Reinhardt
(R AI N HH AA R T)
??
AI WEI
??*
YI WEI
Ivy
(AY V IY)
???
AI MI LI
???
AI MI LI
Emily
(EH M IH L IY)
M(EGP,?)M(EG,?)MI models
transliterations between theM
I
andM
J
models.
For Emily and Reinhardt, the M
J
models gen-
erated correct Chinese transliterations, but theM
I
models did not. Figure 1 shows the probabil-
ity distribution when a transliteration model gen-
erates the first Chinese character in the Chinese
transliteration of Reinhardt with and without Chi-
nese phonemes. Two Chinese characters,  and
, were strong candidates and  is the correct
one in this case. Without Chinese phonemes,
M(E
G
, ?), which is based on P(cg|Reinhardt)
in Figure 1(a) preferring  to , generated the
incorrect transliteration as shown in Table 7. How-
ever, Figure 1(b) shows that  can be selected
if the correct Chinese phoneme sequence “LAI
YIN ...” is given. Three Chinese phoneme se-
quences starting with “LAI YIN ...”, “LAI NA
...”, and “LAI NEI ...” were generated from Rein-
hardt, where “LAI YIN ...” was the best Chinese
phoneme sequence based on the probability distri-
bution in Figure 1(c). As a result, M(E
G
, JC
P
),
which jointly used Chinese phonemes with En-
glish graphemes, generated the correct Chinese
transliteration of Reinhardt based on two probabil-
ity distribution in Figures 1(b) and 1(c). In the case
of Ivy, English phonemes contributed to generat-
ing the correct transliteration in the M(E
GP
, ?)
and M(E
GP
, JC
P
) models.
Chinese transliterations sometimes reflect the
English word’s pronunciation as well as the Chi-
nese character’s meaning (Li et al., 2007). Li
0
0.2
0.4
0.6
0.8
P(
?
|Reinhardt) P(
?
|Reinhardt)
(a) Probability distribution when Chi-
nese phonemes are not given
0
0.2
0.4
0.6
0.8
1
?
?
P(cg|Reinhardt, "LAI YIN ..") P(cg|Reinhardt, "LAI NA ..")
P(cg|Reinhardt, "LAI NEI ..")
(b) Probability distribution when Chinese phonemes are
given
0
0.2
0.4
0.6
0.8
1
P("LAI YIN .."|Reinhardt) P(¬"LAI YIN .."|Reinhardt)
(c) Probability distribution for Chinese phoneme se-
quence “LAI YIN ...” and others
Figure 1: Probability distribution for the first Chi-
nese character in the Chinese transliteration of
Reinhardt: M(E
G
, ?) vs. M(E
G
, JC
P
)
et al. (2007) defined such a Chinese transliter-
ation as a phonetic-semantic transliteration (se-
mantic transliteration) to distinguish it from a
usual phonetic transliteration. One fact that
affects semantic transliteration is gender asso-
ciation (Li et al., 2007). For example, 
(meaining jasmine) is frequently used in Chi-
nese transliterations of female names but sel-
dom in common person names. Because Emily
is often used in female names, the results ob-
tained by the M(E
G
, JC
P
) and M(E
GP
, JC
P
)
models are acceptable. This indicates that Chi-
nese phonemes coupled with English graphemes
or those coupled with English graphemes and
phonemes could provide evidence required for se-
mantic transliteration as well as phonetic translit-
eration. As a result, M(E
GP
, ?), M(E
G
, JC
P
),
665
and M(E
GP
, JC
P
), which used phonemes cou-
pled with English graphemes, achieved higher per-
formance than M(E
G
, ?), which relied only on
English graphemes.
4.2 Effect of Training Data Size
 80
 70
 60
 50
 40
 30
 20
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG,?)
M(EP,?)
M(EGP,?)
M(EG,CP)
M(EP,CP)
M(EGP,CP)
M(EG,JCP)
M(EP,JCP)
M(EGP,JCP)
(a) Basic transliteration models
 80
 70
 60
 50
 40
 30
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG+EP,?,0.7)
M(EG+EGP,?,0.4)
M(EP+EGP,?,0.1)
M(EG+EP,CP,0.8)
M(EG+EGP,CP,0.6)
M(EP+EGP,CP,0.1)
M(EG+EP,JCP,0.7)
M(EG+EGP,JCP,0.6)
M(EP+EGP,JCP,0.1)
(b) Hybrid transliteration models
Figure 2: Performance of each system with differ-
ent training data size
We investigated the effect of training data size
on the performance of each transliteration model.
We randomly selected training data with ratios
from 10 to 90% and compared the performance
of each system trained by different sizes of train-
ing data. The results for the basic translitera-
tion models in Figure 2(a) can be categorized into
three groups. M(E
GP
, ?) and M(E
GP
, JC
P
)
fall into the best group, where they showed the
best performance regardless of training data size.
M(E
G
, ?) and M(E
G
, JC
P
) belong to the mid-
dle group, where they showed lower performance
than the best group if the training data size is
small, but their performance is comparable to the
best group if the size of the training data is large
enough. The others always showed lower perfor-
mance than both the best and middle groups. Fig-
ure 2(b) shows that hybrid transliteration models,
on average, were less sensitive to the training data
size than the basic ones, because the two differ-
ent basic transliteration models used in the hybrid
ones boosted transliteration performance by com-
plementing each other’s weak points.
5 Conclusion
We proposed a new English-to-Chinese transliter-
ation model based on Chinese phonemes and their
corresponding English graphemes and phonemes.
We defined eighteen English-to-Chinese translit-
eration models including our proposed model and
classified them into three classes based on the role
of Chinese phonemes in the transliteration mod-
els. Experiments showed that Chinese phonemes
in our proposed model can contribute to the
performance improvement in English-to-Chinese
transliteration.
Now we can answer Yes to this paper’s key ques-
tion, “Can Chinese phonemes improve machine
transliteration?” Actually, this is the second time
the same question has been answered. The pre-
vious answer, which was unfortunately reported
as No by Li et al. (2004), has been accepted as
true for the last five years; the research issue has
been considered closed. In this paper, we found
a new answer that contradicts the previous an-
swer. We hope that our answer promotes research
on phoneme-based English-to-Chinese translitera-
tion.
Appendix: Illustration of Basic
Transliteration Models inM
J
andM
S
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG:)JC,?(? PG
:)JC,?(?
PP
:)JC,?(?
PGP
(a) M
J
models
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG
:)C,?(?
PGP
:)C,?(?
PP
:)C,?(?
PG
(b) M
S
models
666
References
Y. Al-Onaizan and Kevin Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proc. of ACL ’02, pages 400–408.
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for translit-
eration discovery. In Proceedings of NAACL HLT’
09.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. of IJCNLP 2004, pages
110–119.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In Proc. of IJCAI ’07, pages
1629–1634.
Paul B. Kantor and Ellen M. Voorhees. 2000. The trec-
5 confusion track: Comparing retrieval methods for
scanned text. Information Retrieval, 2:165–176.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of ACL ’07,
pages 648–655.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical ma-
chine transliteration model. In Proc. of HLT-NAACL
2003 Workshop on Building and Using Parallel
Texts, pages 96–103.
Haizhou Li, Min Zhang, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42th Annual Meeting of the As-
sociation of Computational Linguistics, pages 160–
167.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. of
ACL-IJCNLP 2009 Named Entities Workshop.
M.G. Abbas Malik. 2006. Punjabi machine translit-
eration. In Proceedings of the COLING/ACL 2006,
pages 1137–1144.
H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.
2001. Generating phonetic cognates to handle
named entities in English-Chinese cross-language
spoken document retrieval. In Proc. of Auto-
matic Speech Recognition and Understanding, 2001.
ASRU ’01, pages 311–314.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119–151.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximal entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 1–
10.
Richard Schwartz and Yen-Lu Chow. 1990. The N-
Best algorithm: an efficient procedure for finding
top N sentence hypotheses. In Proc. of ICASSP ’90,
pages 81–84.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of ACL ’07,
pages 944–951.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of ACL 2003 Workshop on Multi-
lingual and Mixed-language Named Entity Recogni-
tion, pages 57–64.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. of
COLING ’98, pages 1352–1356.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
Binyong Yin and Mary Felley. 1990. Chinese Roman-
ization: Pronunciation and Orthography. Sinolin-
gua.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using
feature based phonetic method. In Proceedings of
ACL’07, pages 112–119.
667
