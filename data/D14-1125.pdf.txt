Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181–1191,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference
Yoonjung Choi and Janyce Wiebe
Department of Computer Science
University of Pittsburgh
yjchoi, wiebe@cs.pitt.edu
Abstract
Recently, work in NLP was initiated on a
type of opinion inference that arises when
opinions are expressed toward events
which have positive or negative effects
on entities (+/-effect events). This paper
addresses methods for creating a lexicon
of such events, to support such work on
opinion inference. Due to significant
sense ambiguity, our goal is to develop a
sense-level rather than word-level lexicon.
To maximize the effectiveness of different
types of information, we combine a
graph-based method using WordNet
1
relations and a standard classifier using
gloss information. A hybrid between the
two gives the best results. Further, we
provide evidence that the model is an
effective way to guide manual annotation
to find +/-effect senses that are not in the
seed set.
1 Introduction
Opinion mining (or sentiment analysis) identifies
positive or negative opinions in many kinds of
texts such as reviews, blogs, and news articles. It
has been exploited in many application areas such
as review mining, election analysis, and infor-
mation extraction. While most previous research
focusses on explicit opinion expressions, recent
work addresses a type of opinion inference that
arises when opinions are expressed toward events
which have positive or negative effects on enti-
ties (Deng et al., 2013; Deng and Wiebe, 2014).
We call such events +/-effect events.
2
Deng and
Wiebe (2014) show how sentiments toward one
1
WordNet 3.0, http://wordnet.princeton.edu/
2
While the term goodFor/badFor is used in previous pa-
pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,
2014), we have since decided that +/-effect is a better term.
entity may be propagated to other entities via
opinion inference rules. They give the following
example:
(1) The bill would curb skyrocketing
health care costs.
The writer expresses an explicit negative senti-
ment (by skyrocketing) toward the object (health
care costs). The event, curb, has a negative effect
on costs, since they are reduced. We can reason
that the writer is positive toward the event because
it has a negative effect on costs, toward which the
writer is negative. From there, we can reason that
the writer is positive toward the bill, since it is
the agent of the positive event. Deng and Wiebe
(2014) show that such inferences may be exploited
to significantly improve explicit sentiment analy-
sis systems.
However, to achieve its results, the system de-
veloped by Deng and Wiebe (2014) requires that
all instances of +/-effect events in the corpus be
manually provided as input. For the system to
be fully automatic, it needs to be able to recog-
nize +/-effect events automatically. This paper
addresses methods for creating lexicons of such
events, to support such work on opinion inference.
We have discovered that there is significant sense
ambiguity, meaning that words often have mix-
tures of senses among the classes +effect, -effect,
and Null. Thus, we develop a sense-level rather
than word-level lexicon.
One of our goals is to investigate whether
the +/-effect property tends to be shared among
semantically-related senses, and another is to
use a method that applies to all word senses, not
just to the senses of words in a given word-level
lexicon. Thus, we build a graph-based model in
which each node is a WordNet sense, and edges
represent semantic WordNet relations between
senses. In addition, we hypothesized that glosses
also contain useful information. Thus, we develop
1181
a supervised gloss classifier and define a hybrid
model which gives the best overall performance.
Finally, because all WordNet verb senses are
incorporated into the model, we investigate the
ability of the method to identify unlabeled senses
that are likely to be +/-effect senses. We find that
by iteratively labeling the top-weighted unlabeled
senses and rerunning the model, it may be used as
an effective method for guiding annotation efforts.
2 Background
There are many varieties of +/-effect events, in-
cluding creation/destruction (changes in states in-
volving existence), gain/loss (changes in states
involving possession), and benefit/injury (Anand
and Reschke, 2010; Deng et al., 2013). The cre-
ation, gain, and benefit classes are +effect events.
For example, baking a cake has a positive effect on
the cake because it is created;
3
increasing the tax
rate has a positive effect on the tax rate; and com-
forting the child has a positive effect on the child.
The antonymous classes of each are -effect events:
destroying the building has a negative effect on the
building; demand decreasing has a negative effect
on demand; and killing Bill has a negative effect
on Bill.
4
While sentiment (Esuli and Sebastiani, 2006;
Wilson et al., 2005; Su and Markert, 2009) and
connotation lexicons (Feng et al., 2011; Kang et
al., 2014) are related, sentiment, connotation, and
+/-effects are not the same; a single event may
have different sentiment and +/-effect polarities,
for example. Consider the following example:
perpetrate:
S: (v) perpetrate, commit, pull (perform
an act, usually with a negative connota-
tion) “perpetrate a crime”; “pull a bank
robbery”
This sense of perpetuate has a negative
connotation, and is an objective term in
SentiWordNet. However, it has a positive
effect on the object, a crime, since performing a
crime brings it into existence.
3
Deng et al. (2013) point out that +/-effect objects are not
equivalent to benefactive/malefactive semantic roles. An ex-
ample they give is She baked a cake for me: a cake is the ob-
ject of the +effect event baked as just noted, while me is the
filler of its benefactive semantic role (Ziga and Kittil, 2010).
4
Their annotation manual, which gives additional cases, is
available with the annotated data at http://mpqa.cs.pitt.edu/.
As we mentioned, the +/-effect ambiguity can-
not be avoided in a word-level lexicon. In the
+/-effect corpus of Deng et al. (2013),
5
+/-effect
events and their agents and objects are annotated
at the word level. In that corpus, 1,411 +/-effect in-
stances are annotated; 196 different +effect words
and 286 different -effect words appear in these
instances. Among them, 10 words appear in
both +effect and -effect instances, accounting for
9.07% of all annotated instances. They show that
+/-effect events (and the inferences that motivate
this work) appear frequently in sentences with ex-
plicit sentiment. Further, all instances of +/-effect
words that are not identified as +/-effect events are
false hits from the perspective of a recognition sys-
tem.
The following is an example of a word with
senses of different classes:
purge:
S: (v) purge (oust politically) “Deng
Xiao Ping was purged several times
throughout his lifetime” -effect
S: (v) purge (clear of a charge) +effect
S: (v) purify, purge, sanctify (make pure
or free from sin or guilt) “he left the
monastery purified” +effect
S: (v) purge (rid of impurities) “purge
the water”; “purge your mind” +effect
This is part of the WordNet output for the word
purge. In the first sense, the polarity is -effect
since it has a negative effect on the object, Deng
Xizo Ping. However, the other cases have positive
effect on the object. Moreover, although a word
may not have both +effect and -effect senses, it
may have mixtures of ((+effect or -effect) and
Null). A purely word-based approach is blind to
these cases.
3 Related Work
Lexicons are widely used in sentiment analysis
and opinion mining. Several works such as Hatzi-
vassiloglou and McKeown (1997), Turney and
Littman (2003), Kim and Hovy (2004), Strappar-
ava and Valitutti (2004), and Peng and Park (2011)
have tackled automatic lexicon expansion or ac-
quistion. However, in most such work, the lexi-
cons are word-level rather than sense-level.
5
Called the goodFor/badFor corpus in that paper.
1182
For the related (but different) tasks of de-
veloping subjectivity, sentiment and connota-
tion lexicons, some do take a sense-level ap-
proach. Esuli and Sebastiani (2006) construct
SentiWordNet. They assume that terms with
the same polarity tend to have similar glosses. So,
they first expand a manually selected seed set of
senses using WordNet lexical relations such as
also-see and direct antonymy and train two clas-
sifiers, one for positive and another for negative.
As features, a vectorial representation of glosses
is adopted. These classifiers were applied to all
WordNet senses to measure positive, negative, and
objective scores. In extending their work (Esuli
and Sebastiani, 2007), the PageRank algorithm is
applied to rank senses in terms of how strongly
they are positive or negative. In the graph, each
sense is one node, and two nodes are connected
when they contain the same words in their Word-
Net glosses. Moreover, a random-walk step is
adopted to refine the scores in their recent work
(Baccianella et al., 2010). In contrast, our ap-
proach uses WordNet relations and graph propa-
gation in addition to gloss classification.
Gyamfi et al. (2009) construct a classifier to la-
bel the subjectivity of word senses. The hierarchi-
cal structure and domain information in WordNet
are exploited to define features in terms of sim-
ilarity (using the LCS metric in Resnik (1995))
of target senses and a seed set of senses. Also,
the similarity of glosses in WordNet is consid-
ered. Even though they investigated the hierarchi-
cal structure by LCS values, WordNet relations are
not exploited directly.
Su and Markert (2009) adopt a semi-supervised
mincut method to recognize the subjectivity of
word senses. To construct a graph, each node cor-
responds to one WordNet sense and is connected
to two classification nodes (one for subjectivity
and another for objectivity) via a weighted edge
that is assigned by a classifier. For this classifier,
WordNet glosses, relations, and monosemous
features are considered. Also, several WordNet
relations (e.g., antonymy, similiar-to, direct
hypernym, etc.) are used to connect two nodes.
Although they make use of both WordNet glosses
and relations, and gloss information is utilized
for a classifier, this classifier is generated only
for weighting edges between sense nodes and
classification nodes, not for classifying all senses.
Kang et al. (2014) present a unified model that
assigns connotation polarities to both words and
senses. They formulate the induction process as
collective inference over pairwise-Markov Ran-
dom Fields, and apply loopy belief propagation
for inference. Their approach relies on selectional
preferences of connotative predicates; the polarity
of a connotation predicate suggests the polarity of
its arguments. We have not discovered an analo-
gous type of predicate for the problem we address.
Goyal et al. (2010) generate a lexicon of patient
polarity verbs (PPVs) that impart positive or neg-
ative states on their patients. They harvest PPVs
from a Web corpus by co-occurance with Kind and
Evil agents and by bootstrapping over conjunc-
tions of verbs. Riloff et al. (2013) learn positive
sentiment phrases and negative situation phrases
from a corpus of tweets with hashtag “sarcasm”.
However, both of these methods are word-level
rather than sense-level.
Ours is the first NLP research into developing
a sense-level lexicon for events that have negative
or positive effects on entities.
4 +/-Effect Word-Level Seed Lexicon
and Sense Annotations
To create the corpus used in this work, we devel-
oped a word-level seed lexicon, and then manually
annotated all the senses of the words in that lexi-
con.
FrameNet
6
is based on a theory of meaning
called Frame Semantics. In FrameNet, a Lexical
Unit (LU) is a pairing of a word with a meaning,
i.e., it corresponds to a sense in WordNet. Each
LU of a polysemous word belongs to a different
semantic frame, which is a description of a type
of event, relation, or entity and, where appropri-
ate, its participants. For instance, in the Creating
frame, the definition is that a Cause leads to the
formation of a Created entity. It has a positive
effect on the object, Created entity. This frame
contains about 10 LUs such as assemble, create,
yield, and so on. FrameNet consists of about 1,000
semantic frames and about 10,000 LUs.
FrameNet is a useful resource to select +/-effect
words since each semantic frame covers multi-
ple LUs. We believe that using FrameNet to
find +/-effect words is easier than finding +/-effect
words without any information since words may
6
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
1183
be filtered by semantic frames. To select +/-effect
words, an annotator (who is not a co-author) first
identified promising frames as +/-effect and ex-
tracted all LUs from them. Then, he went through
them and picked out the LUs which he judged to
be +effect or -effect. In total, 736 +effect LUs and
601 -effect LUs were selected from 463 semantic
frames.
While Deng et al. (2013) and Deng and Wiebe
(2014) specifically focus on events affecting ob-
jects (i.e., themes), we do not want to limit the
lexicon to only that case. Sometimes, events have
positive or negative effects on agents or other en-
tities as well. Thus, in this paper, we consider
a sense to be +effect (-effect) if it has +effect
(-effect) on an entity, which may be the agent, the
theme, or some other entity.
In a previous paper (Choi et al., 2014), we con-
ducted a study of the sense-level +/-effect prop-
erty. For the evaluation, two annotators (who
are co-authors of that paper) independently anno-
tated senses of selected words, where some are
from pure +effect (-effect) words (i.e., all senses
of the words are classified into the same class)
and some are from mixed words (i.e., the words
have both +effect and -effect senses). In the agree-
ment study, we calculated percent agreement and
? (Artstein and Poesio, 2008), and achieved 0.84
percent agreement and 0.75 ? value.
For a seed set and an evaluation set in this pa-
per, we need annotated sense-level +/-effect data.
Mappings between FrameNet and WordNet are
not perfect. Thus, we opted to manually anno-
tate the senses of the words in the word-level lexi-
con. We first extracted all words from 736 +effect
LUs and 601 -effect LUs; this extracts 606 +effect
words and 537 -effect words (the number of words
is smaller than the number of LUs because one
word can have more than one LU). Among them,
14 words (e.g., crush, order, etc.) are in both the
+effect word set and the -effect word set. That is,
these words have both +effect and -effect mean-
ings. Recall that this annotator was focusing on
frames, not on words - he did not look at all the
senses of all the words. As we will see just below,
when all the senses of all the words are annotated,
a much higher percentage of the words have both
+effect and -effect senses. We will also see that
many of the senses are revealed to be Null, show-
ing that +effect vs. Null and -effect vs. Null ambi-
guities are quite prevalent.
A different annotator (a co-author) then went
through all senses of all the words from the pre-
vious step and manually annotated each sense as
to whether it is +effect, -effect, or Null. Note that
this annotator participated in an agreement study
with positive results in Choi et al. (2014).
For the experiments in this paper, we divided
this annotated data into two equal-sized sets. One
is a fixed test set that is used to evaluate both the
graph model and the gloss classifier. The other set
is used as a seed set by the graph model, and as a
training set by the gloss classifer. Table 1 shows
the distribution of the data. In total, there are 258
+effect senses, 487 -effect senses, and 880 Null
senses. To avoid too large a bias toward the Null
class,
7
we randomly chose half (i.e., the Null set
contains 440 senses). Half of each set is used as
seed and training data, and the other half is used
for evaluation.
+effect -effect Null
# annotated data 258 487 880
# Seed/TrainSet 129 243 220
# TestSet 129 244 220
Table 1: Distribution of annotated data.
5 Graph-based Semi-Supervised
Learning for WordNet Relations
WordNet (Miller et al., 1990) is organized by se-
mantic relations such as hypernymy, troponymy,
grouping, and so on. These semantic relations can
be used to build a network. Since the most fre-
quently encoded relation is the super-subordinate
relation, most verb senses are arranged into hi-
erarchies; verb senses towards the bottom of the
graph express increasingly specific manner. Thus,
by following this hierarchical information, we hy-
pothesized that +/-effect polarity tends to propa-
gate. We use a graph-based semi-supervised learn-
ing (GSSL) method to carry out the label propaga-
tion.
5.1 Graph Formulation
We formulate a graph for semi-supervised learning
as follows. Let G = {X,E,W} be the undirected
graph in which X is the set of nodes, E is the set
7
As mentioned in the introduction, we want our method
to be able to identify unlabeled senses that are likely to be
+/-effect senses (see Section 8); we resize the Null class to
support this goal.
1184
of edges, and W represents the edge weights (i.e.,
the weight of edge E
ij
is W
ij
). The weight matrix
is a non-negative matrix.
Each data point in X = {x
1
, ... ,x
n
} is one
sense. The labeled data of X is represented as
X
L
= {x
1
, ... ,x
l
} and the unlabeled data is rep-
resented as X
U
= {x
l+1
, ... ,x
n
}). The labeled
data X
L
is associated with labels Y
L
= {y
1
, ...
,y
l
}, where y
i
? {1, ..., c} (c is the number of
classes). As is typical in such settings, l  n:
n is 13,767, i.e., the number of verb senses in
WordNet. Seed/TrainSet in Table 1 is the labeled
data.
To connect two nodes, WordNet relations are
utilized. We first connect nodes by the hierar-
chical relations. Since hypernym relations repre-
sent more general senses and troponym relations
represent more specific verb senses, we hypothe-
sized that hypernyms or troponyms of a verb sense
tends to have its same polarity. Verb groups rela-
tions that represent verb senses having a similar
meaning are also promising. Even though verb-
group coverage is not large, its relations are reli-
able since they are manually grouped. The entail-
ment relation is defined as the verb Y is entailed
by X if you must be doing Y by doing X . Since
pairs connected by this relation are co-extensive,
we can assume that both are the same type of
event. The synonym relation is not used because
it is already defined in senses (i.e., each node in
the graph is a synset), and the antonym relation is
also not applied since the weight matrix should be
non-negative. The weight value of all edges is 1.0.
5.2 Label Propagation
Given a constructed graph, the label inference (or
prediction) task is to propagate the seed labels to
the unlabeled nodes. One of the classic GSSL la-
bel propagation methods is the local and global
consistency (LGC) method suggested by Zhou et
al. (2004). The LGC method is a graph transduc-
tion algorithm which is sufficiently smooth with
respect to the intrinsic structure revealed by known
labeled and unlabeled data. The cost function typ-
ically involves a tradeoff between the smoothness
of the predicted labels over the entire graph and
the accuracy of the predicted labels in fitting the
given labeled nodes X
L
. LGC fits in a univariate
regularization framework, where the output ma-
trix is treated as the only variable in optimization,
and the optimal solutions can be easily obtained by
solving a linear system. Thus, we adopt the LGC
method in this paper. Although there are some ro-
bust GSSL methods for handling noisy labels, we
do not need to handle noisy labels because our in-
put is the annotated data.
Let F be a n × c matrix to save the output
values of label propagation. So, we can label
each instance x
i
as a label y
i
= argmax
j?c
F
ij
after the label propagation. The initial discrete la-
bel matrix Y , which is also n × c, is defined as
Y
ij
= 1 if x
i
is labeled as y
i
= j in Y
L
, and
Y
ij
= 0 otherwise. The vertex degree matrix
D = diag([D
11
, ..., Dnn]) is defined by D
ii
=
?
n
j=1
W
ij
.
LGC defines the cost function Q which inte-
grates two penalty components, global smooth-
ness and local fitting (µ is the regularization pa-
rameter):
Q =
1
2
n
?
i=1
n
?
j=1
W
ij
?
F
i
?
D
ii
?
F
j
?
D
jj
?
2
+µ
n
?
i=1
?F
i
? Y
i
?
2
The first part of the cost function is the
smoothness constraint: a good classifying func-
tion should not change too much between nearby
points. That is, if x
i
and x
j
are connected with
an edge, the difference between them should be
small. The second is the fitting constraint: a good
classifying function should not change too much
from the initial label assignment. The final label
prediction matrix F can be obtained by minimiz-
ing the cost function Q.
5.3 Experimental Results
Note that, in the rest of this paper, all tables except
the last one give results on the same fixed test set
(TestSet in Table 1).
We can apply the graph model in two ways.
• UniGraph: All three classes (+effect, -effect,
and Null) are represented in one graph.
• BiGraph: Two separate graphs are first con-
structed and then combined. One graph is for
classifying +effect and Other (i.e., -effect or
Null). This graph is called +eGraph. The
other graph, called -eGraph, is for classify-
ing -effect and Other (i.e., +effect or Null).
1185
UniGraph BiGraph BiGraph*
baseline-
0.411
accuracy
accuracy 0.630 0.623 0.658
+effect P 0.621 0.610 0.642
R 0.655 0.647 0.680
F 0.637 0.628 0.660
-effect P 0.644 0.662 0.779
R 0.720 0.677 0.612
F 0.680 0.670 0.686
Null P 0.615 0.583 0.583
R 0.516 0.550 0.695
F 0.561 0.561 0.634
Table 2: Results of UniGraph, BiGraph, and Bi-
Graph*.
These are combined into one model as fol-
lows. Nodes that are labeled as +effect by
+eGraph and Other by -eGraph are regarded
as +effect, and nodes that are labeled as
-effect by -eGraph and Other by +eGraph are
regarded as -effect. If nodes are labeled as
+effect by +eGraph and -effect by -eGraph,
they are deemed to be Null. Nodes that are
labeled Other by both graphs are also consid-
ered as Null.
We had two motivations for experimenting
with the BiGraph model: (1) SVM, the super-
vised learning method used for gloss classifica-
tion, tends to have better performance on binary
classification tasks, and (2) the two graphs of the
combined model can “negotiate” with each other
via constraints.
In Table 2, we calculate precision (P), recall (R),
and f-measure (F) for all three classes. The base-
line shown in the top row is the accuracy of a ma-
jority class classifier. The first two columns of Ta-
ble 2 show the results of UniGraph and BiGraph
when they are built using the hypernym, troponym,
and verb group relations. UniGraph outperforms
BiGraph in this experiment.
To improve the results by performing some-
thing possible with BiGraph (but not UniGraph),
constraints are added when determining the class.
As we explained, the label of instance x
i
is
determined by F
i
in the graph. When the label
of x
i
is decided to be j, we can say that its con-
fidence value is F
ij
. There are two constraints as
follows.
H+T +V +E
+effect P 0.653 0.642 0.651
R 0.660 0.680 0.683
F 0.656 0.660 0.667
-effect P 0.784 0.779 0.786
R 0.547 0.612 0.604
F 0.644 0.686 0.683
Null P 0.557 0.583 0.564
R 0.735 0.695 0.691
F 0.634 0.634 0.621
Table 3: Effect of each relation
• If a sense is labeled as +effect (-effect), but
the confidence value is less than a threshold,
we count it as Null.
• If a sense is labeled as both +effect and -effect
by BiGraph, we choose the label with the
higher confidence value only if the higher one
is larger than a threshold and the lower one is
less than a threshold.
The thresholds are determined on Seed/TrainSet
by running BiGraph several times with different
thresholds, and choosing the one that gives the
best performance on Seed/TrainSet. (The chosen
value is 0.025 for +effect and 0.03 for -effect).
As can be seen in Table 2, BiGraph with con-
straints (called BiGraph*) outperforms not only
BiGraph without any constraints but also Uni-
Graph. Especially, for BiGraph*, the recall of the
Null class is considerably increased, showing that
constraints not only help overall, but are particu-
larly important for detecting Null cases.
Table 3 gives ablation results, showing the con-
tribution of each WordNet relation in BiGraph*.
With only hierarchical information (i.e., hyper-
nym (H) and troponym (T) relations), it already
shows good performance for all classes. How-
ever, they cannot cover some senses. Among the
13,767 verb senses in WordNet, 1,707 (12.4%)
cannot be labeled because there are not sufficient
hierarchical links to propagate polarity informa-
tion. When adding the verb group (+V) rela-
tion, it shows improvement in both +effect and
-effect. Especially, the recall for +effect and
-effect is significantly increased. In addition, the
coverage of the 13,767 verb senses increases to
95.1%. For entailment (+E), whereas adding it
shows a slight improvement in +effect (and in-
creases coverage by 1.1 percentage points), the
1186
performance is decreased a little bit in the -effect
and Null classes. Since the average f-measure for
all classes is the highest with hypernym (H), tro-
ponym (T), and verb group (V) relations (not en-
tailment), we only consider these three relations
when constructing the graph.
6 Supervised Learning applied to
WordNet Glosses
In WordNet, each sense contains a gloss consist-
ing of a definition and optional example sentences.
Since a gloss consists of several words and there
are no direct links between glosses, we believe that
a word vector representation is appropriate to uti-
lize gloss information as in Esuli and Sebastiani
(2006). For that, we adopt an SVM classifier.
6.1 Features
Two different feature types are used.
Word Features (WF): The bag-of-words
model is applied. We do not ignore stop words
for several reasons. Since most definitions and ex-
amples are not long, each gloss contains a small
number of words. Also, among them, the total vo-
cabulary of WordNet glosses is not large. More-
over, some prepositions such as against are some-
times useful to determine the polarity (+effect or
-effect).
Sentiment Features (SF): Some glosses of
+effect (-effect) senses contain positive (negative)
words. For instance, the definition of {hurt#4,
injure#4} is “cause damage or affect negatively.”
It contains a negative word, negatively. Since a
given event may positively (negatively) affect enti-
ties, some definitions or examples already contain
positive (negative) words to express this. Thus, as
features, we check how many positive (negative)
words a given gloss contains. To detect sentiment
words, the subjectivity lexicon provided by Wil-
son et al. (2005)
8
is utilized.
6.2 Gloss Classifier
We have three classes, +effect, -effect, and Null.
Since SVM shows better performance on binary
classification tasks, we generate two binary clas-
sifiers, one (+eClassifier) to determine whether
a given sense is +effect or Other, and another
(-eClassifier) to classify whether a given sense is
-effect or Other. Then, they are combined as in
BiGraph.
8
Available at http://mpqa.cs.pitt.edu/
6.3 Experimental Results
Seed/TrainSet in Table 1 is used to train the two
classifiers, and TestSet is utilized for the evalua-
tion. So, the training set for +eClassifier consists
of 129 +effect instances and 463 Other instances,
and the training set for -eClassifier contains 243
-effect instances and 349 Other instances. As a
baseline, we adopt a majority class classifier.
Table 4 shows the results on TestSet. Perfor-
mance is better for the -effect than for the +effect
class, perhaps because the -effect class has more
instances.
When sentiment features (SF) are added,
all metric values increase, providing evidence
that sentiment features are helpful to determine
+/-effect classes.
WF WF+SF
baseline accuracy 0.411
accuracy 0.509 0.539
+effect P 0.541 0.588
R 0.354 0.393
F 0.428 0.472
-effect P 0.616 0.672
R 0.500 0.511
F 0.552 0.580
Null P 0.432 0.451
R 0.612 0.657
F 0.507 0.535
Table 4: Results of the gloss classifier.
7 Hybrid Method
To use more combined knowledge, the gloss clas-
sifier and BiGraph* can be combined. That is, for
WordNet gloss information, the gloss classifier is
utilized, and for WordNet relations, BiGraph* is
used. With the Hybrid method, we can see not
only the effect of propagation by WordNet rela-
tions but also the usefulness of gloss information
and sentiment features. Also, while BiGraph*
cannot cover all senses in WordNet, the Hybrid
method can.
The outputs of the gloss classifier and Bi-
Graph* are combined as follows. The label of
the gloss classifier is one of +effect, -effect, Null,
or Both (when a given sense is classified as both
+effect by +eClassifier and -effect by -eClassifier).
Possible labels of BiGraph* are +effect, -effect,
Null, Both, or None (when a given sense is not
1187
labeled by BiGraph*). There are five rules:
• If both labels are +effect (-effect), it is +effect
(-effect).
• If one of them is Both and the other is +effect
(-effect), it is +effect (-effect).
• If the label of BiGraph* is None, believe the
label of the gloss classifier
• If both labels are Both, it is Null
• Otherwise, it is Null
The results for Hybrid are given in the first
row of the lower half of Table 5; the results for
BiGraph* are in the first row of the upper half,
for comparison. Generally, the Hybrid method
shows better performance than the gloss classifier
and BiGraph*. In the Hybrid method, since more
+/-effect senses are detected than by BiGraph*,
while precision is decreased, recall is increased
by more. However, by the same token, the over-
all performance for the Null class is decreased.
Actually, that is expected since the Null class is
determined by the Other class in the gloss clas-
sifier and BiGraph*. Through this experiment, we
see that the Hybrid method is better for classifying
+/-effect senses.
7.1 Model Comparison
To provide evidence for our assumption that dif-
ferent models are needed for different information
to maximize effectiveness, we compare the hy-
brid method with the supervised learning and the
graph-based learning (GSSL) methods, each uti-
lizing both WordNet relations and gloss informa-
tion.
Supervised Learning (onlySL): The gloss clas-
sifier is trained with word features and sentiment
features for WordNet Gloss. To exploit Word-
Net relations in supervised learning, especially
the hierarchical information, we use least com-
mon subsumer (LCS) values as in Gyamfi et al.
(2009), which, recall, performs supervised learn-
ing of subjective/objective senses. The values are
calculated as follows. For a target sense t and a
seed set S, the maximum LCS value between a
target sense and a member of the seed set is found
as:
Score(t, S) = max
s?S
LCS(t, s)
With this LCS feature and the features described
in Section 6, we run SVM on the same training and
test data. For LCS values, the similarity using the
information content proposed by Resnik (1995) is
measured. WordNet Similarity
9
package provides
pre-computed pairwise similarity values for that.
Table 6 shows results of onlySL. Compared to
Table 4, while +effect and Null classes show a
slight improvement, the performance is degraded
for -effect. This means that the added feature is
rather harmful to -effect. Even though the hierar-
chical feature is very helpful to expand +/-effect,
it is not helpful for onlySL since SVM cannot cap-
ture propagation according to the hierarchy.
Graph-based Learning (onlyGraph): In Sec-
tion 5, the graph is constructed by using Word-
Net relations. To apply WordNet gloss informa-
tion in onlyGraph, we calculate a cosine similarity
between glosses. If the similarity value is higher
than a threshold, two nodes are connected with this
similarity value. The threshold is determined by
training and testing on Seed/TrainSet (the chosen
value is 0.3).
Comparing Tables 2 and 6, BiGraph* generally
outperforms onlyGraph (the exception is precision
of +effect). By gloss similarity, many nodes are
connected to each other. However, since uncertain
connections can cause incorrect propagation in the
graph, this negatively affects the performance.
Through this experiment, we see that since each
type of information has a different character, we
need different models to maximize the effective-
ness of each type. Thus, the hybrid method with
different models can have better performance.
Hybrid onlySL onlyGraph
+effect P 0.610 0.584 0.701
R 0.735 0.400 0.364
F 0.667 0.475 0.480
-effect P 0.717 0.778 0.651
R 0.669 0.316 0.562
F 0.692 0.449 0.603
Null P 0.556 0.440 0.473
R 0.520 0.813 0.679
F 0.538 0.571 0.557
Table 6: Comparison to onlySL and onlyGraph.
9
WordNet Similarity,
http://wn-similarity.sourceforge.net/
1188
+effect -effect Null
P R F P R F P R F
BiGraph* Initial 0.642 0.680 0.660 0.779 0.612 0.686 0.583 0.695 0.634
1st 0.636 0.684 0.663 0.770 0.632 0.694 0.591 0.672 0.629
2nd 0.642 0.701 0.670 0.748 0.656 0.699 0.605 0.655 0.629
3rd 0.636 0.708 0.670 0.779 0.652 0.710 0.599 0.669 0.632
4th 0.681 0.674 0.678 0.756 0.674 0.712 0.589 0.669 0.626
Hybrid Initial 0.610 0.735 0.667 0.717 0.669 0.692 0.556 0.520 0.538
1st 0.614 0.713 0.672 0.728 0.681 0.704 0.562 0.523 0.542
2nd 0.613 0.743 0.672 0.716 0.697 0.706 0.559 0.497 0.526
3rd 0.616 0.739 0.672 0.717 0.706 0.712 0.559 0.494 0.525
4th 0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545
Table 5: Results of an iterative approach.
8 Guided Annotation
Recall that Seed/TrainSet and TestSet, the data
used so far, are all the senses of the words in a
word-level +/-effect lexicon. This section presents
evidence that our method can guide annotation ef-
forts to find other words that have +/-effect senses.
A bonus is that the method pinpoints particular
+/-effect senses of those words.
All unlabeled data are senses of words that are
not included in the original lexicon. Since pre-
sumably the majority of verbs do not have any
+/-effect senses, a sense randomly selected from
WordNet is very likely to be Null. We explore an
iterative approach to guided annotation, using Bi-
Graph* and Hybrid as the method for assigning
labels.
The system is initially created as described
above using Seed/TrainSet as the initial seed set.
Each iteration has four steps: 1) rank all unlabeled
data (i.e., the data other than TestSet and the cur-
rent seed set) based on the F
ij
confidence values
(see Section 5.3); 2) choose the top 5% and manu-
ally annotate them (the same annotator as above
did this); 3) add them to the seed set; 4) rerun
the system using the expanded seed set. We per-
formed four iterations in this paper.
The upper and lower parts of Table 5 show the
intial results and the results after each iteration for
BiGraph* and Hybrid. Recall that these are results
on the fixed set, TestSet. Overall for both mod-
els, f-measure increases for both the +effect and
-effect classes as more seeds are added, mainly
due to improvements in recall. The evaluation on
the fixed set is also useful in the annotation process
because it trades off +/-effect vs. Null annotations.
If the new manual annotations were biased, in that
they incorrectly label Null senses as +/-effect, then
the f-measure results would instead degrade on the
fixed TestSet, since the system is created each time
using the increased seed set.
We now consider the accuracy of the system
on the newly labeled annotated data in Step 2.
Note that our method is similar to Active Learn-
ing (Tong and Koller, 2001), in that both auto-
matically identify which unlabeled instances the
human should annotate next. However, in active
learning, the goal is to find instances that are diffi-
cult for a supervised learning system. In our case,
the goal is to find needles in the haystack of Word-
Net senses. In Step 3, we add the newly labeled
senses to the seed set, enabling the model to find
unlabeled senses close to the new seeds when the
system is rerun for the next iteration.
We assess the system’s accuracy on the newly
labeled data by comparing the system’s labels with
the human’s new labels. Accuracy for +effect and
-effect is calculated such as:
Accuracy
+effect
=
# annotated +effect
# top 5% +effect data
Accuracy
?effect
=
# annotated -effect
# top 5% -effect data
That is, the accuracy means that out of the top 5%
of the +effect (-effect) data as scored by the sys-
tem, what percentage are correct as judged by a
human annotator. Table 7 shows the accuracy for
each iteration in the top part and the number of
senses labeled in the bottom part. As can be seen,
the accuracies range between 60% and 78%; these
1189
values are much higher than what would be ex-
pected if labeling senses of words randomly cho-
sen from WordNet.
10
The annotator spent, on av-
erage, approximately an hour to label 100 senses.
For finding new words with +/-effect usages, it
would be much more cost-effective if a significant
percentage of the data chosen for annotation are
senses of words that in fact have +/-effect senses.
1st 2nd 3rd 4th
+effect 65.63% 62.50% 63.79% 59.83%
-effect 73.55% 73.97% 77.78% 70.30%
+effect 128 122 116 117
-effect 155 146 153 145
total 283 268 269 262
Table 7: Accuracy and frequency of the top 5% for
each iteration
9 Conclusion and Future Work
In this paper, we investigated methods for creat-
ing a sense-level +/-effect lexicon. To maximize
the effectiveness of each type of information, we
combined a graph-based method using WordNet
relations and a standard classifier using gloss in-
formation. A hybrid between the two gives the
best results. Further, we provide evidence that the
model is an effective way to guide manual anno-
tation to find +/-effect words that are not in the
seed word-level lexicon. This is important, as the
likelihood that a random WordNet sense (and thus
word) is +effect or -effect is not large.
So as not to limit the inferences that may be
drawn, our annotations include events that are
+effect or -effect either the agent or object. In fu-
ture work, we plan to exploit corpus-based meth-
ods using patterns as in Goyal et al. (2010) com-
bined with semantic role labeling to refine the lex-
icon to distinguish which is the affected entity.
Further, to actually exploit the acquired lexicon to
process corpus data, an appropriate coarse-grained
sense disambiguation process must be added, as
Akkaya et al. (2009) and Akkaya et al. (2011) did
for subjective/objective classification.
We hope the general methodology will be ef-
fective for other semantic properties. In opin-
ion mining and sentiment analysis this is partic-
10
For reference, in 5th iteration, the +effect accuracy is
60.18% and the -effect accuracy is 69.93%, and in 6th itera-
tion, the +effect accuracy is 59.81% and the -effect accuracy
is 69.12%.
ularly needed, because different meanings of pos-
itive and negative are appropriate for different ap-
plications. This is a way to create lexicons that are
customized with respect to one’s own definitions.
It would be promising to combine our method
with other methods to enable it to find +effect
and -effect senses that are outside the coverage
of WordNet. However, a WordNet-based lexicon
gives a substantial base to build from.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and National Sci-
ence Foundation grant #IIS-0916046. We would
like to thank the reviewers for their helpful sug-
gestions and comments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190–199.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of sub-
jectivity word sense disambiguation on contextual
opinion analysis. In Proceedings of CoNLL 2011,
pages 87–96.
Pranna Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555–596.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC, pages 2200–2204.
Yoonjung Choi, Lingjia Deng, and Janyce Wiebe.
2014. Lexical acquisition for opinion inference:
A sense-level lexicon of benefactive and malefac-
tive events. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis (WASSA), pages
107–112. Association for Computational Linguis-
tics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120–125.
1190
Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of implicit
sentiments via implicature constraints. In Proceed-
ings of COLING, page 7988.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417–422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of ACL, pages 424–431.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092–
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77–86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10–18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174–181.
Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin
Choi. 2014. Connotationwordnet: Learning conno-
tation over the word+sense network. In Proceedings
of the 52nd ACL, page 15441554.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367–1373.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235–312.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity. In Proceedings of 14th
IJCAI, pages 448–453.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704–714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083–1086.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of NAACL HLT 2009,
pages 1–9.
Simon Tong and Daphne Koller. 2001. Support vector
machin active learning with applications to text clas-
sification. Journal of Machine Learning Research,
2:45–66.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4):315–346.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347–354.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scholkopf. 2004.
Learning with local and global consistency. Ad-
vances in Neural Information Processing Systems,
16:321–329.
Fernando Ziga and Seppo Kittil. 2010. Benefactives
and malefactives, Typological perspectives and case
studies. John Benjamins Publishing.
1191
