Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523–532,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
A Japanese Predicate Argument Structure Analysis using Decision Lists
Hirotoshi Taira, Sanae Fujita, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho,
Keihanna Science City,
Kyoto 619-0237, Japan
{{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jp
Abstract
This paper describes a new automatic method
for Japanese predicate argument structure
analysis. The method learns relevant features
to assign case roles to the argument of the tar-
get predicate using the features of the words
located closest to the target predicate under
various constraints such as dependency types,
words, semantic categories, parts of speech,
functional words and predicate voices. We
constructed decision lists in which these fea-
tures were sorted by their learned weights. Us-
ing our method, we integrated the tasks of se-
mantic role labeling and zero-pronoun iden-
tification, and achieved a 17% improvement
compared with a baseline method in a sen-
tence level performance analysis.
1 Introduction
Recently, predicate argument structure analysis has
attracted the attention of researchers because this
information can increase the precision of text pro-
cessing tasks, such as machine translation, informa-
tion extraction (Hirschman et al., 1999), question
answering (Narayanan and Harabagiu, 2004) (Shen
and Lapata, 2007), and summarization (Melli et
al., 2005). In English predicate argument structure
analysis, large corpora such as FrameNet (Fillmore
et al., 2001), PropBank (Palmer et al., 2005) and
NomBank (Meyers et al., 2004) have been created
and utilized. Recently, the GDA Corpus (Hashida,
2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al.,
2002) and NAIST Text Corpus (Iida et al., 2007)
were constructed in Japanese, and these corpora
have become the target of an automatic Japanese
predicate argument structure analysis system. We
conducted Japanese predicate argument structure
(PAS) analysis for the NAIST Text Corpus, which
is the largest of these three corpora, and, as far as
we know, this is the first time PAS analysis has been
conducted for whole articles of the corpus.
The NAIST Text Corpus has the following char-
acteristics, i) semantic roles for both predicates and
event nouns are annotated in the corpus, ii) three ma-
jor case roles,1 namely the ga, wo and ni-cases in
Japanese are annotated for the base form of pred-
icates and event nouns, iii) both the case roles in
sentences containing the target predicates and those
outside the sentences (zero-pronouns) are annotated,
and iv) coreference relations are also annotated.
As regards i), recently there has been an increase
in the number of papers dealing with nominalized
predicates (Pradhan et al., 2004) (Jiang and Ng,
2006) (Xue, 2006) (Liu and Ng, 2007). For exam-
ple, ‘trip’ in the sentence “During my trip to Italy, I
met him.” refers not only to the event “I met him”
but also to the event “I traveled to Italy.” As in this
example, nouns sometimes have argument structures
referring to an event. Such nouns are called event
nouns (Komachi et al., 2007) in the NAIST Text
Corpus. At the same time, the problems related to
compound nouns are also important. In Japanese, a
compound noun sometimes simultaneously contains
both an event noun and its arguments. For example,
the compound noun, ‘???? (corporate buyout)’
contains an event noun ‘?? (buyout)’ and its ac-
cusative, ‘?? (corporate).’ However, compound
1Kyoto Text Corpus has about 15 case roles.
523
nouns provide no information about syntactic de-
pendency or about case markers, so it is difficult to
specify the predicate-argument structure. Komachi
et al. investigated the argument structure of event
nouns using the co-occurrence of target nouns and
their case roles in the same sentence (Komachi et
al., 2007). In these approaches, predicates and event
nouns are dealt with separately. Here, we try to
unify these different argument structures using de-
cision lists.
As regards ii), for example, in the causative sen-
tence, ‘??????????????? (Mary
makes Tom fix dinner),’ the basic form of the
causative verb, ‘???? (make fix)’ is ‘?? (fix),’
and its nominative is ‘?? (Tom)’ and the ac-
cusative case role (wo-case) is ‘?? (dinner),’ al-
though the surface case particle is ni (dative). We
must deal with syntactic transformations in passive,
causative, and benefactive constructions when ana-
lyzing the corpus.
As regards iii) and iv), in Japanese, zero pronouns
often occur, especially when the argument has al-
ready been mentioned in previous sentences. There
have been many studies of zero-pronoun identifica-
tion (Walker et al., 1994) (Nakaiwa, 1997) (Iida et
al., 2006).
In this paper, we present a general procedure for
handling both the case role assignment of predicates
and event nouns, and zero-pronoun identification.
We use the decision list learning of rules to find the
closest words with various constraints, because with
decision lists the readability of learned lists is high
and the learning is fast.
The rest of this paper is organized as follows. We
describe the NAIST Text Corpus, which is our tar-
get corpus in Section 2. We describe our proposed
method in Section 3. The result of experiments us-
ing the NAIST Text Corpus and our method are re-
ported in Section 4 and our conclusions are provided
in Section 5.
2 NAIST Text Corpus
In the NAIST Text Corpus, three major obligatory
Japanese case roles are annotated, namely the ga-
case (nominative or subjective case), the wo-case
(accusative or direct object) and the ni-case (da-
tive or in-direct object). The NAIST Text Corpus
is based on the Kyoto Text Corpus Ver. 3.0, which
contains 38,384 sentences in 2,929 texts taken from
news articles and editorials in a Japanese newspaper,
the ‘Mainichi Shinbun’.
We divided these case roles into four types by lo-
cation in the article as in (Iida et al., 2006), i) the
case role depends on the predicate or the predicate
depends on the case role in the intra-sentence (‘de-
pendency relations’), ii) the case role does not de-
pend on the predicate and the predicate does not de-
pend on the case role in the intra-sentence (‘zero-
anaphoric (intra-sentential)’), iii) the case role is
not in the sentence containing the predicate (‘zero-
anaphoric (inter-sentential)’), and iv) the case role
and the predicate are in the same phrase (‘in same
phrase’). Here, we do not deal with exophora.
We show the distribution of the above four types
in test samples in our split of the NAIST Text
Corpus in Tables 1 and 2. In predicates, the
‘dependency relations’ type in the wo-case and
the ni-case occur frequently. In event nouns,
the ‘zero-anaphoric (intra-sentential)’ and ‘zero-
anaphoric (inter-sentential)’ types in the ga-case oc-
cur frequently. With respect to the ‘in same phrase’
type, the wo-case occurs frequently.
3 Predicate Argument Structure Analysis
using Features of Closest Words
In this section, we describe our algorithm. In the
algorithm, we used various constraints when search-
ing for the words located closest to the target predi-
cate. We described these constraints as features with
the direct products of dependency types (ic, oc, ga c,
wo c, ni c, sc, nc, fw and bw), generalization levels
(words, semantic categories, parts of speech), func-
tional words and voices.
3.1 Dependency Types
In Japanese, the functional words in a phrase (Bun-
setsu in Japanese) and the interdependency of bun-
setsu phrases are important for determining the
predicate argument structure. In accordance with
the character of the dependency between the case
roles and the predicates or event nouns, we divided
Japanese word dependency into the following seven
types that cover all dependency types in Japanese.
Additionally, we use two optional dependency types.
524
Table 1: Distribution of case roles for predicates (Test Data)
predicate
ga (Nominative) wo (Accusative) ni (Dative)
all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)
dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)
zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)
zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%)
in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%)
Table 2: Distribution of case roles for event nouns (Test Data)
event noun
ga (Nominative) wo (Accusative) ni (Dative)
all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)
dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)
zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)
zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%)
in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%)
Figure 1: Type ic
3.1.1 Incoming Connection Type (ic)
With this type, the target case role is the head-
word of a bunsetsu phrase and the case role phrase
depends on the target predicate phrase (Figure 1).
3.1.2 Outgoing Connection Type (oc)
With this type, the target case role is the headword
of a phrase and a phrase containing a target predicate
or event noun depends on the case role phrase (Fig-
ure 2).
Figure 2: Type oc
525
Figure 3: Type sc
Figure 4: Type ga c, wo c, ni c
3.1.3 ‘Within the Same Phrase’ Type (sc)
With this type, the target case role and the target
predicate or event noun are in the same phrase (Fig-
ure 3).
3.1.4 ‘Connection into Other Case role Types
(ga c, wo c, ni c)
With these types, a phrase containing the target
case role depends on a phrase containing another
predetermined case role (Figure 4). We use the terms
‘ga c’, ‘wo c’ and ‘ni c’ when the predetermined
case roles are the ga-case, wo-case and ni-case, re-
spectively.
Figure 5: Type nc
3.1.5 Non-connection Type (nc)
With this type, a phrase containing the target case
role and a phrase containing the target predicate or
event noun are in the same article, but these phrases
do not depend on each other (Figure 5).
3.1.6 Optional Type (fw and bw)
Type fw and bw stand for ‘forward’ and ‘back-
ward’ types, respectively. Type fw means the word
located closest to the target predicate or event noun
without considering functional words or voices.
With fw, the word is located between the top of the
article containing the target predicate and the target
predicate or event noun. Similarly, type bw means
the word located closest to the target predicate or
noun, which is located between the targeted predi-
cate or event noun, and the tail of the article con-
taining the predicate.
3.2 Generalization Levels
We used three levels of generalization for every case
role candidate, that is, word, semantic category, and
part of speech. Every word is annotated with a part
of speech in the Kyoto Text Corpus, and we used
these annotations. With regard to semantic cate-
gories, we annotated every word with a semantic
category based on a Japanese thesaurus, Nihongo
Goi Taikei. The thesaurus consists of a hierarchy
of 2,710 semantic classes, defined for over 264,312
nouns, with a maximum depth of twelve (Ikehara et
al., 1997). We mainly used the semantic classes of
526
Figure 6: Top 3 levels of the Japanese thesaurus, ‘Ni-
hongo Goi Taikei’
the third level, and partly the fourth level, which are
similar to semantic roles. We show the top three lev-
els of the Nihongo Goi Taikei common noun the-
saurus in Figure 6. We annotated the words with
their semantic category by hand.
3.3 Functional Word and Voice
We used a functional word in the phrase containing
the target case role and active and passive voices for
the predicate as base features.
3.4 Training Algorithm
The training algorithm used for our method is shown
in Figure 7. First, the algorithm constructs features
that search for the words located closest to the tar-
get predicate under various constraints. Next, the
algorithm learns by using linear Support Vector Ma-
chines (SVMs) (Vapnik, 1995). SVMs learn effec-
tive features by the one vs. rest method for every
case role. We used TinySVM 2 as an SVM imple-
mentation. Moreover, we construct decision lists
sorted by weight from linear SVMs. Finally, the al-
gorithm calculates the existing probabilities of case
roles for every predicate or event noun. This step
2http://chasen.org/t˜aku/software/TinySVM/
produces the criterion that decides whether or not
we will determine the case roles when there is no in-
terdependency between the case role candidate and
the predicate.
Our split of the NAIST Text Corpus has only
62,264 training samples for 2,874 predicates, and we
predict that there will be a shortage of training sam-
ples when adopting traditional learning algorithms,
such as learning algorithms using entropy. So, we
used SVMs with a high generalization capability to
learn the decision lists.
3.5 Test Algorithm
The test algorithm of our method is shown in Fig-
ure 8. In the test phase, we analyzed test samples
using decision lists and the existing probabilities of
case roles learned in the training phase. In step 1, we
determined case roles using a decision list consisting
of features exhibiting case role and predicate inter-
dependency, that is, ic, oc, ga c, wo c, and ni c. This
is because there are many cases in Japanese where
the syntactic constraint is stronger than the seman-
tic constraint when we determine the case roles. In
step 2, we determined case roles using a decision list
of sc (‘in same phrase’) for the case roles that were
not determined in step 1. This step was mainly for
event nouns. Japanese event nouns frequently form
compound nouns that contain case roles. In step 3,
we decided whether or not to proceed to the next
step by using the existing probabilities of case roles.
If the probability was less than a certain threshold
(50%), then the algorithm stopped. In step 4, we de-
termined case roles using a decision list of the fea-
tures that have no interdependency, that is, nc, fw
and bw. This step will be executed when the target
case role is syntactically necessary and determined
by the co-occurrence of the case roles and predicate
or event noun without syntactic clues, such as de-
pendency, functional words and voices.
4 Experimental Results
4.1 Experimental Setting
We performed our experiments using the NAIST
Text Corpus 1.4? (Iida et al., 2007). We used
49,527 predicates and 12,737 event nouns from arti-
cles published from January 1st to January 11th and
the editorials from January to August as training ex-
527
for each predicate pi in all predicates appeared in the training corpus do
feature list(pi) = {} ; n ? 0
clear (x, y)
for each instance pij of pi, in the training corpus do
Clear order() for all features
aij ? the article including pij
Wij ? the number of words in aij
pred index ? the word index of pij in aij
for (m = pred index? 1; m ? 1; m??) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ‘ic’, ‘nc’, ‘ga c’, ‘wo c’ or ‘ni c’ then inc order(n, dep type, wm, pij)
else if dep type == ‘sc’ then inc order(n, dep type, ‘’, ‘’)
endif
inc order(n, ‘fw’, ‘’, ‘’)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
for (m = pred index + 1; m ? Wij ; m + +) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ‘oc’, ‘nc’, ‘ga c’, ‘wo c’ or ‘ni c’ then inc order(n, dep type, wm, pij)
else if dep type == ‘sc’ then inc order(n, dep type, ‘’, ‘’)
endif
inc order(n, ‘bw’, ‘’, ‘’)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
end for
Learn linear SVMs using (x1, y1,ga), ..., (xn, yn,ga)
Learn linear SVMs using (x1, y1,wo), ..., (xn, yn,wo)
Learn linear SVMs using (x1, y1,ni), ..., (xn, yn,ni)
Make the decision list for pi, sorting features by weight.
Calculate the existing probabilities of case roles for pi.
end for
procedure get dependency type(wm, pij)
if phrase(wm) depends on phrase(pij) then return ‘ic’
else if phrase(pij) depends on phrase(wm) then return ‘oc’
else if phrase(wm) depends on phrase(pga) then return ‘ga c’
else if phrase(wm) depends on phrase(pwo) then return ‘wo c’
else if phrase(wm) depends on phrase(pni) then return ‘ni c’
else if phrase(wm) equals phrase(pij) then return ‘sc’
else return ‘nc’
end procedure
procedure inc order(n, dep type, func, voice)
Set a feature fw = (wm, dep type, func, voice) ; order(fw)++ ; if order(fw) == 1 then xn,fw ? 1
Set a feature fs = (sem(wm), dep type, func, voice) ; order(fs)++ ; if order(fs) == 1 then xn,fs ? 1
Set a feature fp = (pos(wm), dep type, func, voice) ; order(fp)++ ; if order(fp) == 1 then xn,fp ? 1
feature list(pi) ? feature list(pi)
?
{fw, fs, fp}
end procedure
Figure 7: Training algorithm
528
Step 1. Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c.
Step 2. Determine case roles using a decision list concerning sc for undetermined case roles in
Step.1.
Step 3. If the existing probability of case roles < 50 % then the program ends.
Step 4. Determine case roles using a decision list concerning nc, fw and bw types.
Figure 8: Test algorithm
amples. We used 11,023 predicates and 3,161 event
nouns from articles published on January 12th and
13th and the September editorials as development
examples. And we used 19,501 predicate and 5,276
event nouns from articles dated January 14th to 17th
and editorials dated October to December as test ex-
amples. This is a typical way to split the data.
We used the annotations in the Kyoto Text Corpus
as the interdependency of bunsetsu phrases. We used
both individual and multiple words as case roles. We
used the phrase boundaries annotated in the NAIST
Text Corpus in the training phase, and used those
annotated automatically by our system using POSs
and simple rules in the test phase. The accuracy of
the automatic annotation is about 90%.
4.2 Baseline Method
To evaluate our algorithm, we conducted experi-
ments using a baseline method. With the method,
we used only nouns that depended on predicates or
event nouns as case role candidates. If the functional
word (post-positional case) in the phrase is ‘ga’,‘wo’
and ‘ni’, we determined the ga-case, wo-case, or ni-
case for the candidates. Next, as regards event nouns
in compound nouns, if there was another word in a
compound noun containing an event noun and it co-
occurred with the event noun as a case role with a
higher probability in the training samples, then the
word was selected for the case role.
4.3 Entropy Method
The conventional approach for making decision lists
utilizes the entropy of samples selected by the
rules (Yarowsky, 1994) (Goodman, 2002). We per-
formed comparative experiments using Yarowsky’s
entropy algorithm (Yarowsky, 1994).
Table 3: Existing probabilities of case roles for predicates
and event nouns
Predicate Existing Probability
or Event Noun ga (NOM) wo (ACC) ni (DAT)
?? (use) 44.72% 82.92% 5.33%
?? (negotiation) 77.41% 30.70% 0.00%
?? (participation) 87.09% 0.00% 72.46%
??? (based on) 81.89% 0.00% 100.00%
4.4 Overall Results
The overall results are shown in Table 7. Here, ‘en-
tropy’ indicates Yarowsky’s algorithm, which uses
entropy (Yarowsky, 1994). Throughout the test data,
the F-measure (%) of our method exceeded that of
the baseline system and the ‘entropy’ system. With
the ga-case (nominative) in particular, the F-measure
increased 9 points.
Table 3 shows some examples of the existing
probabilities of case roles for predicates or event
nouns. When the probabilities are extreme values
such as the ni-case (dative) of?? (negotiation), the
wo-case (accusative) of?? (participation), and the
wo-case and ni-base of ??? (based on), we can
decide to fill the targeted case role or not with high
precision. However, it is difficult to decide to fill
the targeted case role or not when the probability is
close to 50 percent as in the ga-case of?? (use).
We show the learned decision list of the ic type
(the case role depends on the predicate or event
noun), sc type (in the same phrase) and the other
types for event noun?? (negotiation) in Tables 4, 5
and 6, respectively. Here, ‘word’ in the ‘level’
column means ‘base form of predicate’ and ‘sem’
means ’semantic category of predicate.’ In the ic
and sc type decision lists, features with semantic
categories, such as ‘REGION’, ’LOCATION’ and
‘EVENT’, occupy a higher order. In contrast, in
the list of the other types, the features that occupy
the higher order are the features of the word base
529
Table 4: Decision list for ic type of event noun?? (negotiation)
order case dep type level head word functional voice weight
word
1 ga ic word ???????? (North Korea) ? (of) active 0.9820
2 ga ic sem ?? (REGION) ? (of) active 0.6381
3 ga ic word ???? (both Japan and U.S.) ? (of) active 0.5502
4 wo ic word ?????? (establishment of joint ventures) ? (of) active 0.5288
5 wo ic word ?????? (telecommunications) ? (of) active 0.4142
6 wo ic word ???????? (North Korea) ?? (for) active 0.3168
7 wo ic word ?? (ACTION) ? (of) active 0.3083
8 ga ic sem ???? (OOV NOUN) ? (of) active 0.2939
9 wo ic word ????????? (car and auto parts sector) ? (of) active 0.2775
10 wo ic sem ? (LOCATION) ? (of) active 0.2471
Table 5: Decision list for sc type of event noun?? (negotiation)
order case dep type level head word weight
1 wo sc sem ?? (EVENT) 1.1738
2 wo sc word ?? (arrangement) 1.0000
3 ga sc word ???? (airline of Japan and China) 0.9392
4 wo sc sem ?? (MENTAL STATE) 0.8958
5 ga sc word ?????????? (financial services of Japan and U.S.) 0.8371
6 wo sc word ???? (contract extension) 0.7870
7 wo sc word ?? (joint venture) 0.7865
8 wo sc word ????? (intellectual property rights) 0.7224
9 wo sc word ??????? (car and auto parts) 0.7196
10 ga sc word ?? (Japan and North Korea) 0.6771
Table 6: Decision list for other types of event noun?? (negotiation)
order case dep type level head word functional word voice weight
1 ga fw word ?? (Japan and U.S.) 1.9954
2 ga fw word ?? (Taiwan) 1.9952
3 ga fw word ?? (U.S. and North Korea) 1.4979
4 ga fw word ?? (U.K. and China) 1.1773
5 ga nc word ?? (both nations) ? (TOP) active 1.1379
6 wo fw word ????? (diplomatic normalization) 1.0000
7 ga bw word ?? (U.S. and North Korea) 1.0000
8 ga fw word ?? (capital and labor) 1.0000
9 wo fw word ????? (automotive area) 1.0000
10 ga nc word ?? (both sides) ? (TOP) active 1.0000
Table 7: Overall results for NAIST Text Corpus (F-measure(%))
training data test data
sentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT)
baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62
entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77
our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45
530
Table 8: Results for predicates in test sets (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15
dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51
zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66
zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76
in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65
Table 9: Results for event nouns (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88
dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05
zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62
zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51
in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13
form. This means local knowledge of relations be-
tween case roles and predicates or event nouns in
the word level is more important than semantic level
knowledge.
4.5 Results for Predicates in Test Sets
We show the results we obtained for predicates in
Table 8. The results reveal that our method is supe-
rior to the baseline system. Our algorithm is partic-
ularly effective in the ga-case.
4.6 Results for Event Nouns in Test Sets
We show the results we obtained for event nouns in
Table 9. This also shows that our method is superior
to the baseline system. The precision with sc type
is high and our method is effective as regards event
nouns.
5 Conclusion
We presented a new method for Japanese automatic
predicate argument structure analysis using deci-
sion lists based on the features of the words located
closest to the target predicate under various con-
straints. The method learns the relative weights of
these different features for case roles and ranks them
using decision lists. Using our method, we inte-
grated the knowledge of case role determination and
zero-pronoun identification, and generally achieved
a high precision in Japanese PAS analysis. In par-
ticular, we can extract knowledge at various levels
from the corpus for event nouns. In future, we will
use richer constraints and research better ways of
distinguishing whether or not cases are obligatory.
Acknowledgments
We thank Ryu Iida and Yuji Matsumoto of NAIST
for the definitions of the case roles in the NAIST
Text Corpus and functional words, and Franklin
Chang for valuable comments.
References
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proc. of the Pacific Asian
Conference on Language, Information and Computa-
tion (PACLING).
Joshua Goodman. 2002. An incremental decision
list learner. In Proc. of the ACL-02 Conference
on Empirical Methods in Natural Language Process-
ing(EMNLP02), pages 17–24.
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
Lynette Hirschman, Patricia Robinson, Lisa Ferro, Nancy
Chinchor, Erica Brown, Ralph Grishman, and Beth
Sundheim. 1999. Hub-4 Event’99 general guidelines.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proc. of the 21st International Confer-
531
ence on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 625–632.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus
with predicate-argument and coreference relations. In
Proc. of ACL 2007 Workshop on Linguistic Annota-
tion, pages 132–139.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi
Taikei, A Japanese Lexicon. Iwanami Shoten, Tokyo.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese relevance-
tagged corpus (in Japanese). Proc. of the 8th Annual
Meeting of the Association for Natural Language Pro-
cessing, pages 495–498.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Learning-based argument structure
analysis of event-nouns in Japanese. In Proc. of the
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING), pages 120–128.
Chang Liu and Hwee Tou Ng. 2007. Learning predictive
structures for semantic role labeling of NomBank. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 208–215.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of SQUASH,
the SFU question answering summary handler for the
DUC-2005 summarization task. In Proc. of DUC
2005.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In Proc. of HLT-NAACL 2004 Workshop on
Frontiers in Corpus Annotation.
Hiromi Nakaiwa. 1997. Automatic identification of zero
pronouns and their antecedents within aligned sen-
tence pairs. In Proc. of the 3rd Annual Meeting of
the Association for Natural Language Processing (in
Japanese).
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proc. of
the 20th International Conference on Computational
Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Proc.
of the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP/CoNLL), pages 12–21.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag, New York.
M. Walker, M. Iida, and S. Cote. 1994. Japanese dis-
course and the process of centering. Computational
Linguistics, 20(2):193–233.
Nianwen Xue. 2006. Semantic role labeling of nomi-
nalized predicates in Chinese. In Proc. of the HLT-
NAACL, pages 431–438.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restoration
in Spanish and French. In Proc. of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 88–95.
532
