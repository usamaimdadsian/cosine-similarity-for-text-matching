Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1447–1455,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Discovering Relations between Noun Categories 
 
Thahir P Mohamed * Estevam R Hruschka Jr. Tom M Mitchell 
University Of Pittsburgh Federal University of Sao Carlos Carnegie Mellon University 
pmthahir@gmail.com estevam@cs.cmu.edu tom.mitchell@cs.cmu.edu 
   
   
Abstract 
Traditional approaches to Relation Extraction 
from text require manually defining the rela-
tions to be extracted.  We propose here an ap-
proach to automatically discovering relevant 
relations, given a large text corpus plus an ini-
tial ontology defining hundreds of noun cate-
gories (e.g., Athlete, Musician, Instrument).  
Our approach discovers frequently stated rela-
tions between pairs of these categories, using a 
two step process. For each pair of categories 
(e.g., Musician and Instrument) it first co-
clusters the text contexts that connect known 
instances of the two categories, generating a 
candidate relation for each resulting cluster.  It 
then applies a trained classifier to determine 
which of these candidate relations is semanti-
cally valid. Our experiments apply this to a text 
corpus containing approximately 200 million 
web pages and an ontology containing 122 cat-
egories from the NELL system [Carlson et al., 
2010b], producing a set of 781 proposed can-
didate relations, approximately half of which 
are semantically valid.  We conclude this is a 
useful approach to semi-automatic extension of 
the ontology for large-scale information extrac-
tion systems such as NELL. 
1 Introduction 
The Never-Ending Language Learner (NELL) 
(Carlson et al., 2010b)) is a computer system that 
learns continuously to extract facts from the web.  
NELL is given as input an initial ontology that 
specifies the semantic categories (e.g. city, compa-
ny, sportsTeam) and semantic relations (e.g. hasOf-
ficesIn(company,city), teamPlay-
sInCity(sportsTeam,city)) it must extract from the 
web.  In addition, it is provided 10-20 seed positive 
training examples for each of these categories and 
relations, along with hundreds of millions of unla-
beled web page.  Given this input, NELL applies a 
large-scale multitask, semisupervised learning 
method to learn to extract new instances of these 
categories (e.g., city(“London”)) and relations 
(e.g., teamPlaysInCity(“Steelers”,”Pittsburgh”)) 
from the web.  During the past 17 months NELL 
has been running nearly continuously, learning to 
extract over 600 categories and relations, and pop-
ulating a knowledge base containing over 700,000 
instances of these categories and relations with a 
precision of approximately 0.851. 
This paper considers the problem of automati-
cally discovering new relations to extend the on-
tology of systems such as NELL, enabling them to 
increase over time their learning and extraction 
capabilities.  More precisely, we consider the fol-
lowing problem: 
 
Input: 
· An ontology specifying a set of categories 
· A knowledge base containing instances of these 
categories (perhaps including errors) 
· A large text corpus 
 
Output: 
· A set of two-argument relations that are fre-
quently mentioned in the text corpus, and 
whose argument types correspond to categories 
in the input ontology (e.g., RiverFlows-
ThroughCity(<River>,<City>). 
· For each proposed relation, a set of instances 
(i.e. RiverFlowsThroughCity(“Nile”,”Cairo”)). 
· For each proposed relation, a set of text extrac-
tion patterns that can be used to extract addi-
tional instances of the relation (e.g., the text “X 
in the heart of Y”, where X is a known river, 
and Y a known City, suggests extracting 
RiverFlowsThroughCity(X,Y)). 
 
Note the above inputs are easily available from 
NELL in the form of its existing ontology and ex-
tracted knowledge base.  Note also that the outputs 
                                                          
*  Thahir P. Mohamed is currently at Amazon Inc. 
1  NELL?s extracted knowledge can be viewed and 
downloaded at http://rtw.ml.cmu.edu. 
1447
  
of our system are sufficient to initiate NELL?s 
learning of additional extraction methods to further 
populate each proposed relation.  One goal of this 
research is to create a system that can provide 
NELL with an ongoing set of new learning and 
extraction tasks. The system is called OntExt (On-
tology Extension System) 
 
Table 1 shows a sample of successful relations 
and corresponding relation contexts and sample 
seed instances generated by OntExt. 
 
Table 1. Examples of valid relations (generated 
by OntExt), their text extraction patterns and 
extracted instances. 
name(category1- 
main context- 
category2) 
Extraction pat-
terns 
Seed 
Instances 
 
River 
-in heart of- 
City 
 
„in heart of? 
„in the center 
of? 
„which flows 
through? 
“Seine, Paris” 
“Nile, Cairo” 
“Tiber river, Rome” 
“River arno, Florence” 
Food 
-to produce-
Chemical 
„to produce? 
„to make? 
„to form? 
“Salt, Chlorine” 
“Sugar, Carbon diox-
ide” 
“Protein, Serotonin” 
 
StadiumOrVenue 
-in downtown- 
City 
 
„in downtown? 
 
“Ford field, Detroit” 
“Superdome, New Or-
leans” 
“Turner field, Atlanta” 
 
Disease 
-caused by- 
Bacteria 
 
„caused by? 
„is the causa-
tive agent of? 
„is the cause 
of? 
“pneumonia, legionel-
la” 
“mastitis, staphylococ-
cus aureus” 
“gonorrhea, neisseria 
gonorrhoeae” 
Disease 
-destroys-
CellType 
„destroys? 
„attacks? 
"alzheimer, brain cells" 
“vitiligo", melano-
cytes" 
"aids, lymphocytes" 
County 
-county-
StateOrProvince 
„county? 
„county of? 
„county in? 
"sufolk, massachusetts" 
"marin, california" 
"sussex, delaware" 
"osceola, michigan" 
2 Background 
Traditional Relation Extraction 
We define Traditional RE systems as those that 
require the user to specify information about the 
relations to be learned. For instance, SnowBall 
(Agichtein and Gravano 2000) & CPL (Carlson et 
al. 2009) are bootstrapped learning systems that 
require manual input of relation predicates. In the-
se systems, for each relation predicate, the relation 
name (e.g. City „Capital of? Country), the seed in-
stances and the category type (e.g. City, Country, 
Celebrity etc) are provided (for domain and range). 
In CPL (Carlson et al. 2009), learning of rela-
tion/category instances is coupled by using con-
straints such as mutual exclusion relationships 
among the predicates. The authors show that this 
coupling reduces semantic drift, which commonly 
occurs with bootstrapping systems, thus leading to 
improved precision. CPL achieved 89% precision 
for the relation instances extracted (Carlson, Bet-
teridge et al. 2009).  KNOWITALL (Etzioni, Ca-
farella et al. 2005) is a web-scale relation extrac-
tion system, which requires as input the relation 
names. Hence, in these “traditional relation extrac-
tion” methods, the need to manually define the re-
lations to be extracted makes it difficult to work in 
applications having thousands of possible relation 
predicates.  
2.1 Open Relation Extraction 
Open RE methods do not require a user to manual-
ly specify the information about the relations to be 
learned, such as their names, seed examples, etc. 
TextRunner (Banko, Cararella et al. 2007) is such 
an Open Information Extraction system that re-
trieves from the web millions of relational tuples 
between noun phrase entities. TextRunner uses a 
deep linguistic parser to perform self-supervised 
learning and extracts a positive set (i.e. valid rela-
tion between entities) and a negative set (i.e. inva-
lid relationships) of relational tuples based on cer-
tain heuristics. Then, a Naive Bayes classifier is 
built having features such as part-of-speech tags of 
the words in the relation tuples, number of tokens, 
stopwords etc., and uses the labeled instances as 
the training set. This classifier runs on sentences 
from a web corpus to extract millions of relational 
tuples. However, of the 11 million high confident 
relational tuples extracted by this system only 1 
million were concrete facts (Banko, Cararella et al. 
2007). Of these concrete facts 88% were estimated 
to be correct. For instance, (Mountain View, head-
quarters of, Google) is a tuple representing a valid 
concrete fact. The remaining 90% of the tuples are 
abstract or do not have well-formed arguments or 
well-formed relations. For instance, (Einstein, de-
rived, theory) is an abstract tuple as it does not 
1448
  
have enough information to indicate a concrete fact 
(Banko, Cararella et al. 2007) because the specific 
theory which Einstein derived is missing in that 
tuple. In the tuple (45, „went to?, „Boston?), one of 
the arguments (i.e. 45) is not well formed.  
In (Banko and Etzioni, 2008) a Conditional 
Random Field (CRF) classifier is used to perform 
Open Relation Extraction which improves by more 
than 60% the F-score achieved by the Naive Bayes 
model in the TextRunner system. However the 
CRF approach does not solve the problem associ-
ated with extraction of abstract/non-well formed 
tuples. Further, in the same work, it is shown that 
Open RE has a much lower recall in comparison to 
Traditional RE systems. On four common relations 
(Acquisition, Birthplace, InvetorOf, WonAward), 
Open RE attained a recall of 18.4% in comparison 
to 58.4% achieved by Traditional RE (Banko and 
Etzioni 2008). Both Open RE systems discussed 
(Banko, Cararella et al. 2007; Banko and Etzioni 
2008) do not perform learning of the category type 
of the entities involved in the relations. They are 
single-pass and do not perform continuous learning 
to improve/extend on what has been learnt. 
2.2 Unsupervised Methods to Extract Rela-
tions between Named Entities 
In general, traditional RE methods extract concrete 
facts and have much higher recall for a given rela-
tion, than Open RE methods. This is due to the 
knowledge fed into Traditional RE methods such 
as the category type of the entities in the relation 
and seed instances for the relation. Traditional RE 
methods require the relations to be manually de-
fined and extract instances only for them. Open RE 
methods, on the other hand, do not require any 
such domain specific knowledge to be manually 
input.  They extract instances for a wide spectrum 
of relations that are not manually pre-defined.  
To overcome the drawbacks of using Traditional 
and Open RE methods, some researchers have used 
unsupervised learning methods to automatically 
generate new relations (with seeds and contexts) 
between specific categories. These automatically 
generated relations can then be used as input to 
Traditional RE systems.  
Hasegawa et.al (Hasegawa, Sekine et al. 2004), 
propose an unsupervised clustering based ap-
proach. One feature vector for each co-occurring 
NE pair is formed based on the context words in 
which the NE pair co-occurs. Then, a cosine-
similarity metric is applied to each pair of feature 
vectors to generate a “NE-pair x NE-pair” matrix. 
Clustering is done on this matrix and each cluster 
of NE-pairs corresponds to a relation predicate.  
The work by Zhang et.al (Zhang, Su et al. 2005) 
generates a shallow parse tree for each sentence 
containing a NE pair to generate relation instances. 
A tree similarity metric is used to cluster the rela-
tion instances. This method gives improved F-
score over Hasegawa et.al (Hasegawa, Sekine et al. 
2004). Further they use a specialized NE tagger 
built to recognize entities that belong to specific 
predefined categories. The aforementioned meth-
ods (Hasegawa, Sekine et al. 2004) (Zhang, Su et 
al. 2005) were tested on a news corpus to identify 
relations between only a couple of pairs of entity 
types (Person-GeoPoliticalEntity and Company-
Company).  
Both of these methods cluster NE-pairs primari-
ly based on lexical similarity of the context words 
connecting the entities. Hence NE-pairs connected 
by lexically different but semantically similar con-
text patterns (e.g. river „in heart of? city and river 
„flows through? city) would probably not get clus-
tered together. The web data is, however, much 
noisier and has a larger number of entity types (i.e. 
category predicates), thus, another issue is that for 
web scale data NE pairs X NE pairs similarity ma-
trix would not be scalable for many thousands of 
NE-pairs. 
3 Ontology Extension System - OntExt 
The OntExt system for ontology extension, pro-
posed in this paper, combines characteristics from 
both “Traditional RE” and “Open RE,” to discover 
new relations among categories that are already 
present in the ontology, and for which many in-
stances have already been extracted.  
Our proposed method for automatic relation ex-
traction offers the following advantages over the 
methods discussed above.  
• The key idea in our approach is to make use 
of redundancy of information in web data - the 
same relational fact is often stated multiple 
times in large text corpora, using different con-
text patterns.  We use this redundancy to clus-
ter together context patterns which are seman-
tically similar although they may be lexically 
dissimilar. 
1449
  
• Instead of clustering on the ‘NE-pairs X 
NE-pairs’ matrix, clustering is done on a ‘Con-
text-pattern X Context-pattern’ matrix. This is 
much more scalable as the context patterns are 
fewer in number and since our method applies 
several criteria to prune out irrelevant patterns. 
• To accommodate errors in the input catego-
ry instances and ambiguity in web data, we 
build a classifier which learns to distinguish 
valid relations from semantically invalid rela-
tions. 
 
OntExt has 3 components. 1) It starts exploring 
a large web corpus and 2) category instances ex-
tracted by CPL to generate new relations. After the 
relations are generated, 3) a classifier is developed 
to classify semantically valid relations. 
3.1 Pre-processing 
Following along the same strategy used in [Carlson 
et al., 2010], OntExt uses as input a corpus of 2 
billion sentences, which was generated by using 
the OpenNLP2 package to extract, tokenize, and 
POS-tag sentences from the 500 million web page 
English portion of the ClueWeb09 data [Callan and 
Hoy, 2009]. Before performing relation extraction, 
this corpus is preprocessed. First, sentences which 
contain a pair of known category instances are re-
trieved (e.g. the sentence “Ottawa is the capital of 
Canada.”, where „Ottawa? is a known instance of 
the „City? category and „Canada? is a known in-
stance of „Country?).  For every category pair (e.g. 
<City, Country>) the sentences containing known 
instances of both categories are grouped into a set 
S. The text between the two instances is called the 
„context pattern? (e.g. „is the capital of? is a context 
pattern). Three types of pruning are done on this 
set S. 
1. If the context pattern is a rare one (i.e. if the 
context pattern occurs in less than a threshold 
number of sentences), all sentences with that 
context pattern are removed. Thus we retain 
only frequently occurring contexts.  We use a 
threshold requiring at least 5 sentences in the 
experiments presented in Section 4. 
2. Context patterns which co-occur with very 
few instances of either category type are re-
moved. For example, the category pair <Vehi-
cle,SportsTeam> has several sentences such as 
                                                          
2  http://opennlp.sourceforge.net. 
„Car was engulfed in flames?,  „Truck was en-
gulfed in flames? etc. Note that Flames (Calga-
ry Flames) is a SportsTeam. But here flames 
clearly does not refer to a Sportsteam. This 
context „was engulfed in? connects several in-
stance of a „Vehicle? category to a single in-
stance of SportsTeam instance. Hence all sen-
tences with this context are removed. Note this 
context would not have been removed in step 1 
as that is just a threshold on the number of sen-
tences in which any pair occurs. We use a 
threshold requiring at least 3 distinct instances 
of both the domain and the range, for each 
context. 
3. Banko et.al, 2008 show that most binary re-
lational contexts fall under certain types of lex-
ico-synctatic patterns. They include context 
patterns like „C1 Verb C2?, „C1 NP Prep C2?, 
„C1 Verb Prep C2? and „C1 to Verb C2? (C1 
and C2 are category instances). Hence context 
patterns which do not fall under the above 
types are removed from the set S as they are 
not likely to produce relation instances.  
3.2 Relation Generation 
From the previous pre-processing step OntExt re-
trieves for each category pair a pruned set S? of 
sentences. Each sentence has a pair of category 
instances and the context connecting them. 
 
Algorithm 1: Relation Generator 
 
Input: One pair of Categories (C1, C2) and set of 
sentences, each containing a pair of instances 
known to belong to C1 and C2. The phrase con-
necting the instances in the sentence is the context. 
Output: Relations and their seed instances  
 
Steps: 
1. From the input sentences, build a Context by 
Context co-occurrence matrix (Shown in figure 
1). The matrix is then normalized. 
2.  Apply K-means clustering on the matrix to 
cluster the related contexts together. Each clus-
ter corresponds to a possible new relation be-
tween the two input categories. (Weka Ma-
chine Learning package [Hall et al., 2009] was 
used to perform K-means clustering. The value 
of K was set to 5 based on trial and error ex-
periments.) 
1450
  
3. Rank the known instance pairs (belonging to 
C1,C2) for each cluster and take the top 50 as 
seed instances for the relation 
 
The key data structure used by OntExt is a co-
occurrence matrix of the contexts for each category 
pair, as shown in Figure 1. In this matrix, each cell 
corresponds to the number of pairs of category in-
stances that both contexts co-occur with (e.g. the 
sentences “Vioxx can cure Arthritis” and “Vioxx is 
a treatment for Arthritis” provide a case where the 
2 contexts „can cure? and „is a treatment for? co-
occur with an instance pair [Vioxx, Arthritis]). Ini-
tially, the value of Matrix(I,j) is the number of cat-
egory instance pairs that occur with both context i 
and context j.  We then normalize each cell in the 
matrix, dividing it by by the total count for its row. 
?
?
? N
j
jiMatrix
jiMatrixjiMatrix
0
),(
),(),(
 
We also give higher weight to contexts which co-
occur with only a few contexts over ones which are 
generic and co-occur with most contexts. 
|}0),(:)({|*),(),( ?? jiMatrixjContext
NjiMatrixjiMatrix
 
 
Where N is the total number of contexts, and 
|{Context(j) : Matrix(i,j) > 0}| refers to the number 
of cells in the row Matrix(i) which are greater than 
zero. 
For example, for the <drug, disease> category 
pair after 122 contexts were obtained after prepro-
cessing. Contexts such as „to treat?, „for treatment 
of?, „medication? which all indicate the same rela-
tion (drug-to treat-disease) have high co-
occurrence values (see Figure 1). Similarly con-
texts such as „can cause?, „may cause?, „can lead to? 
(indicating the relation drug-can cause-disease) 
have high co-occurrence values (see Figure 1). 
When OntExt performs clustering on this co-
occurrence matrix the contexts with large co-
occurrences get clustered together. Each cluster is 
then used to propose a possible new relation. The 
centroid of each cluster is used to build the relation 
name. If the centroid of a cluster is the context „for 
treatment of?, then the relation name is „drug-for-
treatment-of-disease?. 
OntExt next generates seed instances for the 
proposed relation. The seed instances which co-
occur with contexts corresponding to the cluster 
centroid or close to centroid will be best repre-
sentative of the relation. So the strength of the seed 
instance is inversely proportional to the standard 
deviation of the context from the centroid of the 
relation contexts cluster.  Also the strength of the 
seed instance is directly proportional to the number 
of times it co-occurs with the context. 
 
 
 
Figure 1: This figure shows the Context by Context 
sub-matrix (with 6 contexts) for the category pair 
(Drug, Disease) and the seed instances for each 
relation.  As described in the text, each entry gives 
the normalized count of the number of known 
<drug, disease> pairs that occur with both the row 
context and the column context. 
 
To summarize, each seed instance s (pair of cat-
egory instances) is weighted as follows  
   
Where, 
Pattern_cluster is the cluster of pattern contexts for 
this given relation 
Occ(c,s) is the number of times instance „s? co-
occurs with the pattern context „c? 
sd(c) is the standard deviation of the context from 
the centroid of the pattern cluster.   
Using this metric the instances are ranked and the 
top 50 are output as initial seed instances for the 
proposed relation.   
1451
  
3.3 Classifying semantically valid relations 
More than half of the relations generated in the 
previous step are invalid due to the following rea-
sons 
1. Error in category instances: The category 
instances input to OntExt come from NELL. In 
the version of the knowledge base used in the-
se experiments, the accuracy of these instances 
was 78%. Due to the erroneous category in-
stances some invalid relations are generated by 
OntExt. For instance the generated relation, 
„condiment-wearing-clothing? with seeds 
(pig,dress), (rabbit,pants) etc. Here „pig?  and 
„rabbit? were incorrectly identified by NELL as 
instances of „condiment?. 
2. Semantic Ambiguity: Consider the generated 
relation „bakedgood-baking-magazine? with in-
stances (cookies,time), (cupcakes, people), etc. 
Here the instances „time? and „people? do not 
refer to magazines, although they can in gen-
eral.  Due to the semantic ambiguity of these 
instances this invalid relation got generated 
3. Semantically Incomplete relations: Some of 
the generated relations require a third entity or 
some more contextual information, in order to 
be considered semantically valid. For instance, 
„personUs-said-company? or „newspaper-is-
reporting-that-company?. These don?t stand by 
themselves as two-argument relational facts 
and need more information to be complete 
4. Illogical relations: Some generated relations 
simply have no real semantic meaning. These 
relations are generated due to the category in-
stances appearing together in some unrelated 
contexts. E.g. the generated relation „date-
starting-date? with seeds such as (Wednesday, 
June), (friday, July) and the relation „country-
minister-of- economicsector? with seeds (ja-
pan,agriculture), (india, industry).  
The introduction of these invalid relations can 
adversely affect the performance of NELL. How-
ever, it is a challenging problem to develop auto-
mated ways to distinguish between valid and inva-
lid relations without any domain specific 
knowledge. To approach this problem, we identi-
fied a set of features which can help characterizing 
valid and invalid relations, and which can be gen-
erated automatically. Below is a description of the 
features and the intuition behind their use for this 
classification task. 
Each generated relation has a pair of category 
types (C1, C2), a corresponding set of seed in-
stances (which are pairs of instances belonging to 
C1 and C2) and pattern contexts connecting C1 
and C2.  Let N be the number of seed instance 
pairs and N1 and N2 be number of unique instanc-
es (out of these N instance pairs) belonging to cat-
egories C1 and C2 respectively. 
1. Normalized frequency count: The frequency 
count of each category instance is obtained 
from the corpus and normalized by the catego-
ry instance with maximum count. For a given 
relation, a feature is generated by averaging 
the normalized frequency counts of the in-
stances belonging to C1. Another similar fea-
ture is generated for C2 following the same 
strategy. For example the relation <Profession 
„believe that? Movie> was generated due to 
common words like „predator?, ?earthquake? 
being identified as movie names out of con-
text. These features can help identify such in-
valid relations.   
2. Distribution of extraction patterns: NELL 
learns instances as well as extraction patterns 
for each category (e.g. the category Actor has 
extraction patterns such as „_ got an Oscar 
award?, „_ is the movie?s lead actor?). If a cat-
egory instance co-occurs in the web corpus 
with several extraction patterns belonging to 
other categories, then that instance has large 
ambiguity. We measure ambiguity of an in-
stance (i) belonging to category „C? with re-
spect to another category „M? (where M is not 
a sub type or super type of „C?) as   
Ambiguity(i,M) =  
 withoccurs-co i''  that C''in  patterns extraction of #
 ithoccurs-co i'' that M''in  patterns extraction of # 
We measure the average ambiguity for the set 
of instances (of size N) belonging to category 
C in the generated seeds as follows, 
  
??CiM NMiAmbiguityMax /),((
) 
   
Two features are generated for categories C1  
and C2 in the relation. 
3. Relationship characteristics: We identified a 
few characteristics of the relation which help 
in identifying valid relations.  If in the generat-
ed relation, most instances of C1 co-occur only 
1452
  
with very few instances of C2 (or vice versa) 
then the relation could be weak. For example, 
<Organization „Provides? EconomicSector> - 
the instance „Information? (of category Eco-
nomicSector) connects to a large percentage of 
items in the category „Organization? but does 
not express a meaningful relation. So we con-
sider the instance (in this example „Infor-
mation?, let us call it „maxconnect_instance?) 
co-occurring with maximum number of in-
stances of the other category. The percentage 
of instances it co-occurs with from among the 
total number of instances of the other category 
which are part of the seed instances is taken as 
a feature. Also if that instance is a very com-
mon word (like „information? which in several 
contexts does not refer to „EconomicSector?) 
then this could indicate the presence of an in-
valid relation. So the normalized frequency 
count of this instance (maxconnect_instance) is 
taken as another feature.  
4. Pattern Contexts: The number of pattern con-
texts attained through pattern clustering for the 
relation is taken as another feature. The pres-
ence of several pattern contexts connecting the 
instances between the two categories could in-
dicate that the relation is a valid one.  The 
presence of Hearst patterns (Hearst M, 1992) 
referring to a hyponym (“is-a”) relation in pat-
tern contexts indicates the possibility of a valid 
relation, and is taken as another feature 
Another feature is regarding how specific is 
the context pattern to this relation. If the same 
context connects say C1 instances to instances 
of several other categories apart from C2, then 
this context is not unique to this relation and 
might not indicate a meaningful valid relation-
ship. So the ratio of the number of instances in 
C2 connected to C1 versus the number of in-
stances from all categories connected to C1 by 
the most significant pattern context (i.e. cen-
troid in pattern cluster) is taken as a feature. A 
similar feature is generated for C2 as well. 
4 Experimental Setup and Results 
4.1 CPL System 
CPL (Carlson et al., 2010) is a semi-supervised 
learning system which takes in an input ontology 
(containing category and relation predicates and 
corresponding seed instances) and constraints 
(such as Mutual exclusion rules between predi-
cates). The system iteratively extracts patterns and 
instances for the category/relation predicates from 
a web corpus of around 500 million web pages.  
CPL is one learning component in NELL (the Nev-
er Ending Language Learner) (Carlson et al., 
2010b). 
4.2 Relation Generation: 
We use approximately 22,000 category instances 
belonging to 122 categories extracted by CPL at 
the end of its 20th iteration and the web corpus as 
input to perform the co-clustering described in 
Section 3.2 and generate the new relations. The 
process generated 781 relations. For each relation, 
the relation name, types of the categories involved 
in the relation and the seed instances and patterns 
for each relation were generated. Table 1 in section 
1 shows a sample of valid relations generated by 
this method. 
Tables 2, 3, 4 and 5 show invalid relations for 
each type of invalidity, “Error in the Category In-
stances”, “Semantic Ambiguity”, “Semantically 
Incomplete Relations” and “Illogical Relations” 
respectively. More specifically, Table 2 shows a 
sample of relations generated due to an entity be-
ing labeled incorrectly as to belong to a category. 
The incorrect category instances are in italics. 
Table 3 presents a sample of relations which 
were generated because of semantic ambiguity. 
Instances with ambiguity are in italics. 
Table 4 shows some of the generated relations 
which are semantically incomplete. 
Table 5 presents samples of illogical relations 
which do not establish any concrete fact. 
Table 2. Examples of Incorrect category in-
stances. 
name(category1 
-main context- 
category2) 
Relation 
Contexts 
Seed 
Instances 
SportsGame 
-Beating- 
Country 
„beating? 
 
"tournament,Sri Lanka" 
"champions, France" 
"match, canada" 
Animal 
-will eat-
Condiment 
„will eat? 
„eating? 
 
"wolf, sheep" 
"fox, rabbit" 
"lion, lamb" 
 
 
 
 
 
1453
  
 
Table 3. Examples of Semantically Ambiguous 
relations.  
Name Relation 
Contexts 
Seed 
Instances 
Bird 
-play- 
City 
„play? 
 
"Cardinals, Atlanta" 
"Ravens, Miami" 
"Eagles, Chicago" 
BakedGood 
-baking- 
Magazine 
„baking? 
"time, cakes" 
"people, cookies" 
 
Table 4. Examples of semantically incomplete 
relations. 
Name Relation 
Contexts 
Seed 
Instances 
Personus 
acknowledged 
Date 
 
„acknowledged? 
„warned? 
„met? 
 
"mr obama, tues-
day" 
"george w . bush, 
tuesday" 
"al gore, thursday" 
NewsPaper 
-is reporting 
that- 
Company 
„is reporting 
that? 
„writes that? 
„reported that? 
"financial times, 
apple" 
"wall street jour-
nal, gm" 
"wall street jour-
nal, yahoo" 
 
Table 5. Examples of relations representing 
facts that are not concrete.  
Name Relation 
Contexts 
Seed 
Instances 
Emotion 
-of living in-
StateOrPro 
vince 
„of living in? 
"joy, california" 
"excitement, colora-
do" 
"fear, iowa" 
BodyPart 
-to keep- 
BodyPart 
 
„to keep? 
„guard? 
“hand, eye” 
“nose, throat” 
“eye, brain” 
“elbow, hand” 
4.3 Relation Classification: 
To determine the feasibility of automatically classi-
fying OntExt?s proposed relations as valid or inva-
lid, we trained and tested a classifier using the fea-
tures described above, using manually assigned 
class label for some of the generated relations (252 
relations) as valid or invalid (the criteria for which 
was explained before). 115 of these 252 relations 
were found to be valid by manual evaluation. This 
shows the need for a machine learning classifier to 
identify valid/invalid relations. The various fea-
tures described earlier (such as normalized fre-
quency count, relationship characteristics, pattern 
context features, distribution of extraction patterns) 
were generated for each relation. Ten-fold cross 
validation experiments were carried out with vari-
ous classifiers.  A Random Forest classifier per-
formed the best.  Precision, recall and ROC-area is 
shown in the table below (ROC area is the area 
under the ROC curve which plots the classifier 
performance by having the True Positive Rate on 
the Y-axis and False Positive Rate on the X-axis). 
 
Table 6. Classifier performance. 
RelationType Precision Recall ROC Area 
Valid 71.6 72.2 0.804 
Invalid 76.5 75.9 0.804 
Weighted 
Avg. 
74.2 74.2 0.804 
 
These results indicate that the system is able to 
learn to identify semantically valid relations with-
out using any manually input information. The val-
id relations generated can be input to NELL, al-
lowing it to iteratively learn additional instances 
for each proposed relation. 
5 Conclusion and Future work: 
Open Relation Extraction and Traditional Relation 
Extraction have their respective strengths and 
weaknesses. The OntExt system proposed in this 
work combines the strengths of both of those 
methods. The relation predicates automatically 
generated by our approach are typed, have a mean-
ingful name identifying the relation, and are ac-
companied by suggested context patterns and seed 
instances.  These relations can be input to NELL to 
learn more instances for the relation. We propose 
in the future to integrate this relation generation 
system into NELL, to iteratively extend NELL?s 
initial ontology, providing an ongoing stream of 
new learning tasks.  After every fixed set of 
NELL?s iterations, its growing knowledge base 
would be input to the relation generation system 
which will in turn feed NELL with new relation 
predicates.  One additional area for future research 
is to extend OntExt to discover new categories in 
addition to new relations. 
1454
  
Acknowledgements  
We gratefully acknowledge support for this re-
search from Darpa, Google, Yahoo! and the Brazil-
ian research agency CNPq. We also gratefully 
acknowledge Dr. Madhavi Ganapathiraju (at Uni-
versity of Pittsburgh) for her support and encour-
agement. 
References  
Agichtein, E. and L. Gravano (2000). "Snowball: Ex-
tracting relations from large plain-text collections." 
Procs. of the Fifth ACM International Conference on 
Digital Libraries. 
Banko, M., M. Cararella, et al. (2007). "Open infor-
mation extraction from the web." In Procs. of IJCAI. 
Banko, M. and O. Etzioni (2008). "The Tradeoffs Be-
tween Open and Traditional Relation Extraction." In 
Proceedings of ACL-08. 
Callan, J., and Hoy, M. (2009). Clueweb09 data set. 
http://boston.lti.cs.cmu.edu/Data/clueweb09/. 
Carlson, A., J. Betteridge, et al. (2009). "Coupling 
Semi-Supervised Learning of Categories and Rela-
tions." Proceedings of the NAACL HLT 2009 Work-
shop on Semi-supervised Learning for Natural Lan-
guage Processing. 
A. Carlson, J. Betteridge, et al. (2010). “Coupled Semi-
Supervised Learning for Information Extraction,” 
Proceedings of the ACM International Conference on 
Web Search and Data Mining (WSDM), 2010. 
A. Carlson, J. Betteridge, et al., (2010b). “Toward an 
Architecture for Never-Ending Language Learning,” 
Proceedings of the Conference on Artificial Intelli-
gence (AAAI), 2010. 
Etzioni, O., M. Cafarella, et al. (2005). "Unsupervised 
named-entity extraction from the web: An experi-
mental study." Artificial Intelligence. 
Hasegawa, T., S. Sekine, et al. (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al. (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNLP 05. 
Hasegawa, T., S. Sekine, et al. (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al. (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNL 
Hearst, M. (1992) Automatic Acquisition of Hyponyms 
from Large Text Corpora. Proc. of the Fourteenth In-
ternational Conference on Computational Linguistics, 
Nantes, F 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten (2009); 
The WEKA Data Mining Software: An Update; 
SIGKDD Explorations, Volume 11, Issue 1.
 
1455
