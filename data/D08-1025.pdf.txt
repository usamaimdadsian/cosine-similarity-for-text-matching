Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 234–243,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
A noisy-channel model of rational human sentence comprehension under
uncertain input
Roger Levy
Department of Linguistics
University of California – San Diego
9500 Gilman Drive #0108
La Jolla, CA 92093-0108
rlevy@ling.ucsd.edu ?
Abstract
Language comprehension, as with all other
cases of the extraction of meaningful struc-
ture from perceptual input, takes places un-
der noisy conditions. If human language
comprehension is a rational process in the
sense of making use of all available infor-
mation sources, then we might expect uncer-
tainty at the level of word-level input to af-
fect sentence-level comprehension. However,
nearly all contemporary models of sentence
comprehension assume clean input—that is,
that the input to the sentence-level com-
prehension mechanism is a perfectly-formed,
completely certain sequence of input tokens
(words). This article presents a simple model
of rational human sentence comprehension
under noisy input, and uses the model to in-
vestigate some outstanding problems in the
psycholinguistic literature for theories of ra-
tional human sentence comprehension. We
argue that by explicitly accounting for input-
level noise in sentence processing, our model
provides solutions for these outstanding prob-
lems and broadens the scope of theories of hu-
man sentence comprehension as rational prob-
abilistic inference.
?Part of this work has benefited from presentation at the
21st annual meeting of the CUNY Sentence Processing Confer-
ence in Chapel Hill, NC, 14 March 2008, and at a seminar at the
Center for Research on Language, UC San Diego. I am grateful
to Klinton Bicknell, Andy Kehler, and three anonymous review-
ers for comments and suggestions, Cyril Allauzen for guidance
regarding the OpenFST library, and to Mark Johnson, Mark-
Jan Nederhof, and Noah Smith for discussion of renormalizing
weighted CFGs.
1 Introduction
Considering the adversity of the conditions under
which linguistic communication takes place in ev-
eryday life—ambiguity of the signal, environmental
competition for our attention, speaker error, and so
forth—it is perhaps remarkable that we are as suc-
cessful at it as we are. Perhaps the leading expla-
nation of this success is that (a) the linguistic sig-
nal is redundant, and (b) diverse information sources
are generally available that can help us obtain infer
the intended message (or something close enough)
when comprehending an utterance (Tanenhaus et al.,
1995; Altmann and Kamide, 1999; Genzel and Char-
niak, 2002, 2003; Aylett and Turk, 2004; Keller,
2004; Levy and Jaeger, 2007). Given the difficulty
of this task coupled with the availability of redun-
dancy and useful information sources, it would seem
rational for all available information to be used to
its fullest in sentence comprehension. This idea is
either implicit or explicit in several interactivist the-
ories of probabilistic language comprehension (Ju-
rafsky, 1996; Hale, 2001; Narayanan and Jurafsky,
2002; Levy, 2008). However, these theories have
implicitly assumed a partitioning of interactivity that
distinguishes the word as a fundamental level of
linguistic information processing: word recognition
is an evidential process whose output is nonethe-
less a specific “winner-takes-all” sequence of words,
which is in turn the input to an evidential sentence-
comprehension process. It is theoretically possible
that this partition is real and is an optimal solution
to the problem of language comprehension under
gross architectural constraints that favor modularity
234
(Fodor, 1983). On the other hand, it is also possible
that this partition has been a theoretical convenience
but that, in fact, evidence at the sub-word level plays
an important role in sentence processing, and that
sentence-level information can in turn affect word
recognition. If the latter is the case, then the ques-
tion arises of how we might model this type of infor-
mation flow, and what consequences it might have
for our understanding of human language compre-
hension. This article employs the well-understood
formalisms of probabilistic context-free grammars
(PCFGs) and weighted finite-state automata (wF-
SAs) to propose a novel yet simple noisy-channel
probabilistic model of sentence comprehension un-
der circumstances where there is uncertainty about
word-level representations. Section 2 introduces this
model. We use this new model to investigate two
outstanding problems for the theory of rational sen-
tence comprehension: one involving global infer-
ence—the beliefs that a human comprehender ar-
rives at regarding the meaning of a sentence after
reading it in its entirety (Section 3)—and one involv-
ing incremental inference—the beliefs that a com-
prehender forms and updates moment by moment
while reading each part of it (Section 4). The com-
mon challenge posed by each of these problems is
an apparent failure on the part of the comprehender
to use information made available in one part of a
sentence to rule out an interpretation of another part
of the sentence that is inconsistent with this informa-
tion. In each case, we will see that the introduction
of uncertainty into the input representation, coupled
with noisy-channel inference, provides a unified so-
lution within a theory of rational comprehension.
2 Sentence comprehension under
uncertain input
The use of generative probabilistic grammars for
parsing is well understood (e.g., Charniak, 1997;
Collins, 1999). The problem of using a probabilistic
grammar G to find the “best parse” T for a known
input string w is formulated as1
1By assumption, G is defined such that its complete pro-
ductions T completely specify the string, such that P (w|T ) is
non-zero for only one value of w.
argmax
T
PG(T |w) (I)
but a generative grammar directly defines the joint
distribution PG(T,w) rather than the conditional
distribution. In this case, Bayes’ rule is used to find
the posterior:
PG(T |w) =
P (T,w)
P (w) (II)
? P (T,w) (III)
If the input string is unknown, the problem
changes. Suppose we have some noisy evidence I
that determines a probability distribution over input
strings P (w|I). We can still use Bayes’ rule to ob-
tain the posterior:
PG(T |I) =
P (T, I)
P (I) (IV)
?
?
w
P (I|T,w)P (w|T )P (T ) (V)
Likewise, if we are focused on inferring which
words were seen given an uncertain input, we have
PG(w|I) ?
?
T
P (I|T,w)P (w|T )P (T ) (VI)
2.1 Uncertainty for a Known Input
This paper considers situations such as controlled
psycholinguistic experiments where we (the re-
searchers) know the sentence w? presented to a
comprehender, but do not know the specific input I
that the comprehender obtains. In this case, if we
are, for example, interested in the expected infer-
ences of a rational comprehender about what word
string she was exposed to, the probability distribu-
tion of interest is
P (w|w?) =
?
I
PC(w|I,w?)PT (I|w?) dI (VII)
where PC is the probability distribution used by the
comprehender to process perceived input, and PT
is the “true” probability distribution over the inputs
235
that might actually be perceived given the true sen-
tence. Since the comprehender does not observe w?
we must have conditional independence between w
and w? given I . We can then apply Bayes’ rule to
(VII) to obtain
P (w|w?) =
?
I
PC(I|w)PC(w)
PC(I)
PT (I|w?) dI
(VIII)
= PC(w)
?
I
PC(I|w)PT (I|w?)
PC(I)
dI
(IX)
? PC(w)Q(w,w?) (X)
where Q(w,w?) is proportional to the integral term
in Equation (IX). The term PC(w) corresponds
to the comprehender’s prior beliefs; the integral
term is the effect of input uncertainty. If com-
prehenders model noise rationally, then we should
have PC(I|w) = PT (I|w), and thus Q(w,w?)
becomes a symmetric, non-negative function of w
and w?; hence the effect of input uncertainty can
be modeled by a kernel function on input string
pairs. (Similar conclusions result when the poste-
rior distribution of interest is over structures T .) It
is an open question which kernel functions might
best model the inferences made in human sentence
comprehension. Most obviously the kernel func-
tion should account for noise (environmental, per-
ceptual, and attentional) introduced into the signal
en route to the neural stage of abstract sentence
processing. In addition, this kernel function might
also be a natural means of accounting for modeling
error such as disfluencies (Johnson and Charniak,
2004), word/phrase swaps, and even well-formed ut-
terances that the speaker did not intend. For pur-
poses of this paper, we limit ourselves to a simple
kernel based on the Levenshtein distance LD(w,w?)
between words and constructed in the form of a
weighted finite-state automaton (Mohri, 1997).
2.2 The Levenshtein-distance kernel
Suppose that the input word string w? consists of
words w1...n. We define the Levenshtein-distance
kernel as follows. Start with a weighted finite-state
automaton in the log semiring over the vocabulary
? with states 0 . . . n, state 0 being the start state
0
a/1
cat/3
sat/3
1
<eps>/1
a
cat/2
sat/2
a/1
cat/3
sat/3
2
<eps>/3
a/2
cat
sat/1
a/1
cat/3
sat/3
3
<eps>/3
a/2
cat/1
sat
a/1
cat/3
sat/3
Figure 1: The Levenshtein-distance kernel for multi-
word string edits. KLD(w?) is shown for ? =
{cat,sat,a}, w? = (a cat sat), and ? = 1. State 0
is the start state, and State 3 is the lone (zero-cost)
final state.
and n the (zero-cost) final state. We add two types
of arcs to this automaton: (a) substitution/deletion
arcs (i ? 1, w?) ? i, i ? 1, . . . , n, each with cost
?LD(wi, w?), for all w? ? ? ? {?}; and (b) in-
sertion loop arcs (j, w?) ? j, j ? 0, . . . , n, each
with cost ?LD(?, w?), for all w? ? ?.2 The result-
ing wFSA KLD(w?) defines a function over w such
that the summed weight of paths through the wFSA
accepting w is logQ(w,w?). This kernel allows for
the possibility of word substitutions (represented by
the transition arcs with labels that are neither wi nor
?), word deletions (represented by the transition arcs
with ? labels), and even word insertions (represented
by the loop arcs). The unnormalized probability of
each type of operation is exponential in the Leven-
shtein distance of the change induced by the oper-
ation. The term ? is a free parameter, with smaller
values corresponding to noisier input. Figure 1 gives
an example of the Levenshtein-distance kernel for a
simple vocabulary and sentence.3
2For purposes of computing the Levenshtein distance be-
tween words, the epsilon label ? is considered to be a zero-
length letter string.
3The Levenshtein-distance kernel can be seen to be sym-
metric in w,w? as follows. Any path accepting w in the
wFSA generated from w? involves the following non-zero-
cost transitions: insertions w?I1...i, deletions wD1...j , and substi-
tutions (w ? w?)S1...k. For each such path P , there will be
exactly one path P ? in the wFSA generated from w that ac-
cepts w? with insertions wD1...j , deletions w?I1...i, and substitu-
tions (w? ? w)S1...k. Due to the symmetry of the Levenshtein
distance, the paths P and P ? will have identical costs. There-
fore the kernel is indeed symmetric.
236
2.3 Efficient computation of posterior beliefs
The problem of finding structures or strings with
high posterior probability given a particular input
string w? is quite similar to the problem faced in
the parsing of speech, where the acoustic input I to
a parser can be represented as a lattice of possible
word sequences, and the edges of the lattice have
weights determined by a model of acoustic realiza-
tion of words, P (I|w) (Collins et al., 2004; Hall
and Johnson, 2003, 2004). The two major differ-
ences between lattice parsing and our problem are
(a) we have integrated out the expected effect of
noise, which is thus implicit in our choice of kernel;
and (b) the loops in the Levenshtein-distance kernel
mean that the input to parsing is no longer a lattice.
This latter difference means that some of the tech-
niques applicable to string parsing and lattice pars-
ing – notably the computation of inside probabilities
– are no longer possible using exact methods. We
return to this difference in Sections 3 and 4.
3 Global inference
One clear prediction of the uncertain-input model of
(VII)–(X) is that under appropriate circumstances,
the prior expectations PC(w) of the comprehen-
der should in principle be able to override the lin-
guistic input actually presented, so that a sentence
is interpreted as meaning—and perhaps even be-
ing—something other than it actually meant or was.
At one level, it is totally clear that comprehenders
do this on a regular basis: the ability to do this
is required for someone to act as a copy editor—
that is, to notice and (crucially) correct mistakes
on the printed page. In many cases, these types
of correction happen at a level that may be below
consciousness—thus we sometimes miss a typo but
interpret the sentences as it was intended, or ignore
the disfluency of a speaker. What has not been pre-
viously proposed in a formal model, however, is that
this can happen even when an input is a completely
grammatical sentence. Here, we argue that an ef-
fect demonstrated by Christianson et al. (2001) (see
also Ferreira et al., 2002) is an example of expec-
tations overriding input. When presented sentences
of the forms in (1) using methods that did not per-
mit rereading, and asked questions of the type Did
the man hunt the deer?, experimental participants
gave affirmative responses significantly more often
for sentences of type (1a), in which the substring the
man hunted the deer appears, than for either (1b) or
(1c).
(1) a. While the man hunted the deer ran into
the woods. (GARDENPATH)
b. While the man hunted the pheasant the
deer ran into the woods. (TRANSITIVE)
c. While the man hunted, the deer ran into
the woods. (COMMA)
This result was interpreted by Christianson et al.
(2001) and Ferreira et al. (2002) as reflecting (i)
the fact that there is a syntactic garden path in
(1a)—after reading the first six words of the sen-
tence, the preferred interpretation of the substring
the man hunted the deer is as a simple clause in-
dicating that the deer was hunted by the man—and
(ii) that readers were not always successful at revis-
ing away this interpretation when they saw the dis-
ambiguating verb ran, which signals that the deer
is actually the subject of the main clause, and that
hunted must therefore be intransitive. Furthermore
(and crucially), for (1a) participants also responded
affirmatively most of the time to questions of the
type Did the deer run into the woods? This result
is a puzzle for existing models of sentence compre-
hension because no grammatical analysis exists of
any substring of (1a) for which the deer is both the
object of hunted and the subject of ran. In fact, no
formal model has yet been proposed to account for
this effect.
The uncertain-input model gives us a means of
accounting for these results, because there are near
neighbors of (1a) for which there is a global gram-
matical analysis in which either the deer or a coref-
erent NP is in fact the object of the subordinate-
clause verb hunted. In particular, inserting the word
it either before or after the deer creates such a near
neighbor:
(2) a. While the man hunted the deer it ran
into the woods.
b. While the man hunted it the deer ran
into the woods.
We formalize this intuition within our model by us-
ing the wFSA representation of the Levenshtein-
237
ROOT ? S PUNCT. 0.0
S ? SBAR S 6.3
S ? SBAR PUNCT S 4.6
PUNCT S ? , S 0.0
S ? NP VP 0.1
SBAR ? IN S 0.0
NP ? DT NN 1.9
NP ? NNS 4.4
NP ? NNP 3.3
NP ? DT NNS 4.5
NP ? PRP 1.3
NP ? NN 3.1
VP ? VBD RB 9.7
VP ? VBD PP 2.2
VP ? VBD NP 1.2
VP ? VBD RP 8.3
VP ? VBD 2.0
VP ? VBD JJ 3.4
PP ? IN NP 0.0
Figure 2: The PCFG used in the global-inference
study of Section 3. Rule weights given as negative
log-probabilities in bits.
distance kernel. A probabilistic context-free gram-
mar (PCFG) representing the comprehender’s gram-
matical knowledge can be intersected with that
wFSA using well-understood techniques, generating
a new weighted CFG (Bar-Hillel et al., 1964; Neder-
hof and Satta, 2003). This intersection thus repre-
sents the unnormalized posterior PC(T,w|w?). Be-
cause there are loops in the wFSA generated by the
Levenshtein-distance kernel, exact normalization of
the posterior is not tractable (though see Neder-
hof and Satta, 2003; Chi, 1999; Smith and John-
son, 2007 for possible approaches to approximat-
ing the normalization constant). We can, however,
use the lazy k-best algorithm of Huang and Chiang
(2005; Algorithm 3) to obtain the word-string/parse-
tree pairs with highest posterior probability.
3.1 Experimental Verification
To test our account of the rational noisy-channel in-
terpretation of sentences such as (1), we defined a
small PCFG using the phrasal rules listed in Figure
2, with rule probabilities estimated from the parsed
Brown corpus.4 Lexical rewrite probabilities were
determined using relative-frequency estimation over
the entire parsed Brown corpus. For each of the sen-
tence sets like (1) used in Experiments 1a, 1b, and 2
of Christianson et al. (2001) that have complete lex-
ical coverage in the parsed Brown corpus (22 sets
in total), a noisy-input wFSA was constructed us-
ing KLD, permitting all words occurring more than
2500 times in the parsed Brown corpus as possi-
ble edit/insertion targets.5 Figure 3 shows the av-
erage proportion of parse trees among the 100 best
parses in the intersection between this PCFG and the
wFSA for each sentence for which an interpretation
is available such that the deer or a coreferent NP is
the direct object of hunted.6 The Levenshtein dis-
tance penalty ? is a free parameter in the model, but
the results are consistent for a wide range of ?: in-
terpretations of type (2) are more prevalent both in
terms of number mass for (1a) than for either (1b)
or (1c). Furthermore, across 9 noise values for 22
sentence sets, there were never more interpretations
of type (2) for COMMA sentences than for the cor-
responding GARDENPATH sentences, and in only
one case were there more such interpretations for
a TRANSITIVE sentence than for the corresponding
GARDENPATH sentence.
4 Incremental comprehension and error
identification
We begin taking up the role of input uncertainty for
incremental comprehension by posing a question:
4Counts of these rules were obtained using
tgrep2/Tregex tree-matching patterns (Rohde,
2005; Levy and Andrew, 2006), available online at
http://idiom.ucsd.edu/˜rlevy/papers/
emnlp2008/tregex_patterns. We have also in-
vestigated the use of broad-coverage PCFGs estimated using
standard treebank-based techniques, but found that the compu-
tational cost of inference with treebank-sized grammars was
prohibitive.
5The word-frequency cutoff was introduced for computa-
tional speed; we have obtained qualitatively similar results with
lower word-frequency cutoffs.
6We took a parse tree to satisfy this criterion if the NP
the deer appeared either as the matrix-clause subject or the
embedded-clause object, and a pronoun appeared in the other
position. In a finer-grained grammatical model, number/gender
agreement would be enforced between such a pronoun and the
NP in the posterior, so that the probability mass for these parses
would be concentrated on cases where the pronoun is it.
238
0 5 10 15 20
0
5
10
15
20
25
Levenshtein edit distance penalty (bits)
# 
M
is
le
ad
in
g 
pa
rs
es
 in
 to
p 
10
0
GardenPath
Comma
Transitive
Figure 3: Results for 100-best global inference, as
a function of the Levenshtein distance penalty ? (in
bits).
what is the optimal way to read a sentence on a page
(Legge et al., 1997)? Presumably, the goal of read-
ing is to find a good compromise between scanning
the contents of the sentence as quickly as possible
while achieving an accurate understanding of the
sentence’s meaning. To a first approximation, hu-
mans solve this problem by reading each sentence in
a document from beginning to end, regardless of the
actual layout; whether this general solution is best
understood in terms of optimality or rather as para-
sitic on spoken language comprehension is an open
question beyond the immediate scope of the present
paper. However, about 10–15% of eye movements in
reading are regressive (Rayner, 1998), and we may
usefully refine our question to when a regressive eye
movement might be a good decision. In traditional
models of sentence comprehension, the optimal an-
swer would certainly be “never”, since past observa-
tions are known with certainty. But once uncertainty
about the past is accounted for, it is clear that there
may in principle be situations in which regressive
saccades may be the best choice.
What are these situations? One possible answer
would be: when the uncertainty (e.g., measured by
entropy) about an earlier part of the sentence is high.
There are some cases in which this is probably the
correct answer: many regressive eye movements are
very small and the consensus in the eye-movement
literature is that they represent corrections for motor
error at the saccadic level. That is, the eyes over-
shoot the intended target and regress to obtain in-
formation about what was missed. However, mo-
tor error can account only for short, isolated regres-
sions, and about one-sixth of regressions are part of
a longer series back into the sentence, into a much
earlier part of the text which has already been read.
We propose that these regressive saccades might be
the best choice when the most recent observed in-
put significantly changes the comprehender’s beliefs
about the earlier parts of the sentence. To make the
discussion more concrete, we turn to another recent
result in the psycholinguistic literature that has been
argued to be problematic for rational theories of sen-
tence comprehension.
It has been shown (Tabor et al., 2004) that sen-
tences such as (3) below induce considerable pro-
cessing difficulty at the word tossed, as measured in
word-by-word reading times:
(3) The coach smiled at the player tossed a fris-
bee. (LOCALLY COHERENT)
Both intuition and controlled experiments reveal that
this difficulty seems due at least in part to the cat-
egory ambiguity of the word tossed, which is oc-
casionally used as a participial verb but is much
more frequently used as a simple-past verb. Al-
though tossed in (3) is actually a participial verb in-
troducing a reduced relative clause (and the player
is hence its recipient), most native English speakers
find it extremely difficult not to interpret tossed as a
main verb and the player as its agent—far more dif-
ficult than for corresponding sentences in which the
critical participial verb is morphologically distinct
from the simple past form ((4a), (4c); c.f. threw) or
in which the relative clause is unreduced and thus
clearly marked ((4b), (4c)).
(4) a. The coach smiled at the player thrown a
frisbee. (LOCALLY INCOHERENT)
b. The coach smiled at the player who was
tossed a frisbee.
c. The coach smiled at the player who was
thrown a frisbee.
The puzzle here for rational approaches to sentence
comprehension is that the preceding top-down con-
text provided by The coach smiled at. . . should com-
pletely rule out the possibility of seeing a main
verb immediately after player, hence a rational com-
239
prehender should not be distracted by the part-of-
speech ambiguity.7
4.1 An uncertain-input solution
The solution we pursue to this puzzle lies in the fact
that (3) has many near-neighbor sentences in which
the word tossed is in fact a simple-past tense verb.
Several possibilities are listed below in (5):
(5) a. The coach who smiled at the player
tossed a frisbee.
b. The coach smiled as the player tossed a
frisbee.
c. The coach smiled and the player tossed
a frisbee.
d. The coach smiled at the player who
tossed a frisbee.
e. The coach smiled at the player that
tossed a frisbee.
f. The coach smiled at the player and
tossed a frisbee.
The basic intuition we follow is that simple-past
verb tossed is much more probable where it appears
in any of (5a)-(5f) than participial tossed is in (3).
Therefore, seeing this word causes the comprehen-
der to shift her probability distribution about the ear-
lier part of the sentence away from (3), where it had
been peaked, toward its near neighbors such as the
examples in (5). This change in beliefs about the
past is treated as an error identification signal (EIS).
In reading, a sensible response to an EIS would be
a slowdown or a regressive saccade; in spoken lan-
guage comprehension, a sensible response would be
to allocate more working memory resources to the
comprehension task.
4.2 Quantifying the Error Identification Signal
We quantify our proposed error identification sig-
nal as follows. Consider the probability distribution
over the input up to, but not including, a position j
in a sentence w:
7This preceding context sharply distinguishes (3) from
better-known, traditional garden-path sentences such as The
horse raced past the barn fell, in which preceding context can-
not be used to correctly disambiguate the part of speech of the
ambiguous verb raced.
P (w[0,j)) (XI)
We use the subscripting [0, j) to illustrate that this
interval is “closed” through to include the beginning
of the string, but “open” at position j—that is, it in-
cludes all material before position j but does not in-
clude anything at that position or beyond. Let us
then define the posterior distribution after seeing all
input up through and including word i as Pi(w[0,j)).
We define the EIS induced by reading a word wi as
follows:
D
(
Pi(w[0,i))||Pi?1(w[0,i))
) (XII)
?
?
w?{w[0,i)}
Pi (w) log
Pi (w)
Pi?1 (w)
(XIII)
where D(q||p) is the Kullback-Leibler divergence,
or relative entropy, from p to q, a natural way of
quantifying the distance between probability distri-
butions (Cover and Thomas, 1991) which has also
been argued for previously in modeling attention and
surprise in both visual and linguistic cognition (Itti
and Baldi, 2005; Levy, 2008).
4.3 Experimental Verification
As in Section 3, we use a small probabilistic gram-
mar covering the relevant structures in the problem
domain to represent the comprehender’s knowledge,
and a wFSA based on the Levenshtein-distance ker-
nel to represent noisy input. We are interested in
comparing the EIS at the word tossed in (3) versus
the EIS at the word thrown in (4a). In this case,
the interval w[0,j) contains all the material that could
possibly have come before the word tossed/thrown,
but does not contain material at or after the position
introduced by the word itself. Loops in the prob-
abilistic grammar and the Levenshtein-distance ker-
nel pose a challenge, however, to evaluating the EIS,
because the normalization constant of the resulting
grammar/input intersection is essential to evaluat-
ing Equation (XIII). To circumvent this problem,
we eliminate loops from the kernel by allowing only
one insertion per inter-word space.8 (See Section 5
for a possible alternative).
8Technically, this involves the following transformation of
a Levenshtein-distance wFSA. First, eliminate all loop arcs.
240
ROOT ? S 0.00
S ? S-base CC S-base 7.3
S ? S-base 0.01
S-base ? NP-base VP 0
NP ? NP-base RC 4.1
NP ? NP-base 0.5
NP ? NP-base PP 2.0
NP-base ? DT N N 4.7
NP-base ? DT N 1.9
NP-base ? DT JJ N 3.8
NP-base ? PRP 1.0
NP-base ? NNP 3.1
VP/NP ? V NP 4.0
VP/NP ? V 0.1
VP ? V PP 2.0
VP ? V NP 0.7
VP ? V 2.9
RC ? WP S/NP 0.5
RC ? VP-pass/NP 2.0
RC ? WP FinCop VP-pass/NP 4.9
PP ? IN NP 0
S/NP ? VP 0.7
S/NP ? NP-base VP/NP 1.3
VP-pass/NP ? VBN NP 2.2
VP-pass/NP ? VBN 0.4
Figure 4: The grammar used for the incremental-
inference experiment of Section 4. Rule weights
given as negative log-probabilities in bits.
Figure 4 shows the (finite-state) probabilistic
grammar used for the study, with rule probabilities
once again determined from the parsed Brown cor-
pus using relative frequency estimation. To calcu-
late the distribution over strings after exposure to
the i-th word in the sentence, we “cut” the input
wFSA such that all transitions and arcs after state
2i+2 were removed and replaced with a sequence of
states j = 2i + 3, . . . ,m, with zero-cost transitions
(j?1, w?) ? j for all w? ? ??{?}, and each new j
Next, map every state i onto a state pair in a new wFSA
(2i, 2i+1), with all incoming arcs in i being incoming into 2i,
all outgoing arcs from i being outgoing from 2i + 1, and new
transition arcs (2i, w?) ? 2i + 1 for each w? ? ? ? {?} with
cost LD(?, w?). Finally, add initial state 0 to the new wFSA
with transition arcs to state 1 for all w? ? ? ? {?} with cost
LD(?, w?). A final state i in the old wFSA corresponds to a
final state 2i + 1 in the new wFSA.
0 1 2 3 4 5 6 7
0.
0
0.
1
0.
2
0.
3
0.
4
Levenshtein edit distance penalty (bits)
EI
S
Locally coherent
Locally incoherent
Figure 5: The Error Identification Signal (EIS) for
(3) and (4a), as a function of the Levenshtein dis-
tance penalty ? (in bits)
being a zero-cost final state.9 Because the intersec-
tion between this “cut” wFSA and the probabilistic
grammar is loop-free, it can be renormalized, and
the EIS can be calculated without difficulty. All the
computations in this section were carried out using
the OpenFST library (Allauzen et al., 2007).
Figure 5 shows the average magnitude of the EIS
for sentences (3) versus (4a) at the critical word po-
sition tossed/thrown. Once again, the Levenshtein-
distance penalty ? is a free parameter in the model,
so we show model behavior as a function of ?, for
the eight sentence pairs in Experiment 1 of Tabor
et al. with complete lexical and syntactic coverage
for the grammar of Figure 4. For values of ? where
the EIS is non-negligible, it is consistently larger at
the critical word (tossed in (3), thrown in (4a)) in
the COHERENT condition than in the INCOHERENT
condition. Across a range of eight noise levels, 67%
of sentence pairs had a higher EIS in the COHERENT
condition than in the INCOHERENT condition. Fur-
thermore, the cases where the INCOHERENT condi-
tion had a larger EIS occurred only for noise levels
below 1.1 and above 3.6, and the maximum such EIS
was quite small, at 0.067. Overall, the model’s be-
havior is consistent with the experimental results of
Tabor et al. (2004), and can be explained through the
intuition described at the end of Section 4.1.
9The number of states added had little effect on results, so
long as at least as many states were added as words remained in
the sentence.
241
5 Conclusion
In this paper we have outlined a simple model of ra-
tional sentence comprehension under uncertain in-
put and explored some of the consequences for out-
standing problems in the psycholinguistic literature.
The model proposed here will require further em-
pirical investigation in order to distinguish it from
other proposals that have been made in the liter-
ature, but if our proposal turns out to be correct
it has important consequences for both the theory
of language processing and cognition more gener-
ally. Most notably, it furthers the case for ratio-
nality in sentence processing; and it eliminates one
of the longest-standing modularity hypotheses im-
plicit in work on the cognitive science of language:
a partition between systems of word recognition
and sentence comprehension (Fodor, 1983). Unlike
the pessimistic picture originally painted by Fodor,
however, the interactivist picture resulting from our
model’s joint inference over possible word strings
and structures points to many rich details that still
need to be filled in. These include questions such as
what kernel functions best account for human com-
prehenders’ modeling of noise in linguistic input,
and what kinds of algorithms might allow represen-
tations with uncertain input to be computed incre-
mentally.
The present work could also be extended in sev-
eral more technical directions. Perhaps most notable
is the problem of the normalization constant for the
posterior distribution over word strings and struc-
tures; this problem was circumvented via a k-best
approach in Section 3 and by removing loops from
the Levenshtein-distance kernel in Section 4. We
believe, however, that a more satisfactory solution
may exist via sampling from the posterior distribu-
tion over trees and strings. This may be possible
either by estimating normalizing constants for the
posterior grammar using iterative weight propaga-
tion and using them to obtain proper production rule
probabilities (Chi, 1999; Smith and Johnson, 2007),
or by using reversible-jump Markov-chain Monte
Carlo (MCMC) techniques to sample from the pos-
terior (Green, 1995), and estimating the normaliz-
ing constant with annealing-based techniques (Gel-
man and Meng, 1998) or nested sampling (Skilling,
2004). Scaling the model up for use with treebank-
size grammars is another area for technical improve-
ment.
Finally, we note that the model here could poten-
tially find practical application in grammar correc-
tion. Although the noisy channel has been in use for
many years in spelling correction, our model could
be used more generally for grammar corrections, in-
cluding insertions, deletions, and (with new noise
functions) potentially changes in word order.
References
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,
and Mohri, M. (2007). OpenFst: A general
and efficient weighted finite-state transducer library.
In Proceedings of the Ninth International Confer-
ence on Implementation and Application of Au-
tomata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
http://www.openfst.org.
Altmann, G. T. and Kamide, Y. (1999). Incremental in-
terpretation at verbs: restricting the domain of subse-
quent reference. Cognition, 73(3):247–264.
Aylett, M. and Turk, A. (2004). The Smooth Sig-
nal Redundancy Hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31–56.
Bar-Hillel, Y., Perles, M., and Shamir, E. (1964). On for-
mal properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application. Addison-Wesley.
Charniak, E. (1997). Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
AAAI, pages 598–603.
Chi, Z. (1999). Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
Christianson, K., Hollingworth, A., Halliwell, J. F., and
Ferreira, F. (2001). Thematic roles assigned along the
garden path linger. Cognitive Psychology, 42:368–
407.
Collins, C., Carpenter, B., and Penn, G. (2004). Head-
driven parsing for word lattices. In Proceedings of
ACL.
Collins, M. (1999). Head-Driven Statistical Models for
Natural Language Parsing. PhD thesis, University of
Pennsylvania.
Cover, T. and Thomas, J. (1991). Elements of Information
Theory. John Wiley.
Ferreira, F., Ferraro, V., and Bailey, K. G. D. (2002).
Good-enough representations in language comprehen-
sion. Current Directions in Psychological Science,
11:11–15.
242
Fodor, J. A. (1983). The Modularity of Mind. MIT Press.
Gelman, A. and Meng, X.-L. (1998). Simulating nor-
malizing constants: from importance sampling to
bridge sampling to path sampling. Statistical Science,
13(2):163–185.
Genzel, D. and Charniak, E. (2002). Entropy rate con-
stancy in text. In Proceedings of ACL.
Genzel, D. and Charniak, E. (2003). Variation of entropy
and parse trees of sentences as a function of the sen-
tence number. In Empirical Methods in Natural Lan-
guage Processing, volume 10.
Green, P. J. (1995). Reversible jump Markov chain Monte
Carlo and Bayesian model determination. Biometrika,
82:711–732.
Hale, J. (2001). A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of NAACL, vol-
ume 2, pages 159–166.
Hall, K. and Johnson, M. (2003). Language modeling
using efficient best-first bottom-up parsing. In Pro-
ceedings of the IEEE workshop on Automatic Speech
Recognition and Understanding.
Hall, K. and Johnson, M. (2004). Attention shifting for
parsing speech. In Proceedings of ACL.
Huang, L. and Chiang, D. (2005). Better k-best parsing.
In Proceedings of the International Workshop on Pars-
ing Technologies.
Itti, L. and Baldi, P. (2005). Bayesian surprise attracts
human attention. In Advances in Neural Information
Processing Systems.
Johnson, M. and Charniak, E. (2004). A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL.
Jurafsky, D. (1996). A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive Sci-
ence, 20(2):137–194.
Keller, F. (2004). The entropy rate principle as a pre-
dictor of processing effort: An evaluation against eye-
tracking data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 317–324, Barcelona.
Legge, G. E., Klitz, T. S., and Tjan, B. S. (1997). Mr.
Chips: An ideal-observer model of reading. Psycho-
logical Review, 104(3):524–553.
Levy, R. (2008). Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.
Levy, R. and Andrew, G. (2006). Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proceedings of the 2006 conference on Lan-
guage Resources and Evaluation.
Levy, R. and Jaeger, T. F. (2007). Speakers optimize in-
formation density through syntactic reduction. In Ad-
vances in Neural Information Processing Systems.
Mohri, M. (1997). Finite-state transducers in language
and speech processing. Computational Linguistics,
23(2):269–311.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Advances in Neural Informa-
tion Processing Systems, volume 14, pages 59–65.
Nederhof, M.-J. and Satta, G. (2003). Probabilistic pars-
ing as intersection. In Proceedings of the International
Workshop on Parsing Technologies.
Rayner, K. (1998). Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3):372–422.
Rohde, D. (2005). TGrep2 User Manual, version 1.15
edition.
Skilling, J. (2004). Nested sampling. In Fischer, R.,
Preuss, R., and von Toussaint, U., editors, Bayesian in-
ference and maximum entropy methods in science and
engineering, number 735 in AIP Conference Proceed-
ings, pages 395–405.
Smith, N. A. and Johnson, M. (2007). Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics, 33(4):477–491.
Tabor, W., Galantucci, B., and Richardson, D. (2004).
Effects of merely local syntactic coherence on sen-
tence processing. Journal of Memory and Language,
50(4):355–370.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.,
and Sedivy, J. C. (1995). Integration of visual and
linguistic information in spoken language comprehen-
sion. Science, 268:1632–1634.
243
