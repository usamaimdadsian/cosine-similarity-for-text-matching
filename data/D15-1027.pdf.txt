Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
A Model of Zero-Shot Learning of Spoken Language Understanding
Majid Yazdani
Computer Science Department
University of Geneva
majid.yazdani@unige.ch
James Henderson
Xerox Research Center Europe
james.henderson@xrce.xerox.com
Abstract
When building spoken dialogue systems
for a new domain, a major bottleneck is
developing a spoken language understand-
ing (SLU) module that handles the new
domain’s terminology and semantic con-
cepts. We propose a statistical SLU model
that generalises to both previously unseen
input words and previously unseen out-
put classes by leveraging unlabelled data.
After mapping the utterance into a vector
space, the model exploits the structure of
the output labels by mapping each label
to a hyperplane that separates utterances
with and without that label. Both these
mappings are initialised with unsupervised
word embeddings, so they can be com-
puted even for words or concepts which
were not in the SLU training data.
1 Introduction
Spoken Language Understanding (SLU) in dia-
logue systems is the task of taking the utterance
output by a speech recognizer and assigning it a
semantic label that represents the dialogue actions
of that utterance accompanied with their associ-
ated attributes and values. For example, the utter-
ance ”I would like Chinese food” is labelled with
inform(food=Chinese), in which inform is the dia-
logue action that provides the value of the attribute
food that is Chinese.
Dialogue systems often use hand-crafted gram-
mars for SLU, such as Phoenix (Ward, 1994),
which are expensive to develop, and expensive
to extend or adapt to new attributes and values.
Statistical SLU models are usually trained on the
data obtained from a specific domain and loca-
tion, using a structured output classifier that can
be discriminative (Pradhan et al., 2004; Kate and
Mooney, 2006; Henderson et al., 2012) or genera-
tive (Schwartz et al., 1996; He and Young, 2005).
Gathering and annotating SLU data is costly and
time consuming and therefore SLU datasets are
small compare to the number of possible labels.
Because training sets for a new domain are
small, or non-existent, learning is often an in-
stance of Zero-shot or One-shot learning prob-
lems (Palatucci et al., 2009; L. Fei-Fei; Fergus,
2006), in which zero or few examples of some
output classes are available during the training.
For example, in the restaurant reservation domain,
not all possible combinations of foods and dia-
logue actions may be included in the training set.
The general idea to solve this type of problems is
to map the input and class labels to a semantic
space of usually lower dimension in which simi-
lar classes are represented by closer points in the
space (Palatucci et al., 2009; Weston et al., 2011;
Weston et al., 2010). Usually unsupervised knowl-
edge sources are used to form semantic codes of
the labels that helps us to generalize to unseen la-
bels.
On the other hand, there are also different ways
to express the same meaning, and similarly, most
of them can not be included in the training set.
For instance, the system may have seen ”Please
give me the telephone number” in training, but the
user might ask ”Please give me the phone” at test
time. This problem, feature sparsity, is a common
issue in many NLP tasks. Decomposition of in-
put feature parameters using vector-matrix mul-
tiplication (Bengio et al., 2003; Collobert et al.,
2011; Collobert and Weston, 2008) has addressed
this sparsity issue successfully in previous work.
In this way, by sharing the word representations
and composition matrices, we can overcome fea-
ture sparsity by producing similar representations
for similar utterances.
In order to represent words and concepts we
use word embeddings, which are a form of vec-
tor space model. Word embeddings have proven
to be effective models of semantic representation
244
of words in various NLP tasks (Baroni et al., 2014;
Yazdani and Popescu-Belis, 2013; Collobert et al.,
2011; Collobert and Weston, 2008; Huang et al.,
2012; Mikolov et al., 2013b). In addition to pa-
rameter sharing, these representations enable us
to leverage large scale unlabelled data. Because
word embeddings trained on unlabeled data reflect
the similarity between words, they help the model
generalize from the words in the original training
corpora to the words in the new extended domain,
and help generalize from small amounts of data in
the extended domain.
The contribution of this paper is to build a rep-
resentation learning classifier for the SLU task that
can generalize to unseen words and labels. For ev-
ery utterance we learn how to compose the word
vectors to form the semantics of that utterance for
this task of language understanding. Furthermore,
we learn how to compose the semantics of each la-
bel from the semantics of the words used to name
that label. This enables us to generalize to unseen
labels.
In this work we use the word2vec software of
Mikolov et al. (2013a)
1
to induce unsupervised
word embeddings that are used to initialize word
embedding parameters. For this, we use an En-
glish Wikipedia dump as our unlabelled training
corpus, which is a diverse broad-coverage corpus.
It has been shown (Baroni et al., 2014; Mikolov
et al., 2013b) that these embeddings capture lex-
ical similarities even when they are trained on a
diverse corpus like Wikipedia. We test our models
on a restaurant booking domain. We investigate
domain adaptation by adding new attribute types
(e.g. goodformeal) and new attribute values (e.g.
Hayes Valley as a restaurant location). Our exper-
iments indicate that our model has better perfor-
mance compared to a hand-crafted system as well
as a SVM baseline.
2 SLU Datasets
The dialogue utterances used to build the SLU
dataset were collected during a trial of online di-
alogue policy adaptation for a restaurant reserva-
tion system based in San Francisco. The trial be-
gan with (area, pricerange and food), and adapted
the Interaction Manager online to handle the ad-
ditional attribute types near, allowedforkids, and
goodformeal (Ga?sic et al., 2014). User utterances
from these trials were transcribed and annotated
1
https://code.google.com/p/word2vec/
with dialogue acts by an expert, and afterwards
edited by another expert
2
. Each user utterance was
annotated with a set of labels, where each label
consists of an act type (e.g. inform, request), an
attribute type (e.g. foodtype, pricerange), and an
attribute value (e.g. Chinese, Cheap).
The dataset is separated into four subsets,
SFCore, SF1Ext, SF2Ext and SF3Ext, each
with an increasing set of attribute types, as speci-
fied in Table 1. This table also gives the total num-
ber of utterances in each data set. For our first ex-
periment, we split each dataset into about 15% for
the testing set and 85% for the training set. For our
second experiment we use each extended subset
for testing and its preceding subsets for training.
Ontology Attribute types ( # of values ) # of utterances
SFCore food(59), area(155), pricerange(3) 1103
SF1Ext SFCore + near(39) 1810
SF2Ext SF1Ext + allowedforkids(2) 1571
SF3Ext SF2Ext +goodformeal(4) 1518
Table 1: Domains for San Francisco (SF) restau-
rants expanding in complexity
3 A Dialogue Act Representation
Learning Classifier
The SLU model is run on each hypothesis output
by the ASR component, and tries to predict the
correct set of dialogue act labels for each hypoth-
esis. This problem is in general an instance of
multi-label classification, because a single utter-
ance can have multiple dialogue act labels. Also,
these labels are structured, since each label consist
of an act type, an attribute type, and an attribute
value. Each label component also has canonical
text associated with it, which is the text used to
name the label component (e.g. “Chinese” as a
value).
The number of possible dialogue acts grows
rapidly as the domain is extended with new at-
tribute types and values, making this task one of
multi-label classification with a very large number
of labels. One natural approach to this task is to
train one binary classifier for each possible label,
to decide whether or not to include it in the output.
In our case, this requires training a large number
of classifiers, and it is impossible to generalize to
2
This data is publically available from
https://sites.google.com/site/
parlanceprojectofficial/home/
datarepository
245
dialogue acts that include attributes or values that
were not in the training set since there won’t be
any parameter sharing among label classifiers.
In our alternative approach, we build the rep-
resentation of the utterance and the representation
of the label from their constituent words, then we
check if these representations match or not. In the
following we explain in details this representation
learning model.
3.1 Utterance Representation Learning
In this section we explain how to build the utter-
ance representation from its constituent words. In
addition to words, we use bigrams, since they have
been shown previously to be effective features for
this task (Henderson et al., 2012). Following the
success in transfer learning from parsing to under-
standing tasks (Henderson et al., 2013; Socher et
al., 2013), we use dependency parse bigrams in
our features as well. We learn to build a local rep-
resentation at each word position in the utterance
by using the word representation, adjacent word
representations, and the head word representation.
Let ?(w) be a d dimensional vector representing
the word w, and ?(U
i
) be a h dimensional vector
which is the local representation at word position
i. We compute the local representation as follows:
?(U
i
) = ?(?(w
i
)W
word
+ ?(w
h
)W
parse
R
k
+
?(w
j
)W
previous
+ ?(w
k
)W
next
) (1)
in which w
h
is the head word with the depen-
dency relation R
k
to w
i
, and w
j
and w
k
are the
previous and next words. W
word
is a d × h ma-
trix that transforms the word embedding to hidden
representation inputs. W
parse
R
k
is a d × h ma-
trix for the relation R
k
that similarly transforms
the head word embedding (so W
parse
is a tensor),
and W
previous
and W
next
similarly transform the
previous and next words’ embeddings. Figure 1
depicts this representation building at each word.
3.2 Label Representation Learning
One standard way to address the problem of multi-
label classification is building binary classifiers
for each possible label. Large margin classifiers
have been shown to be an effective tool for this
task (Pradhan et al., 2004; Kate and Mooney,
2006). We use the same idea of binary classifiers
to learn one hyperplane per label, which separates
the utterances with this label from all other utter-
ances, with a large margin. In the standard way of
Figure 1: The multi-label classifier
building the classifier, each label’s hyperplane is
independent of other labels. To extend this model
to a zero-shot learning classifier, we use parame-
ter sharing among label hyperplanes so that similar
labels have similar hyperplanes.
We exploit the structure of labels by assuming
that each hyperplane representation is a compo-
sition of representations of the label’s constituent
components, namely dialogue action, attribute and
attribute value. We learn the composition function
and the constituent representations while training
the classifiers, using the labelled SLU data. The
constituent representations are initialised as the
word embeddings for the label constituent’s name
string, such as “inform”, “food” and “Chinese”,
where these embeddings are trained on the unla-
belled data. Figure 1 depicts the classifier model.
We define the hyperplane of the label a
j
(att
k
=
val
l
) with its normal vector W
a
j
,att
k
,val
l
as:
W
a
j
,att
k
,val
l
= ?([?(a
j
), ?(att
k
), ?(val
l
)]W
ih
)W
ho
where ?(·) is the same mapping to d dimensional
word vectors that is used above in the utterance
representation, W
ih
is a 3d× h matrix and W
ho
is
a h × h matrix. The score of each local represen-
tation vector ?(U
i
) is its distance from this label
hyperplane, which is computed as the dot product
of the local vector ?(U
i
) with the normal vector
W
a
j
,att
k
,val
l
.
We sum these local scores for each po-
sition i to build the whole utterance score:
?
i
?(U
i
)W
T
a
j
,att
k
,val
l
. Alternatively we can think
of this computation as summing the local vectors
to get a whole-utterance representation ?(U) =
?
i
?(U
i
) and then doing the dot product. The
246
pooling method (sum) used in the model is (inten-
tionally) over-simplistic. We did not want to dis-
tract from the main contribution of the paper, and
our dataset did not justify any more complex solu-
tion since utterances are short. It can be replaced
by more powerful approaches if it is needed.
To train a large margin classifier, we train all
the parameters such that the score of an utterance
is bigger than a margin for its labels and less than
the negative margin for all other labels. Thus, the
loss function is as follows:
min
?
?
2
?
2
+
?
U
max(0, 1?y
?
i
?(U
i
)W
T
a
j
,att
k
,val
l
)
(2)
where ? is all the parameters of the model, namely
?(w
i
) (word embeddings), W
word
, W
Parse
,
W
previous
, W
next
, W
ih
, and W
ho
. y is either 1 or
?1 depending whether the input U has that label
or not.
To optimize this large margin classifier we per-
form stochastic gradient descent by using the ada-
grad algorithm on this primal loss function, sim-
ilarly to Pegasos SVM (Shalev-Shwartz et al.,
2007), but here we backpropagate the errors to
the representations to train the word embeddings
and composition functions. In each iteration of the
stochastic training algorithm, we randomly select
an utterance and its labels as positive examples
and choose randomly another utterance with a dif-
ferent label as a negative example. When choos-
ing the negative sample randomly, we sample ut-
terances with the same dialogue act but different
attribute or value with 4 times higher probability
than utterances with a different dialogue act. This
biased negative sampling speeds up the training
process since it provides more difficult training ex-
amples to the learner.
The model is able to address the adaptivity is-
sues because the utterance and the dialogue act
representations are in the same space using the
same shared parameters ?(w), which are ini-
tialised with unsupervised word embeddings. It
has been shown that such word embeddings cap-
ture word similarities and hence the classifier is
no longer ignorant about any new attribute type or
attribute value. Also, there is parameter sharing
between dialogue acts because these word/label
embeddings are shared, and the matrices for the
composition of these representations are the same
across all dialogue acts. This can help overcome
sparsity in the SLU training set by transferring
learning between similar situations and similar
dialogue act triples. For example, if the train-
ing set does not contain any examples of the act
”request(postcode)”, but many examples of ”re-
quest(phone)”, sharing the parameters can help
with the recognition of ”request(postcode)” in ut-
terances similar to ”request(phone)”. Moreover,
the SLU model is to some extent robust against
paraphrasing in the input utterance because it
maps the utterance to a semantic space, and uses
parse bigrams. More sophisticated vector-space
semantic representations of the utterance are an
area for future work, but should be largely orthog-
onal to the contribution of this paper.
To find the set of compatible dialogue acts for a
given utterance, we should check all possible dia-
logue acts. This can severely slow down SLU. To
avoid testing all possible dialogue combinations,
we build three different classifiers: The first one
recognises the act types in the utterance, the sec-
ond one recognises the attribute types for each of
the chosen act types, and the third classifier recog-
nises the full dialogue acts as we described above,
but only for the chosen pairs of act types and at-
tribute types.
4 SLU Experiments
In the first experiment, we measure SLU perfor-
mance trained on all available data, by building a
dataset that is the union of all the above datasets.
This measures the performance of SLU when there
is a small amount of data for an extended do-
main. This dataset, similarly to SF3Ext, has 6
main attribute types. Table 2 shows the perfor-
mance of this model. We report as baselines the
performance of the Phoenix system (hand crafted
for this domain) and a binary linear SVM trained
on the same data. The hidden layers have size
h=d=50. For this experiment, we split each dataset
into about 15% for the testing set and 85% for the
training set.
System Outputs Precision Recall F-core
Phoenix 516 84.10 41.65 55.71
SVM 690 65.03 52.45 58.06
Our 932 90.24 81.15 85.45
Table 2: Performance on union of data (SF-
Core+SF1Ext+SF2Ext+SF3Ext)
Our SLU model can adapt well to the extended
domain with more attribute types. We observe
247
Test set
model, train set SF1Ext SF2Ext SF3Ext
P—R—F P—R—F P—R—F
Our SFcore 73.36—66.11—69.54 74.61—59.73—66.34 72.54—53.86—61.81
SVM SFcore 50.66— 38.7— 43.87 49.64—34.70— 40.84 48.99—30.91—37.90
Our SF1Ext 83.18—66.08—73.65 78.32—59.98—67.93
SVM SF1Ext 58.72—41.71—48.77 53.25—34.88—42.15
Our SF2Ext 84.12—67.78—75.07
SVM SF2Ext 59.27—42.80—49.70
Table 3: SLU performance: trained on a smaller domain and tested on more inclusive domains.
particularly that the recall is almost twice as high
as the hand-crafted baseline. This shows that our
SLU can recognise most of the dialogue acts in
an utterance, where the rule-based Phoenix sys-
tem and a classifier without composed output can-
not. Overall there are 1042 dialogue acts in the
test set. SLU recall is very important in the over-
all dialogue system performance, as the effect of a
missed dialogue act is hard to handle for the Inter-
action Manager. Both hand-crafted and our system
show relatively high precision.
In the next experiment, we measure how well
the new SLU model performs in an extended do-
main without any training examples from that ex-
tended domain. We train a SLU model on each
subset, and test it on each of the more inclusive
subsets. Table 3 shows the results.
Not surprisingly, the performance is better if
SLU is trained on a similar domain to the test do-
main, and adding more attribute types and values
decreases the performance more. But our SLU
can generalise very well to the extended domain,
achieving much better generalisation that the SVM
model.
4.1 Conclusion
In this paper, we describe a new SLU model
that is designed for improved domain adaptation.
The multi-label classification problem of dialogue
act recognition is addressed with a classifier that
learns to build an utterance representation and a
dialogue act representation, and decides whether
or not they are compatible. The dialogue act repre-
sentation is a vector composition of its constituent
labels’ embeddings, and is trained as the hyper-
plane of a large margin binary classifier for that di-
alogue act. The utterance representation is trained
as a composition of word embeddings. Since the
utterance and the dialogue act representations are
both built using unsupervised word embeddings
and share these embedding parameters, the model
can address the issues of domain adaptation. Word
embeddings capture word similarities, and hence
the classifier is able to generalise from known at-
tribute types or values to similar novel attribute
types or values. We tested this SLU model on
datasets where the number of attribute types and
values is increased, and show much better re-
sults than the baselines, especially in recall. The
model succeeds in both adapting to an extended
domain using relatively few training examples and
in recognising novel attribute types and values.
Acknowledgments
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE), and Hasler
foundation project no. 15019, Deep Neural Net-
work Dependency Parser for Context-aware Rep-
resentation Learning. The authors also would like
to thank Dr.Helen Hastie for her help in annotating
the dataset.
References
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 238–247.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 160–167.
248
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
M Ga?sic, D Kim, P Tsiakoulis, C Breslin, M Hender-
son, M Szummer, B Thomson, and S Young. 2014.
Incremental on-line adaptation of pomdp-based dia-
logue managers to extended domains.
Yulan He and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. Computer
Speech and Language, 19:85–106.
Matthew Henderson, Milica Ga?si´c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Comput. Linguist., 39(4):949–998.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, ACL-44, pages 913–920.
R.; Perona L. Fei-Fei; Fergus. 2006. One-shot learning
of object categories. IEEE Transactions on Pattern
Analysis Machine Intelligence, 28:594–611, April.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT-2013).
Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning
with semantic output codes. In Advances in Neu-
ral Information Processing Systems 22, pages 1410–
1418.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, and
James H. Martin. 2004. Shallow semantic parsing
using support vector machines.
Richard Schwartz, Scott Miller, David Stallard, and
John Makhoul. 1996. Language understanding us-
ing hidden understanding models. In Spoken Lan-
guage, 1996. ICSLP 96. Proceedings., Fourth Inter-
national Conference on, volume 2, pages 997–1000.
IEEE.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of the 24th Interna-
tional Conference on Machine Learning, pages 807–
814.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642.
Wayne Ward. 1994. Extracting information in sponta-
neous speech. In ICSLP.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Mach.
Learn., 81.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary image
annotation. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intelli-
gence - Volume Volume Three, pages 2764–2770.
Majid Yazdani and Andrei Popescu-Belis. 2013. Com-
puting text semantic relatedness using the contents
and links of a hypertext encyclopedia. Artif. Intell.,
194:176–202.
249
