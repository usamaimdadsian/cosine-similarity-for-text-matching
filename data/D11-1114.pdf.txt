Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Exact Inference for Generative Probabilistic
Non-Projective Dependency Parsing
Shay B. Cohen
School of Computer Science
Carnegie Mellon University, USA
scohen@cs.cmu.edu
Carlos Go´mez-Rodr?´guez
Departamento de Computacio´n
Universidade da Corun˜a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We describe a generative model for non-
projective dependency parsing based on a sim-
plified version of a transition system that has
recently appeared in the literature. We then
develop a dynamic programming parsing al-
gorithm for our model, and derive an inside-
outside algorithm that can be used for unsu-
pervised learning of non-projective depend-
ency trees.
1 Introduction
Dependency grammars have received considerable
attention in the statistical parsing community in
recent years. These grammatical formalisms of-
fer a good balance between structural expressiv-
ity and processing efficiency. Most notably, when
non-projectivity is supported, these formalisms can
model crossing syntactic relations that are typical in
languages with relatively free word order.
Recent work has reduced non-projective parsing
to the identification of a maximum spanning tree in a
graph (McDonald et al., 2005; Koo et al., 2007; Mc-
Donald and Satta, 2007; Smith and Smith, 2007).
An alternative to this approach is to use transition-
based parsing (Yamada and Matsumoto, 2003; Nivre
and Nilsson, 2005; Attardi, 2006; Nivre, 2009;
Go´mez-Rodr?´guez and Nivre, 2010), where there is
an incremental processing of a string with a model
that scores transitions between parser states, condi-
tioned on the parse history. This paper focuses on
the latter approach.
The above work on transition-based parsing has
focused on greedy algorithms set in a statistical
framework (Nivre, 2008). More recently, dynamic
programming has been successfully used for pro-
jective parsing (Huang and Sagae, 2010; Kuhlmann
et al., 2011). Dynamic programming algorithms for
parsing (also known as chart-based algorithms) al-
low polynomial space representations of all parse
trees for a given input string, even in cases where
the size of this set is exponential in the length of
the string itself. In combination with appropriate
semirings, these packed representations can be ex-
ploited to compute many values of interest for ma-
chine learning, such as best parses and feature ex-
pectations (Goodman, 1999; Li and Eisner, 2009).
In this paper we move one step forward with re-
spect to Huang and Sagae (2010) and Kuhlmann et
al. (2011) and present a polynomial dynamic pro-
gramming algorithm for non-projective transition-
based parsing. Our algorithm is coupled with a
simplified version of the transition system from At-
tardi (2006), which has high coverage for the type
of non-projective structures that appear in various
treebanks. Instead of an additional transition oper-
ation which permits swapping of two elements in
the stack (Titov et al., 2009; Nivre, 2009), Attardi’s
system allows reduction of elements at non-adjacent
positions in the stack. We also present a generat-
ive probabilistic model for transition-based parsing.
The implication for this, for example, is that one can
now approach the problem of unsupervised learning
of non-projective dependency structures within the
transition-based framework.
Dynamic programming algorithms for non-
projective parsing have been proposed by Kahane et
al. (1998), Go´mez-Rodr?´guez et al. (2009) and Kuhl-
mann and Satta (2009), but they all run in exponen-
tial time in the ‘gap degree’ of the parsed structures.
To the best of our knowledge, this paper is the first to
1234
introduce a dynamic programming algorithm for in-
ference with non-projective structures of unbounded
gap degree.
The rest of this paper is organized as follows. In
§2 and §3 we outline the transition-based model we
use, together with a probabilistic generative inter-
pretation. In §4 we give the tabular algorithm for
parsing, and in §5 we discuss statistical inference
using expectation maximization. We then discuss
some other aspects of the work in §6 and conclude
in §7.
2 Transition-based Dependency Parsing
In this section we briefly introduce the basic defini-
tions for transition-based dependency parsing. For a
more detailed presentation of this subject, we refer
the reader to Nivre (2008). We then define a spe-
cific transition-based model for non-projective de-
pendency parsing that we investigate in this paper.
2.1 General Transition Systems
Assume an input alphabet ? with a special symbol
$ ? ? , which we use as the root of our parse struc-
tures. Throughout this paper we denote the input
string as w = a0 · · · an?1, n ? 1, where a0 = $ and
ai ? ? \ {$} for each i with 1 ? i ? n? 1.
A dependency tree for w is a directed tree Gw =
(Vw, Aw), where Vw = {0, . . . , n ? 1} is the set of
nodes, and Aw ? Vw × Vw is the set of arcs. The
root of Gw is the node 0. The intended meaning
is that each node in Vw encodes the position of a
token in w. Furthermore, each arc in Aw encodes a
dependency relation between two tokens. We write
i ? j to denote a directed arc (i, j) ? Aw, where
node i is the head and node j is the dependent.
A transition system (for dependency parsing) is a
tuple S = (C, T, I, Ct), whereC is a set of configur-
ations, defined below, T is a finite set of transitions,
which are partial functions t:C ? C, I is a total
initialization function mapping each input string to
a unique initial configuration, and Ct ? C is a set of
terminal configurations.
A configuration is defined relative to some input
string w, and is a triple (?, ?,A), where ? and ? are
disjoint lists called stack and buffer, respectively,
and A ? Vw × Vw is a set of arcs. Elements of
? and ? are nodes from Vw and, in the case of the
stack, a special symbol ¢ that we will use as initial
stack symbol. If t is a transition and c1, c2 are con-
figurations such that t(c1) = c2, we write c1 `t c2,
or simply c1 ` c2 if t is understood from the context.
Given an input string w, a parser based on S in-
crementally processes w from left to right, starting
in the initial configuration I(w). At each step, the
parser nondeterministically applies one transition, or
else it stops if it has reached some terminal config-
uration. The dependency graph defined by the arc
set associated with a terminal configuration is then
returned as one possible analysis for w.
Formally, a computation of S is a sequence ? =
c0, . . . , cm, m ? 1, of configurations such that, for
every iwith 1 ? i ? m, ci?1 `ti ci for some ti ? T .
In other words, each configuration in a computa-
tion is obtained as the value of the preceding con-
figuration under some transition. A computation is
called complete whenever c0 = I(w) for some in-
put string w, and cm ? Ct.
We can view a transition-based dependency
parser as a device mapping strings into graphs (de-
pendency trees). Without any restriction on trans-
ition functions in T , these functions might have an
infinite domain, and could thus encode even non-
recursively enumerable languages. However, in
standard practice for natural language parsing, trans-
itions are always specified by some finite mean. In
particular, the definition of each transition depends
on some finite window at the top of the stack and
some finite window at the beginning of the buffer
in each configuration. In this case, we can view a
transition-based dependency parser as a notational
variant of a push-down transducer (Hopcroft et al.,
2000), whose computations output sequences that
directly encode dependency trees. These transducers
are nondeterministic, meaning that several trans-
itions can be applied to some configurations. The
transition systems we investigate in this paper fol-
low these principles.
We close this subsection with some additional
notation. We denote the stack with its topmost ele-
ment to the right and the buffer with its first ele-
ment to the left. We indicate concatenation in the
stack and buffer by a vertical bar. For example, for
k ? Vw, ?|k denotes some stack with topmost ele-
ment k and k|? denotes some buffer with first ele-
ment k. For 0 ? i ? n ? 1, ?i denotes the buffer
1235
[i, i + 1, . . . , n ? 1]; for i ? n, ?i denotes [] (the
empty buffer).
2.2 A Non-projective Transition System
We now turn to give a description of our trans-
ition system for non-projective parsing. While a
projective dependency tree satisfies the requirement
that, for every arc in the tree, there is a direc-
ted path between its headword and each of the
words between the two endpoints of the arc, a non-
projective dependency tree may violate this condi-
tion. Even though some natural languages exhibit
syntactic phenomena which require non-projective
expressive power, most often such a resource is used
in a limited way.
This idea is demonstrated by Attardi (2006), who
proposes a transition system whose individual trans-
itions can deal with non-projective dependencies
only to a limited extent, depending on the distance
in the stack of the nodes involved in the newly con-
structed dependency. The author defines this dis-
tance as the degree of the transition, with transitions
of degree one being able to handle only projective
dependencies. This formulation permits parsing a
subset of the non-projective trees, where this subset
depends on the degree of the transitions. The repor-
ted coverage in Attardi (2006) is already very high
when the system is restricted to transitions of degree
two or three. For instance, on training data for Czech
containing 28,934 non-projective relations, 27,181
can be handled by degree two transitions, and 1,668
additional dependencies can be handled by degree
three transitions. Table 1 gives additional statistics
for treebanks from the CoNLL-X shared task (Buch-
holz and Marsi, 2006).
We now turn to describe our variant of the trans-
ition system of Attardi (2006), which is equivalent to
the original system restricted to transitions of degree
two. Our results are based on such a restriction. It is
not difficult to extend our algorithms (§4) to higher
degree transitions, but this comes at the expense of
higher complexity. See §6 for more discussion on
this issue.
Let w = a0 · · · an?1 be an input string over ?
defined as in §2.1, with a0 = $. Our transition sys-
tem for non-projective dependency parsing is
S(np) = (C, T (np), I(np), C(np)t ),
Language Deg. 2 Deg. 3 Deg. 4
Arabic 180 21 7
Bulgarian 961 41 10
Czech 27181 1668 85
Danish 876 136 53
Dutch 9072 2119 171
German 15827 2274 466
Japanese 1484 143 9
Portuguese 3104 424 37
Slovene 601 48 13
Spanish 66 7 0
Swedish 1566 226 79
Turkish 579 185 8
Table 1: The number of non-projective relations of vari-
ous degrees for several treebanks (training sets), as repor-
ted by the parser of Attardi (2006). Deg. stands for ‘de-
gree.’ The parser did not detect non-projective relations
of degree higher than 4.
where C is the same set of configurations defined
in §2.1. The initialization function I(np) maps each
string w to the initial configuration ([¢], ?0, ?). The
set of terminal configurationsC(np)t contains all con-
figurations of the form ([¢, 0], [], A), for any set of
arcs A.
The set of transition functions is defined as
T (np) = {shb | b ? ?} ? {la1, ra1, la2, ra2},
where each transition is specified below. We let vari-
ables i, j, k, l range over Vw, and variable ? is a list
of stack elements from Vw ? {¢}:
shb : (?, k|?,A) ` (?|k, ?,A) if ak = b;
la1 : (?|i|j, ?,A) ` (?|j, ?,A ? {j ? i});
ra1 : (?|i|j, ?,A) ` (?|i, ?, A ? {i? j});
la2 : (?|i|j|k, ?,A) ` (?|j|k, ?,A ? {k ? i});
ra2 : (?|i|j|k, ?,A) ` (?|i|j, ?,A ? {i? k}).
Each of the above transitions is undefined on config-
urations that do not match the forms specified above.
As an example, transition la2 is not defined for a
configuration (?, ?,A) with |?| ? 2, and transition
shb is not defined for a configuration (?, k|?,A)
with b 6= ak, or for a configuration (?, [], A).
Transition shb removes the first node from the buf-
fer, in case this node represents symbol b ? ? ,
1236
and pushes it into the stack. These transitions are
called shift transitions. The remaining four trans-
itions are called reduce transitions, i.e., transitions
that consume nodes from the stack. Notice that in
the transition system at hand all the reduce trans-
itions decrease the size of the stack by one ele-
ment. Transition la1 creates a new arc with the top-
most node on the stack as the head and the second-
topmost node as the dependent, and removes the
latter from the stack. Transition ra1 is symmetric
with respect to la1. Transitions la1 and ra1 have
degree one, as already explained. When restricted
to these three transitions, the system is equivalent
to the so-called stack-based arc-standard model of
Nivre (2004). Transition la2 and transition ra2 are
very similar to la1 and ra1, respectively, but with
the difference that they create a new arc between
the topmost node in the stack and a node which is
two positions below the topmost node. Hence, these
transitions have degree two, and are the key com-
ponents in parsing of non-projective dependencies.
We turn next to describe the equivalence between
our system and the system in Attardi (2006). The
transition-based parser presented by Attardi pushes
back into the buffer elements that are in the top pos-
ition of the stack. However, a careful analysis shows
that only the first position in the buffer can be af-
fected by this operation, in the sense that elements
that are pushed back from the stack are never found
in buffer positions other than the first. This means
that we can consider the first element of the buffer
as an additional stack element, always sitting on the
top of the top-most stack symbol.
More formally, we can define a function mc :
C ? C that maps configurations in the original al-
gorithm to those in our variant as follows:
mc((?, k|?,A)) = (?|k, ?,A)
By applying this mapping to the source and target
configuration of each transition in the original sys-
tem, it is easy to check that c1 ` c2 in that parser if
and only if mc(c1) ` mc(c2) in our variant. We ex-
tend this and define an isomorphism between com-
putations in both systems, such that a computation
c0, . . . , cm in the original parser is mapped to a com-
putation mc(c0), . . . ,mc(cm) in the variant, with
both generating the same dependency graph A. This
???
??? 2n2n? 12n? 21 2 3
Figure 1: A dependency structure of arbitrary gap degree
that can be parsed with Attardi’s parser.
proves that our notational variant is in fact equival-
ent to Attardi’s parser.
A relevant property of the set of dependency
structures that can be processed by Attardi’s parser,
even when restricted to transitions of degree two, is
that the number of discontinuities present in each of
their subtrees, defined as the gap degree by Bod-
irsky et al. (2005), is not bounded. For example, the
dependency graph in Figure 1 has gap degree n? 1,
and it can be parsed by the algorithm for any arbit-
rary n ? 1 by applying 2n shb transitions to push
all the nodes into the stack, followed by (2n ? 2)
ra2 transitions to create the crossing arcs, and finally
one ra1 transition to create the dependency 1? 2.
As mentioned in §1, the computational complex-
ity of the dynamic programming algorithm that will
be described in later sections does not depend on the
gap degree, contrary to the non-projective depend-
ency chart parsers presented by Go´mez-Rodr?´guez et
al. (2009) and by Kuhlmann and Satta (2009), whose
running time is exponential in the maximum gap de-
gree allowed by the grammar.
3 A Generative Probabilistic Model
In this section we introduce a generative probabil-
istic model based on the transition system of §2.2.
In formal language theory, there is a standard way
of giving a probabilistic interpretation to a non-
deterministic parser whose computations are based
on sequences of elementary operations such as trans-
itions. The idea is to define conditional probability
distributions over instances of the transition func-
tions, and to ‘combine’ these probabilities to assign
probabilities to computations and strings.
One difficulty we have to face with when dealing
with transition systems is that the notion of compu-
tation, defined in §2.1, depends on the input string,
because of the buffer component appearing in each
configuration. This is a pitfall to generative model-
1237
ing, where we are interested in a system whose com-
putations lead to the generation of any string. To
overcome this problem, we observe that each com-
putation, defined as a sequence of stacks and buffers
(the configurations) can equivalently be expressed as
a sequence of stacks and transitions.
More precisely, consider a computation ? =
c0, . . . , cm, m ? 1. Let ?i, be the stack associated
with ci, for each i with 0 ? i ? m. Let also C? be
the set of all stacks associated with configurations in
C. We can make explicit the transitions that have
been used in the computation by rewriting ? in the
form ?0 `t1 ?1 · · ·?m?1 `tm ?m. In this way, ?
generates a string that is composed by all symbols
that are pushed into the stack by transitions shb, in
the left to right order.
We can now associate a probability to (our repres-
entation of) sequence ? by setting
p(?) =
m?
i=1
p(ti | ?i?1). (1)
To assign probabilities to complete computations we
should further multiply p(?) by factors ps(?0) and
pe(?m), where ps and pe are start and end probabil-
ity distributions, respectively, both defined over C?.
Note however that, as defined in §2.2, all initial con-
figurations are associated with stack [¢] and all final
configurations are associated with stack [¢, 0], thus
ps and pe are deterministic. Note that the Markov
chain represented in Eq. 1 is homogeneous, i.e., the
probabilities of the transition operations do not de-
pend on the time step.
As a second step we observe that, according to the
definition of transition system, each t ? T has an in-
finite domain. A commonly adopted solution is to
introduce a special function, called history function
and denoted by H , defined over the set C? and tak-
ing values over some finite set. For each t ? T and
?, ?? ? C?, we then impose the condition
p(t | ?) = p(t | ??)
whenever H(?) = H(??). Since H is finitely val-
ued, and since T is a finite set, the above condition
guarantees that there will only be a finite number of
parameters p(t | ?) in our model.
So far we have presented a general discussion of
how to turn a transition-based parser into a gener-
ative probabilistic model, and have avoided further
specification of the history function. We now turn
our attention to the non-projective transition system
of §2.2. To actually transform that system into a
parametrized probabilistic model, and to develop an
associated efficient inference procedure as well, we
need to balance between the amount of information
we put into the history function and the computa-
tional complexity which is required for inference.
We start the discussion with a na?¨ve model using a
history function defined by a fixed size window over
the topmost portion of the stack. More precisely,
each transition is conditioned on the lexical form of
the three symbols at the top of the stack ?, indic-
ated as b3, b2, b1 ? ? below, with b1 referring to the
topmost symbol. The parameters of the model are
defined as follows.
p(shb | b3, b2, b1) = ?shbb3,b2,b1 , ?b ? ? ,
p(la1 | b3, b2, b1) = ?la1b3,b2,b1 ,
p(ra1 | b3, b2, b1) = ?ra1b3,b2,b1 ,
p(la2 | b3, b2, b1) = ?la2b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?ra2b3,b2,b1 .
The parameters above are subject to the follow-
ing normalization conditions, for every choice of
b3, b2, b1 ? ? :
?la1b3,b2,b1 + ?
ra1
b3,b2,b1 + ?
la2
b3,b2,b1+
?ra2b3,b2,b1 +
?
b??
?shbb3,b2,b1 = 1 .
This na?¨ve model presents two practical problems.
The first problem relates to the efficiency of an in-
ference algorithm, which has a quite high computa-
tional complexity, as it will be discussed in §5. A
second problem arises in the probabilistic setting.
Using this model would require estimating many
parameters which are based on trigrams. This leads
to higher sample complexity to avoid sparse counts:
we would need more samples to accurately estimate
the model.
We therefore consider a more elaborated model,
which tackles both of the above problems. Again,
let b3, b2, b1 ? ? indicate the lexical form of the
three symbols at the top of the stack. We define the
1238
distributions p(t | ?) as follows:
p(shb | b1) = ?shbb1 , ?b ? ? ,
p(la1 | b2, b1) = ?rdb1 · ?
la1
b2,b1 ,
p(ra1 | b2, b1) = ?rdb1 · ?
ra1
b2,b1 ,
p(la2 | b3, b2, b1) = ?rdb1 · ?
rd2
b2,b1 · ?
la2
b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?rdb1 · ?
rd2
b2,b1 · ?
ra2
b3,b2,b1 .
The parameters above are subject to the following
normalization conditions, for every b3, b2, b1 ? ? :
?
b??
?shbb1 + ?
rd
b1 = 1 , (2)
?la1b2,b1 + ?
ra1
b2,b1 + ?
rd2
b2,b1 = 1 , (3)
?la2b3,b2,b1 + ?
ra2
b3,b2,b1 = 1 . (4)
Intuitively, parameter ?rdb denotes the probability
that we perform a reduce transition instead of a shift
transition, given that we have seen lexical form b at
the top of the stack. Similarly, parameter ?rd2b2,b1 de-notes the probability that we perform a reduce trans-
ition of degree 2 (see §2.2) instead of a reduce trans-
ition of degree 1, given that we have seen lexical
forms b1 and b2 at the top of the stack.
We observe that the above model has a num-
ber of parameters |? | + 4 · |? |2 + 2 · |? |3 (not
all independent). This should be contrasted with
the na?¨ve model, that has a number of parameters
4 · |? |3 + |? |4.
4 Tabular parsing
We present here a dynamic programming algorithm
for simulating the computations of the system from
§2–3. Given an input string w, our algorithm pro-
duces a compact representation of the set ? (w),
defined as the set of all possible computations of
the model when processing w. In combination with
the appropriate semirings, this method can provide
for instance the highest probability computation in
? (w), or else the probability of w, defined as the
sum of all probabilities of computations in ? (w).
We follow a standard approach in the literature
on dynamic programming simulation of stack-based
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989). More recently, this approach has also
been applied by Huang and Sagae (2010) and by
??????
c 0
c 1
c m
? h 1
h 1? i
? h 2 h 3
minimum
stack
length in
c 1 , . . . , cm
i i + 1
i + 1
buffer size
stack size
st
ac
k
b
u
ff
er
j
Figure 2: Schematic representation of the computations
? associated with item [h1, i, h2h3, j].
Kuhlmann et al. (2011) to the simulation of pro-
jective transition-based parsers. The basic idea in
this approach is to decompose computations of the
parser into smaller parts, group them into equival-
ence classes and recombine to obtain larger parts of
computations.
Let w = a0 · · · an?1, Vw and S(np) be defined as
in §2. We use a structure called item, defined as
[h1, i, h2h3, j],
where 0 ? i < j ? n and h1, h2, h3 ? Vw must
satisfy h1 < i and i ? h2 < h3 < j. The intended
interpretation of an item can be stated as follows; see
also Figure 2.
• There exists a computation ? of S(np) on w hav-
ing the form c0, . . . , cm, m ? 1, with c0 =
(?|h1, ?i, A) and cm = (?|h2|h3, ?j , A?) for
some stack ? and some arc sets A and A?;
• For each iwith 1 ? i < m, the stack ?i associated
with configuration ci has the list ? at the bottom
and satisfies |?i| ? |?|+ 2.
Some comments on the above conditions are in
order here. Let t1, · · · , tm be the sequence of trans-
itions in T (np) associated with computation ?. Then
we have t1 = shai , since |?1| ? |?| + 2. Thus we
conclude that |?1| = |?|+ 2.
The most important consequence of the definition
of item is that each transition ti with 2 ? i ? m
does not depend on the content of the ? portion of
the stack ?i. To see this, consider transition ci?1 `ti
ci. If ti = shai , the content of ? is irrelevant at
1239
this step, since in our model shai is conditioned only
on the topmost stack symbol of ?i?1, and we have
|?i?1| ? |?|+ 2.
Consider now the case of ti = la2. From |?i| ?
|?| + 2 we have that |?i?1| ? |?| + 3. Again, the
content of ? is irrelevant at this step, since in our
model la2 is conditioned only on the three topmost
stack symbols of ?i?1. A similar argument applies
to the cases of ti ? {ra2, la1, ra1}.
From the above, we conclude that if we apply the
transitions t1, . . . , tm to stacks of the form ?|h1, the
resulting computations have all identical probabilit-
ies, independently of the choice of ?.
Each computation satisfying the two conditions
above will be called an I-computation associ-
ated with item [h1, i, h2h3, j]. Notice that an I-
computation has the overall effect of replacing node
h1 sitting above a stack ? with nodes h2 and h3.
This is the key property in the development of our
algorithm below.
We specify our dynamic programming algorithm
as a deduction system (Shieber et al., 1995). The
deduction system starts with axiom [¢, 0, ¢0, 1], cor-
responding to an initial stack [¢] and to the shift of
a0 = $ from the buffer into the stack. The set ? (w)
is non-empty if and only if item [¢, 0, ¢0, n] can be
derived using the inference rules specified below.
Each inference rule is annotated with the type of
transition it simulates, along with the arc constructed
by the transition itself, if any.
[h1, i, h2h3, j]
[h3, j, h3j, j + 1]
(shaj )
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h5, j]
(la1;h5 ? h4)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra1;h4 ? h5)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h4h5, j]
(la2;h5 ? h2)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra2;h2 ? h5)
The above deduction system infers items in a
bottom-up fashion. This means that longer compu-
tations over substrings of w are built by combining
shorter ones. In particular, the inference rule shaj
asserts the existence of I-computations consisting of
a single shaj transition. Such computations are rep-
resented by the consequent item [h3, j, h3j, j + 1],
indicating that the index of the shifted word aj is
added to the stack by pushing it on top of h3.
The remaining four rules implement the reduce
transitions of the model. We have already ob-
served in §2.2 that all available reduce transitions
shorten the size of the stack by one unit. This al-
lows us to combine pairs of I-computations with
a reduce transition, resulting in a computation that
is again an I-computation. More precisely, if we
concatenate an I-computation asserted by an item
[h1, i, h2h3, k] with an I-computation asserted by an
item [h3, k, h4h5, j], we obtain a computation that
has the overall effect of increasing the size of the
stack by 2, replacing the topmost stack element h1
with stack elements h2, h4 and h5. If we now apply
any of the reduce transitions from the inventory of
the model, we will remove one of these three nodes
from the stack, and the overall result will be again
an I-computation, which can then be asserted by a
certain item. For example, if we apply the reduce
transition la1, the consequent item is [h1, i, h2h5, j],
since an la1 transition removes the second topmost
element from the stack (h4). The other reduce trans-
itions remove a different element, and thus their
rules produce different consequent items.
The above argument shows the soundness of the
deduction system, i.e., an item I = [h1, i, h2h3, j]
is only generated if there exists an I-computation
? = c0, . . . , cm with c0 = (?|h1, ?i, A) and cm =
(?|h2|h3, ?j , A?). To prove completeness, we must
show the converse result, i.e., that the existence of
an I-computation ? implies that item I is inferred.
We first do this under the assumption that the infer-
ence rule for the shift transitions do not have an ante-
cedent, i.e., items [h1, j, h1j, j + 1] are considered
as axioms. We proceed by using strong induction on
the length m of the computation ?.
For m = 1, ? consists of a single transition shaj ,
and the corresponding item I = [h1, j, h1j, j + 1]
is constructed as an axiom. For m > 1, let ? be
as specified above. The transition that produced
1240
cm must have been a reduce transition, otherwise
? would not be an I-computation. Let ck be the
rightmost configuration in c0, . . . , cm?1 whose stack
size is |?| + 2. Then it can be shown that the com-
putations ?1 = c0, . . . , ck and ?2 = ck, . . . , cm?1
are again I-computations. Since ?1 and ?2 have
strictly fewer transitions than ?, by the induction hy-
pothesis, the system constructs items [h1, i, h2h3, k]
and [h3, k, h4h5, j], where h2 and h3 are the stack
elements at the top of ck. Applying to these items
the inference rule corresponding to the reduce trans-
ition at hand, we can construct item I .
When the inference rule for the shift transition has
an antecedent [h1, i, h2h3, j], as indicated above, we
have the overall effect that I-computations consist-
ing of a single transition shifting aj on the top of h3
are simulated only in case there exists a computation
starting with configuration ([¢], ?0) and reaching a
configuration of the form (?|h2|h3, ?j). This acts as
a filter on the search space of the algorithm, but does
not invalidate the completeness property. However,
in this case the proof is considerably more involved,
and we do not report it here.
An important property of the deduction system
above, which will be used in the next section, is
that the system is unambiguous, that is, each I-
computation is constructed by the system in a
unique way. This can be seen by observing that, in
the sketch of the completeness proof reported above,
there always is an unique choice of ck that decom-
poses I-computation ? into I-computations ?1 and
?2. In fact, if we choose a configuration ck? other
than ck with stack size |?| + 2, the computation
??2 = ck? , . . . , cm?1 will contain ck as an interme-
diate configuration, which violates the definition of
I-computation because of an intervening stack hav-
ing size not larger than the size of the stack associ-
ated with the initial configuration.
As a final remark, we observe that we can keep
track of all inference rules that have been applied
in the computation of each item by the above al-
gorithm, by encoding each application of a rule as
a reference to the pair of items that were taken as
antecedent in the inference. In this way, we ob-
tain a parse forest structure that can be viewed as a
hypergraph or as a non-recursive context-free gram-
mar, similar to the case of parsing based on context-
free grammars. See for instance Klein and Manning
(2001) or Nederhof (2003). Such a parse forest en-
codes all valid computations in ? (w), as desired.
The algorithm runs in O(n8) time. Using meth-
ods similar to those specified in Eisner and Satta
(1999), we can reduce the running time to O(n7).
However, we do not further pursue this idea here,
and proceed with the discussion of exact inference,
found in the next section.
5 Inference
We turn next to specify exact inference with our
model, for computing feature expectations. Such
inference enables, for example, the derivation of
an expectation-maximization algorithm for unsuper-
vised parsing.
Here, a feature is a function over computations,
providing the count of a pattern related to a para-
meter. We denote by f la2b3,b2,b1(?), for instance,the number of occurrences of transition la2 within
? with topmost stack symbols having word forms
b3, b2, b1 ? ? , with b1 associated with the topmost
stack symbol.
Feature expectations are computed by using an
inside-outside algorithm for the items in the tabu-
lar algorithm. More specifically, given a string w,
we associate each item [h1, i, h2h3, j] defined as in
§4 with two quantities:
I([h1, i, h2h3, j]) =
?
?=([h1],?i),...,([h2,h3],?j)
p(?) ; (5)
O([h1, i, h2h3, j]) =
?
?,?=([¢],?0),...,(?|h1,?i)
??=(?|h2|h3,?j),...,([¢,0],?n)
p(?) · p(??) . (6)
I([h1, i, h2h3, j]) and O([h1, i, h2h3, j]) are called
the inside and the outside probabilities, respect-
ively, of item [h1, i, h2h3, j]. The tabular algorithm
of §4 can be used to compute the inside probabilit-
ies. Using the gradient transformation (Eisner et al.,
2005), a technique for deriving outside probabilities
from a set of inference rules, we can also compute
O([h1, i, h2h3, j]). The use of the gradient trans-
formation is valid in our case because the tabular al-
gorithm is unambiguous (see §4).
Using the inside and outside probabilities, we can
now efficiently compute feature expectations for our
1241
Ep(?|w)[f la2b3,b2,b1(?)] =
?
??? (w)
p(? | w) · f la2b3,b2,b1(?) =
1
p(w) ·
?
??? (w)
p(?) · f la2b3,b2,b1(?)
= 1p(w) ·
?
?,i,k,j,
h1,h2,h3,h4,h5,
s.t. ah2=b3,
ah4=b2, ah5=b1
?
?0=([¢],?0),...,(?|h1,?i),
?1=(?|h1,?i),...,(?|h2|h3,?k),
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j),
?3=(?|h2|h5,?j),...,([¢,0],?n)
p(?0) · p(?1) · p(?2) · p(la2 | b3, b2, b1) · p(?3)
=
?rdb1 · ?
rd2
b2,b1 · ?
la2
b3,b2,b1
p(w) ·
?
?,i,j,
h1,h2,h5, s.t.
ah2=b3, ah5=b1
?
?0=([¢],?0),...,(?|h1,?i),
?3=(?|h2|h5,?j),...,([¢,0],?n)
p(?0) · p(?3) ·
·
?
k,h3,h4,
s.t. ah4=b2
?
?1=(?|h1,?i),...,(?|h2|h3,?k)
p(?1) ·
?
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j)
p(?2)
Figure 3: Decomposition of the feature expectationEp(?|w)[f la2b3,b2,b1(?)] into a finite summation. Quantity p(w) aboveis the sum over all probabilities of computations in ? (w).
model. Figure 3 shows how to express the expect-
ation of feature f la2b3,b2,b1(?) by means of a finitesummation. Using Eq. 5 and 6 and the relation
p(w) = I([¢, 0, ¢0, n]) we can then write:
Ep(?|w)[f la2b3,b2,b1(?)] =
?rdb1 · ?
rd2
b2,b1 · ?
la2
b3,b2,b1
I([¢, 0, ¢0, n]) ·
·
?
i,j,h1,h4,h5,
s.t. ah4=b2, ah5=b1
O([h1, i, h4h5, j]) ·
·
?
k,h2,h3,
s.t. ah2=b3
I([h1, i, h2h3, k]) · I([h3, k, h4h5, j]) .
Very similar expressions can be derived for the ex-
pectations for features f ra2b3,b2,b1(?), f la1b2,b1(?), and
f ra1b2,b1(?). As for feature f shbb1 (?), b ? ? , the aboveapproach leads to
Ep(?|w)[f shbb1 (?)] =
=
?shbb1
I([¢, 0, ¢0, n]) ·
?
?,i,h, s.t.
ah=b1, ai=b
O([h, i, hi, i+ 1]) .
As mentioned above, these expectations can be
used, for example, to derive an EM algorithm for our
model. The EM algorithm in our case is not com-
pletely straightforward because of the way we para-
metrize the model. We give now the re-estimation
steps for such an EM algorithm. We assume that all
expectations below are taken with respect to a set of
parameters ? from iteration s ? 1 of the algorithm,
and we are required to update these ?. To simplify
notation, let us assume that there is only one stringw
in the training corpus. For each b1 ? ? , we define:
Zb1 =
?
b2??
Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
]
+
?
b3,b2??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
;
Zb2,b1 =
?
b3??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
.
We then have, for every b ? ? :
?shbb1 (s)?
Ep(?|w)[f shbb1 (?)]
Zb1 +
?
b??? Ep(?|w)[f
shb?
b1 (?)]
.
1242
Furthermore, we have:
?la1b2,b1(s)?
Ep(?|w)[f la1b2,b1(?)]
Zb2,b1 + Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
] ,
and:
?la2b3,b2,b1(s)?
Ep(?|w)[f la2b3,b2,b1(?)]
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
] .
The rest of the parameter updates can easily be de-
rived using the above updates because of the sum-
to-1 constraints in Eq. 2–4.
6 Discussion
We note that our model inherits spurious ambigu-
ity from Attardi’s model. More specifically, we can
have different derivations, corresponding to differ-
ent system computations, that result in identical de-
pendency graphs and strings. While running our
tabular algorithm with the Viterbi semiring effi-
ciently computes the highest probability computa-
tion in ? (w), spurious ambiguity means that find-
ing the highest probability dependency tree is NP-
hard. This latter result can be shown using proof
techniques similar to those developed by Sima’an
(1996). We leave it for future work how to eliminate
spurious ambiguity from the model.
While in the previous sections we have described
a tabular method for the transition system of Attardi
(2006) restricted to transitions of degree up to two, it
is possible to generalize the model to include higher-
degree transitions. In the general formulation of At-
tardi parser, transitions of degree d create links in-
volving nodes located d positions beneath the top-
most position in the stack:
lad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i2| . . . |id+1, ?, A ? {id+1 ? i1});
rad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i1|i2| . . . |id, ?, A ? {i1 ? id+1}).
To define a transition system that supports trans-
itions up to degree D, we use a set of
items of the form [s1 . . . sD?1, i, e1 . . . eD, j], cor-
responding (in the sense of §4) to compu-
tations of the form c0, . . . , cm, m ? 1,
with c0 = (?|s1| . . . |sD?1, ?i, A) and cm =
(?|e1| . . . |eD, ?j , A?). The deduction steps corres-
ponding to reduce transitions in this general system
have the general form
[s1 . . . sD?1, i, e1m1 . . .mD?1, j]
[m1 . . .mD?1, j, e2 . . . eD+1, w]
[s1 . . . sD?1, i, e1 . . . ec?1ec+1 . . . eD+1, w]
(ep ? ec)
where the values of p and c differ for each transition:
to obtain the inference rule corresponding to a lad
transition, we make p = D + 1 and c = D + 1? d;
and to obtain the rule for a rad transition, we make
p = D + 1? d and c = D + 1. Note that the parser
runs in timeO(n3D+2), whereD stands for the max-
imum transition degree, so each unit increase in the
transition degree adds a cubic factor to the parser’s
polynomial time complexity. This is in contrast to a
previous tabular formulation of the Attardi parser by
Go´mez-Rodr?´guez et al. (2011), which ran in expo-
nential time.
The model for the transition system we give in this
paper is generative. It is not hard to naturally extend
this model to the discriminative setting. In this case,
we would condition the model on the input string to
get a conditional distribution over derivations. It is
perhaps more natural in this setting to use arbitrary
weights for the parameter values, since the compu-
tation of a normalization constant (the probability of
a string) is required in any case. Arbitrary weights
in the generative setting could be more problematic,
because it would require computing a normalization
constant corresponding to a sum over all strings and
derivations.
7 Conclusion
We presented in this paper a generative probabilistic
model for non-projective parsing, together with the
description of an efficient tabular algorithm for pars-
ing and doing statistical inference with the model.
Acknowledgments
The authors thank Marco Kuhlmann for helpful
comments on an early draft of the paper. The authors
also thank Giuseppe Attardi for the help received to
extract the parsing statistics. The second author has
been partially supported by Ministerio de Ciencia e
Innovacio´n and FEDER (TIN2010-18552-C03-02).
1243
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166–170,
New York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143–151,
Vancouver, Canada.
Manuel Bodirsky, Marco Kuhlmann, and Mathias Mo¨hl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Gram-
mar and Ninth Meeting on Mathematics of Language,
pages 195–203, Edinburgh, UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
149–164, New York, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 457–464, College Park, MD,
USA.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In Proceedings
of HLT-EMNLP, pages 281–290.
Carlos Go´mez-Rodr?´guez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1492–1501, Uppsala, Sweden.
Carlos Go´mez-Rodr?´guez, David J. Weir, and John Car-
roll. 2009. Parsing mildly non-projective dependency
structures. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 291–299, Athens, Greece.
Carlos Go´mez-Rodr?´guez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics (in press), 37(3).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to Automata Theory.
Addison-Wesley, 2nd edition.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077–1086,
Uppsala, Sweden.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 18th International Conference on Compu-
tational Linguistics (COLING-ACL), pages 646–652,
Montre´al, Canada.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the IWPT, pages
123–134.
Terry Koo, Amir Globerson, Xavier Carreras, and Mi-
chael Collins. 2007. Structured prediction models
via the matrix-tree theorem. In Proceedings of the
EMNLP-CoNLL, pages 141–150.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective depend-
ency parsing. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 478–486, Athens, Greece.
Marco Kuhlmann, Carlos Go´mez-Rodr?´guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon,
USA.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbru¨cken, July 29–
August 2, 1974, number 14 in Lecture Notes in Com-
puter Science, pages 255–269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40–51, Singa-
pore.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Tenth International Conference on Parsing
Technologies (IWPT), pages 121–132, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Human Language
Technology Conference (HLT) and Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523–530, Vancouver, Canada.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and knuth’s algorithm. Computational Linguistics,
29(1):135–143.
1244
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 99–106, Ann Arbor, USA.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50–57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513–553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the 47th An-
nual Meeting of the ACL and the Fourth International
Joint Conference on Natural Language Processing of
the AFNLP, pages 351–359, Singapore.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1–2):3–
36.
Khalil Sima’an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of COLING, pages 1175–
1180.
David A. Smith and Noah A. Smith. 2007. Probab-
ilistic models of nonprojective dependency trees. In
Proceedings of the EMNLP-CoNLL, pages 132–140.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI, pages 281–290.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195–206, Nancy,
France.
1245
