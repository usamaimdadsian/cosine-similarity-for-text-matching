Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 814–824,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Unsupervised Information Extraction with Distributional Prior Knowledge
Cane Wing-ki Leung1, Jing Jiang1, Kian Ming A. Chai2, Hai Leong Chieu2, Loo-Nin Teow2
1School of Information Systems, Singapore Management University, Singapore
2DSO National Laboratories, Singapore
{caneleung,jingjiang}@smu.edu.sg, {ckianmin,chaileon,tloonin}@dso.org.sg
Abstract
We address the task of automatic discovery of
information extraction template from a given
text collection. Our approach clusters candi-
date slot fillers to identify meaningful tem-
plate slots. We propose a generative model
that incorporates distributional prior knowl-
edge to help distribute candidates in a docu-
ment into appropriate slots. Empirical results
suggest that the proposed prior can bring sub-
stantial improvements to our task as compared
to a K-means baseline and a Gaussian mixture
model baseline. Specifically, the proposed
prior has shown to be effective when coupled
with discriminative features of the candidates.
1 Introduction
Information extraction (IE) is the task of extract-
ing information from natural language texts to fill a
database record following a structure called a tem-
plate. Such templates are usually defined based
on the domain of interest. For example, the do-
main in the Sixth Message Understanding Confer-
ence (MUC-6, 1995) is management succession, and
the pre-defined template consists of the slots posi-
tion, the person leaving, the person joining, and the
organization.
Previous research on IE often requires the pre-
definition of templates. Template construction is
usually done manually by domain experts, and an-
notated documents are often created to facilitate su-
pervised learning approaches to IE. However, both
manual template construction and data annotation
are labor-intensive. More importantly, templates and
annotated data usually cannot be re-used in new do-
mains due to domain dependency. It is therefore nat-
ural to consider the problem of unsupervised tem-
plate induction and information extraction. This is
the topic of this paper.
There have been a few previous attempts to ad-
dress the unsupervised IE problem (Shinyama and
Sekine, 2006; Sekine, 2006; Rosenfeld and Feld-
man, 2006; Filatova et al., 2006). These approaches
have a commonality: they try to cluster candidate
slot fillers, which are often nouns and noun phrases,
into slots of the template to be constructed. How-
ever, most of them have neglected the following im-
portant observation: a single document or text seg-
ment tends to cover different slots rather than re-
dundantly fill the same slot. In other words, during
clustering, candidates within the same text segment
should be more likely to be distributed into different
clusters.
In this paper, we propose a generative model that
incorporates this distributional prior knowledge. We
define a prior distribution over the possible label
assignments in a document or a text segment such
that a more diversified label assignment is preferred.
This prior is based on the Poisson distribution. We
also compare a number of generative models for
generating slot fillers and find that the Gaussian mix-
ture model is the best. We then combine the Poisson-
based label assignment prior with the Gaussian mix-
ture model to perform slot clustering. We find that
compared with a K-means baseline and a Gaussian
mixture model baseline, our combined model with
the proposed label assignment prior substantially
performs better on two of the three data sets we use
for evaluation. We further analyze the results on the
third data set and find that the proposed prior will
have little effect if there are no good discriminative
features to begin with. In summary, we find that
814
our Poisson-based label assignment prior is effective
when coupled with good discriminative features.
2 Related Work
One common approach to unsupervised IE is based
on automatic IE pattern acquisition on a cluster of
similar documents. For instance, Sudo et al. (2003)
and Sekine (2006) proposed different methods for
automatic IE pattern acquisition for a given domain
based on frequent subtree discovery in dependency
parse trees. These methods leveraged heavily on the
entity types of candidates when assigning them to
template slots. As a consequence, potentially dif-
ferent semantic roles of candidates having the same
entity type could become indistinguishable (Sudo et
al., 2003; Sekine, 2006). This problem is alleviated
in our work by exploiting distributional prior knowl-
edge about template slots, which is shown effective
when coupled with discriminative features of can-
didates. Filatova et al. (2006) also considered fre-
quent subtrees in dependency parse trees, but their
goal was to build templates around verbs that are
statistically important in a given domain. Our work,
in contrast, is not constrained to verb-centric tem-
plates. We aim to identify salient slots in the given
domain by clustering.
Marx et al. (2002) proposed the cross-component
clustering algorithm for unsupervised IE. Their al-
gorithm assigned a candidate from a document to
a cluster based on the candidate’s feature similarity
with candidates from other documents only. In other
words, the algorithm did not consider a candidate’s
relationships with other candidates in the same doc-
ument. Our work is based on a different perspec-
tive: we model label assignments for all candidates
in the same document with a distributional prior that
prefers a document to cover more distinct slots. We
show empirically that this prior improves slot clus-
tering results greatly in some cases.
Also related to our work is open domain IE, which
aims to perform unsupervised relation extraction.
TEXTRUNNER (Banko et al., 2007), for example,
automatically extracts all possible relations between
pairs of noun phrases from a given corpus. The main
difference between open domain IE and our work
is that open domain IE does not aim to induce do-
main templates, whereas we focus on a single do-
main with the goal of inducing a template that de-
scribes salient information structure of that domain.
Furthermore, TEXTRUNNER and related studies on
unsupervised relation extraction often rely on highly
redundant information on the Web or in large cor-
pus (Hasegawa et al., 2004; Rosenfeld and Feldman,
2006; Yan et al., 2009), which is not assumed in our
study.
We propose a generative model with a distribu-
tional prior for the unsupervised IE task, where
slot fillers correspond to observations in the model,
and their labels correspond to hidden variables we
want to learn. In the machine learning literature,
researchers have explored the use of similar prior
knowledge in the form of constraints through model
expectation. For example, Grac¸a et al. (2007) pro-
posed to place constraints on the posterior proba-
bilities of hidden variables in a generative model,
while Druck et al. (2008) studied a similar problem
in a discriminative, semi-supervised setting. These
studies model constraints as features, and enforce
the constraints through expected feature values. In
contrast, we place constraints on label assignments
through a probabilistic prior on the distribution of
slots. The proposed prior is simple and easy to inter-
pret in a generative model. Nevertheless, it will be
interesting to explore how the proposed prior can be
implemented within the posterior constraint frame-
work.
3 Problem Overview
In this section, we first formally define our unsuper-
vised IE problem. We then provide an overview of
our solution, which is based on a generative model.
3.1 Problem Definition
We assume a collection of documents or short text
segments from a certain domain. These documents
or text segments describe different events or enti-
ties, but they are about the same topic or aspect of
the domain. Examples of such collections include
a collection of sentences describing the educational
background of famous scientists and a collection of
aviation incident reports. Our task is to automati-
cally discover an IE template from this collection.
The discovered template should contain a set of slots
that play different semantic roles in the domain.
815
Input text:
Topic: Graduate Student Seminar Lunch
Dates: 13-Apr-95
Time: 12:00 PM - 1:30 PM
PostedBy: Edmund J. Delaney on 5-Apr-95 at 16:24 from andrew.cmu.edu
Abstract:
The last Graduate Student Seminar Series lunch will be held on Thursday, April 13 from noon-1:30 p.m. in room
207, Student Activities Center. Professor Sara Kiesler of SDS will speak on Carving A Successful Research Niche.
Output:
Slot Slot Filler(s)
Slot 1 (start time) 12:00PM
Slot 2 (end time) 1:30PM, 1:30 p.m.
Slot 3 (location) room 207, Student Activities Center
Slot 4 (speaker) Professor Sara Kiesler
Slot 4 (irrelevant information) Edmund J. Delaney
Figure 1: An input text from a seminar announcement collection and the discovered IE template. Note that the slots
are automatically discovered and the slot names are manually assigned.
To construct such a template, we start with identi-
fying candidate slot fillers, hereafter referred to as
candidates, from the input text. Then we cluster
these candidates with the aim that each cluster will
represent a semantically meaningful slot. Figure 1
gives an example of an input text from a collection
of seminar announcements and the resulting tem-
plate discovered from the collection. As we can see,
the template contains some semantically meaningful
slots such as the start time, end time, location and
speaker of a seminar. Moreover, it also contains a
slot that covers an irrelevant candidate. We call such
slots covering irrelevant candidates garbage slots.
We can make two observations on the mapping
from candidates to template slots from real data,
such as the text in Figure 1. Firstly, a template
slot may be filled by more than one candidate from
a single document, although this number has been
observed to be small. For example, the template
slot end time in Figure 1 has two slot fillers: “1:30
PM” from the semi-structured header and “1:30
p.m.” from within the abstract. Secondly, a docu-
ment tends to contain candidates that cover different
template slots. We believe that this observation is a
consequence of the fact that a document will tend to
convey as much information as possible. We further
exploit these observations in Section 4.
3.2 A General Solution
Recall that our general solution to the unsupervised
IE problem is to cluster candidate slot fillers in order
to identify meaningful slots. We leave the details of
how to extract the candidates to Section 7.1. In this
section, we assume that we have a set of candidates
x = {xi,j}, where xi,j is the j-th candidate from
the i-th document in the collection. We cluster these
candidates intoK groups for a givenK.
Let yi,j ? {1, . . . ,K} denote the cluster label for
xi,j and y denote the set of all the yi,j’s. Let xi and
yi denote the sets of all the xi,j’s and the yi,j’s in the
i-th document respectively. We assume a generative
model for x and y as follows. For the i-th document
in our collection, we assume that the number of can-
didates is known and we draw a label assignment yi
according to some distribution parameterized by ?.
Then for the j-th candidate, we generate xi,j from
yi,j according to a generative model parameterized
by ?. Since the labels y are hidden, the observed
log-likelihood of the parameters given the observa-
tions x is
L(?,?) = log p(x; ?,?)
=
?
i
log
?
yi
p(xi,yi; ?,?)
=
?
i
log
?
yi
p(yi; ?)
?
j
p(xi,j |yi,j ; ?). (1)
816
D
ni
K
(a) A multinomial prior.
D
ni
KK-1K-1
(b) The proposed Poisson-based
prior.
Figure 2: Generative models with different label assign-
ment priors. D denotes the number of documents in the
given collection, ni denotes the number of candidates in
the i-th document, andK is the number of slots (clusters).
For a given functional form of p(yi; ?) and
p(xi,j |yi,j ; ?), the best model parameters can be es-
timated by maximizing Eq. (1). In the next section,
we detail two designs of the prior p(yi; ?), followed
by different generative models for the distribution
p(xi,j |yi,j ; ?) in Section 5. Then we describe the
estimation of model parameters in Section 6.
4 Label Assignment Prior
The label assignment prior, p(yi; ?), models the
generation of labels for candidates in a document.
In this section, we first describe a commonly used
multinomial prior, and then introduce the proposed
Poisson-based prior for the unsupervised IE task.
4.1 A Multinomial Prior
Usually, one would assume that the labels for
the different candidates in the same document
are generated independently, that is, p(yi; ?) =?
j p(yi,j ; ?). Under this model, we assume that
each yi,j is generated from a multinomial distribu-
tion parameterized by?, where ?y denotes the prob-
ability of generating label y. Our objective function
in Eq. (1) then becomes:
L(?,?) = log p(x; ?,?)
=
?
i,j
log
?
y
?yp(xi,j |y; ?). (2)
Figure 2(a) depicts a generative model with this
multinomial prior in plate notation. Note that the in-
dependence assumption on label assignment in this
model does not capture our observation that candi-
dates in a document are likely to cover different se-
mantic roles.
4.2 The Proposed Poisson-based Prior
We propose a prior distribution that favors more
diverse label assignments. Our proposal takes
into consideration the following three observations.
Firstly, candidates in the same document are likely
to cover different semantic roles. The proposed prior
distribution should therefore assign higher probabil-
ity to a label assignment that covers more distinct
slots. Secondly, the same piece of information is not
likely to be repeated many times in a document. Our
design thus allows a slot to generate multiple fillers
in a document, up to a limited number of times.
Thirdly, there may exist candidates that do not be-
long to slots in the extracted template. Therefore, we
introduce a dummy slot or garbage slot to the label
set to collect such candidates. Yet, we shall not as-
sume any prior/domain knowledge about candidates
generated by the garbage slot as they are essentially
irrelevant in the given domain.
We now detail the prior that exploits the above
observations. First, we fix the K-th slot (or cluster)
in the label set to be the garbage slot. For each of
the non-garbage slot k = 1, . . . ,K ? 1, we also fix
the maximum number of fillers that can be gener-
ated, which we denote by ?k. There is no ?K for the
garbage slot because the number of fillers is not con-
strained for this slot. This allows all candidates in a
document to be generated by the garbage slot. Let
ni be the number of candidates in the i-th document.
Given K, {?k}K?1k=1 and ni, the set of possible labelassignments for the i-th document can be generated.
We illustrate this with an example. Let K = 2 and
?1 = 1. The label set is {1, 2}, where 2 represents
the garbage slot. Let the number of candidates be
ni = 2. The possible label assignments within this
setting are (1, 2), (2, 1) and (2, 2).
The set of possible label assignments for the i-
th document is the sample space on which we place
the prior distribution p(yi; ?). We need a prior that
gives a higher probability to a more diverse label
assignment. For a given yi for the i-th document,
let ni,k be the number of candidates in the docu-
ment that have been assigned to slot k. That is,
ni,k def=
?ni
j=1 1(yi,j = k), where 1(·) is the indica-
tor variable. We propose the following distribution
based on the Poisson distribution:
817
p(yi; ?) def= Z?1i
K?1?
k=1
Poisson(ni,k; ?k), (3)
where Zi is the normalizing constant, and ?k is the
mean parameter of the k-th Poisson distribution,
k = 1, . . .K ? 1. The absence of a factor that
depends on ni,K reflects the lack of prior knowl-
edge on the number of garbage slot fillers. Fig-
ure 2(b) depicts the proposed generative model with
the Poisson-based prior in plate notation.
5 Generating Slot Fillers
Different existing generative models can be used to
model the generation of a slot filler given a label, that
is, p(x|y; ?). We explore four of them for our task,
namely, the naive Bayes model, the Bernoulli mix-
ture model, the Gaussian mixture model, and a lo-
cally normalized logistic regression model proposed
by Berg-Kirkpatrick et al. (2010).
5.1 Multinomial Naive Bayes
In the multinomial naive Bayes model, features of
an observation x are assumed to be independent and
each generated from a multinomial distribution. We
first introduce some notations. Let f denote a fea-
ture (e.g. entity type) and Vf denote the set of possi-
ble values for f . Let xf ? Vf be the value of feature
f in x (e.g. person). For a given label y, feature f
follows a multinomial distribution parameterized by
?y,f , where ?y,f,v denotes the probability of feature
f taking the value v ? Vf given label y. The func-
tional form of the conditional probability of x given
a label y is then
p(x|y; ?) =
?
f
p(xf |y; ?) =
?
f
?y,f,xf . (4)
5.2 Bernoulli Mixture Model
In the naive Bayes model our features are defined
to be categorical. For the Bernoulli mixture model,
as well as the Gaussian mixture model and the lo-
cally normalized logistic regression model in the
next subsections, we first convert each observation
x into a binary feature vector x ? {0, 1}F where F
is the number of binary features. An example of a
binary features is “the entity type is person”.
We assume that, for a given label y, observations
are generated from a multivariate Bernoulli distribu-
tion parameterized by?y,f , where?y,f,v denotes the
probability of feature f taking the value v ? {0, 1}
given label y. The conditional probability of x given
y can then be written as
p(x|y; ?) =
?
f
p(xf = 1|y; ?)xf · p(xf = 0|y; ?)1?xf
=
?
f
?y,f,xf . (5)
5.3 Gaussian Mixture Model
In the Gaussian mixture model, we assume that a
given label y generates observations with a mul-
tivariate Gaussian distribution N (µy,?y), where
µy ? RF is the mean and ?y ? RF×F is the co-
variance matrix of the Gaussian. If we assume that
the different feature dimensions are independent and
have the same variance, that is, ?y = ?2yI , where I
is the identity matrix, then the conditional density of
x given y is
p(x|y; ?) = 1(2??2y)F/2
exp
(
??x? µy?
2
2?2y
)
. (6)
5.4 Locally Normalized Logistic Regression
Berg-Kirkpatrick et al. (2010) proposed a method
for incorporating features into generative models for
unsupervised learning. Their method models the
generation of x given y as a logistic function param-
eterized by a weight vector wy, defined as follows:
p(x|y; ?) = exp?x,wy??
x? exp?x?,wy?
. (7)
?x,w? denotes the inner product between x and w.
The denominator considers all data points x? in the
data set, thus Eq. (7) gives a probability distribution
over data points for a given y.
818
6 Parameter Estimation
We can apply the Expectation-Maximization (EM)
algorithm (Dempster et al., 1977) to maximize
the log-likelihood functions under both multinomial
prior in Eq. (2) and the proposed Poisson-based prior
in Eq. (1). For the multinomial prior, there are stan-
dard closed form solutions for the naive Bayes, the
Bernoulli mixture and the Gaussian mixture models.
For locally normalized logistic regression, model
parameters can also be learned via EM, but with
a gradient-based M-step (Berg-Kirkpatrick et al.,
2010). We leave out the details here and focus on pa-
rameter estimation in the proposed generative model
with the Poisson-based prior.
We assume that in the Poisson-based prior, the
parameters {?k}K?1k=1 and {?k}K?1k=1 are fixed ratherthan learned in this work. For the distribution
p(x|y; ?), let ?(t?1) and ?(t) denote parameter es-
timates from two consecutive EM iterations. At the
t-th iteration, the E-step updates the responsibilities
of each label assignment yi for each document:
?i,yi = p(yi|xi; ?,?(t?1))
= p(yi; ?)p(xi|yi; ?
(t?1))?
y?i p(y
?
i; ?)p(xi|y?i; ?(t?1))
, (8)
where ?i is a distribution over all possible label as-
signments yi’s for the i-th document. The M-step
updates the estimates of ?(t) based on the current
values of ?i’s and ?(t?1). This is done by maximiz-
ing the following objective function:
?
i
?
yi
?i,yi log
(
p(yi; ?)
?
j
p(xi,j |yi,j ; ?(t?1))
)
. (9)
The exact formulas used in the M-step for
updating ? depend on the functional form of
p(xi,j |yi,j ; ?). As an example, we give the formulas
for the Gaussian mixture model, in which? contains
the set of means {µ(t)k }Kk=1 and variances {?(t)k }Kk=1.Taking the derivatives of Eq. (9) with respect to µk
and to ?k, and then setting the derivations to zero,
we can solve for µk and for ?k to get:
µ(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)xi,j?
i
?
yi ?i,yi
?
j 1(yi,j = k)
, (10)
?(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)||xi,j ? µ
(t)
k ||2
F?i
?
yi ?i,yi
?
j 1(yi,j = k)
, (11)
where 1(·) is the indicator variable. We skip the
derivations here due to space limit.
Closed form solutions also exist for the naive
Bayes and the Bernoulli mixture models. For lo-
cally normalized logistic regression, parameters can
be learned with a gradient-based M-step as in the
multinomial prior setting. Existing optimization al-
gorithms, such as L-BFGS, can be used for optimiz-
ing model parameters in the M-step as discussed in
(Berg-Kirkpatrick et al., 2010).
7 Experiments
In this section, we first describe the data sets we used
in our experiments, detailing the target slots and can-
didates in each data set, as well as features we ex-
tract for the candidates. We then describe our evalu-
ation metrics, followed by experimental results.
7.1 Data Sets
We use three data sets for evaluating our unsuper-
vised IE task. Note that to speed up computation,
we only include documents or text segments con-
taining no more than 10 candidates in our experi-
ments. The first data set contains a set of seminar an-
nouncements (Freitag and McCallum, 1999), anno-
tated with four slot labels, namely stime (start time),
etime (end time), speaker and location. We used as
candidates all strings labeled in the annotated data
as well as all named entities found by the Stanford
NER tagger for CoNLL (Finkel et al., 2005). There
are 309 seminar announcements with 2262 candi-
dates in this data set.
The second data set is a collection of para-
graphs describing aviation incidents, taken from the
Wikipedia article on “List of accidents and incidents
involving commercial aircraft” (Wikipedia, 2009).
Each paragraph in the article contains one to a few
sentences describing an incident. In this domain, we
take each paragraph as a separate document, and all
hyperlinked phrases in the original Wikipedia arti-
cle as candidates. For evaluation, we manually an-
notated the paragraphs of incidents from 2006 to
2009 with five slot labels: the flight number (FN),
the airline (AL), the aircraft model (AC), the exact
819
location (LO) of the incident (e.g. airport name),
and the country (CO) where the incident occurred.
The entire data set consists of 564 paragraphs with
2783 candidates. The annotated portion consists of
74 paragraphs with 395 candidates.
The third data set comes from the management
succession domain used in the Sixth Message Un-
derstanding Conference (MUC-6, 1995). We extract
from the original data set all sentences that were
tagged with a management succession event, and use
as candidates all tagged strings in those sentences.
This domain has four target slots, namely PersonIn
(the person moving into a new position), PersonOut
(the person leaving a position), Org (the corpora-
tion’s name) and Post (the position title). Sentences
containing candidates with multiple labels (candi-
dates annotated as both PersonIn and PersonOut) are
discarded. The extracted data set consists of 757
sentences with 2288 candidates.
7.2 Features
To extract features for candidates, we first normal-
ize each word to its lower-case, with digits replaced
by the token digit. We extract the following fea-
tures for every candidate: the candidate phrase it-
self, its head word, the unigram and bigram be-
fore and after the candidate in the sentence where
it appeared, its entity type (person, location, or-
ganization, and date/time), as well as features de-
rived from dependency parse trees. Specifically, we
first apply the Stanford lexical parser to our data
(de Marneffe et al., 2006). Then for each candi-
date, we follow its dependencies in the correspond-
ing dependency parse tree until we find a relation
r ? {nsubj, csubj, dobj, iobj, pobj} in which the
candidate is the dependent. We then construct a fea-
ture (r, v) where v is governor of the relation.
7.3 Evaluation Baseline and Method
We use the standard K-means algorithm (Macqueen,
1967) as a non-generative baseline, since K-means is
commonly used for clustering. To evaluate cluster-
ing results, we match each slot in the labeled data to
the cluster that gives the best F1-measure when eval-
uated for the slot. We report the precision (P), re-
call (R) and F1-measure for individual slot labels, as
well as the macro- and micro- average results across
all labels for each experiment. We conduct 10 trials
of experiment on each model and each data set with
different random initializations. We report the trials
that give the smallest within-cluster sum-of-squares
(WCSS) distance for K-means, and those that give
the highest log-likelihood of data for all other mod-
els. Experimental trials are run until the change in
WCSS/log-likelihood between two EM iterations is
smaller than 1 × 10?6. All trials converged within
30 minutes.
All models we evaluate involve a parameter K,
which is the number of values that y can take on.
The value of K is manually fixed in this study. As
noted, we use a garbage slot to capture irrelevant
candidates, thus the value of K is set to the number
of target slots plus 1 for each data set. We empir-
ically set the adjustable parameters in the proposed
prior, and the weight of the regularization term in the
locally normalized logistic regression model (Berg-
Kirkpatrick et al., 2010), denoted by ?. Exact set-
tings are given in the next subsection. Note that the
focus of our experiments is on evaluating the effec-
tiveness of the proposed prior. We leave the task of
learning the various parameter values to future work.
7.4 Results
Evaluation on existing generative models
We first evaluate the existing generative models
described in Section 5 with the multinomial prior.
Table 1 summarizes the performance of Naive Bayes
(NB), the Bernoulli mixture model (BMM), the
Gaussian mixture model (GMM), the locally nor-
malized logistic regression (LNLR) model, and K-
means. We only show the F1 measures in the table
due to space limit.
We first observe that NB does not perform well
for our task. LNLR, which is an interesting contri-
bution in its own right, does not seem to be suitable
for our task as well. While NB and LNLR are infe-
rior to K-means for all three data sets, BMM shows
mixed results. Specifically, BMM outperforms K-
means for aviation incidents, but performs poorly
for seminar announcements. GMM and K-means
achieve similar results, which is not surprising be-
cause K-means can be viewed as a special case of
the spherical GMM we used (Duda et al., 2001).
Overall speaking, results show that GMM is the
best among the four generative models for the distri-
820
(a) Results on seminar announcements. No macro- and micro-average result is reported
for NB and BMM as they merged the etime cluster with the stime cluster. Numbers in
brackets are the respective measures of the stime cluster when evaluated for etime.
Model stime etime speaker location Macro-avg Micro-avg Parameter
NB 0.558 (0.342) 0.276 0.172 — — Nil
BMM 0.822 (0.440) 0.412 0.402 — — Nil
GMM 0.450 0.530 0.417 0.426 0.557 0.455 Nil
LNLR 0.386 0.239 0.200 0.208 0.264 0.266 ? = .0005
K-means 0.560 0.574 0.335 0.426 0.538 0.452 Nil
(b) Results on aviation incidents. Target slots are airline (AL) , flight number (FN), aircraft
model (AC), location (LO) and country (CO).
Model AL FN AC LO CO Macro-avg Micro-avg Parameter
NB 0.896 0.473 0.676 0.504 0.533 0.618 0.628 Nil
BMM 0.862 0.794 0.656 0.695 0.614 0.741 0.724 Nil
GMM 0.859 0.914 0.635 0.576 0.538 0.730 0.692 Nil
LNLR 0.597 0.352 0.314 0.286 0.291 0.379 0.396 ? = .0005
K-means 0.859 0.936 0.661 0.576 0.538 0.729 0.701 Nil
(c) Results on management succession events. Target slots are person joining (PersonIn),
person leaving (PersonOut), organization (Org), and position (Post).
Model PersonIn PersonOut Org Post Macro-avg Micro-avg Parameter
NB 0.545 0.257 0.473 0.455 0.459 0.437 Nil
BMM 0.550 0.437 0.800 0.767 0.650 0.648 Nil
GMM 0.583 0.432 0.813 0.803 0.679 0.676 Nil
LNLR 0.419 0.245 0.319 0.399 0.351 0.346 ? = .0002
K-means 0.372 0.565 0.835 0.814 0.645 0.665 Nil
Table 1: Performance summary of the different generative models and K-means in terms of F1.
Data set Parameter Value
Seminar announcements {?k}4k=1 {2}4k=1
{?k}4k=1 {1}4k=1
Aviation incidents {?k}5k=1 {1}5k=1
{?k}5k=1 {1}5k=1
Management succession {?k}4k=1 {1,2,2,2}
{?k}4k=1 {1,2,2,2}
Table 2: Parameter settings for p(yi; ?).
bution p(x|y; ?). We proceed with incorporating the
proposed prior into GMM for further explorations.
Effectiveness of the proposed prior
We evaluate the effectiveness of the proposed
prior by combining it with GMM. Specifically, the
combined model follows Eq. (1), with p(yi; ?) com-
puted using the Poisson-based formula in Eq. (3) and
p(xi,j |yi,j ; ?) following Eq. (6) as in GMM.
We empirically determine the parameters used in
p(yi; ?) to maximize data’s log-likelihood as noted.
Table 2 reports the values of {?k}K?1k=1 and {?k}K?1k=1for different data sets. Recall that ?k specifies the
maximum number of candidates that the k-th slot can
generate, and its value is observed to be small in real
data. ?k specifies the expected number of candidates
that the k-th slot will generate.
Table 3 reports the performance of the combined
model (“GMM with prior”) on the three data sets,
along with results of GMM and K-means for easy
comparison. The combined model improves over
both GMM and K-means for seminar announce-
ments and aviation incidents, as can be seen from the
models’ macro- and micro-average performance.
The advantages brought by the proposed prior are
mainly reflected in slots that are difficult to clus-
ter under GMM and K-means. Taking seminar an-
nouncements as an example, GMM and K-means
achieve high precision but low recall for stime, and
low precision but high recall for etime. When exam-
ining the clusters produced by these two models, we
found one small cluster that contains mostly stime
fillers (thus high precision but low recall), and an-
other much larger cluster that contains mostly etime
fillers together with most of the remaining stime
fillers (thus low precision but high recall for etime).
821
(a) Results on seminar announcements.
Model Metric stime etime speaker location Macro-avg Micro-avg
GMM with Prior P 0.964 0.983 0.232 0.253 0.608 0.416
R 0.680 0.932 0.952 0.481 0.761 0.738
F1 0.798 0.957 0.374 0.331 0.676 0.532
GMM P 1.000 0.362 0.300 0.436 0.524 0.407
R 0.291 0.984 0.686 0.416 0.594 0.518
F1 0.450 0.530 0.417 0.426 0.557 0.455
K-means P 0.890 0.434 0.222 0.436 0.496 0.389
R 0.408 0.847 0.679 0.416 0.588 0.541
F1 0.560 0.574 0.335 0.426 0.538 0.452
(b) Results on aviation incidents.
Model Metric AL FN AC LO CO Macro-avg Micro-avg
GMM with Prior P 1.000 1.000 1.000 0.741 0.833 0.915 0.908
R 0.753 0.877 0.465 0.588 0.727 0.682 0.673
F1 0.859 0.935 0.635 0.656 0.777 0.782 0.773
GMM P 1.000 1.000 1.000 0.563 0.433 0.799 0.724
R 0.753 0.842 0.465 0.588 0.709 0.672 0.664
F1 0.859 0.914 0.635 0.576 0.538 0.730 0.692
K-means P 1.000 0.981 0.830 0.563 0.433 0.761 0.711
R 0.753 0.895 0.549 0.588 0.709 0.699 0.691
F1 0.859 0.936 0.661 0.576 0.538 0.729 0.701
(c) Results on management succession events.
Model Metric PersonIn PersonOut Org Post Macro-avg Micro-avg
GMM with Prior P 0.458 0.610 0.720 0.774 0.640 0.642
R 0.784 0.352 0.969 0.846 0.738 0.731
F1 0.578 0.447 0.826 0.809 0.686 0.683
GMM P 0.464 0.605 0.725 0.792 0.647 0.648
R 0.782 0.336 0.925 0.815 0.715 0.707
F1 0.583 0.432 0.813 0.803 0.679 0.676
K-means P 0.382 0.515 0.733 0.839 0.607 0.639
R 0.363 0.625 0.969 0.791 0.687 0.693
F1 0.372 0.565 0.835 0.814 0.645 0.665
Table 3: Comparison between the combined model (GMM with the proposed prior), GMM and K-means.
This shows that GMM, when used with the multi-
nomial prior, and K-means have difficulties sepa-
rating candidates from these two slots. In contrast,
the combined model improves the recall of stime to
68%, as compared to 29.1% achieved by GMM with
the multinomial prior and 40.8% by K-means, with-
out sacrificing precision. It also improves the preci-
sion of etime from 36.2% to 98.3%.
For aviation incidents, the advantage of the pro-
posed prior is reflected in the location (LO) and
country (CO) slots, which may confuse the various
models as they both belong to the entity type loca-
tion. The proposed prior improves the precision of
these two slots greatly by trying to distribute them
into appropriate slots in the clustering process.
The three models achieve very similar perfor-
mance on management succession events as Ta-
ble 3(c) shows. Surprisingly, incorporating the
Poisson-based prior into GMM does not seem useful
in separating PersonIn and PersonOut slot fillers. To
investigate the possible reasons for this, we exam-
ine feature values in the centriods of the two clusters
learned by the three models.
Tables 4 and 5 respectively list the top-10 features
in the PersonIn cluster and the PersonOut cluster
learned by the combined model1, and their corre-
sponding values in the centriods of the two clusters.
The two clusters share 3 of the top-5 features, some
1We made similar observations from centriods learned in
GMM and K-Means, which are therefore not reported here.
822
Values in the centriod of:
Top-10 features PersonIn PersonOut
type:?person? 0.9985 1
unigram after:, 0.7251 0.3404
unigram before:?s? 0.2705 0
bigram after:, ?digits? 0.2105 0.1879
bigram after:, who 0.1404 0.0567
unigram before:, 0.1067 0.0035
dobj:succeeds 0.0906 0
unigarm before:succeeds 0.0892 0
nsubj:resigned 0.0746 0.0284
unigram before:said 0.0673 0
Table 4: Top-10 features in the PersonIn cluster, as
learned by GMM with the proposed prior.
of them being general context features that might not
help characterizing candidates from different slots
(e.g. the unigram after the candidate is a comma).
Both lists also contain features from dependency
parse trees. Note that the “dobj:succeeds” feature
in the PersonIn cluster is in fact contributed by Per-
sonOut slot fillers, while the “nsubj:succeeds” fea-
ture in the PersonOut cluster is contributed by Per-
sonIn slot fillers. Although listed among the top-
10, these features have relatively low values in the
learned centriods (about 0.1). These observations
may suggest that the management succession data
set lacks strong, discriminative features for all mod-
els to effectively distinguish between PersonIn and
PersonOut candidates in an unsupervised manner.
To conclude, the proposed prior is effective in as-
signing different but confusing candidate slot fillers
into appropriate slots, when there exist reasonable
features that can be exploited in the label assign-
ment process. This is evident by the improvements
the proposed prior brings to GMM in the seminar
announcement and aviation incident data sets.
8 Conclusions
We propose a generative model that incorporates
distributional prior knowledge about template slots
in a document for the unsupervised IE task. Specifi-
cally, we propose a Poisson-based prior that prefers
label assignments to cover more distinct slots in the
same document. The proposed prior also allows a
slot to generate multiple fillers in a document, up to
a certain number of times depending on the domain
of interest.
We experimented with four existing generative
Values in the centriod of:
Top-10 features PersonOut PersonIn
type:?person? 1 0.9985
unigram before:mr. 0.9894 0
bigram before:?s? mr. 0.5213 0
unigram after:, 0.3404 0.7251
bigram after:, ?digits? 0.1879 0.2105
unigram after:was 0.1667 0.0556
nsubj:president 0.1667 0.0117
nsubj:succeeds 0.1028 0.0102
bigram before:, mr. 0.0957 0
unigram after:’s 0.0745 0.0073
Table 5: Top-10 features in the PersonOut cluster, as
learned by GMM with the proposed prior.
models for the task of clustering slot fillers with
a multinomial prior, which assumes that labels are
generated independently in a document. We then
evaluate the effectiveness of the proposed prior by
incorporating it into the Gaussian mixture model
(GMM), which is shown to be the best among the
four existing models in our experiments. By incor-
porating the proposed prior into GMM, we can ob-
tain significantly better clustering results on two out
of three data sets.
Further improvements to this work are possible.
Firstly, we assume that some adjustable parameters
in the proposed prior can be manually fixed, such as
the number of template slots in the output and the
maximum numbers of fillers that can be generated
by different slots. We are looking into methods for
automatically learning such parameters. This will
help improve the applicability of our work to differ-
ent domains as an unsupervised model. Secondly,
we currently consider in the prior a probability dis-
tribution over all possible label assignments for ev-
ery document. This can be computationally expen-
sive if input documents are long, or when we aim
to discover large templates with large values of K.
An alternative is to consider an approximate solution
that evaluates, for instance, only the top few label as-
signments that are likely to maximize the likelihood
of our observations. This remains as an interesting
future work of this study.
Acknowledgments
This work is supported by DSO National Laborato-
ries. We thank the anonymous reviewers for their
helpful comments.
823
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In International Joint
Conference on Artificial Intelligence, pages 2670–
2676.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Coˆte´,
John DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 582–590.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1–38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595–602.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley-Interscience, 2nd
edition.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain
templates. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ’06,
pages 207–214, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 363–
370.
Dayne Freitag and Andrew Kachites McCallum. 1999.
Information extraction with HMMs and shrinkage. In
Proceedings of the AAAI-99 Workshop on Machine
Learning for Information Extraction.
Joa˜o Grac¸a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 415, Morristown, NJ, USA. Association
for Computational Linguistics.
J. B. Macqueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, Volume 1, pages 281–297.
Zvika Marx, Ido Dagan, and Eli Shamir. 2002. Cross-
component clustering for template induction. In Pro-
ceedings of the 2002 ICML Workshop on Text Learn-
ing.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference. Morgan Kaufmann, San Fran-
cisco, CA.
Benjamin Rosenfeld and Ronen Feldman. 2006. URES
: An unsupervised Web relation extraction system.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 667–674.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL Main con-
ference poster sessions, pages 731–738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
304–311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ’03,
pages 224–231, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Wikipedia. 2009. List of accidents and
incidents involving commercial aircraft.
http://en.wikipedia.org/wiki/List of accidents and
incidents involving commercial aircraft.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining Wikipedia texts using in-
formation from the web. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
824
