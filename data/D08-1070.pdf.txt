Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Learning with Probabilistic Features for Improved Pipeline Models
Razvan C. Bunescu
School of EECS
Ohio University
Athens, OH 45701
bunescu@ohio.edu
Abstract
We present a novel learning framework for
pipeline models aimed at improving the com-
munication between consecutive stages in a
pipeline. Our method exploits the confidence
scores associated with outputs at any given
stage in a pipeline in order to compute prob-
abilistic features used at other stages down-
stream. We describe a simple method of in-
tegrating probabilistic features into the linear
scoring functions used by state of the art ma-
chine learning algorithms. Experimental eval-
uation on dependency parsing and named en-
tity recognition demonstrate the superiority of
our approach over the baseline pipeline mod-
els, especially when upstream stages in the
pipeline exhibit low accuracy.
1 Introduction
Machine learning algorithms are used extensively
in natural language processing. Applications range
from fundamental language tasks such as part of
speech (POS) tagging or syntactic parsing, to higher
level applications such as information extraction
(IE), semantic role labeling (SRL), or question an-
swering (QA). Learning a model for a particular lan-
guage processing problem often requires the output
from other natural language tasks. Syntactic pars-
ing and dependency parsing usually start with a tex-
tual input that is tokenized, split in sentences and
POS tagged. In information extraction, named en-
tity recognition (NER), coreference resolution, and
relation extraction (RE) have been shown to benefit
from features that use POS tags and syntactic depen-
dencies. Similarly, most SRL approaches assume
a parse tree representation of the input sentences.
The common practice in modeling such dependen-
cies is to use a pipeline organization, in which the
output of one task is fed as input to the next task
in the sequence. One advantage of this model is
that it is very simple to implement; it also allows
for a modular approach to natural language process-
ing. The key disadvantage is that errors propagate
between stages in the pipeline, significantly affect-
ing the quality of the final results. One solution
is to solve the tasks jointly, using the principled
framework of probabilistic graphical models. Sut-
ton et al. (2004) use factorial Conditional Random
Fields (CRFs) (Lafferty et al., 2001) to jointly pre-
dict POS tags and segment noun phrases, improving
on the cascaded models that perform the two tasks
in sequence. Wellner et al. (2004) describe a CRF
model that integrates the tasks of citation segmen-
tation and citation matching. Their empirical results
show the superiority of the integrated model over the
pipeline approach. While more accurate than their
pipeline analogues, probabilistic graphical models
that jointly solve multiple natural language tasks are
generally more demanding in terms of finding the
right representations, the associated inference algo-
rithms and their computational complexity. Recent
negative results on the integration of syntactic pars-
ing with SRL (Sutton and McCallum, 2005) provide
additional evidence for the difficulty of this general
approach. When dependencies between the tasks
can be formulated in terms of constraints between
their outputs, a simpler approach is to solve the tasks
separately and integrate the constraints in a linear
programming formulation, as proposed by Roth and
670
Yih (2004) for the simultaneous learning of named
entities and relations between them. More recently,
Finkel et al. (2006) model the linguistic pipelines
as Bayesian networks on which they perform Monte
Carlo inference in order to find the most likely out-
put for the final stage in the pipeline.
In this paper, we present a new learning method
for pipeline models that mitigates the problem of er-
ror propagation between the tasks. Our method ex-
ploits the probabilities output by any given stage in
the pipeline as weights for the features used at other
stages downstream. We show a simple method of
integrating probabilistic features into linear scoring
functions, which makes our approach applicable to
state of the art machine learning algorithms such as
CRFs and Support Vector Machines (Vapnik, 1998;
Scho¨lkopf and Smola, 2002). Experimental results
on dependency parsing and named entity recogni-
tion show useful improvements over the baseline
pipeline models, especially when the basic pipeline
components exhibit low accuracy.
2 Learning with Probabilistic Features
We consider that the task is to learn a mapping from
inputs x ? X to outputs y ? Y(x). Each input
x is also associated with a different set of outputs
z ? Z(x) for which we are given a probabilistic
confidence measure p(z|x). In a pipeline model, z
would correspond to the annotations performed on
the input x by all stages in the pipeline other than
the stage that produces y. For example, in the case
of dependency parsing, x is a sequence of words, y
is a set of word-word dependencies, z is a sequence
of POS tags, and p(z|x) is a measure of the confi-
dence that the POS tagger has in the output z. Let
? be a representation function that maps an exam-
ple (x, y, z) to a feature vector ?(x, y, z) ? Rd, and
w ? Rd a parameter vector. Equations (1) and (2)
below show the traditional method for computing
the optimal output yˆ in a pipeline model, assuming
a linear scoring function defined by w and ?.
yˆ(x) = argmax
y?Y(x)
w · ?(x, y, zˆ(x)) (1)
zˆ(x) = argmax
z?Z(x)
p(z|x) (2)
The weight vector w is learned by optimizing a pre-
defined objective function on a training dataset.
In the model above, only the best annotation zˆ
produced by upstream stages is used for determining
the optimal output yˆ. However, zˆ may be an incor-
rect annotation, while the correct annotation may be
ignored because it was assigned a lower confidence
value. We propose exploiting all possible annota-
tions and their probabilities as illustrated in the new
model below:
yˆ(x) = argmax
y?Y(x)
w · ?(x, y) (3)
?(x, y) =
?
z?Z(x)
p(z|x) · ?(x, y, z) (4)
In most cases, directly computing ?(x, y) is unfeasi-
ble, due to a large number of annotations inZ(x). In
our dependency parsing example, Z(x) contains all
possible POS taggings of sentence x; consequently
its cardinality is exponential in the length of the sen-
tence. A more efficient way of computing ?(x, y)
can be designed based on the observation that most
components ?i of the original feature vector ? utilize
only a limited amount of evidence from the example
(x, y, z). We define (x˜, y˜, z˜) ? Fi(x, y, z) to cap-
ture the actual evidence from (x, y, z) that is used by
one instance of feature function ?i. We call (x˜, y˜, z˜)
a feature instance of ?i in the example (x, y, z).
Correspondingly, Fi(x, y, z) is the set of all fea-
ture instances of ?i in example (x, y, z). Usually,
?i(x, y, z) is set to be equal with the number of in-
stances of ?i in example (x, y, z), i.e. ?i(x, y, z) =
|Fi(x, y, z)|. Table 1 illustrates three feature in-
stances (x˜, y˜, z˜) generated by three typical depen-
dency parsing features in the example from Figure 1.
Because the same feature may be instantiated multi-
?1 : DT? NN ?2 : NNS? thought ?3 : be? in
y˜ 10?11 2?4 7?9
z˜ DT10 NN11 NNS2
x˜ thought4 be7 in9
|Fi| O(|x|2) O(|x|) O(1)
Table 1: Feature instances.
ple times in the same example, the components of
each feature instance are annotated with their po-
sitions relative to the example. Given these defi-
nitions, the feature vector ?(x, y) from (4) can be
671
Figure 1: Dependency Parsing Example.
rewritten in a component-wise manner as follows:
?(x, y) = [?1(x, y) . . . ?d(x, y)] (5)
?i(x, y) =
?
z?Z(x)
p(z|x) · ?i(x, y, z)
=
?
z?Z(x)
p(z|x) · |Fi(x, y, z)|
=
?
z?Z(x)
p(z|x)
?
(x˜,y˜,z˜)?Fi(x,y,z)
1
=
?
z?Z(x)
?
(x˜,y˜,z˜)?Fi(x,y,z)
p(z|x)
=
?
(x˜,y˜,z˜)?Fi(x,y,Z(x))
?
z?Z(x),z?z˜
p(z|x)
where Fi(x, y,Z(x)) stands for:
Fi(x, y,Z(x)) =
?
z?Z(x)
Fi(x, y, z)
We introduce p(z˜|x) to denote the expectation:
p(z˜|x) =
?
z?Z(x),z?z˜
p(z|x)
Then ?i(x, y) can be written compactly as:
?i(x, y) =
?
(x˜,y˜,z˜)?Fi(x,y,Z(x))
p(z˜|x) (6)
The total number of terms in (6) is equal with the
number of instantiations of feature ?i in the exam-
ple (x, y) across all possible annotations z ? Z(x),
i.e. |Fi(x, y,Z(x))|. Usually this is significantly
smaller than the exponential number of terms in (4).
The actual number of terms depends on the particu-
lar feature used to generate them, as illustrated in the
last row of Table 1 for the three features used in de-
pendency parsing. The overall time complexity for
calculating ?(x, y) also depends on the time com-
plexity needed to compute the expectations p(z˜|x).
When z is a sequence, p(z˜|x) can be computed ef-
ficiently using a constrained version of the forward-
backward algorithm (to be described in Section 3).
When z is a tree, p(z˜|x) will be computed using a
constrained version of the CYK algorithm (to be de-
scribed in Section 4).
The time complexity can be further reduced if in-
stead of ?(x, y) we use its subcomponent ?ˆ(x, y)
that is calculated based only on instances that appear
in the optimal annotation zˆ:
?ˆ(x, y) = [?ˆ1(x, y) . . . ?ˆd(x, y)] (7)
?ˆi(x, y) =
?
(x˜,y˜,z˜)?Fi(x,y,zˆ)
p(z˜|x) (8)
The three models are summarized in Table 2 below.
In the next two sections we illustrate their applica-
yˆ(x) = argmax
y?Y(x)
w · ?(x, y)
M1 ?(x, y) = ?(x, y, zˆ(x))
zˆ(x) = argmax
z?Z(x)
p(z|x)
yˆ(x) = argmax
y?Y(x)
w · ?(x, y)
M2 ?(x, y) = [?1(x, y) . . . ?d(x, y)]
?i(x, y) =
?
(x˜,y˜,z˜)?Fi(x,y,Z(x))
p(z˜|x)
yˆ(x) = argmax
y?Y(x)
w · ?ˆ(x, y)
M3 ?ˆ(x, y) = [?ˆ1(x, y) . . . ?ˆd(x, y)]
?ˆi(x, y) =
?
(x˜,y˜,z˜)?Fi(x,y,zˆ)
p(z˜|x)
Table 2: Three Pipeline Models.
tion to two common tasks in language processing:
dependency parsing and named entity recognition.
3 Dependency Parsing Pipeline
In a traditional dependency parsing pipeline (model
M1 in Table 2), an input sentence x is first aug-
672
mented with a POS tagging zˆ(x), and then pro-
cessed by a dependency parser in order to obtain
a dependency structure yˆ(x). To evaluate the new
pipeline models we use MSTPARSER1, a linearly
scored dependency parser developed by McDonald
et al. (2005). Following the edge based factorization
method of Eisner (1996), the score of a dependency
tree in the first order version is defined as the sum of
the scores of all edges in the tree. Equivalently, the
feature vector of a dependency tree is defined as the
sum of the feature vectors of all edges in the tree:
M1: ?(x, y) =
?
u?v?y
?(x, u?v, zˆ(x))
M2: ?(x, y) =
?
u?v?y
?(x, u?v)
M3: ?ˆ(x, y) =
?
u?v?y
?ˆ(x, u?v)
For each edge u? v ? y, MSTPARSER generates
features based on a set of feature templates that take
into account the words and POS tags at positions u,
v, and their left and right neighbors u±1, v±1. For
example, a particular feature template T used inside
MSTPARSER generates the following POS bigram
features:
?i(x, u?v, z) =
{
1, if ?zu, zv? = ?t1, t2?
0, otherwise
where t1, t2 ? P are the two POS tags associated
with feature index i. By replacing y with u? v in
the feature expressions from Table 2, we obtain the
following formulations:
M1:?i(x, u?v) =
{
1, if ?zˆu, zˆv?=?t1, t2?
0, otherwise
M2:?i(x, u?v) = p(z˜=?t1, t2?|x)
M3: ?ˆi(x, u?v) =
{
p(z˜=?t1, t2?|x), if ?zˆu, zˆv?=?t1, t2?
0, otherwise
where, following the notation from Section 2,
z˜ = ?zu, zv? is the actual evidence from z that is
used by feature i, and zˆ is the top scoring annotation
produced by the POS tagger. The implementation in
MSTPARSER corresponds to the traditional pipeline
model M1. Given a method for computing feature
1URL: http://sourceforge.net/projects/mstparser
probabilities p(z˜ = ?t1, t2?|x), it is straightforward
to modify MSTPARSER to implement models M2
and M3 – we simply replace the feature vectors ?
with ? and ?ˆ respectively. As mentioned in Sec-
tion 2, the time complexity of computing the fea-
ture vectors ? in model M2 depends on the com-
plexity of the actual evidence z˜ used by the fea-
tures. For example, the feature template T used
above is based on the POS tags at both ends of a de-
pendency edge, consequently it would generate |P|2
features in model M2 for any given edge u ? v.
There are however feature templates used in MST-
PARSER that are based on the POS tags of up to 4
tokens in the input sentence, which means that for
each edge they would generate |P|4 ? 4.5M fea-
tures. Whether using all these probabilistic features
is computationally feasible or not also depends on
the time complexity of computing the confidence
measure p(z˜|x) associated with each feature.
3.1 Probabilistic POS features
The new pipeline models M2 and M3 require an
annotation model that, at a minimum, facilitates
the computation of probabilistic confidence values
for each output. We chose to use linear chain
CRFs (Lafferty et al., 2001) since CRFs can be eas-
ily modified to compute expectations of the type
p(z˜|x), as needed by M2 and M3.
The CRF tagger was implemented in MAL-
LET (McCallum, 2002) using the original feature
templates from (Ratnaparkhi, 1996). The model
was trained on sections 2–21 from the English Penn
Treebank (Marcus et al., 1993). When tested on sec-
tion 23, the CRF tagger obtains 96.25% accuracy,
which is competitive with more finely tuned systems
such as Ratnaparkhi’s MaxEnt tagger.
We have also implemented in MALLET a con-
strained version of the forward-backward procedure
that allows computing feature probabilities p(z˜|x).
If z˜ = ?ti1ti2 ...tik? specifies the tags at k positions
in the sentence, then the procedure recomputes the
? parameters for all positions between i1 and ik by
constraining the state transitions to pass through the
specified tags at the k positions. A similar approach
was used by Culotta et al. in (2004) in order to asso-
ciate confidence values with sequences of contigu-
ous tokens identified by a CRF model as fields in an
information extraction task. The constrained proce-
673
dure requires (ik ? i1)|P|2 = O(N |P|2) multipli-
cations in an order 1 Markov model, where N is the
length of the sentence. Because MSTPARSER uses
an edge based factorization of the scoring function,
the constrained forward procedure will need to be
run for each feature template, for each pair of tokens
in the input sentence x. If the evidence z˜ required by
the feature template T constrains the tags at k posi-
tions, then the total time complexity for computing
the probabilistic features p(z˜|x) generated by T is:
O(N3|P|k+2)=O(N |P|2) ·O(N2) ·O(|P|k) (9)
As mentioned earlier, some feature templates used
in the dependency parser constrain the POS tags at 4
positions, leading to a O(N3|P|6) time complexity
for a length N sentence. Experimental runs on the
same machine that was used for CRF training show
that such a time complexity is not yet feasible, espe-
cially because of the large size of P (46 POS tags).
In order to speed up the computation of probabilis-
tic features, we made the following two approxima-
tions:
1. Instead of using the constrained forward-
backward procedure, we enforce an indepen-
dence assumption between tags at different po-
sitions and rewrite p(z˜ = ?ti1ti2 ...tik?|x) as:
p(ti1ti2 ...tik |x) ?
k
?
j=1
p(tij |x)
The marginal probabilities p(tij |x) are easily
computed using the original forward and back-
ward parameters as:
p(tij |x) =
?ij (tij |x)?ij (tij |x)
Z(x)
This approximation eliminates the factor
O(N |P|2) from the time complexity in (9).
2. If any of the marginal probabilities p(tij |x) is
less than a predefined threshold (? |P|)?1, we
set p(z˜|x) to 0. When ? ? 1, the method is
guaranteed to consider at least the most proba-
ble state when computing the probabilistic fea-
tures. Looking back at Equation (4), this is
equivalent with summing feature vectors only
over the most probable annotations z ? Z(x).
The approximation effectively replaces the fac-
tor O(|P|k) in (9) with a quasi-constant factor.
The two approximations lead to an overall time com-
plexity of O(N2) for computing the probabilistic
features associated with any feature template T , plus
O(N |P|2) for the unconstrained forward-backward
procedure. We will use M ?2 to refer to the model
M2 that incorporates the two approximations. The
independence assumption from the first approxima-
tion can be relaxed without increasing the asymp-
totic time complexity by considering as independent
only chunks of contiguous POS tags that are at least
a certain number of tokens apart. Consequently,
the probability of the tag sequence will be approxi-
mated with the product of the probabilities of the tag
chunks, where the exact probability of each chunk
is computed in constant time with the constrained
forward-backward procedure. We will use M ??2 to
refer to the resulting model.
3.2 Experimental Results
MSTPARSER was trained on sections 2–21 from the
WSJ Penn Treebank, using the gold standard POS
tagging. The parser was then evaluated on section
23, using the POS tagging output by the CRF tagger.
For model M1 we need only the best output from
the POS tagger. For models M ?2 and M ??2 we com-
pute the probability associated with each feature us-
ing the corresponding approximations, as described
in the previous section. In model M ??2 we consider
as independent only chunks of POS tags that are 4
tokens or more apart. If the distance between the
chunks is less than 4 tokens, the probability for the
entire tag sequence in the feature is computed ex-
actly using the constrained forward-backward pro-
cedure. Table 3 shows the accuracy obtained by
models M1, M ?2(?) and M ??2 (?) for various values
of the threshold parameter ? . The accuracy is com-
M1 M ?2(1) M ?2(2) M ?2(4) M ??2 (4)
88.51 88.66 88.67 88.67 88.70
Table 3: Dependency parsing results.
puted over unlabeled dependencies i.e. the percent-
age of words for which the parser has correctly iden-
tified the parent in the dependency tree. The pipeline
674
Figure 2: Named Entity Recognition Example.
model M ?2 that uses probabilistic features outper-
forms the traditional pipeline model M1. As ex-
pected, M ??2 performs slightly better than M ?2, due
to a more exact computation of feature probabilities.
Overall, only by using the probabilities associated
with the POS features, we achieve an absolute er-
ror reduction of 0.19%, in a context where the POS
stage in the pipeline already has a very high accu-
racy of 96.25%. We expect probabilistic features to
yield a more substantial improvement in cases where
the pipeline model contains less accurate upstream
stages. Such a case is that of NER based on a com-
bination of POS and dependency parsing features.
4 Named Entity Recognition Pipeline
In Named Entity Recognition (NER), the task is to
identify textual mentions of predefined types of en-
tities. Traditionally, NER is modeled as a sequence
classification problem: each token in the input sen-
tence is tagged as being either inside (I) or outside
(O) of an entity mention. Most sequence tagging
approaches use the words and the POS tags in a
limited neighborhood of the current sentence posi-
tion in order to compute the corresponding features.
We augment these flat features with a set of tree
features that are computed based on the words and
POS tags found in the proximity of the current to-
ken in the dependency tree of the sentence. We
argue that such dependency tree features are better
at capturing predicate-argument relationships, espe-
cially when they span long stretches of text. Figure 2
shows a sentence x together with its POS tagging z1,
dependency links z2, and an output tagging y. As-
suming the task is to recognize mentions of people,
the word sailors needs to be tagged as inside. If we
extracted only flat features using a symmetric win-
dow of size 3, the relationship between sailors and
thought would be missed. This relationship is use-
ful, since an agent of the predicate thought is likely
to be a person entity. On the other hand, the nodes
sailors and thought are adjacent in the dependency
tree of the sentence. Therefore, their relationship
can be easily captured as a dependency tree feature
using the same window size.
For every token position, we generate flat features
by considering all unigrams, bigrams and trigrams
that start with the current token and extend either to
the left or to the right. Similarly, we generate tree
features by considering all unigrams, bigrams and
trigrams that start with the current token and extend
in any direction in the undirected version of the de-
pendency tree. The tree features are also augmented
with the actual direction of the dependency arcs be-
tween the tokens. If we use only words to create
n-gram features, the token sailors will be associated
with the following features:
• Flat: sailors, the sailors, ?S? the sailors,
sailors mistakenly, sailors mistakenly thought.
• Tree: sailors, sailors ? the, sailors ?
thought, sailors? thought? must, sailors?
thought? mistakenly.
We also allow n-grams to use word classes such as
POS tags and any of the following five categories:
?1C? for tokens consisting of one capital letter, ?AC?
for tokens containing only capital letters, ?FC? for
tokens that start with a capital letter, followed by
small letters, ?CD? for tokens containing at least one
digit, and ?CRT? for the current token.
The set of features can then be defined as a Carte-
sian product over word classes, as illustrated in Fig-
ure 3 for the original tree feature sailors? thought
? mistakenly. In this case, instead of one com-
pletely lexicalized feature, the model will consider
12 different features such as sailors? VBD? RB,
NNS? thought? RB, or NNS? VBD? RB.
675
??
?CRT?
NNS
sailors
?
?×[?]×
[
VBD
thought
]
×[?]×
[
RB
mistakenly
]
Figure 3: Dependency tree features.
The pipeline model M2 uses features that appear
in all possible annotations z = ?z1, z2?, where z1
and z2 are the POS tagging and the dependency
parse respectively. If the corresponding evidence is
z˜ = ?z˜1, z˜2?, then:
p(z˜|x) = p(z˜2|z˜1, x)p(z˜1|x)
For example, NNS2 ? thought4 ? RB3 is a feature
instance for the token sailors in the annotations from
Figure 2. This can be construed as having been gen-
erated by a feature template T that outputs the POS
tag ti at the current position, the word xj that is the
parent of xi in the dependency tree, and the POS tag
tk of another dependent of xj (i.e. ti ? xj ? tk).
The probability p(z˜|x) for this type of features can
then be written as:
p(z˜|x) = p(i?j?k|ti, tk, x) · p(ti, tk|x)
The two probability factors can be computed exactly
as follows:
1. The M2 model for dependency parsing from
Section 3 is used to compute the probabilistic
features ?(x, u? v|ti, tk) by constraining the
POS annotations to pass through tags ti and tk
at positions i and k. The total time complexity
for this step is O(N3|P|k+2).
2. Having access to ?(x, u? v|ti, tk), the factor
p(i?j?k|ti, tk, x) can be computed in O(N3)
time using a constrained version of Eisner’s al-
gorithm, as will be explained in Section 4.1.
3. As described in Section 3.1, computing the
expectation p(ti, tk|x) takes O(N |P2|) time
using the constrained forward-backward algo-
rithm.
The current token position i can have a total of N
values, while j and k can be any positions other
than i. Also, ti and tk can be any POS tag from
P . Consequently, the feature template T induces
O(N3|P|2) feature instances. Overall, the time
complexity for computing the feature instances gen-
erated by T is O(N6|P|k+4), as results from:
O(N3|P|2) · (O(N3|P|k+2) +O(N3) +O(N |P|2))
While still polynomial, this time complexity is fea-
sible only for small values ofN . In general, the time
complexity for computing probabilistic features in
the full model M2 increases with both the number
of stages in the pipeline and the complexity of the
features.
Motivated by efficiency, we decided to use the
pipeline model M3 in which probabilities are com-
puted only over features that appear in the top scor-
ing annotation zˆ = ?zˆ1, zˆ2?, where zˆ1 and zˆ2 repre-
sent the best POS tagging, and the best dependency
parse respectively. In order to further speed up the
computation of probabilistic features, we made the
following approximations:
1. We consider the POS tagging and the depen-
dency parse independent and rewrite p(z˜|x) as:
p(z˜|x) = p(z˜1, z˜2|x) ? p(z˜1|x)p(z˜2|x)
2. We enforce an independence assumption be-
tween POS tags. Thus, if z˜1 = ?ti1ti2 ...tik?
specifies the tags at k positions in the sentence,
then p(z˜1|x) is rewritten as:
p(ti1ti2 ...tik |x) ?
k
?
j=1
p(tij |x)
3. We also enforce a similar independence as-
sumption between dependency links. Thus, if
z˜2 = ?u1 ? v1...uk ? vk? specifies k depen-
dency links, then p(z˜2|x) is rewritten as:
p(u1?v1...uk?vk|x) ?
k
?
l=1
p(ul?vl|x)
For example, the probability p(z˜|x) of the feature
instance NNS2 ? thought4 ? RB3 is approximated
as:
p(z˜|x) ? p(z˜1|x) · p(z˜2|x)
p(z˜1|x) ? p(t2 =NNS|x) · p(t3 =RB|x)
p(z˜2|x) ? p(2?4|x) · p(3?4|x)
We will use M ?3 to refer to the resulting model.
676
4.1 Probabilistic Dependency Features
The probabilistic POS features p(ti|x) are computed
using the forward-backward procedure in CRFs, as
described in Section 3.1. To completely specify the
pipeline model for NER, we also need an efficient
method for computing the probabilistic dependency
features p(u? v|x), where u? v is a dependency
edge between positions u and v in the sentence x.
MSTPARSER is a large-margin method that com-
putes an unbounded score s(x, y) for any given sen-
tence x and dependency structure y ? Y(x) using
the following edge-based factorization:
s(x, y) =
?
u?v?y
s(x, u?v) = w
?
u?v?y
?(x, u?v)
The following three steps describe a general method
for associating probabilities with output substruc-
tures. The method can be applied whenever a struc-
tured output is associated a score value that is un-
bounded in R, assuming that the score of the entire
output structure can be computed efficiently based
on a factorization into smaller substructures.
S1. Map the unbounded score s(x, y) from R
into [0, 1] using the softmax function (Bishop, 1995):
n(x, y) = e
s(x,y)
?
y?Y(x) es(x,y)
The normalized score n(x, y) preserves the ranking
given by the original score s(x, y). The normaliza-
tion constant at the denominator can be computed in
O(N3) time by replacing the max operator with the
sum operator inside Eisner’s chart parsing algorithm.
S2. Compute a normalized score for the sub-
structure by summing up the normalized scores of
all the complete structures that contain it. In our
model, dependency edges are substructures, while
dependency trees are complete structures. The nor-
malized score will then be computed as:
n(x, u?v) =
?
y?Y(x),u?v?y
n(x, y)
The sum can be computed in O(N3) time using a
constrained version of the algorithm that computes
the normalization constant in step S1. This con-
strained version of Eisner’s algorithm works in a
similar manner with the constrained forward back-
ward algorithm by restricting the dependency struc-
tures to contain a predefined edge or set of edges.
S3. Use the isotonic regression method of
Zadrozny and Elkan (2002) to map the normalized
scores n(x, u? v) into probabilities p(u? v|x). A
potential problem with the softmax function is that,
depending on the distribution of scores, the expo-
nential transform could dramatically overinflate the
higher scores. Isotonic regression, by redistributing
the normalized scores inside [0, 1], can alleviate this
problem.
4.2 Experimental Results
We test the pipeline model M ?3 versus the traditional
model M1 on the task of detecting mentions of per-
son entities in the ACE dataset2. We use the standard
training – testing split of the ACE 2002 dataset in
which the training dataset is also augmented with the
documents from the ACE 2003 dataset. The com-
bined dataset contains 674 documents for training
and 97 for testing. We implemented the CRF model
in MALLET using three different sets of features:
Tree, Flat, and Full corresponding to the union of
all flat and tree features. The POS tagger and the de-
pendency parser were trained on sections 2-21 of the
Penn Treebank, followed by an isotonic regression
step on section 23 for the dependency parser. We
compute precision recall (PR) graphs by varying a
threshold on the token level confidence output by the
CRF tagger, and summarize the tagger performance
using the area under the curve. Table 4 shows the re-
sults obtained by the two models under the three fea-
ture settings. The model based on probabilistic fea-
Model Tree Flat Full
M ?3 76.78 77.02 77.96
M1 74.38 76.53 77.02
Table 4: Mention detection results.
tures consistently outperforms the traditional model,
especially when only tree features are used. Depen-
dency parsing is significantly less accurate than POS
tagging. Consequently, the improvement for the tree
based model is more substantial than for the flat
2URL: http://www.nist.gov/speech/tests/ace
677
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
Probabilistic
Traditional
Figure 4: PR graphs for tree features.
model, confirming our expectation that probabilis-
tic features are more useful when upstream stages in
the pipeline are less accurate. Figure 4 shows the PR
curves obtained for the tree-based models, on which
we see a significant 5% improvement in precision
over a wide range of recall values.
5 Related Work
In terms of the target task – improving the perfor-
mance of linguistic pipelines – our research is most
related to the work of Finkel et al. (2006). In their
approach, output samples are drawn at each stage
in the pipeline conditioned on the samples drawn
at previous stages, and the final output is deter-
mined by a majority vote over the samples from
the final stage. The method needs very few sam-
ples for tasks such as textual entailment, where the
final outcome is binary, in agreement with a theo-
retical result on the rate of convergence of the vot-
ing Gibbs classifier due to Ng and Jordan (2001).
While their sampling method is inherently approx-
imate, our full pipeline model M2 is exact in the
sense that feature expectations are computed exactly
in polynomial time whenever the inference step at
each stage can be done in polynomial time, irrespec-
tive of the cardinality of the final output space. Also,
the pipeline models M2 and M3 and their more effi-
cient alternatives propagate uncertainty during both
training and testing through the vector of probabilis-
tic features, whereas the sampling method takes ad-
vantage of the probabilistic nature of the outputs
only during testing. Overall, the two approaches
can be seen as complementary. In order to be ap-
plicable with minimal engineering effort, the sam-
pling method needs NLP researchers to write pack-
ages that can generate samples from the posterior.
Similarly, the new pipeline models could be easily
applied in a diverse range of applications, assum-
ing researchers develop packages that can efficiently
compute marginals over output substructures.
6 Conclusions and Future Work
We have presented a new, general method for im-
proving the communication between consecutive
stages in pipeline models. The method relies on
the computation of probabilities for count features,
which translates in adding a polynomial factor to the
overall time complexity of the pipeline whenever the
inference step at each stage is done in polynomial
time, which is the case for the vast majority of infer-
ence algorithms used in practical NLP applications.
We have also shown that additional independence
assumptions can make the approach more practical
by significantly reducing the time complexity. Ex-
isting learning based models can implement the new
method by replacing the original feature vector with
a more dense vector of probabilistic features3. It is
essential that every stage in the pipeline produces
probabilistic features, and to this end we have de-
scribed an effective method for associating proba-
bilities with output substructures.
We have shown for NER that simply using the
probabilities associated with features that appear
only in the top annotation can lead to useful im-
provements in performance, with minimal engineer-
ing effort. In future work we plan to empirically
evaluate NER with an approximate version of the
full model M2 which, while more demanding in
terms of time complexity, could lead to even more
significant gains in accuracy. We also intend to com-
prehensively evaluate the proposed scheme for com-
puting probabilities by experimenting with alterna-
tive normalization functions.
Acknowledgements
We would like to thank Rada Mihalcea and the
anonymous reviewers for their insightful comments
and suggestions.
3The Java source code will be released on my web page.
678
References
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recogntion. Oxford University Press.
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In Proceed-
ings of Human Language Technology Conference and
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), Boston, MA.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th Conference on Computational linguis-
tics, pages 340–345, Copenhagen, Denmark.
Jenny R. Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annota-
tion pipelines. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 618–626, Sydney, Australia.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of 18th International Conference on
Machine Learning (ICML-2001), pages 282–289,
Williamstown, MA.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313–330.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics (ACL-
05), pages 91–98, Ann Arbor, Michigan.
Andrew Y. Ng and Michael I. Jordan. 2001. Conver-
gence rates of the Voting Gibbs classifier, with appli-
cation to bayesian feature selection. In Proceedings of
18th International Conference on Machine Learning
(ICML-2001), pages 377–384, Williamstown, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part of speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-96), pages 133–141, Philadel-
phia, PA.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
In Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004),
pages 1–8, Boston, MA.
Bernhard Scho¨lkopf and Alexander J. Smola. 2002.
Learning with kernels - support vector machines, regu-
larization, optimization and beyond. MIT Press, Cam-
bridge, MA.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In CoNLL-05 Shared
Task.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of 21st In-
ternational Conference on Machine Learning (ICML-
2004), pages 783–790, Banff, Canada, July.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Proceedings of 20th
Conference on Uncertainty in Artificial Intelligence
(UAI-2004), Banff, Canada, July.
Bianca Zadrozny and Charles Elkan. 2002. Trans-
forming classifier scores into accurate multiclass prob-
ability estimates. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-2002), Ed-
monton, Alberta.
679
