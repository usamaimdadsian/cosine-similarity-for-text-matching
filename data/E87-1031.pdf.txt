PLANNING FOR PROBLEM FORMULATION 
IN ADVICE-G IV ING D IALOGUE 
Paul Decitre, Thomas  Grossi, C14o Jullien, Jean-Philippe Solvay 
Cap Sogeti Innovation 
Centre de Recherche de Grenoble 
Chemin  du Vieux Ch ine  
38240 Meylan,  France 
Abst rac t  
We dist inguish three main,  overlapping activities in an 
advice-giving dialogue: problem formulat ion,  resolution, 
and explanat ion.  This  paper focuses on a problem for- 
mulat ion activity in a dialogue module  which interacts 
on one side with an expert  problem solver for financial 
investing and on the other side with a natura l  language 
front-end. Several strategies which reflect specific aspects 
of person-machine advice-giving dialogues are realized by 
incorporating planning at a high-level of dialogue. 
In t roduct ion  
As performances and scope of intelligent systems in- 
crease and the interaction of a system with a user gains in 
complexity, it becomes desirable to provide an easy initial 
access to a system for the novice user. Natural language 
is a med ium presumably known by most users. For the 
system however, it not only requires understanding nat- 
ural language "utterances" (on a keyboard) but also rec- 
ogni~.ing the intentions behind these utterances. It leads 
to a full-fledged dialogue involving much reasoning at the 
pragmatic level of the communication process. The com- 
petence of most intelligent systems is usually bound to 
a restricted application domain and we can imagine that 
part of a dialogue is domain-dependent while another is 
domain-independent. Our  efforts aim at designing a dia- 
logue module making these different aspects explicit and 
interacting with other knowledge-based agents. This work 
contributes to Esprit Project 816 EsteamX:  An architec- 
ture for distributed problem solving by cooperating data 
and knowledge bases. Advice-giving systems for financial 
investment have been chosen as a first testbed application. 
This paper describes preliminary research on the dialogue 
module of such a system and the resulting prototype. 
An  advice-giving dialogue comprises three main activ- 
ities, which may overlap: 
- l~wblsm form,dug/on, where the various needs and ca- 
pabilities of the user are elicited; 
- reso/uh'on, in which a possible solution to the problem 
is determined; 
- ezpla~;tion, which aims at convincing the user that 
IThe project is supported in part by the Commission of 
the European Communities 
the solution is in fact what she/he needs. 
Our work concentrates on a problem formulation activity 
in a dialogue module which cooperates with a problem 
solver and a natural language front-end. The problem 
solver selects adequate securities for basic investment sit- 
uations of a private investor and is being developed as 
part of the same Esprit project \[Bruffaerts 1986\]. The 
natural language front-end based on functional grammars 
is the result of separate research at Cap Sogeti Innovation 
\[Fimbel 1985, Lancel 1986\]. 
Computat iona l  Aspects  
Dialogue and communicat ion  theory are a broad field 
of studies drawing on several disciplines, among them phi- 
losophy, cognitive science, and artificial intelligence. The 
flurry of research devoted to these topics in recent years 
is largely enough to convince us we could not seriously 
hope to tackle the "general" problem. We have therefore 
limited our interest to person-machine advice-giving dia- 
logue and we focus on two essential characteristics of this 
kind of dialogue: 
- the system has intentions and extensive knowledge 
about the domain which are a pr/or/unknown to the 
user; 
- the user 's  intent ions must  be interpreted in terms of 
the sys tem's  abilities or inabilities. 
We can see the first point as a manifestation of the "ex- 
pertness" of the system, and the second as a manifestation 
of the Unoviceness~ of the user. 
We briefly recall some other research issues connected 
to our work and then elaborate on the specific aspects of 
person-machine advice-giving dialogue. 
Many  efforts have been devoted to developing a general 
theory for speech act understanding \[Searle 1969, Allen 
1980, Cohen 1979\]. Recognizing the illocutionary force 
of a speech act allows the system to reason about the 
intentions of the user and to behave accordingly. Most 
work in this field addresses only isolated speech acts or 
sometimes single utterances and is not concerned with a 
possible dialogue setting. Recent attempts, however, for 
reformulating speech act analysis inside a general theory 
of action {Cohen 1986\] or for applying default reasoning 
to speech act understanding \[Perrault 1986\] may yet facil- 
itate an extension to a whole dialogue. Another line of re- 
search has taken into account the dialogue dimension \[Lit- 
man 1984, Wilensky 1984, Carberry 1986\] and shown the 
strong interrelation between dialogue and plans but has 
186 
been mostly concerned with information-seeking dialogues 
in which the user seems to have an implicit knowledge of 
what the system can or cannot do. These dialogues of- 
ten produce patterns of the type "question from the user 
requiring adequate answer from the system" and seldom 
consider a possible initiative on the system's behalf. Dia- 
logue parsing is yet another approach which attempts to 
formalize the surface structure of a dialogue \[Reichman 
1984, Wachtel 1986 I. It could however lead to some rigid- 
ity in the interaction between the user and the system; 
for instance, it may  not provide the adequate primitive 
elements to detect and repair communicative failures in 
the dialogue. A possible way out would be to account 
for this surface structure of dialogue within a theory of 
pragrnatics \[Airenti 1986\]. 
In person-machine advice-giving dialogue the challenge 
we face is how to make the expert ise of the system acces- 
sible to the user in order to satisfy her /h is  needs: the "ex- 
pertness" of the system and the "novicenees" of the user 
force a compromise between the system control l ing the 
dialogue and the user expressing her /h imse l f  freely. We 
chose to rely on the system for conduct ing the dialogue 
without however ignoring the initiative of the user, which 
is to be examined within the intentional framework of the 
system. In the course of our research, we have derived a 
few general strategies which typify our approach. 
• Whenever  possible, the system should set a clear back- 
ground to the conversation. This  is part icularly true of 
the beginning of a session, where the system should not 
leave the user in the dark but  should at once define its 
own competence and suggest possible options to the user. 
This initial sett ing will reflect the global- purpose of the 
dialogue and its expected unfolding. 
• Each step of a dialogue takes place in a certain context. 
We must  ensure a common perception of this context by 
the user and the system if we want a meaningful  exchange 
between them. 
s It is worth tak ing advantage of what the system can 
expect from the user when the latter "takes the floor" to 
guide the search of a correct interpretat ion and quickly 
decide the best-suited reaction. We should make these 
expectat ions of the system apparent in our model of di- 
alogue. Nevertheless, we want the system to allow for 
user digressions, such as the introduct ion of a new topic 
or the correction of a previous s tatement .  It is impor-  
tant to note that  a sophist icated ialogue management  
which would allow the system to react adequately to this 
unexpected behavior  of the user should not impair  the 
straightforward and most  probable reaction described just  
before. It should rather  be called upon as a second best 
choice when the first one has failed, thus defining a pref- 
erence hierarchy among possible reactions of the system. 
(In other words, first the c lear-minded and obedient user, 
then the muddle-minded onel) 
• The form of the interaction between a user and an 
advice-giver evolves with the experience they have of each 
other and the increase of their mutua l  knowledge, either in 
the course of a same session or through several sessions. 
The dialogue system should gradual ly lead the user to- 
ward a s impler and more e/~lcient interface by suggest ing 
the adequate jargon and steps which would allow the user 
to quicker and better  formulate her /h is  problem \[Slator 
19861 . 
Descr ip t ion  o f  the  P ro to type  
• Wor ld  
The  World of our dialogue module consists of a set 
of objects among which several relations and inheritance 
mechanisms are defined. For instance, there are classical 
is-a links, part-of links (a cash-need is part of the invest- 
ment plan) and specification links (an amount is "specified" 
by a number and a currency). 
Parts of this semantic network are shared with other 
agents than the dialogue agent, or at least have the same 
representation in other agents. This is the case between 
the dialogue module and the problem solver for the prob- 
lem formulation phase: a model of the expected problem 
is represented in the World. 
For our application', the expected problem consists of 
an investment plan expressed in terms of the basic invest- 
ment situations for which the problem solver is able to se- 
lect the adequate securities. It may  include an emergency- 
fund, i.e., an amount of money  which should be available 
at random time within a given delay, or a cash-need, i.e., 
an amount of money which should be available at a given 
date. These financial objects in the problem model are 
related to objects describing goals and situations of the 
user's everyday world through rsqu/rvrn~nat links. For in- 
stance, buying a car in five years may necessitate a cash- 
need, while covering unexpected expenses may ask for an 
emergency-fund.  These reqm'mment links will guide the 
recognition of the user plan when resolving references. 
Other  domain knowledge for the problem formulat ion di- 
alogue is encoded in terms of the problem model objects 
and includes preferred sequences for the interaction with 
the user and constraints on these objects. 
For the dialogue module,  the user is considered as an- 
other agent and her /h is  intentions and mental  states are 
represented in terms of posit ions toward objects of the di- 
alogue. Examples of such posit ions are 'user understands  
X' ,  ' system wants to know the value of X', or 'user wants 
X to take a certain value'. We can view the objects and 
posit ions as represent ing respectively static and dynamic 
information in the  system and allowing the exchange of 
information between agents. 
• Focus -Stack  and  Agenda 
We can characterize each step of the dialogue by a given 
attent ional  focus and a given task for the system. In our 
dialogue module these correspond respectively to a par- 
t i tu lar  object - -  or set of objects - -  under  discussion and 
to an action of the system. 
Dur ing the dialogue, the focus of attent ion obviously 
evolves along a chronological dimension: one subject at 
a t ime. But  a deeper analysis (el., for example,  \[Grosz 
1985\]) reveals a layered structure . In the current pro- 
totype, these layers of foci come into play in refinement 
and digression. Ref inement occurs when the t reatment  
of a complex object is split into sub-dialogues about its 
parts: dur ing such a sub-dialogue, the "parent" and "sib- 
ling" objects const i tute background context layers. A 
typical digression takes place when the system suspends 
informat ion-gather ing to give an explanat ion and comes 
back to the suspended step of the dialogue. The system 
keeps track of the active layers of foci in the Focus-Stack. 
The sequence of actions the system has currently 
planned to perform are stored in the Agenda.  
187 
• A rch i tec ture  
The dialogue module contains four sub-modules: the 
INTERPRETER and the GENERATOR are in charge of relat- 
ing logical form expressions of the natural language front- 
end to meanings about the World, the EXECUTOR carries 
out communicative-games for interacting with the user, 
and the REACTOR activates metaplans for updating the 
Agenda and the Focus-Stack. 
The next sections of this paper investigates in greater 
detail how the metaplans and communicative-games 
model the possible actions and strategies of the system 
and enter into the dialogue planning process. An  account 
on other aspects of this prototype may be found in a pre- 
vious technical report \[Decitre 1986\]. 
H igh-Leve l  P lann ing  in  the  REACTOR 
From the dialogue module's point of view, the entire 
conversation results from the goal, "Obtain an investment 
plan problem specification from the user". The goal is ex- 
panded according to the problem model into appropriate 
subgoMs, which are pushed onto the Agenda for sequen- 
tial execution. As each subgoal is considered, it may be 
further expanded as necessary. In other words, the de- 
composition of the communicative intentions (obtaining 
specifications) reflects the decomposition of the task in- 
tentions (investing). There exist two types of metaplans: 
the metaplans for expanding the Agenda and the meta, 
plans for revising it according to some initiative from the 
user. 
• Expans ion  
As an illustration of the first type of metaplans, let 
us consider what happens at the beginning of an advice- 
giving session. When the dialogue starts, the Agenda con- 
sists solely of a single action t~atCinsest-plaa ). A treat action 
basically corresponds to a sequence of three steps: presen- 
tation of the object to the user, asking for values which 
specify this object, and finally asking for confirmation. 
But the expansion of treat actions can vary according to 
the type of their argument. For instance, an object may be 
either simple or complex, it may also be visible or trans- 
parent. A transparent object is part of the structure of the 
problem model but remains invisible to the user. This is 
the case for p~b~insest -p /an)  which consists of the set 
of the parts of an investment plan, /.e., {emews~U-yum/, 
cadL-need*,/o~-term}. These transparent objects attempt 
to model the differences which may exist between how 
the problem model is organised and how it may be per- 
ceived by the user. For a complex object, the expansion 
introduces treatment~ for the parts of the complex object, 
whereas imple objects have only specifications. 
Let us just show how these expansion metaplans ac- 
count for the first two of our general strategies. 
The expansion of the initial goal tvest(inee#t-plan~ posts 
a prcsent(ineest-plan) onto the Agenda. The presentation 
of a complex object such as in,eat-p/an reflects how it will 
be expanded, since the same source of information, i.e., 
the problem model, is used for presentation and expan- 
sion, and thus provides a background setting for the di- 
alogue. The order - -  in this case obligatory - -  in which 
the sub-objects of in~eJt-plan are considered is: first, the 
tota/-amount for the plan; second, the pa~tion(in~est-plan). 
The latter is a transparent object for which adequate pre- 
sentation rules are defined: the presentation of a partition 
simply entails a presentation of all parts. The natural lan- 
guage front-end actually generates the following descrip- 
tion: 
system-"investment-plan: An investment plan is 
characterized by a total amount and is usually com- 
posed of an emergency-fund, one or several cash-needs 
and a long-term investment." 
Update of the Focus-Stack is also governed by the expan- 
sion, and a layer containing all the objects introduced in 
this presentation is pushed onto the stack. The present 
example gives \[toto3-amount, emergen~p-fun~l, co.h-need, Ion4- 
term\]. We see again the effect of transparency: the parts 
themselves are directly pushed onto the stack and not the 
partition. This layer will constitute the backup layer of 
the Focus-Stack associated to the overall dialogue setting. 
At this stage, the next action on the Agenda is 
t~at(totoi-~mount) which may be further expanded in pu~h- 
focua, o,~k-i~Jo-game, ckeek-com~ete, pop-focus. The ask-info- 
game is a communicative game which asks a question 
about the total-amount object: 
system - 'What  is the total amount of your plan of 
investment?" 
and waits for the response of the user. The communica- 
tive game is designed to induce the user to specialize 
her/his focus of attention toward the refined context otal- 
~mount, and pud~-Jocu~(total-~nount) places this object on 
the Focus-Stack, updating it correspondingly. 
• Rev is ion  
Our plan generation is simplified because the execution 
of one subgoal cannot invalidate another, so a constant 
monitoring of preconditions is obviated; but this is more 
than made up by the difficulW in accommodating possible 
changes to the plan necessitated ble the user's input. The 
choice of a planning process which either expands or re- 
pairs an existing plan reflects our third strategy. Indeed, 
the natural expansion of a plan can be seen as correspond- 
ing to the expected behavior of the user and the revisions 
only happen when the user takes the initiative. In this 
approach, the reasoning which takes place when the user 
follows the expected course is reduced to its minimum and 
only digressions require extra efforts. 
Interactions with the user are handled through com- 
municative games and a special metaplan reacts when a 
communicative game appears on top of the Agenda. This 
metaplan triggers the execution of the game and ann- 
lyres the outcome of the execution to decide consequently 
the updates to the Agenda. If the game has completely 
succeeded, /.e. all responses of the user fit the expecta- 
tions, the communicative game is simply removed from 
the Agenda and replaced by ok-~e~'t actions for each new 
position expressed by the user. Otherwise there exist un- 
expected responses and different actions are pushed onto 
the Agenda in such a way that the expected positions will 
be analysed first by means of ok-react actions, then un- 
expected positions concerning the current focus and un- 
expected positions outside the current focus by means of 
not-ok-react actions. For all these not-ok-re~ct actions, there 
are metaplans to consider the precise situation and to 
decide an appropriate reaction, with rearrangement and 
other modifications made as necessary to the Agenda of 
pending actions. Delaying the expansion of plans until it 
188 
becomes necessary to execute them facilitates taking into 
account the effect of the user's responses on goals not 
yet addressed, as in, for example, the verification of con- 
straints which the various parts of the problem definition 
impose on one another, or in noticing that the value of a 
missing variable can be computed from the combination 
of other values the user has already given. 
What  sorts of snags can occur in a dialogue that might 
force the system to revise its plans? Our problem model 
provides certain relations which must hold between val- 
ues provided by the user. The user might, however, 
give a value which is in conflict either with one of these 
constraints or with values previously given. We must 
point out the sticking-point and help the user resolve 
the conflict. The serify-cor~straird metaplan pushes a me~- 
con~trsint-game onto the Agenda. This game will present 
the local constraint which led to refusing the new posi- 
tion expressed by the user and the justifications which 
relate this local constraint to the global constraints of the 
problem model. Consider, for instance, a simple equality 
constraint between the total amount and the sum of the 
amounts  of the parts. Wi th  a $20,000 total-amount and 
a $5,000 amount for the eme~encp/um/, a $16,000 assign- 
value position for the amount of the ca.sh-~cd would bring 
system -"The amount of your cash-need should be 
less than or equal to $15.000 for consistency with the 
total amount." 
We also have preferences (and somet imes obligations) 
in the ordering of the various points to be addressed ur- 
ing the conversation, but  the user  might  not respect hem. 
For instance, the user might  at any moment  decide to 
change subject,  in which case we must  consider the effects 
of the switch: if, for example,  she /he  asks to back up in 
the conversation to change someth ing  which was of neces- 
sity addressed before the current subject,  this could force 
revision of all the values given since that  point up to the 
present. Based on the following situations, we identify 
three classes of change-~b'ject metaplans,  which can trig- 
ger when the new posit ion expressed by the user bears 
on a context which is not the current  focus and modify 
accordingly the Agenda:  
- the  current focus must  be treated before the new 
subject introduced by the user (according to se- 
quencing policies in the problem model),  
- the  subject the user would like to examine has al- 
ready been treated and a modif ication would have 
consequences on what has been discussed since, 
- there is no sequencing difficulty. 
If the user asks for explanation of some point which 
she/he doesn't understand, the system enters a digression 
in the dialogue, after which the original topic is resumed. 
Low-Leve l  P lann ing  and  the  EXECUTOR 
As discussed above, the decomposit ion of a plan often 
engenders the need for interaction with the user. This  
is done through the communicat ive games. Basically a 
communicat ive game aims at representing a pair of turns 
between the user  and the system,  e.g., quest ion/answer.  
(In fact, we also need to model one-turn games for the 
transit ions between phaees, e.g., in t roduct ion / resumpt ion  
of a new/old subject).  A l though we can never be sure 
the second turn  will take place as desired, the interest 
of representing games is to provide local expectat ions for 
the interpretation of the response of the user. It should 
be noted that our intention in using these communicative 
games is not to impose a structure on the dialogue be- 
tween the user and the system: these games correspond 
to an ideal dialogue in which the user would always re- 
spond as expected. The actual dialogue is a succession 
of communicative games which may fail, thereby reacti- 
vating the high-level planning process described in the 
previous section. 
With each communicative game is associated an out- 
meaning which indicates the semantic content to be con- 
veyed to the user when the game is executed. This oral- 
meaning is expressed in the internal language of the dia- 
logue module in which mostly appear objects of the prob- 
lem model. Adequate references in logical form to these 
objects are provided by the GENERATOR of the dialogue 
module. The referring process utilizes: 
- the semantic representation of the World; 
- the Focus-Stack, especially the current focus which 
may be elliptically referred to; 
- the conceptual state of the user. 
This conceptual state is based on initial assumptions, e.g., 
whether a concept is a prior/familiar to the user, and 
on what has already transpired during the dialogue, e.g. 
whether a concept has already been explained, or how 
the user has previously referred to an object of the prob- 
lem model. The GENERATOR takes this information to 
adapt its description and link unknown concepts to fa- 
miliar ones. Thus the user progressively learns what the 
problem model consists of and how it relates to her/his 
familiar concepts: a simple but efficient approach to the 
evolving interaction between the user and the system held 
above as our fourth desirable strategy for person-machine 
advice-giving dialogues. 
Symmetrically a communicative game is also charac- 
terized by an ia-ezpeeted meaning which stands for the ex- 
pected response of the user, usually in terms of positions 
on the current focus or on parts of the current focus. The 
user's sentence is put into logical form by the natural lan- 
guage front-end and poseible meanings are proposed by 
the INTERPRETER. The latter has to determine which 
object of the problem model the description of the user 
could refer to. Each interpretation attempt is done within 
a context, that is a particular object which is the root of 
the search process. Interpretation is based on two search 
strategies: the first uses specificat/on links, while the sec- 
ond uses d~cr/m,'~nt properties and re~'rement links. Two 
types of reference can be recognized. Direct reference uses 
only the first strategy following the R~','fwah'on links start- 
ing from the context object and allows for elliptical an- 
swers to questions. Indirect reference uses successively 
both strategies: a search based on the dimerirnlnant prop- 
erties determines candidate objects with a ~q~'rement link 
to the context object, then these candidates constitute the 
starting points for searching along apecificat/on links. The 
user does not have the same structured view of the finan- 
cial world as the system do, and hence will not necessarily 
refer to things as we would like. The user will talk about 
Uthe car I want to buy in five years" which requires a 
cash-need. Interpretation attempts are ordered according 
to the stack of loci: the most salient focus (or layer of loci) 
is selected as context (or set of contexts), then the deeper 
foci are successively tried. The INTERPRETER only tries 
189 
a deeper focus if no interpretation has been found at a 
higher layer. Moreover, for each layer, the INTERPRETER 
tries to solve the direct reference before the indirect one 
and returns all possible interpretations within the first 
layer and type of reference which permitted to solve the 
reference. The structure of past loci partly reflects the 
evolution of our task structure \[Grosz 1985\] and allows 
the user to refer back to past segments of the dialogue. 
This structure is more supple than a mechanism which 
relies solely on unachieved goals because not only is the 
focus of a completed task not lost, but its location within 
this structure is influenced by the problem model in order 
to optimize subsequent recovery. 
Additional knowledge is contained in game descrip- 
tions: a feature in-react complements in-ezpreted in provid- 
ing a set of game-specific rules for interpreting the literal 
meaning of the user's response returned by the INTER- 
PRETER into its intended meaning within the particular 
game considered. A simple example consists of trans- 
formation rules for yes-ok/no answers depending on the 
game. 
Conc lus ion  
This work incorporates planning by the system at a 
high level of dialogue, and nevertheless leaves a great deal 
of initiative to the user. This flexibility is enhanced by 
the wide range of input styles which are allowed by the 
interpretation f input according to focus and indirect ref- 
erence. At the moment we have a prototype of a dia- 
logue module written in Prolog which implements general 
strategies for person-machine advice-giving dialogue. The 
naturM-language front-end, written in C, has been inter- 
faced with the prototype, but the generation side would 
require further investigation. Generalizing the planning 
component and integrating more sophisticated plan recog- 
nition techniques are some of the other issues addressed 
in a next prototype. Work is also under way to extend 
the concept base in our knowledge world to enrich the 
conversation with the user. 
Re ferences  
Airenti G., Bara B.G., and Colombetti M., uCogni- 
tire Pragmatic.s," Research Report URIA 86-1, Unit~. di 
Ricerca di Intelligen-.a Artiflciale, Universit~ di Milano, 
1986. 
Allen J., ~A Plan-Based Analysis of Indirect Speech 
Acts," JournoJ olthe Assoeiah'on ol Computation~ Lin~ietics, 
vol. 15, 1980. 
Bruffeerts A., Henin E., and Marlair V., ~An Expert Sys- 
tem Prototype for Financial Counseling," Research Re- 
port 507, Philips Research Laboratory Brussels, 1986. 
Carberry S., "User Models: the Problem of Disparity," 
Procredinos of the Xlth International Cortlcr~nce on Computa- 
t/ona/L/ngu/~ics, pp. 29-34, Bonn (FR Germany), 1986. 
Cohen P.R. and Perrault C.R., "Elements of a Plan-Based 
Theory of Speech Acts," Cognitive 8cicnre, no. 3, pp. 177- 
212, 1979. 
Cohen P.R., "The Role of Speech Acts in Natural Lan- 
guage Understanding," Tutorials of the XIth International 
Conference on Computational Linguistics, Bonn (FR Ger- 
many), 1986. 
Decitre P., Grossi T., Juilien C., and Solvay J.P., "A Sum- 
mary Description of a Dialoguer Prototype," Technical 
Report CRG 86-1, Cap Sogeti Innovation, 1986. 
Fimbel E., Groscot H., Lancel J.M., and Simonin N., "Us- 
ing a Text Model for Analysis and Generation," Proce~/in¢8 
of the S~o~ Conference of the European CTmpter of the AsJo- 
ciah'on /or Computational La'nguiatics, Geneva (Switzerland), 
1985. 
Gross B.J., "Discourse Structure and the Proper Treat- 
ment of Interruptions," Proceeding~ of the IXth IJCAI, Los 
Angeles (USA), 1985. 
Lancel J.M., Rousselot F., and Simonin N., "A Gram- 
mar  Used for Parsing and Generation," Procredings of the 
Xlth International Conference on Computational Linguiatic,, 
pp. 536-539, Bonn (FR Germany), 1986. 
Litman D.J. and Allen J.F., "A Plan Recognition Model 
for Subdialogues in Conversations," Technical Report 141, 
University of Rochester, 1984. 
Perrault C.R., "An Application of Default Logic to Speech 
Act Theory," Proceedings ol the NATO Workshop on ~ruc- 
ture o/Mulh'mod~ DioJoguee Includ.in~ Voice, Venaco (France), 
1986. 
Reichman R., "Extended Person-Machine Interface," At- 
tibia\] Intelligence, vol. 22, pp. 157-218, 1984. 
Searle J., Speech Acts: An Eama# in the Ph~osoph# olLar~uaoe, 
Cambridge University Press, 1969. 
Slator B.M., Anderson M.P., and Conley W., "Pygmalion 
at the Interface," Communications of the ACM, vol. 29 , 
no. ?, pp. 599-604, 1986. 
Wachtel T., ~Pragmatic Sensitivity in NL Interfaces and 
the Structure of Conversation," Proeee&'ngs o/the Xlth Inter- 
nohion~d Conleren~ on Computational Lingui~ics, pp. 35-41, 
Bonn (FR Germany), 1986. 
Wileusky R., Arens Y., and Chin D., "Talking to UNIX 
in English: An Overview of UC, ~ C'ommun/cat/o~ /the 
ACM, vol. 27 , no. 6, pp. 574-593, 1984. 
190 
