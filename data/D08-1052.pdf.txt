Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
LTAG Dependency Parsing with Bidirectional Incremental Construction
Libin Shen
BBN Technologies
lshen@bbn.com
Aravind K. Joshi
University of Pennsylvania
joshi@cis.upenn.edu
Abstract
In this paper, we first introduce a new archi-
tecture for parsing, bidirectional incremental
parsing. We propose a novel algorithm for in-
cremental construction, which can be applied
to many structure learning problems in NLP.
We apply this algorithm to LTAG dependency
parsing, and achieve significant improvement
on accuracy over the previous best result on
the same data set.
1 Introduction
The phrase “Bidirectional Incremental” may appear
self-contradictory at first sight, since incremental
parsing usually means left-to-right parsing in the
context of conventional parsing. In this paper, we
will extend the meaning of incremental parsing.
The idea of bidirectional parsing is related to
the bidirectional sequential classification method de-
scribed in (Shen et al., 2007). In that paper, a tagger
assigns labels to words of highest confidence first,
and then these labels in turn serve as the context of
later labelling operations. The bidirectional tagger
obtained the best results in literature on POS tagging
on the standard PTB dataset.
We extend this method from labelling to structure
learning, The search space of structure learning is
much larger, so that it is appropriate to exploit con-
fidence scores in search.
In this paper, we are interested in LTAG depen-
dency parsing because TAG parsing is a well known
problem of high computational complexity in reg-
ular parsing. In order to get a focus for the learn-
ing algorithm, we work on a variant of LTAG based
parsing in which we learn the word dependency re-
lations encoded in LTAG derivations instead of the
full-fledged trees.
1.1 Parsing
Two types of parsing strategies are popular in nat-
ural language parsing, which are chart parsing and
incremental parsing.
Suppose the input sentence is w1w2...wn. Let cell
[i, j] represent wiwi+1...wj , a substring of the sen-
tence. As far as CFG parsing is concerned, a chart
parser computes the possible structures over all pos-
sible cells [i, j], where 1 ? i ? j ? n. The order
of computing on these n(n + 1)/2 cells is based on
some partial order , such that [p1, p2]  [q1, q2] if
q1 ? p1 ? p2 ? q2. In order to employ dynamic
programming, one can only use a fragment of a hy-
pothesis to represent the whole hypothesis, which
is assumed to satisfy conditional independence as-
sumption. It is well known that richer context rep-
resentation gives rise to better parsing performance
(Johnson, 1998). However, the need for tractability
does not allow much internal information to be used
to represent a hypothesis. The designs of hypothe-
ses in (Collins, 1999; Charniak, 2000) show a del-
icate balance between expressiveness and tractabil-
ity, which play an important role in natural language
parsing.
Some recent work on incremental parsing
(Collins and Roark, 2004; Shen and Joshi, 2005)
showed another way to handle this problem. In
these incremental parsers, tree structures are used
to represent the left context. In this way, one can
access the whole tree to collect rich context in-
formation at the expense of being limited to beam
search, which only maintains k-best results at each
495
step. Compared to chart parsing, incremental pars-
ing searches for the analyses for only 2n ? 1 cells,
[1, 1], [2, 2], [1, 2], .., [i, i], [1, i], .., [1, n], incremen-
tally, while complex structures are used for the anal-
yses for each cell, which satisfy conditional inde-
pendence under a much weaker assumption.
In this paper, we call this particular approach
left-to-right incremental parsing, since one can also
search from right to left incrementally in a similar
way. A major problem of the left-to-right approach
is that one can only utilize the structural information
on the left side but not the right side.
1.2 Parsing as Bidirectional Construction
A natural way to handle this problem is to employ
bidirectional search, which means we can dynami-
cally search the space in two directions. So we ex-
pand the idea of incremental parsing by introducing
greedy search. Specifically, we look for the hypothe-
ses over the cell [1, n] by building analyses over
2n? 1 cells [ai,1, ai,2], i = 1, .., 2n? 1 step by step,
where [a2n?1,1, a2n?1,2] = [1, n]. Furthermore, for
any [ai,1, ai,2]
• ai,1 = ai,2, or
• ?j, k, such that [ai,1, ai,2] = [aj,1, ak,2], where
j < i, k < i and aj,2 + 1 = ak,1.
It is easy to show that the set {[ai,1, ai,2] | 1 ?
i ? 2n? 1} forms a tree relation, which means that
each cell except the last one will be used to build an-
other cell just once. In this framework, we can begin
with several starting points in a sentence and search
in any direction. So left-to-right parsing is only a
special case of incremental parsing defined in this
way. We still use complex structures to represent
the partial analyses, so as to employ both top-down
and bottom-up information as in (Collins and Roark,
2004; Shen and Joshi, 2005). Furthermore, we can
utilize the rich context on both sides of the partial
results.
Similar to bidirectional labelling in (Shen et al.,
2007), there are two learning tasking in this model.
First, we need to learn which cell we should choose.
At each step, we can select only one path. Sec-
ondly, we need to learn which operation we should
take for a given cell. We maintain k-best candidates
for each cell instead of only one, which differenti-
ates this model from normal greedy search. So our
model is more robust. Furthermore, we need to find
an effective way to iterate between these two tasks.
Instead of giving an algorithm specially designed
for parsing, we generalize the problem for graphs. A
sentence can be viewed as a graph in which words
are viewed as vertices and neighboring words are
connected with an arc. In Sections 2 and 3, we
will propose decoding and training algorithms re-
spectively for graph-based incremental construction,
which can be applied to many structure learning
problems in NLP.
We will apply this algorithm to dependency pars-
ing of Lexicalized Tree Adjoining Grammar (Joshi
and Schabes, 1997). Specifically, we will train and
evaluate an LTAG dependency parser over the LTAG
treebank described in Shen et al. (2008). We report
the experimental results on PTB section 23 of the
LTAG treebank. The accuracy on LTAG dependency
is 90.5%, which is 1.2 points over 89.3%, the previ-
ous best result (Shen and Joshi, 2005) on the same
data set.
It should be noted that PTB-based bracketed la-
belling is not an appropriate evaluation metric here,
since the experiments are on an LTAG treebank.
The derived trees in the LTAG treebank are different
from the CFG trees in PTB. Hence, we do not use
metrics such as labeled precision and labeled recall
for evaluation.
2 Graph-based Incremental Construction
2.1 Idea and Data Structures
Now we define the problem formally. We will use
dependency parsing as an example to illustrate the
idea.
We are given a connected graph G(V,E) whose
hidden structure is U , where V = {vi}, E ?
V × V is a symmetric relation, and U = {uk} is
composed of a set of elements that vary with ap-
plications. As far as dependency parsing is con-
cerned, the input graph is simply a chain of ver-
tices, where E(vi?1, vi), and its hidden structure is
{uk = (vsk , vek , bk)}, where vertex vek depends on
vertex vsk with label bk.
A graph-based incremental construction algo-
rithm looks for the hidden structure in a bottom-up
496
style.
Let xi and xj be two sets of connected vertexes
in V , where xi ? xj = ? and they are directly con-
nected via an edge in E. Let yxi be a hypothesized
hidden structure of xi, and yxj a hypothesized hid-
den structure of xj .
Suppose we choose to combine yxi and yxj with
an operation r to build a hypothesized hidden struc-
ture for xk = xi ? xj . We say the process of con-
struction is incremental if the output of the opera-
tion, yxk = r(xi, xj, yxi, yxj) ? yxi ? yxj for all
the possible xi, xj , yxi, yxj and operation r. As far
as dependency parsing is concerned, incrementality
means that we cannot remove any links coming from
the substructures.
Once yxk is built, we can no longer use yxi or
yxj as a building block. It is easy to see that left
to right incremental construction is a special case of
our approach. So the question is how we decide the
order of construction as well as the type of operation
r. For example, in the very first step of dependency
parsing, we need to decide which two words are to
be combined as well as the dependency label to be
used.
This problem is solved statistically, based on the
features defined on the substructures involved in the
operation and their context. Suppose we are given
the weights of these features, we will show in the
next section how these parameters guide us to build
a set of hypothesized hidden structures with beam
search. In Section 3, we will present a Perceptron
like algorithm (Collins, 2002; Daume´ III and Marcu,
2005) to obtain the parameters.
Now we introduce the data structure to be used in
our algorithms.
A fragment is a connected sub-graph of G(V,E).
Each fragment x is associated with a set of hypothe-
sized hidden structures, or fragment hypotheses for
short: Y x = {yx1 , ..., yxk}. Each yx is a possible frag-
ment hypothesis of x.
It is easy to see that an operation to combine two
fragments may depend on the fragments in the con-
text, i.e. fragments directly connected to one of the
operands. So we introduce the dependency relation
over fragments. Suppose there is a dependency re-
lation D ? F × F , where F ? 2V is the set of all
fragments in graph G. D(xi, xj) means that any op-
eration on a fragment hypothesis of xi depends on
the features in the fragment hypothesis of xj , and
vice versa.
We are especially interested in the following two
dependency relations.
• level-0 dependency: D0(xi, xj) ?? i = j.
• level-1 dependency: D1(xi, xj) ?? xi and
xj are directly connected in G.
Level-0 dependency means that the features of
a hypothesis for a vertex xi do not depend on the
hypotheses for other vertices. Level-1 dependency
means that the features depend on the hypotheses of
nearby vertices only.
The learning algorithm for level-0 dependency is
similar to the guided learning algorithm for labelling
as described in (Shen et al., 2007). Level-1 depen-
dency requires more data structures to maintain the
hypotheses with dependency relations among them.
However, we do not get into the details of level-1
formalism in this papers for two reasons. One is the
limit of page space and depth of a conference pa-
per. On the other hand, our experiments show that
the parsing performance with level-1 dependency is
close to what level-0 dependency could provides.
Interested readers could refer to (Shen, 2006) for
detailed description of the learning algorithms for
level-1 dependency.
2.2 Algorithms
Algorithm 1 shows the procedure of building hy-
potheses incrementally on a given graph G(V,E).
Parameter k is used to set the beam width of search.
Weight vector w is used to compute score of an op-
eration.
We have two sets, H and Q, to maintain hypothe-
ses. Hypotheses in H are selected in beam search,
and hypotheses in Q are candidate hypotheses for
the next step of search in various directions.
We first initiate the hypotheses for each vertex,
and put them into set H . For example, in depen-
dency parsing, the initial value is a set of possible
POS tags for each single word. Then we use a queue
Q to collect all the possible hypotheses over the ini-
tial hypotheses H .
Whenever Q is not empty, we search for the hy-
pothesis with the highest score according to a given
weight vector w. Suppose we find (x, y). We select
497
Algorithm 1 Incremental Construction
Require: graph G(V,E);
Require: beam width k;
Require: weight vector w;
1: H ? initH();
2: Q? initQ(H);
3: repeat
4: (x?, y?)? arg max(x,y)?Q score(y);
5: H ? updateH(H,x?);
6: Q? updateQ(Q,H, x?);
7: until (Q = ?)
DT MD NNVB CDNN
NN NN
student will take four coursesthe
Figure 1: After initialization
top k-best hypotheses for segment x from Q and use
them to update H . Then we remove from Q all the
hypotheses for segments that have overlap with seg-
ment x. In the end, we build new candidate hypothe-
ses with the updated selected hypothesis set H , and
add them to Q.
2.3 An Example
We use an example of dependency parsing to illus-
trate the incremental construction algorithm first.
Suppose the input sentence is the student will take
four courses. We are also given the candidate POS
tags for each word. So the graph is just a linear struc-
ture in this case. We use level-0 dependency and set
beam width to two.
We use boxes to represent fragments. The depen-
dency links are from the parent to the child.
Figure 1 shows the result after initialization. Fig-
ure 2 shows the result after the first step, combining
the fragments of four and courses. Figure 3 shows
the result after the second step, combining the and
student, and figure 4 shows the result after the third
step, combining take and four courses. Due to lim-
ited space, we skip the rest operations.
2.4 Description
Now we will explain the functions in Algorithm 1
one by one.
DT NN VBMD CD NN
NN NN CD NN
student will take four coursesthe
Figure 2: Step 1
DT NN VBMD CD NN
DT NN NN NN CD NN
student will take four coursesthe
Figure 3: Step 2
• initH() initiates hypotheses for each vertex.
Here we set the initial fragment hypotheses,
Y xi = {yxi1 , ..., yxik }, where xi = {vi} con-
tains only one vertex.
• initQ(H) initiates the queue of candidate op-
erations over the current hypotheses H . Sup-
posed there exist segments xi and xj which are
directly connected in G. We apply all possi-
ble operations to all fragment hypotheses for xj
and xj , and add the result hypotheses in Q. For
example, we generate (x, y) with some opera-
tion r, where segment x is xi ? xj .
All the candidate operations are organized with
respect to the segments. For each segment, we
maintain top k candidates according to their
scores.
• updateH(H,x) is used to update hypotheses in
H . First, we remove from H all the hypotheses
whose corresponding segment is a sub-set of x.
Then, we add into H the top k hypotheses for
segment x.
• updateQ(Q,H, x) is also designed to complete
two tasks. First, we remove from Q all the
hypotheses whose corresponding segment has
overlap with segment x. Then, we add new
candidate hypotheses depending on x in a way
498
DT NN VBMD CD NN
DT NN NN CD NN
student will take four courses
MD
the
Figure 4: Step 3
Algorithm 2 Parameter Optimization
1: w? 0;
2: for (round r = 0; r < R; r++) do
3: load graph Gr(V,E), gold standard Hr;
4: initiate H and Q;
5: repeat
6: (x?, y?)? arg max(x,y)?Q score(y);
7: if (y? is compatible with Hr) then
8: update H and Q;
9: else
10: y˜ ? positive(Q,x?);
11: promote(w, y˜);
12: demote(w, y?);
13: update Q with w;
14: end if
15: until (Q = ?)
16: end for
similar to the initQ(H) function. For each seg-
ment, we maintain the top k candidates for each
segment.
3 Parameter Optimization
In the previous section, we described an algorithm
for graph-based incremental construction for a given
weight vector w. In Algorithm 2, we present a Per-
ceptron like algorithm to obtain the weight vector
for the training data.
For each given training sample (Gr,Hr), where
Hr is the gold standard hidden structure of graph
Gr, we first initiate cut T , hypotheses HT and can-
didate queue Q by calling initH and initQ as in Al-
gorithm 1.
Then we use the gold standard Hr to guide the
search. We select candidate (x?, y?) which has the
highest operation score in Q. If y? is compatible with
Hr, we update H and Q by calling updateH and
updateQ as in Algorithm 1. If y? is incompatible
with Hr, we treat y? as a negative sample, and search
for a positive sample y˜ in Q with positive(Q,x?).
If there exists a hypothesis y˜x? for fragment x?
which is compatible with Hr, then positive(Q,x?)
returns y˜x? . Otherwise positive(Q,x?) returns the
candidate hypothesis which is compatible with Hr
and has the highest operation score in Q.
Then we update the weight vector w with y˜ and
y?. At the end, we update the candidate Q by using
the new weights w.
In order to improve the performance, we use Per-
ceptron with margin in the training (Krauth and
Me´zard, 1987). The margin is proportional to the
loss of the hypothesis. Furthermore, we use aver-
aged weights (Collins, 2002; Freund and Schapire,
1999) in Algorithm 1.
4 LTAG Dependency Parsing
We apply the new algorithm to LTAG dependency
parsing on an LTAG Treebank (Shen et al., 2008)
extracted from Penn Treebank (Marcus et al., 1994)
and Proposition Bank (Palmer et al., 2005). Penn
Treebank was previously used to train and evalu-
ate various dependency parsers (Yamada and Mat-
sumoto, 2003; McDonald et al., 2005). In these
works, Magerman’s rules are used to pick the head
at each level according to the syntactic labels in a
local context.
The dependency relation encoded in the LTAG
Treebank reveals deeper information for the follow-
ing two reasons. First, the LTAG architecture itself
reveals deeper dependency. Furthermore, the PTB
was reconciled with the Propbank in the LTAG Tree-
bank extraction (Shen et al., 2008).
We are especially interested in the two types of
structures in the LTAG Treebank, predicate adjunc-
tion and predicate coordination. They are used to
encode dependency relations which are unavailable
in other approaches. On the other hand, these struc-
tures turn out to be a big problem for the general rep-
resentation of dependency relations, including ad-
junction and coordination. We will show that the
algorithm proposed here provides a nice solution for
this problem.
499
has
says now
he
attach
attach
packagesunion
adjoin
attach
attach
Figure 5: Predicate Adjunction
4.1 Representation of the LTAG Treebank
In the LTAG Treebank (Shen et al., 2008), each word
is associated with a spinal template, which repre-
sents the projection from the lexical item to the root.
Templates are linked together to form a derivation
tree. The topology of the derivation tree shows a
type of dependency relation, which we call LTAG
dependency here.
There are three types of operations in the LTAG
Treebank, which are attachment, adjunction, and co-
ordination. Attachment is used to represent both
substitution and sister adjunction in the traditional
LTAG. So it is similar to the dependency relation in
other approaches.
The LTAG dependency can be a non-projective
relation thanks to the operation of adjunction. In
the LTAG Treebank, raising verbs and passive ECM
verbs are represented as auxiliary trees to be ad-
joined. In addition, adjunction is used to handle
many cases of discontinuous arguments in Prop-
bank. For example, in the following sentence,
ARG1 of says in Propbank is discontinuous, which
is First Union now has packages for seven customer
groups.
• First Union, he says, now has packages for
seven customer groups.
In the LTAG Treebank, the subtree for he says ad-
joins onto the node of has, which is the root of the
derivation tree, as shown in Figure 5.
Another special aspect of the LTAG Treebank is
the representation of predicate coordination. Figure
6 is the representation of the following sentence.
• I couldn’t resist rearing up on my soggy loafers
and saluting.
The coordination between rearing and saluting is
represented explicitly with a coord-structure, and
resist
rearing saluting
and
I
attach
attach attach
coordination
Figure 6: Predicate Coordination
continuedstock
pounded
amid
attach
adjoin
attach
Figure 7: Non-projective Adjunction
this coord-structure attaches to resist. It is shown
in (Shen et al., 2008) that coord-structures could en-
code the ambiguity of argument sharing, which can
be non-projective also.
4.2 Incremental Construction
We build LTAG derivation trees incrementally. A
hypothesis of a fragment is represented with a par-
tial derivation tree. When the fragment hypotheses
of two nearby fragments combine, the partial deriva-
tion trees are combined into one.
It is trivial to combine two partial derivation trees
with attachment. We simply attach the root of one
tree to some node on the other tree which is visible to
this root node. Adjunction is similar to attachment,
except that an adjoined subtree may be visible from
the other side of the derivation tree. For example, in
sentence
• The stock of UAL Corp. continued to be
pounded amid signs that British Airways ...
continued adjoins onto pounded, and amid attaches
to continued from the other side of the derivation
tree (pounded is between continued and amid), as
shown in Figure 7.
The predicate coordination is decomposed into a
set of operations to meet the need for incremen-
tal processing. Suppose a coordinated structure at-
taches to the parent node on the left side. We build
this structure incrementally by attaching the first
500
resist
rearing saluting
and
I
attach
attach
attach
conjoin
Figure 8: Conjunction
?
$
?
+
  	
@@R
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.fl
QQs
@@R @@R
m1.1
m1.1.1
s1 s2
m1
m1.2
m
mr
m2 s
attach
Figure 9: Representation of nodes
conjunct to the parent and conjoining other con-
juncts to first one. In this way, we do not need to
force the coordination to be built before the attach-
ment. Either can be executed first. A sample is
shown in Figure 8.
4.3 Features
In this section, we will describe the features used in
LTAG dependency parsing. An operation is repre-
sented by a 4-tuple
• op = (type, dir, posleft, posright),
where type ? {attach, adjoin, conjoin} and dir
is used to represent the direction of the operation.
posleft and posright are the POS tags of the two
operands.
Features are defined on POS tags and lexical items
of the nodes in the context. In order to represent the
features, we use m for the main-node of the oper-
ation, s for the sub-node, mr for the parent of the
main-node, m1..mi for the children of m, and s1..sj
for the children of s, as shown in Figure 9. The in-
dex always starts from the side where the operation
takes place. We use the Gorn addresses to represent
the nodes in the subtrees rooted on m and s.
Furthermore, we use lk and rk to represent the
nodes in the left and right context of the flat sen-
tence. We use hl and hr to represent the head of the
hypothesis trees on the left and right context respec-
tively. Let x be a node. We use x.p to represent the
POS tag of node x, and x.w to represent the lexical
item of node x.
Table 1 show the features used in LTAG depen-
dency parsing. There are seven classes of features.
The first three classes of features are those defined
on only one operand, on both operands, and on the
siblings respectively. If gold standard POS tags are
used as input, we define features on the POS tags in
the context. If level-1 dependency is used, we define
features on the root node of the hypothesis partial
derivation trees in the neighborhood.
Half check and full check features are designed
for grammatical check. For example, in Figure 9,
node s attaches onto node m from left. Then nothing
can attach onto s from the right side. The children of
the right side of s are fixed, so we use the half check
features to check the completeness of the children
of the right half for s. Furthermore, we notice that
all the rightmost descendants of s and the leftmost
descendants of m at each level become unavailable
for any further operation. So their children are fixed
after this operation. All these nodes are in the form
of m1.1...1 or s1.1...1. We use full check features to
check the children from both sides for these nodes.
In the discussion above, we ignored adjunction
and conjunction. We need to slightly refine the con-
ditions of checking. Due to the limit of space, we
skip these cases.
5 Experiments
We use the same data set as in (Shen and Joshi,
2005). We use Sec. 2-21 of the LTAG Treebank for
training, Sec. 22 for feature selection, and Sec. 23
for test. Table 2 shows the comparison of different
models. Beam size is set to five in our experiments.
With level-0 dependency, our system achieves an ac-
curacy of 90.3% at the speed of 4.25 sentences a sec-
ond on a Xeon 3G Hz processor with JDK 1.5. With
level-1 dependency, the parser achieves 90.5% at
3.59 sentences a second. Level-1 dependency does
not provide much improvement due to the fact that
level-0 features provide most of the useful informa-
tion for this specific application.
It is interesting to compare our system with other
dependency parsers. The accuracy on LTAG depen-
501
category description templates
one operand Features defined on only one operand. For each
template tp, [type, dir, tp] is used as a feature.
(m.p), (m.w), (m.p,m.w), (s.p),
(s.w), (s.p, s.w)
two operands Features defined on both operands. For each tem-
plate tp, [op, tp] is used as a feature. In addition,
[op] is also used as a feature.
(m.w), (s.w), (m.w, s.w)
siblings Features defined on the children of the
main nodes. For each template tp,
[op, tp], [op,m.w, tp], [op,mr.p, tp] and
[op,mr.p,m.w, tp] are used as features.
(m1.p), (m1.p,m2.p), ..,
(m1.p,m2.p, ..,mi.p)
POS context In the case that gold standard POS tags are used
as input, features are defined on the POS tags of
the context. For each template tp, [op, tp] is used
as a feature.
(l2.p), (l1.p), (r1.p), (r2.p),
(l2.p, l1.p), (l1.p, r1.p),
(r1.p, r2.p)
tree context In the case that level-1 dependency is employed,
features are defined on the trees in the context.
For each template tp, [op, tp] is used as a feature.
(hl.p), (hr.p)
half check Suppose s1, ..., sk are all the children of s which
are between s and m in the flat sentence. For
each template tp, [tp] is used as a feature.
(s.p, s1.p, s2.p, .., sk.p),
(m.p, s.p, s1.p, s2.p, .., sk.p)
and (s.w, s.p, s1.p, s2.p, .., sk.p),
(s.w,m.p, s.p, s1.p, s2.p, .., sk.p)
if s.w is a verb
full check Let x1, x2, .., xk be the children of x, and xr
the parent of x. For any x = m1.1...1 or s1.1...1,
template tp, [tp(x)] is used as a feature.
(x.p, x1.p, x2.p, .., xk.p),
(xr.p, x.p, x1.p, x2.p, .., xk.p) and
(x.w, x.p, x1.p, x2.p, .., xk.p),
(x.w, xr.p, x.p, x1.p, x2.p, .., xk.p)
if x.w is a verb
Table 1: Features defined on the context of operation
model accuracy%
Shen and Joshi, 2005 89.3
level-0 dependency 90.3
level-1 dependency 90.5
Table 2: Experiments on Sec. 23 of the LTAG Treebank
dency is comparable to the numbers of the previ-
ous best systems on dependency extracted from PTB
with Magerman’s rules, for example, 90.3% in (Ya-
mada and Matsumoto, 2003) and 90.9% in (McDon-
ald et al., 2005). However, their experiments are on
the PTB, while ours is on the LTAG corpus.
It should be noted that it is more difficult to learn
LTAG dependencies. Theoretically, the LTAG de-
pendencies reveal deeper relations. Adjunction can
lead to non-projective dependencies, and the depen-
dencies defined on predicate adjunction are linguis-
tically more motivated, as shown in the examples in
Figure 5 and 7. The explicit representation of predi-
cate coordination also provides deeper relations. For
example, in Figure 6, the LTAG dependency con-
tains resist ? rearing and resist ? saluting,
while the Magerman’s dependency only contains
resist ? rearing. The explicit representation of
predicate coordination will help to solve for the de-
pendencies for shared arguments.
6 Discussion
In our approach, each fragment in the graph is asso-
ciated with a hidden structure, which means that we
cannot reduce it to a labelling task. Therefore, the
problem of interest to us is different from previous
502
work on graphical models, such as CRF (Lafferty et
al., 2001) and MMMN (Taskar et al., 2003).
McAllester et al. (2004) introduced Case-Factor
Diagram (CFD) to transform a graph based con-
struction problem to a labeling problem. However,
adjunction, prediction coordination, and long dis-
tance dependencies in LTAG dependency parsing
make it difficult to implement. Our approach pro-
vides a novel alternative to CFD.
Our learning algorithm stems from Perceptron
training in (Collins, 2002). Variants of this method
have been successfully used in many NLP tasks, like
shallow processing (Daume´ III and Marcu, 2005),
parsing (Collins and Roark, 2004; Shen and Joshi,
2005) and word alignment (Moore, 2005). Theoret-
ical justification for those algorithms can be applied
to our training algorithm in a similar way.
In our algorithm, dependency is defined on com-
plicated hidden structures instead of on a graph.
Thus long distance dependency in a graph becomes
local in hidden structures, which is desirable from
linguistic considerations.
The search strategy of our bidirectional depen-
dency parser is similar to that of the bidirectional
CFG parser in (Satta and Stock, 1994; Ageno and
Rodrguez, 2001; Kay, 1989). A unique contribu-
tion of this paper is that selection of path and deci-
sions about action are trained simultaneously with
discriminative learning. In this way, we can employ
context information more effectively.
7 Conclusion
In this paper, we introduced bidirectional incremen-
tal parsing, a new architecture of parsing. We pro-
posed a novel algorithm for graph-based incremen-
tal construction, and applied this algorithm to LTAG
dependency parsing, revealing deep relations, which
are unavailable in other approaches and difficult to
learn. We evaluated the parser on an LTAG Tree-
bank. Experimental results showed significant im-
provement over the previous best system. Incre-
mental construction can be applied to other structure
learning problems of high computational complex-
ity, for example, such as machine translation and se-
mantic parsing.
References
A. Ageno and H. Rodrguez. 2001. Probabilistic mod-
elling of island-driven parsing. In International Work-
shop on Parsing Technologies.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL).
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference of Empirical Methods in Natural Lan-
guage Processing.
H. Daume´ III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the 22nd In-
ternational Conference on Machine Learning.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
A. K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
– 124. Springer-Verlag.
M. Kay. 1989. Head-driven parsing. In Proceedings of
Workshop on Parsing Technologies.
W. Krauth and M. Me´zard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745–752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmen-
tation and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
D. McAllester, M. Collins, and F. Pereira. 2004. Case-
factor diagrams for structured probabilistic modeling.
In UAI 2004.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL).
503
R. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
G. Satta and O. Stock. 1994. Bi-Directional Context-
Free Grammar Parsing for Natural Language Process-
ing. Artificial Intelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Pars-
ing. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing.
L. Shen, G. Satta, and A. K. Joshi. 2007. Guided Learn-
ing for Bidirectional Sequence Classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL).
L. Shen, L. Champollion, and A. K. Joshi. 2008. LTAG-
spinal and the Treebank: a new resource for incremen-
tal, dependency and semantic parsing. Language Re-
sources and Evaluation, 42(1):1–19.
L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis,
University of Pennsylvania.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Proceedings of the 17th Annual
Conference Neural Information Processing Systems.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with Support Vector Machines. In
IWPT 2003.
504
