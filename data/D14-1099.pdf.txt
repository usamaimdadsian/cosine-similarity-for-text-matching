Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927,
October 25-29, 2014, Doha, Qatar.
c
©2014 Association for Computational Linguistics
A Polynomial-Time Dynamic Oracle
for Non-Projective Dependency Parsing
Carlos G
´
omez-Rodr´?guez
Departamento de
Computaci´on
Universidade da Coru˜na, Spain
cgomezr@udc.es
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
The introduction of dynamic oracles has
considerably improved the accuracy of
greedy transition-based dependency pars-
ers, without sacrificing parsing efficiency.
However, this enhancement is limited to
projective parsing, and dynamic oracles
have not yet been implemented for pars-
ers supporting non-projectivity. In this
paper we introduce the first such oracle,
for a non-projective parser based on At-
tardi’s parser. We show that training with
this oracle improves parsing accuracy over
a conventional (static) oracle on a wide
range of datasets.
1 Introduction
Greedy transition-based parsers for dependency
grammars have been pioneered by Yamada and
Matsumoto (2003) and Nivre (2003). These meth-
ods incrementally process the input sentence from
left to right, predicting the next parsing action,
called transition, on the basis of a compact rep-
resentation of the derivation history.
Greedy transition-based parsers can be very
efficient, allowing web-scale parsing with high
throughput. However, the accuracy of these meth-
ods still falls behind that of transition-based pars-
ers using beam-search, where the accuracy im-
provement is obtained at the cost of a decrease
in parsing efficiency; see for instance Zhang and
Nivre (2011), Huang and Sagae (2010), Choi and
McCallum (2013). As an alternative to beam-
search, recent research on transition-based parsing
has therefore explored possible ways of improving
accuracy at no extra cost in parsing efficiency.
The training of transition-based parsers relies
on a component called the parsing oracle, which
maps parser configurations to optimal transitions
with respect to a gold tree. A discriminative model
is then trained to simulate the oracle’s behavior,
and is later used for decoding. Traditionally, so-
called static oracles have been exploited in train-
ing, where a static oracle is defined only for con-
figurations that have been reached by computa-
tions with no mistake, and it returns a single ca-
nonical transition among those that are optimal.
Very recently, Goldberg and Nivre (2012),
Goldberg and Nivre (2013) and Goldberg et al.
(2014) showed that the accuracy of transition-
based parsers can be substantially improved using
dynamic oracles. A dynamic oracle returns the
set of all transitions that are optimal for a given
configuration, with respect to the gold tree, and
is well-defined and correct for every configuration
that is reachable by the parser.
Na¨?ve implementations of dynamic oracles run
in exponential time, since they need to simulate
all possible computations of the parser for the in-
put configuration. Polynomial-time implementa-
tions of dynamic oracles have been proposed by
the above mentioned authors for several project-
ive dependency parsers. To our knowledge, no
polynomial-time algorithm has been published for
transition-based parsers based on non-projective
dependency grammars.
In this paper we consider a restriction of a
transition-based, non-projective parser originally
presented by Attardi (2006). This restriction
was further investigated by Kuhlmann and Nivre
(2010) and Cohen et al. (2011). We provide an im-
plementation for a dynamic oracle for this parser
running in polynomial time.
We experimentally compare the parser trained
with the dynamic oracle to a baseline obtained
by training with a static oracle. Significant ac-
curacy improvements are achieved on many lan-
guages when using our dynamic oracle. To our
knowledge, these are the first experimental results
on non-projective parsing based on a dynamic or-
acle.
917
2 Preliminary Definitions
Transition-based dependency parsing was origin-
ally introduced by Yamada and Matsumoto (2003)
and Nivre (2003). In this section we briefly sum-
marize the notation we use for this framework and
introduce the notion of dynamic oracle.
2.1 Transition-Based Dependency Parsing
We represent an input sentence as a string w =
w
0
· · ·w
n
, n ? 1, where each w
i
with i 6= 0 is a
lexical symbol and w
0
is a special symbol called
root. Set V
w
= {i | 0 ? i ? n} denotes the sym-
bol occurrences in w. For i, j ? V
w
with i 6= j,
we write i ? j to denote a grammatical depend-
ency of some unspecified type betweenw
i
andw
j
,
where w
i
is the head and w
j
is the dependent.
A dependency tree t for w is a directed tree
with node set V
w
and with root node 0. An arc of t
is a pair (i, j), encoding a dependency i ? j; we
will often use the latter notation to denote arcs.
A transition-based dependency parser typically
uses a stack data structure to process the input
string from left to right, in a way very similar
to the classical push-down automaton for context-
free languages (Hopcroft et al., 2006). Each stack
element is a node from V
w
, representing the root
of a dependency tree spanning some portion of the
input w, and no internal state is used. At each step
the parser applies some transition that updates the
stack and/or consumes one symbol from the input.
Transitions may also construct new dependencies,
which are added to the current configuration of the
parser.
We represent the stack as an ordered sequence
? = [h
d
, . . . , h
1
], d ? 0, of nodes h
i
? V
w
, with
the topmost element placed at the right. When
d = 0, we have the empty stack ? = []. We use
the vertical bar to denote the append operator for
?, and write ? = ?
?
|h
1
to indicate that h
1
is the
topmost element of ?.
The portion of the input string still to be pro-
cessed by the parser is called the buffer. We
represent the buffer as an ordered sequence ? =
[i, . . . , n] of nodes from V
w
, with i the first ele-
ment of the buffer. We denote the empty buffer
as ? = []. Again, we use the vertical bar to de-
note the append operator, and write ? = i|?
?
to
indicate that i is the first symbol occurrence of ?;
consequently, we have ?
?
= [i+ 1, . . . , n].
In a transition-based parser, the parsing pro-
cess is defined through the technical notions of
configuration and transition. A configuration of
the parser relative to w is a triple c = (?, ?,A),
where ? and ? are a stack and a buffer, respect-
ively, and A is the set of arcs that have been built
so far. A transition is a partial function map-
ping the set of parser configurations into itself.
Each transition-based parser is defined by means
of some finite inventory of transitions. We will
later introduce the specific inventory of transitions
for the parser that we investigate in this paper.
We use the symbol ` to denote the binary relation
formed by the union of all transitions of a parser.
With the notions of configuration and transition
in place, we can define a computation of the
parser on w as a sequence c
0
, c
1
, . . . , c
m
, m ? 0,
of configurations relative tow, under the condition
that c
i?1
` c
i
for each i with 1 ? i ? m. We use
the reflexive and transitive closure of `, written
`
?
, to represent computations.
2.2 Configuration Loss and Dynamic Oracles
A transition-based dependency parser is a non-
deterministic device, meaning that a given con-
figuration can be mapped into several configur-
ations by the available transitions. However, in
several implementations the parser is associated
with a discriminative model that, on the basis of
some features of the current configuration, always
chooses a single transition. In other words, the
model is used to run the parser as a pseudo-de-
terministic device. The training of the discriminat-
ive model relies on a component called the parsing
oracle, which maps parser configurations to “op-
timal” transitions with respect to some reference
dependency tree, which we call the gold tree.
Traditionally, so-called static oracles have been
used which return a single, canonical transition
and they do so only for configurations that can
reach the gold tree, that is, configurations repres-
enting parsing histories with no mistake. In re-
cent work, Goldberg and Nivre (2012), Goldberg
and Nivre (2013) and Goldberg et al. (2014) have
introduced dynamic oracles, which return the set
of all transitions that are optimal with respect to
a gold tree, and are well-defined and correct for
every configuration that is reachable by the parser.
These authors have shown that the accuracy of
transition-based dependency parsers can be sub-
stantially improved if dynamic oracles are used in
place of static ones. In what follows, we provide
a mathematical definition of dynamic oracles, fol-
lowing Goldberg et al. (2014).
918
(?, k|?,A) `
sh
(?|k, ?,A)
(?|i|j, ?,A) `
la
(?|j, ?,A ? {j ? i})
(?|i|j, ?,A) `
ra
(?|i, ?, A ? {i? j})
(?|i|j|k, ?,A) `
la
2
(?|j|k, ?,A ? {k ? i})
(?|i|j|k, ?,A) `
ra
2
(?|i|j, ?,A ? {i? k})
Figure 1: Transitions of the non-projective parser.
Let t
1
and t
2
be dependency trees for w, with
arc sets A
1
and A
2
, respectively. The loss of t
1
with respect to t
2
is defined as
L(t
1
, t
2
) = |A
1
\A
2
| . (1)
Note that L(t
1
, t
2
) = L(t
2
, t
1
), since |A
1
| = |A
2
|.
Furthermore L(t
1
, t
2
) = 0 if and only if t
1
and t
2
are the same tree.
Let c be a configuration of a transition-based
parser relative to w. Let also D(c) be the set of all
dependency trees that can be obtained in a com-
putation of the form c `
?
c
f
, where c
f
is a final
configuration, that is, a configuration that has con-
structed a dependency tree for w. We extend the
loss function in (1) to configurations by letting
L(c, t
2
) = min
t
1
?D(c)
L(t
1
, t
2
) . (2)
Let t
G
be the gold tree for w. Quantity L(c, t
G
)
can be used to define a dynamic oracle as follows.
For any transition `
?
in the finite inventory of our
parser, we use the functional notation ?(c) = c
?
in
place of c `
?
c
?
. We then let
oracle(c, t
G
) =
{? | L(?(c), t
G
)? L(c, t
G
) = 0} . (3)
In words, (3) provides the set of transitions that do
not increase the loss of c; we call these transitions
optimal for c.
A na¨?ve way of implementing (3) would be
to explicitly compute the set D(c) in (2), which
has exponential size. More interestingly, the im-
plementation of dynamic oracles proposed by the
above cited authors all run in polynomial time.
These oracles are all defined for projective pars-
ing. In this paper, we present a polynomial-time
oracle for a non-projective parser.
3 Non-Projective Dependency Parsing
In this section we introduce a parser for non-
projective dependency grammars that is derived
from the transition-based parser originally presen-
ted by Attardi (2006), and was further investigated
by Kuhlmann and Nivre (2010) and Cohen et al.
(2011). Our definitions follow the framework in-
troduced in Section 2.1.
We start with some additional notation. Let t be
a dependency tree for w and let k be a node of t.
Consider the complete subtree t
?
of t rooted at k,
that is, the subtree of t induced by k and all of the
descendants of k in t. The span of t
?
is the sub-
sequence of tokens in w represented by the nodes
of t
?
. Node k has gap-degree 0 if the span of t
?
forms a (contiguous) substring of w. A depend-
ency tree is called projective if all of its nodes
have gap-degree 0; a dependency tree which is not
projective is called non-projective.
Given w as input, the parser starts with the ini-
tial configuration ([], [0, . . . , n], ?), consisting of
an empty stack, a buffer with all the nodes repres-
enting the symbol occurrences in w, and an empty
set of constructed dependencies (arcs). The parser
stops when it reaches a final configuration of the
form ([0], [], A), consisting of a stack with only the
root node and of an empty buffer; in any such con-
figuration, set A always implicitly defines a valid
dependency tree (rooted in node 0).
The core of the parser consists of an invent-
ory of five transitions, defined in Figure 1. Each
transition is specified using the free variables ?,
?, A, i, j and k. As an example, the schema
(?|i|j, ?,A) `
la
(?|j, ?,A? {j ? i}) means that
if a configuration c matches the antecedent, then a
new configuration is obtained by instantiating the
variables in the consequent accordingly.
The transition `
sh
, called shift, reads a new
token from the input sentence by removing it from
the buffer and pushing it into the stack. Each
of the other transitions, collectively called reduce
transitions, has the effect of building a dependency
between two nodes in the stack, and then removing
the dependent node from the stack. The removal
of the dependent ensures that the output depend-
ency tree is built in a bottom-up order, collecting
all of the dependents of each node i before linking
i to its head.
The transition `
la
, called left-arc, creates a left-
ward arc where the topmost stack node is the
head and the second topmost node is the depend-
ent, and removes the latter from the stack. The
transition `
ra
, called right-arc, is defined sym-
metrically, so that the topmost stack node is at-
919
? ? ?
h h h
h
1 1 2
3minimum
stack length
at c     c1
c c c0 1
stack length
...
...
m
m
Figure 2: General form of the computations asso-
ciated with an item [h
1
, h
2
, h
3
].
tached as a dependent of the second topmost node.
The combination of the shift, left-arc and right-
arc transitions provides complete coverage of pro-
jective dependency trees, but no support for non-
projectivity, and corresponds to the so-called arc-
standard parser introduced by Nivre (2004).
Support for non-projective dependencies is
achieved by adding the transitions `
la
2
and `
ra
2
,
which are variants of the left-arc and right-arc
transitions, respectively. These new transitions
create dependencies involving the first and the
third topmost nodes in the stack. The creation of
dependencies between non-adjacent stack nodes
might produce crossing arcs and is the key to the
construction of non-projective trees.
Recall that transitions are partial functions,
meaning that they might be undefined for some
configurations. Specifically, the shift transition is
only defined for configurations with a non-empty
buffer. Similarly, the left-arc and right-arc trans-
itions can only be applied if the length of the stack
is at least 2, while the transitions `
la
2
and `
ra
2
re-
quire at least 3 nodes in the stack.
Transitions `
la
2
and `
ra
2
were originally intro-
duced by Attardi (2006) together with other, more
complex transitions. The parser we define here
is therefore more restrictive than Attardi (2006),
meaning that it does not cover all the non-pro-
jective trees that can be processed by the ori-
ginal parser. However, the restricted parser has re-
cently attracted some research interest, as it covers
the vast majority of non-projective constructions
appearing in standard treebanks (Attardi, 2006;
Kuhlmann and Nivre, 2010), while keeping sim-
plicity and interesting properties like being com-
patible with polynomial-time dynamic program-
ming (Cohen et al., 2011).
4 Representation of Computations
Our oracle algorithm exploits a dynamic program-
ming technique which, given an input string, com-
bines certain pieces of a computation of the parser
from Section 3 to obtain larger pieces. In order
to efficiently encode pieces of computations, we
borrow a representation proposed by Cohen et al.
(2011), which is introduced in this section.
Let w = a
0
· · · a
n
and V
w
be specified as in
Section 2, and let w
?
be some substring of w. (The
specification of w
?
is not of our concern in this
section.) Let also h
1
, h
2
, h
3
? V
w
. We are inter-
ested in computations of the parser processing the
substring w
?
and having the form c
0
, c
1
, . . . , c
m
,
m ? 1, that satisfy both of the following condi-
tions, exemplified in Figure 2.
• For some sequence of nodes ? with |?| ? 0,
the stack associated with c
0
has the form ?|h
1
and the stack associated with c
m
has the form
?|h
2
|h
3
.
• For each intermediate configuration c
i
, 1 ?
i ? m ? 1, the stack associated with c
i
has
the form ??
i
, where ?
i
is a sequence of nodes
with |?
i
| ? 2.
An important property of the above definition
needs to be discussed here, which is at the heart of
the polynomial-time algorithm in the next section.
If in c
0
, c
1
, . . . , c
m
we replace ? with a different
sequence ?
?
, we obtain a valid computation for w
?
constructing exactly the same dependencies as the
original computation. To see this, let c
i?1
`
?
i
c
i
for each i with 1 ? i ? m. Then `
?
1
must be a
shift, otherwise |?
1
| ? 2 would be violated. Con-
sider now a transition `
?
i
with 2 ? i ? m that
builds some dependency. From |?
i
| ? 2 we derive
|?
i?1
| ? 3. We can easily check from Figure 1
that none of the nodes in ? can be involved in the
constructed dependency.
Intuitively, the above property asserts that the
sequence of transitions `
?
1
,`
?
2
, . . . ,`
?
m
can be
applied to parse substring w
?
independently of the
context ?. This suggests that we can group into
an equivalence class all the computations satisfy-
ing the conditions above, for different values of
?. We indicate such class by means of the tuple
[h
1
, h
2
h
3
], called item. It is easy to see that each
item represents an exponential number of compu-
tations. In the next section we will show how we
can process items with the purpose of obtaining an
efficient computation for dynamic oracles.
920
5 Dynamic Oracle Algorithm
Our algorithm takes as input a gold tree t
G
for
string w and a parser configuration c = (?, ?,A)
relative to w, specified as in Section 2. We assume
that t
G
can be parsed by the non-projective parser
of Section 3 starting from the initial configuration.
5.1 Basic Idea
The algorithm consists of two separate stages, in-
formally discussed in what follows. In the first
stage we identify some tree fragments of t
G
that
can be constructed by the parser after reaching
configuration c, in a way that does not depend on
the content of ?. This means that these fragments
can be precomputed by looking only into ?. Fur-
thermore, since these fragments are subtrees of t
G
,
their computation has no effect on the overall loss
of a computation on w.
For each fragment t with the above properties,
we replace all the nodes in ? that are also nodes
of t with the root node of t itself. The result of the
first stage is therefore a new node sequence shorter
than ?, which we call the reduced buffer ?
R
.
In the second stage of the algorithm we use a
variant of the tabular method developed by Co-
hen et al. (2011), which was originally designed
to simulate all computations of the parser in Sec-
tion 3 on an input string w. We run the above
method on the concatenation of the stack and the
reduced buffer, with some additional constraints
that restrict the search space in two respects. First,
we visit only those computations of the parser
that step through configuration c. Second, we
reach only those dependency trees that contain all
the tree fragments precomputed in the first stage.
We can show that such search space always con-
tains at least one dependency tree with the desired
loss, which we then retrieve performing a Viterbi
search.
5.2 Preprocessing of the Buffer
Let t be a complete subtree of t
G
, having root
node k in ?. Consider the following two condi-
tions, defined on t.
• Bottom-up completeness: No arc i ? j in t
is such that i is a node in ?, i 6= k, and j is a
node in ?.
• Zero gap-degree: The nodes of t that are in ?
form a (contiguous) substring of w.
We claim that if t satisfies the above conditions,
then we can safely reduce the nodes of t appearing
in ?, replacing them with node k. We only report
here an informal discussion of this claim, and omit
a formal proof.
As a first remark, recall that our parser imple-
ments a purely bottom-up strategy. This means
that after a tree has been constructed, all of its
nodes but the root are removed from the parser
configuration. Then the Bottom-up completeness
condition guarantees that if we remove from ? all
nodes of t but k, the nodes of t that are in ? can still
be processed in a way that does not affect the loss,
since their parent must be either k or a node that is
neither in ? nor in ?. Note that the nodes of t that
are neither in ? nor in ? are irrelevant to the pre-
computation of t from ?, since these nodes have
already been attached and are no longer available
to the parser.
As a second remark, the Zero gap-degree con-
dition guarantees that the span of t over the nodes
of ? is not interleaved by nodes that do not belong
to t. This is also an important requirement for the
precomputation of t from ?, since a tree fragment
having a discontinuous span over ? might not be
constructable independently of ?. More specific-
ally, parsing such fragment implies dealing with
the nodes in the discontinuities, and this might re-
quire transitions involving nodes from ?.
We can now use the sufficient condition above
to compute ?
R
. We process ? from left to right.
For each node k, we can easily test the Bottom-up
completeness condition and the Zero gap-degree
condition for the complete subtree t of t
G
rooted
at k, and perform the reduction if both conditions
are satisfied. Note that in this process a node k
resulting from the reduction of t might in turn be
removed from ? if, at some later point, we reduce
a supertree of t.
5.3 Computation of the Loss
We describe here our dynamic programming al-
gorithm for the computation of the loss of an in-
put configuration c. We start with some additional
notation. Let ? = ??
R
be the concatenation of ?
and ?
R
, which we treat as a string of nodes. For
integers i with 0 ? i ? |?| ? 1, we write ?[i] to
denote the (i + 1)-th node of ?. Let also ` = |?|.
Symbol ` is used to mark the boundary between
the stack and the reduced buffer in ?, thus ?[i] with
i < ` is a node of ?, while ?[i] with i ? ` is a node
of ?
R
.
Algorithm 1 computes the loss of c by pro-
cessing the sequence ? in a way quite similar to the
921
standard nested loop implementation of the CKY
parser for context-free grammars (Hopcroft et al.,
2006). The algorithm uses a two-dimensional ar-
ray T whose indexes range from 0 to |?| = ` +
|?
R
|, and only the cells T [i, j] with i < j are
filled.
We view each T [i, j] as an association list
whose keys are items [h
1
, h
2
h
3
], defined in the
context of the substring ?[i] · · · ?[j ? 1] of ?; see
Section 4. The value stored at T [i, j]([h
1
, h
2
h
3
])
is the minimum loss contribution due to the com-
putations represented by [h
1
, h
2
h
3
]. For technical
reasons, we assume that our parser starts with a
symbol $ 6? V
w
in the stack, denoting the bottom
of the stack.
We initialize the table by populating the cells
of the form T [i, i + 1] with information about
the trivial computations consisting of a single `
sh
transition that shifts the node ?[i] into the stack.
These computations are known to have zero loss
contribution, because a `
sh
transition does not cre-
ate any arcs. In the case where the node ?[i] be-
longs to ?, i.e., i < `, we assign loss contribution
0 to the entry T [i, i + 1]([?[i? 1], ?[i? 1]?[i]])
(line 3 of Algorithm 1), because ?[i] is shifted with
?[i? 1] at the top of the stack. On the other hand,
if ?[i] is in ?, i.e., i ? `, we assign loss contri-
bution 0 to several entries in T [i, i + 1] (line 6)
because, at the time ?[i] is shifted, the content of
the stack depends on the transitions executed be-
fore that point.
After the above initialization, we consider
pairs of contiguous substrings ?[i] · · · ?[k ? 1] and
?[k] · · · ?[j ? 1] of ?. At each inner iteration
of the nested loops of lines 7-11 we update cell
T [i, j] based on the content of the cells T [i, k] and
T [k, j]. We do this through the procedure PRO-
CESSCELL(T , i, k, j), which considers all pairs
of keys [h
1
, h
2
h
3
] in T [i, k] and [h
3
, h
4
h
5
] in
T [k, j]. Note that we require the index h
3
to match
between both items, meaning that their computa-
tions can be concatenated. In this way, for each
reduce transition ? in our parser, we compute the
loss contribution for a new piece of computation
defined by concatenating a computation with min-
imum loss contribution in the first item and a com-
putation with minimum loss contribution in the
second item, followed by the transition ? . The fact
that the new piece of computation can be repres-
ented by an item is exemplified in Figure 3 for the
case ? = `
ra
2
.
?
h1
c0
?
h
h
2
3
c
?
h
h
2
3
c
?
h
h
2
c
h5
4
?
h
h
4
5
c +1
la  : create arc
h     h  and remove
h   from stack
2
5 2
2
[h , h h ]1 2 3 la 2?
?
h1
c0
?
h
h
4
5
c +1
... ...
+ +[h , h h ]3 4 5
[h , h h ]1 4 5
...
?
m m r r
r
Figure 3: Concatenation of two computa-
tions/items and transition `
ra
2
, resulting in a new
computation/item.
The computed loss contribution is used to up-
date the entry in T [i, j] corresponding to the item
associated with the new computation. Observe
how the loss contribution provided by the arc cre-
ated by ? is computed by the ?
G
function at lines
17, 20, 23 and 26, which is defined as:
?
G
(i? j) =
{
0, if i? j is in t
G
;
1, otherwise.
(4)
We remark that the nature of our problem al-
lows us to apply several shortcuts and optimiza-
tions that would not be possible in a setting where
we actually needed to parse the string ?. First, the
range of variable i in the loop in line 8 starts at
max{0, `?d}, rather than at 0, because we do not
need to combine pairs of items originating from
nodes in ? below the topmost node, as the items
resulting from such combinations correspond to
computations that do not contain our input config-
uration c. Second, when we have set values for i
such that i+2 < `, we can omit calling PROCESS-
CELL for values of the parameter k ranging from
i+2 to `?1, as those calls would use as their input
one of the items described above, which are not of
interest. Finally, when processing substrings that
are entirely in ?
R
(i ? `) we can restrict the trans-
itions that we explore to those that generate arcs
that either are in the gold tree t
G
, or have a parent
node which is not present in ? (see conditions in
922
Algorithm 1 Computation of the loss function
1: T [0, 1]([$, $0])? 0 . shift node 0 on top of empty stack symbol $
2: for i? 1 to `? 1 do
3: T [i, i+ 1]([?[i? 1], ?[i? 1]?[i]])? 0 . shift node ?[i] with ?[i? 1] on top of the stack
4: for i? ` to |?| do
5: for h? 0 to i? 1 do
6: T [i, i+ 1]([?[h], ?[h]?[i]])? 0 . shift node ?[i] with ?[h] on top of the stack
7: for d? 2 to |?| do . consider substrings of length d
8: for i? max{0, `? d} to |?| ? d do . i = beginning of substring
9: j ? i+ d . j ? 1 = end of substring
10: PROCESSCELL(T , i, i+ 1, j) . We omit the range k = i+ 2 to max{i+ 2, `} ? 1
11: for k ? max{i+ 2, `} to j do . factorization of substring at k
12: PROCESSCELL(T , i, k, j)
13: return T [0, |?|]([$, $0]) +
?
i?[0,`?1]
L
c
(?[i], t
G
)
14: procedure PROCESSCELL(T , i, k, j)
15: for each key [h
1
, h
2
h
3
]) defined in T [i, k] do
16: for each key [h
3
, h
4
h
5
]) defined in T [k, j] do . h
3
must match between the two entries
17: loss
la
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
4
)
18: if (i < `) ? ?
G
(h
5
? h
4
) = 0 ? (h
5
6? ?) then
19: T [i, j]([h
1
, h
2
h
5
])? min{loss
la
, T [i, j]([h
1
, h
2
h
5
])} . cell update `
la
20: loss
ra
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
4
? h
5
)
21: if (i < `) ? ?
G
(h
4
? h
5
) = 0 ? (h
4
6? ?) then
22: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
23: loss
la
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
2
)
24: if (i < `) ? ?
G
(h
5
? h
2
) = 0 ? (h
5
6? ?) then
25: T [i, j]([h
1
, h
4
h
5
])? min{loss
la
2
, T [i, j]([h
1
, h
4
h
5
])} . cell update `
la
2
26: loss
ra
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
2
? h
5
)
27: if (i < `) ? ?
G
(h
2
? h
5
) = 0 ? (h
2
6? ?) then
28: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
2
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
2
lines 18, 21, 24, 27), because we know that incor-
rectly attaching a buffer node as a dependent of an-
other buffer node, when the correct head is avail-
able, can never be an optimal decision in terms of
loss.
Once we have filled the table T , the loss for
the input configuration c can be obtained from the
value of the entry T [0, |?|]([$, $0]), representing
the minimum loss contribution among computa-
tions that reach the input configuration c and parse
the whole input string. To obtain the total loss,
we add to this value the loss contribution accu-
mulated by the dependency trees with root in the
stack ? of c. This is represented in Algorithm 1 as
?
i?[0,`?1]
L
c
(?[i], t
G
), where L
c
(?[i], t
G
) is the
count of the descendants of ?[i] (the (i+1)-th ele-
ment of ?) that had been assigned the wrong head
by the parser with respect to t
G
.
5.4 Sample Run
Consider the Czech sentence and the gold depend-
ency tree t
G
shown in Figure 4(a). Given the con-
figuration c = (?, ?,A) where ? = [0, 1, 3, 4],
? = [5, . . . , 13] and A = {3 ? 2}, we trace the
two stages of the algorithm.
Preprocessing of the buffer The complete sub-
tree rooted at node 7 satisfies the Bottom-up com-
pleteness and the Zero gap-degree conditions in
Section 5.2, so the nodes 5, . . . , 12 in ? can be
replaced with the root 7. Note that all the nodes in
the span 5, . . . , 12 have all their (gold) dependents
in that span, with the exception of the root 7, with
its dependent node 1 still in the stack. No other
reduction is possible, and we have ?
R
= [7, 13].
The corresponding fragment of t
G
is represented
in Figure 4(b).
Computation of the loss Let ? = ??
R
. Al-
gorithm 1 builds the two-dimensional array T in
Figure 4(c). Each cell T [i, j] contains an asso-
ciation list, whose (key:value) pairs map items to
their loss contribution. Figure 4(c) only shows the
pairs involved in the minimum-loss computation.
Lines 1-6 of Algorithm 1 initialize the cells in
the diagonal, T [0, 1], . . . , T [5, 6]. The boundary
between stack and buffer is ` = 4, thus cells
T [0, 1], T [1, 2], and T [2, 3] contain only one ele-
ment, while T [3, 4], T [4, 5] and T [5, 6] contain as
many as the previous elements in ?, although not
all of them are shown in the figure.
Lines 7-12 fill the superdiagonals until T [0, 6]
is reached. The cells T [0, 2], T [0, 3] and T [1, 3]
923
-Root- V be?z?ne´m provozu vs?ak telefonn´? linky nermaj´? takivou kvalitu jako v laborator?i .
0 1 2 3 4 5 6 7 8 9 10 11 12 13
(a) Non-projective dependency tree from the Prague Dependency Treebank.
-Root- V provozu vs?ak nermaj´? .
0 1 3 4 7 13
? ?R
(b) Fragment of dependency tree in (a) after buffer
reduction.
i j 1 2 3 4 5 6
0 [$,$ 0]:0 ? ? . . . [$,$ 0]:1 [$,$ 0]:1
1 [0,0 1]:0 ? . . . [0,0 4]:1 . . .
2 [1,1 3]:0 [1,1 4]:1 [1,4 7]:1 . . .
3 [3,3 4]:0 [3,4 7]:1 . . .
4 [4,4 7]:0 . . .
5 [0,0 13]:0
(c) Relevant portion of T computed by Algorithm 1, with the
loss of c in the yellow entry.
Figure 4: Example of loss computation given the sentence in (a) and considering a configuration c with
? = [0, 1, 3, 4] and ? = [5, . . . , 13].
are left empty because ` = 4. Once T [0, 6]
is calculated, it contains only the entry with key
[$, $, 0], with the associated value 1 representing
the minimum number of wrong arcs that the pars-
ing algorithm has to build to reach a final con-
figuration from c. Then, Line 13 retrieves the
loss of the configuration, computed as the sum of
T [0, 6]([$, $, 0]) with the termL
c
, representing the
erroneous arcs made before reaching c.
Note that in our example the loss of c is 1, even
though L
c
= 0, meaning that there are no wrong
arcs in A. Indeed, given c, there is no single com-
putation that builds all the remaining arcs in t
G
.
This is reflected in T , where the path to reach the
item with minimum loss has to go through either
T [3, 5] or T [2, 4], which implies building the erro-
neous arc (w
7
? w
3
) or (w
4
? w
3
), respectively.
6 Computational Analysis
The first stage of our algorithm can be easily im-
plemented in time O(|?| |t
G
|), where |t
G
| is the
number of nodes in t
G
, which is equal to the length
n of the input string.
For the worst-case complexity of the second
stage (Algorithm 1), note that the number
of cell updates made by calling PROCESS-
CELL(T , i, k, j) with k < ` is O(|?|
3
|?|
2
|?
R
|).
This is because these updates can only be caused
by procedure calls on line 10 (as those on line 12
always set k ? `) and therefore the index k always
equals i + 1, while h
2
must equal h
1
because the
item [h
1
, h
2
h
3
] is one of the initial items created
on line 3. The variables i, h
1
and h
3
must index
nodes on the stack ? as they are bounded by k,
while j ranges over ?
R
and h
4
and h
5
can refer to
nodes either on ? or on ?
R
.
On the other hand, the number of cell updates
triggered by calls to PROCESSCELL such that k ?
` is O(|?|
4
|?
R
|
4
), as they happen for four indices
referring to nodes of ?
R
(k, j, h
4
, h
5
) and four
indices that can range over ? or ?
R
(i, h
1
, h
2
, h
3
).
Putting everything together, we conclude that
the overall complexity of our algorithm is
O(|?| |t
G
|+ |?|
3
|?|
2
|?
R
|+ |?|
4
|?
R
|
4
).
In practice, quantities |?|, |?
R
| and |?| are signi-
ficantly smaller than n, providing reasonable train-
ing times as we will see in Section 7. For instance,
when measured on the Czech treebank, the aver-
age value of |?| is 7.2, with a maximum of 87.
Even more interesting, the average value of |?
R
|
is 2.6, with a maximum of 23. Comparing this to
the average and maximum values of |?|, 11 and
192, respectively, we see that the buffer reduction
is crucial in reducing training time.
Note that, when expressed as a function of n,
our dynamic oracle has a worst-case time com-
plexity of O(n
8
). This is also the time complexity
of the dynamic programming algorithm of Cohen
et al. (2011) we started with, simulating all com-
putations of our parser. In contrast, the dynamic
oracle of Goldberg et al. (2014) for the projective
case achieves a time complexity ofO(n
3
) from the
dynamic programming parser by Kuhlmann et al.
(2011) running in time O(n
5
).
924
The reason why we do not achieve any asymp-
totic improvement is that some helpful properties
that hold with projective trees are no longer satis-
fied in the non-projective case. In the projective
(arc-standard) case, subtrees that are in the buf-
fer can be completely reduced. As a consequence,
each oracle step always combines an inferred entry
in the table with either a node from the stack or a
node from the reduced buffer, asymptotically re-
ducing the time complexity. However, in the non-
projective (Attardi) case, subtrees in the buffer can
not always be completely reduced, for the reasons
mentioned in the second-to-last paragraph of Sec-
tion 5.2. As a consequence, the oracle needs to
make cell updates in a more general way, which
includes linking pairs of elements in the reduced
buffer or pairs of inferred entries in the table.
-Root- John was not as good for the job as Kate .
0 1 2 3 4 5 6 7 8 9 10 11
Figure 5: Non-projective dependency tree adapted
from the Penn Treebank.
An example of why this is needed is provided
by the gold tree in Figure 5. Assume a config-
uration c = (?, ?,A) where ? = [0, 1, 2, 3, 4],
? = [5, . . . , 11], and A = ?. It is easy to see that
the loss of c is greater than zero, since the gold tree
is not reachable from c: parsing the subtree rooted
at node 5 requires shifting 6 into the stack, and
this makes it impossible to build the arcs 2 ? 5
and 2? 6. However, if we reduced the subtree in
the buffer with root 5, we would incorrectly obtain
a loss of 0, as the resulting tree is parsable if we
start with `
sh
followed by `
la
and `
ra
2
. Note that
there is no way of knowing whether it is safe to
reduce the subtree rooted at 5 without using non-
local information. For example, the arc 2 ? 6 is
crucial here: if 6 depended on 5 or 4 instead, the
loss would be zero. These complications are not
found in the projective case, allowing for the men-
tioned asymptotic improvement.
7 Experimental Evaluation
For comparability with previous work on dynamic
oracles, we follow the experimental settings repor-
ted by Goldberg et al. (2014) for their arc-standard
dynamic oracle. In particular, we use the same
training algorithm, features, and root node posi-
tion. However, we train the model for 20 itera-
static dynamic
UAS LAS UAS LAS
Arabic 80.90 71.56 82.23 72.63
Basque 75.96 66.74 74.32 65.59
Catalan 90.55 85.20 89.94 84.96
Chinese 84.72 79.93 85.34 81.00
Czech 79.83 72.69 82.08 74.44
English 85.52 84.46 87.38 86.40
Greek 79.84 72.26 81.55 74.14
Hungarian 78.13 68.90 76.27 68.14
Italian 83.08 78.94 84.43 80.45
Turkish 79.57 69.44 79.41 70.32
Bulgarian 89.46 85.99 89.32 85.92
Danish 85.58 81.25 86.03 81.59
Dutch 79.05 75.69 80.13 77.22
German 88.34 86.48 88.86 86.94
Japanese 93.06 91.64 93.56 92.18
Portuguese 84.80 81.38 85.36 82.10
Slovene 76.33 68.43 78.20 70.22
Spanish 79.88 76.84 80.25 77.45
Swedish 87.26 82.77 87.24 82.49
PTB 89.55 87.18 90.47 88.18
Table 1: Unlabelled Attachment Score (UAS) and
Labelled Attachment Score (LAS) using a static
and a dynamic oracle. Evaluation on CoNLL 2007
(first block) and CoNLL 2006 (second block) data-
sets is carried out including punctuation, evalu-
ation on the Penn Treebank excludes it.
tions rather than 15, as the increased search space
and spurious ambiguity of Attardi’s non-project-
ive parser implies that more iterations are required
to converge to a stable model. A more detailed
description of the experimental settings follows.
7.1 Experimental Setup
Training We train a global linear model using
the averaged perceptron algorithm and a labelled
version of the parser described in Section 3. We
perform on-line training using the oracle defined
in Section 5: at each parsing step, the model’s
weights are updated if the predicted transition res-
ults into an increase in configuration loss, but
the process continues by following the predicted
transition independently of the loss increase.
As our baseline we train the model using the
static oracle defined by (Cohen et al., 2012). This
oracle follows a canonical computation that cre-
ates arcs as soon as possible, and prioritizes the
`
la
transition over the `
la
2
transition in situations
925
where both create a gold arc. The static oracle
is not able to deal with configurations that can-
not reach the gold dependency tree, so we con-
strain the training algorithm to follow the zero-loss
transition provided by the oracle.
While this version of Attardi’s parser has been
shown to cover the vast majority of non-projective
sentences in several treebanks (Attardi, 2006; Co-
hen et al., 2012), there still are some sentences
which are not parsable. These sentences are
skipped during training, but not during test and
evaluation of the model.
Datasets We evaluate the parser performance
over CoNLL 2006 and CoNLL 2007 datasets.
If a language is present in both datasets, we
use the latest version. We also include res-
ults over the Penn Treebank (PTB) (Marcus et
al., 1993) converted to Stanford basic dependen-
cies (De Marneffe et al., 2006). For the CoNLL
datasets we use the provided part-of-speech tags
and the standard training/test partition; for the
PTB we use automatically assigned tags, we train
on sections 2-21 and test on section 23.
7.2 Results and Analysis
In Table 1 we report the unlabelled (UAS) and la-
belled (LAS) attachment scores for the static and
the dynamic oracles. Each figure is an average
over the accuracy provided by 5 models trained
with the same setup but using a different random
seed. The seed is only used to shuffle the sentences
in random order during each iteration of training.
Our results are consistent with the results re-
ported by Goldberg and Nivre (2013) and Gold-
berg et al. (2014). For most of the datasets, we
obtain a relevant improvement in both UAS and
LAS. For Dutch, Czech and German, we achieve
an error reduction of 5.2%, 11.2% and 4.5%, re-
spectively. Exceptions to this general trend are
Swedish and Bulgarian, where the accuracy differ-
ences are negligible, and the Basque, Catalan and
Hungarian datasets, where the performance actu-
ally decreases.
If instead of testing on the standard test sets we
use 10-fold cross-validation and average the res-
ulting accuracies, we obtain improvements for all
languages in Table 1 but Basque and Hungarian.
More specifically, measured (UAS, LAS) pairs for
Swedish are (86.85, 82.17) with dynamic oracle
against (86.6, 81.93) with static oracle; for Bul-
garian (88.42, 83.91) against (88.20, 83.55); and
for Catalan (88.33, 83.64) against (88.06, 83.13).
This suggests that the negligible or unfavourable
results in Table 1 for these languages are due to
statistical variability given the small size of the test
sets.
As for Basque, we measure (75.54, 67.58)
against (76.77, 68.20); similarly, for Hungarian
we measure (75.66, 67.66) against (77.22, 68.42).
Unfortunately, we have no explanation for these
performance decreases, in terms of the typology
of the non-projective patterns found in these two
datasets. Note that Goldberg et al. (2014) also
observed a performance decrease on the Basque
dataset in the projective case, although not on
Hungarian.
The parsing times measured in our experiments
for the static and the dynamic oracles are the same,
since the oracle algorithm is only used during the
training stage. Thus the reported improvements in
parsing accuracy come at no extra cost for parsing
time. In the training stage, the extra processing
needed to compute the loss and to explore paths
that do not lead to a gold tree made training about
4 times slower, on average, for the dynamic oracle
model. This confirms that our oracle algorithm is
fast enough to be of practical interest, in spite of its
relatively high worst-case asymptotic complexity.
8 Conclusions
We have presented what, to our knowledge, are
the first experimental results for a transition-based
non-projective parser trained with a dynamic or-
acle. We have also shown significant accuracy im-
provements on many languages over a static oracle
baseline.
The general picture that emerges from our ap-
proach is that dynamic programming algorithms
originally conceived for the simulation of trans-
ition-based parsers can effectively be used in the
development of polynomial-time algorithms for
dynamic oracles.
Acknowledgments
The first author has been partially funded by Min-
isterio de Econom´?a y Competitividad/FEDER
(Grant TIN2010-18552-C03-02) and by Xunta de
Galicia (Grant CN2012/008). The third author has
been partially supported by MIUR under project
PRIN No. 2010LYA9RH 006.
926
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166–
170, New York, USA.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1052–
1062, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Shay B. Cohen, Carlos G´omez-Rodr´?guez, and Giorgio
Satta. 2011. Exact inference for generative prob-
abilistic non-projective dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1234–
1245, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Shay B. Cohen, Carlos G´omez-Rodr´?guez, and Gior-
gio Satta. 2012. Elimination of spurious ambigu-
ity in transition-based dependency parsing. CoRR,
abs/1206.6735.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449–454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc. of
the 24
th
COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Associ-
ation for Computational Linguistics, 2(April):119–
130.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2006. Introduction to Automata Theory, Lan-
guages, and Computation (3rd Edition). Addison-
Wesley Longman Publishing Co., Inc., Boston, MA,
USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, July.
Marco Kuhlmann and Joakim Nivre. 2010. Transition-
based techniques for non-projective dependency
parsing. Northern European Journal of Language
Technology, 2(1):1–19.
Marco Kuhlmann, Carlos G´omez-Rodr´?guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149–160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50–57, Barcelona, Spain.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195–206.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
927
