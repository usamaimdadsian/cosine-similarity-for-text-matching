Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172–1181,
Singapore, 6-7 August 2009. c©2009 ACL and AFNLP
Large-Scale Verb Entailment Acquisition from the Web
Chikara Hashimoto? Kentaro Torisawa† Kow Kuroda‡
Stijn De Saeger§ Masaki Murata¶ Jun’ichi Kazama?
National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
{
? ch,† torisawa,‡ kuroda,§ stijn,¶murata,? kazama}@nict.go.jp
Abstract
Textual entailment recognition plays a
fundamental role in tasks that require in-
depth natural language understanding. In
order to use entailment recognition tech-
nologies for real-world applications, a
large-scale entailment knowledge base is
indispensable. This paper proposes a con-
ditional probability based directional sim-
ilarity measure to acquire verb entailment
pairs on a large scale. We targeted 52,562
verb types that were derived from 108
Japanese Web documents, without regard
for whether they were used in daily life
or only in specific fields. In an evaluation
of the top 20,000 verb entailment pairs ac-
quired by previous methods and ours, we
found that our similarity measure outper-
formed the previous ones. Our method
also worked well for the top 100,000 re-
sults.
1 Introduction
We all know that if you snored, you must have
been sleeping, that if you are divorced, you must
have been married, and that if you won a lawsuit,
you must have sued somebody. These relation-
ships between events where one is the logical con-
sequence of the other are called entailment. Such
knowledge plays a fundamental role in tasks that
require in-depth natural language understanding,
e.g., answering questions and using natural lan-
guage interfaces.
This paper proposes a novel method for verb
entailment acquisition. Using a Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents, we automat-
ically acquired such verb pairs as snore ? sleep
and divorce ? marry, where entailment holds be-
tween the verbs in the pair.1 Our definition of “en-
tailment” is the same as that in WordNet3.0; v
1
entails v
2
if v
1
cannot be done unless v
2
is, or has
been, done.2
Our method follows the distributional similar-
ity hypothesis, i.e., words that occur in the same
context tend to have similar meanings. Just as in
the methods of Lin and Pantel (2001) and Szpek-
tor and Dagan (2008), we regard the arguments
of verbs as the context in the hypothesis. How-
ever, unlike the previous methods, ours is based
on conditional probability and is augmented with
a simple trick that improves the accuracy of verb
entailment acquisition. In an evaluation of the top
20,000 verb entailment pairs acquired by the pre-
vious methods and ours, we found that our similar-
ity measure outperformed the previous ones. Our
method also worked well for the top 100,000 re-
sults,
Since the scope of Natural Language Process-
ing (NLP) has advanced from a formal writing
style to a colloquial style and from restricted to
open domains, it is necessary for the language re-
sources for NLP, including verb entailment knowl-
edge bases, to cover a broad range of expressions,
regardless of whether they are used in daily life
or only in specific fields that are highly techni-
cal. As we will discuss later, our method can ac-
quire, with reasonable accuracy, verb entailment
pairs that deal not only with common and familiar
verbs but also with technical and unfamiliar ones
like podcast ? download and jibe ? sail.
Note that previous researches on entailment ac-
quisition focused on templates with variables or
word-lattices (Lin and Pantel, 2001; Szpektor and
Dagan, 2008; Barzilay and Lee, 2003; Shinyama
1Verb entailment pairs are described as v
1
? v
2
(v
1
is
the entailing verb and v
2
is the entailed one) henceforth.
2WordNet3.0 provides entailment relationships between
synsets like divorce, split up?marry, get married, wed, con-
join, hook up with, get hitched with, espouse.
1172
et al., 2002). Certainly these templates or word
lattices are more useful in such NLP applications
as Q&A than simple entailment relations between
verbs. However, our contention is that entailment
certainly holds for some verb pairs (like snore ?
sleep) by themselves, and that such pairs consti-
tute the core of a future entailment rule database.
Although we focused on verb entailment, our
method can also acquire template-level entailment
pairs with a reasonable accuracy.
The rest of this paper is organized as follows.
In §2, related works are described. §3 presents our
proposed method. After this, an evaluation of our
method and the existing methods is presented in
Section 4. Finally, we conclude the paper in §5.
2 Related Work
Previous studies on entailment, inference rules,
and paraphrase acquisition are roughly classi-
fied into those that require comparable corpora
(Shinyama et al., 2002; Barzilay and Lee, 2003;
Ibrahim et al., 2003) and those that do not (Lin
and Pantel, 2001; Weeds and Weir, 2003; Geffet
and Dagan, 2005; Pekar, 2006; Bhagat et al., 2007;
Szpektor and Dagan, 2008).
Shinyama et al. (2002) regarded newspaper arti-
cles that describe the same event as a pool of para-
phrases, and acquired them by exploiting named
entity recognition. They assumed that named en-
tities are preserved across paraphrases, and that
text fragments in the articles that share several
comparable named entities should be paraphrases.
Barzilay and Lee (2003) also used newspaper ar-
ticles on the same event as comparable corpora
to acquire paraphrases. They induced paraphras-
ing patterns by sentence clustering. Ibrahim et al.
(2003) relied on multiple English translations of
foreign novels and sentence alignment to acquire
paraphrases. We decided not to take this approach
since using comparable corpora limits the scale
of the acquired paraphrases or entailment knowl-
edge bases. Although obtaining comparable cor-
pora has been simplified by the recent explosion
of the Web, the availability of plain texts is incom-
parably better.
Entailment acquisition methods that do not re-
quire comparable corpora are mostly based on the
distributional similarity hypothesis and use plain
texts with a syntactic parser. Basically, they parse
texts to obtain pairs of predicate phrases and their
arguments, which are regarded as features of the
predicates with appropriately assigned weights.
Lin and Pantel (2001) proposed a paraphrase ac-
quisition method (non-directional similarity mea-
sure) called DIRT which acquires pairs of binary-
templates (predicate phrases with two argument
slots) that are paraphrases of each other. DIRT em-
ploys the following similarity measure proposed
by Lin (1998):
Lin(l, r) =
?
f?F
l
?F
r
[w
l
(f) + w
r
(f)]
?
f?F
l
w
l
(f) +
?
f?F
r
w
r
(f)
where l and r are the corresponding slots of two
binary templates, F
s
is s’s feature vector (argu-
ment nouns), and w
s
(f) is the weight of f ? F
s
(PMI between s and f ). The intuition behind this
is that the more nouns two templates share, the
more semantically similar they are. Since we ac-
quire verb entailment pairs based on unary tem-
plates (Szpektor and Dagan, 2008) we used the
Lin formula to acquire unary templates directly
rather than using the DIRT formula, which is the
arithmetic-geometric mean of Lin’s similarities for
two slots in a binary template.
Bhagat et al. (2007) developed an algorithm
called LEDIR for learning the directionality of
non-directional inference rules like those pro-
duced by DIRT. LEDIR implements a Direction-
ality Hypothesis: when two binary semantic re-
lations tend to occur in similar contexts and the
first one occurs in significantly more contexts than
the second, then the second most likely implies the
first and not vice versa.
Weeds and Weir (2003) proposed a general
framework for distributional similarity that mainly
consists of the notions of what they call Precision
(defined below) and Recall:
Precision(l, r) =
?
f?F
l
?F
r
w
l
(f)
?
f?F
l
w
l
(f)
where l and r are the targets of a similarity mea-
surement, F
s
is s’s feature vector, and w
s
(f) is the
weight of f ? F
s
. The best performing weight is
PMI. Precision is a directional similarity measure
that examines the coverage of l’s features by those
of r’s, with more coverage indicating more simi-
larity.
Szpektor and Dagan (2008) proposed a direc-
tional similarity measure called BInc (Balanced-
Inclusion) that consists of Lin and Precision, as
BInc(l, r) =
?
Lin(l, r) × Precision(l, r)
1173
where l and r are the target templates. For weight-
ing features, they used PMI. Szpektor and Dagan
(2008) also proposed a unary template, which is
defined as a template consisting of one argument
slot and one predicate phrase. For example, X take
a nap ? X sleep is an entailment pair consisting
of two unary templates. Note that the slot X must
be shared between templates. Though most of the
previous entailment acquisition studies focused on
binary templates, unary templates have an obvi-
ous advantage over binary ones; they can handle
intransitive predicate phrases and those that have
omitted arguments. The Japanese language, which
we deal with here, often omits arguments, and thus
the advantage of unary templates is obvious.
As shown in §4, our method outperforms Lin,
Precision, and BInc in accuracy.
Szpector et al. (2004) addressed broad coverage
entailment acquisition. But their method requires
an existing lexicon to start, while ours does not.
Apart from the dichotomy of the compara-
ble corpora and the distributional similarity ap-
proaches, Torisawa (2006) exploited the structure
of Japanese coordinated sentences to acquire verb
entailment pairs. Pekar (2006) used the local
structure of coherent text by identifying related
clauses within a local discourse. Zanzotto et al.
(2006) exploited agentive nouns. For example,
they acquired win ? play from “the player wins.”
Geffet and Dagan (2005) proposed the Distribu-
tional Inclusion Hypotheses, which claimed that if
a word v entails another word w, then all the char-
acteristic features of v are expected to appear with
w, and vice versa. They applied this to noun en-
tailment pair acquisition, rather than verb pairs.
3 Proposed Method
This section presents our method of verb entail-
ment acquisition. First, the basics of Japanese are
described. Then, we present the directional sim-
ilarity measure that we developed in §3.2. §3.3
describes the structure and acquisition of the web-
based data from which entailment pairs are de-
rived. Finally, we show how we acquire verb en-
tailment pairs using our proposed similarity mea-
sure and the web-based data in §3.4.
3.1 Basics of Japanese
Japanese explicitly marks arguments including the
subject and object by postpositions, and is a head-
final language. Thus, a verb phrase consisting of
an object hon (book) and a verb yomu (read), for
example, is expressed as hon-wo yomu (book-ACC
read) “read a book” with the accusative postpo-
sition wo marking the object.3 Accordingly, we
refer to a unary template as ?p, v? hereafter, with
p and v referring to the postposition and a verb.
Also, we abbreviate a template-level entailment
?p
l
, v
l
? ? ?p
r
, v
r
? as l ? r for simplicity. We
define a unary template as a template consisting
of one argument slot and one predicate, following
Szpektor and Dagan (2008).
3.2 Directional Similarity Measure based on
Conditional Probability
The directional similarity measure that we devel-
oped and called Score is defined as follows:
Score(l, r) = Score
base
(l, r) × Score
trick
(l, r)
where l and r are unary templates, and Score in-
dicates the probability of l ? r. Score
base
, which
is the base of Score, is defined as follows:
Score
base
(l, r) =
?
f?F
l
?F
r
P (r|f)P (f |l)
where F
s
is s’s feature vector (nouns including
compounds). The intention behind the definition
of Score
base
is to emulate the conditional proba-
bility P (v
r
|v
l
)
4 in a distributional similarity style
function. Note that P (v
r
|v
l
) should be 1 when en-
tailment v
l
? v
r
holds (i.e., v
r
is observed when-
ever v
l
is observed) and we have reliable proba-
bility values. Then, if we can directly estimate
P (v
r
|v
l
), it is reasonable to assume v
l
? v
r
if
P (v
r
|v
l
) is large enough. However, we cannot es-
timate P (v
r
|v
l
) directly since it is unlikely that we
will observe the verbs v
r
and v
l
at the same time.
(People do not usually repeat v
r
and v
l
in the same
document to avoid redundancy.) Thus, instead of
a direct estimation, we substitute Score
base
(l, r)
as defined above. In other words, we assume
P (v
r
|v
l
) ? P (r|l) ? ?
f?F
l
?F
r
P (f |l)P (r|f).
Actually, Score
base
originally had another mo-
tivation, inspired by Torisawa (2005), for which no
postposition but the instrumental postposition de
was relevant. In this discussion, all of the nouns
(fs) that are marked by the instrumental postposi-
tion are seen as “tools,” and P (f |l) is interpreted
3ACC represents an accusative postposition in Japanese.
Likewise, NOM, DAT, INS, and TOP are the symbols for the
nominative, dative, instrumental, and topic postpositions.
4Remember that v
l
and v
r
are the verbs of unary tem-
plates l and r.
1174
as a measure of how typically the tool f is used
to perform the action denoted by (the v
l
of) l; if
P (f |l) is large enough, f is a typical tool used in
l. On the other hand, P (r|f) indicates the proba-
bility of (the v
r
of) r being the purpose for using
the tool f . See (1) for an example.
(1) konro-de chouri-suru
cooking.stove-INS cook
‘cook (something) using a cooking stove.’
The purpose of using a cooking stove is to cook.
Torisawa (2005) has pointed out that when r ex-
presses the purpose of using a tool f , P (r|f) tends
to be large. This predicts that P (r|cooking stove)
is large, where r is ?de, cook?.
According to this observation, if f is a single
purpose tool and P (f |l), the probability of f be-
ing the tool by which l is performed, and P (r|f),
the probability of r being the purpose of using the
tool f , are large enough, then the typical perfor-
mance of the action v
l
should contain some ac-
tions that can be described by v
r
, i.e., the pur-
pose of using f . Moreover, if all the typical tools
(fs) used in v
l
are also used for v
r
, most perfor-
mances of the action v
l
should contain a part de-
scribed by the action v
r
. In summary, this means
that when ?
f?F
l
?F
r
P (r|f)P (f |l), Score
base
, has
a large value, we can expect v
l
? v
r
.
For example, let v
l
be deep-fry and v
r
be cook.
Note that v
l
? v
r
holds for this example. There
are many tools that are used for deep-frying,
such as cooking stove, pot, or pan. This means
that P (cooking stove|l), P (pot|l), or P (pan|l) are
large. On the other hand, the purpose of using all
of these tools is cooking, based on common sense.
Thus, probabilities such as P (r|cooking stove)
and P (r|pan) should have large values. Accord-
ingly, ?
f?F
l
?F
r
P (f |l)P (r|f), Score
base
, should
be relatively large for deep-fry ? cook,
Actually, we defined Score
base
based on the
above assumption However, through a series of
preliminary experiments, we found that the same
score could be applied without losing the preci-
sion to the other postpositions. Thus, we gener-
alized the framework so that it could deal with
most postpositions, namely ga (NOM), wo (ACC),
ni (DAT), de (INS), and wa (TOP). Note that this
is a variation of the distributional inclusion hy-
pothesis (Geffet and Dagan, 2005), but that we do
not use mutual information as in previous works,
based on the hypothesis discussed above. Actu-
ally, as shown in §4, our conditional probability
based method outperformed the mutual informa-
tion based metrics in our experiments.
On the other hand, Score
trick
implements an-
other assumption that if only one feature con-
tributes to Score
base
and the contribution of the
other nouns is negligible, if any, the similarity is
unreliable. Accordingly, for Score
trick
, we uni-
formly ignore the contribution of the most domi-
nant feature from the similarity measurement.
Score
trick
(l, r)
= Score
base
(l, r) ? max
f?F
l
?F
r
P (r|f)P (f |l)
As shown in §4, this trick actually improved the
entailment acquisition accuracy.
We used maximum likelihood estimation to ob-
tain P (r|f) and P (f |l) in the above discussion.
Bannard and Callison-Burch (2005) and Fujita
and Sato (2008) also proposed directional simi-
larity measures based on conditional probability,
which are very similar to Score
base
, although ei-
ther their method’s prerequisites or the targets of
the similarity measurements were different from
ours. The method of Bannard and Callison-Burch
(2005) requires bilingual parallel corpora, and
uses the translations of expressions as its feature.
Fujita and Sato (2008) dealt with productive pred-
icate phrases, while our target is non-productive
lexical units, i.e., verbs. Thus, this is the first
attempt to apply a conditional probability based
similarity measure to verb entailment acquisition.
In addition, the trick implemented in Score
trick
is
novel.
3.3 Preparing Template-Feature Tuples
Our method starts from a dataset called template-
feature tuples, which was derived from the Web
in the following way: 1) Parse the Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents with Japanese
dependency parser KNP (Kawahara and Kuro-
hashi, 2006b). 2) Extract triples ?n, p, v? consist-
ing of nouns (n), postpositions (p), and verbs (v),
where an n marked by a p depends on a v from
the parsed Web text. 3) From the triple database,
construct template-feature tuples ?n, ?p, v?? by re-
garding ?p, v? as a unary template and n as one of
its features. 4) Convert the verbs into their canon-
ical forms as defined by KNP. 5) Filter out tuples
that fall into one of the following categories: 5-
1) Freq(?p, v?) < 20. 5-2) Its verb is passivized,
1175
causativized, or negated. 5-3) Its verb is semanti-
cally vague like be, do, or become. 5-4) Its post-
position is something other than ga (NOM), wo
(ACC), ni (DAT), de (INS), or wa (TOP).
The resulting unary template-feature tuples in-
cluded 127,808 kinds of templates that consisted
of 52,562 verb types and five kinds of postpo-
sitions. The verbs included compound words
like bosi-kansen-suru (mother.to.child-infection-
do) “infect from mothers to infants.”
3.4 Acquiring Entailment Pairs
We acquired verb entailment pairs using the fol-
lowing procedure: i) From the template-feature
tuples mentioned in §3.3, acquire unary template
pairs that exhibit an entailment relation between
them using the directional similarity measure in
§3.2. ii) Convert the acquired unary templates
?p, v? into naked verbs v by stripping the postpo-
sitions p. iii) Remove the duplicated verb pairs
resulting from stripping ps. To be precise, when
we removed the duplicated pairs, we left the high-
est ranked one. iv) Retrieve N-best verb pairs as
the final output from the result of iii). That is, we
first acquired unary template pairs and then trans-
formed them into verb pairs.
Although this paper focuses on verb entailment
acquisition, we also evaluated the accuracy of
template-level entailment acquisition, in order to
show that our similarity measure works well, not
only for verb entailment acquisition, but also for
template entailment acquisition (See §4.4). we
created two kinds of unary templates: the “Scoring
Slots” template and the “Nom(inative) Slots” tem-
plate. The first is simply the result of the procedure
i); all of the templates have slots that are used for
similarity scoring. The second one was obtained
in the following way: 1) Only templates whose p
is not a nominative are sampled from the result of
the procedure i). 2) Their ps are all changed to a
nominative. Templates of the second kind are used
to show that the corresponding slots between tem-
plates (nominative, in this case) that are not used
for similarity scoring can be incorporated to re-
sulting template-level entailment pairs if the scor-
ing function really captures the semantic similarity
between templates.
Note that, for unary template entailment pairs
like (2) to be well-formed, the two unary slots (X-
wo) between templates must share the same noun
as the index i indicates. This is relevant in §4.4.
(2) X
i
-wo musaborikuu ? X
i
-wo taberu
X
i
-ACC gobble X
i
-ACC eat
4 Evaluation
We compare the accuracy of our method with that
of the alternative methods in §4.1. §4.2 shows
the effectiveness of the trick. We examine the en-
tailment acquisition accuracy for frequent verbs in
§4.3, and evaluate the performance of our method
when applied to template-level entailment acquisi-
tion in §4.4. Finally, by showing the accuracy for
verb pairs obtained from the top 100,000 results,
we claim that our method provides a good start-
ing point from which a large-scale verb entailment
resource can be constructed in §4.5.
For the evaluation, three human annotators (not
the authors) checked whether each acquired entail-
ment pair was correct. The average of the three
Kappa values for each annotator pair was 0.579
for verb entailment pairs and 0.568 for template
entailment pairs, both of which indicate the mid-
dling stability of this evaluation annotation.
4.1 Experiment 1: Verb Pairs
We applied Score, BInc, Lin, and Precision to the
template-feature tuples (§3.3), obtained template
entailment pairs, and finally obtained verb entail-
ment pairs by removing the postpositions from the
templates as described in §3. As a baseline, we
created pairs from randomly chosen verbs.
Since we targeted all of the verbs that ap-
peared on the Web (under the condition of
Freq(?p, v?) ? 20), the annotators were con-
fronted with technical terms and slang that they
did not know. In such cases, they consulted dic-
tionaries (either printed or machine readable ones)
and the Web. If they still could not find the mean-
ing of a verb, they labeled the pair containing the
unknown verb as incorrect.
We used the accuracy = # of correct pairs# of acquired pairs as
an evaluation measure. We regarded a pair as cor-
rect if it was judged correct by one (Accuracy-1),
two (Accuracy-2), or three (Accuracy-3) annota-
tors.
We evaluated 200 entailment pairs sampled
from the top 20,000 for each method (# of ac-
quired pairs = 200). For fairness, the evaluation
samples for each method were shuffled and placed
in one file from which the annotators worked. In
this way, they were unable to know which entail-
ment pair came from which method.
1176
Note that the verb entailment pairs produced
by Lin do not provide the directionality of en-
tailment. Thus, the annotators decided the direc-
tionality of these entailment pairs as follows: i)
Copy 200 original samples and reverse the order
of v
1
and v
2
. ii) Shuffle the 400 Lin samples
(the original and reversed samples) with the other
ones. iii) Evaluate all of the shuffled pairs. Each
Lin pair was regarded as correct if either direction
was judged correct. In other words, we evaluated
the upper bound performance of the LEDIR algo-
rithm.
Table 1 shows the accuracy of the acquired
verb entailment pairs for each method. Figure 1
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
BInc 0.450 0.255 0.125
Precision 0.725 0.545 0.385
Lin 0.590 0.370 0.160
Random 0.050 0.010 0.005
Table 1: Accuracy of verb entailment pairs.
shows the accuracy figures for the N-best entail-
ment pairs for each method, with N being 1,000,
2,000, . . ., or 20,000. We observed the following
points from the results. First, Score outperformed
all the other methods. Second, Score and Pre-
cision, which are directional similarity measures,
worked well, while Lin, which is a symmetric one,
performed poorly even though the directionality of
its output was determined manually.
Looking at the evaluated samples, Score suc-
cessfully acquired pairs in which the entailed
verbs generalized entailing verbs that were techni-
cal terms. (3) shows examples of Score’s outputs.
(3) a. RSS-haisin-suru ? todokeru
RSS-feed-do deliver
“feed the RSS data”
b. middosippu-maunto-suru ? tumu
midship-mounting-do mount
“have (engine) midship-mounted”
The errors made by DIRT (4) and BInc (5) in-
cluded pairs consisting of technical terms.
(4) kurakkingu-suru
software.cracking-do
‘crack a (security) system’
? koutiku-hosyu-suru
building-maintenance-do
“build and maintain a system”
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Figure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-suru
tank-raising-do
“raise (fish) in a tank”
? siken-houryuu-suru
test-discharge-do
“stock (with fish) experimentally”
These terms are related in some sense, but they
are not entailment pairs.
4.2 Experiment 2: Effectiveness of the Trick
Next, we investigated the effectiveness of the trick
described in §3. We evaluated Score, Score
trick
,
and Score
base
. Table 2 shows the accuracy figures
for each method. Figure 2 shows the accuracy fig-
ures for the N-best outputs for each method. The
1177
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
Score
trick
0.725 0.610 0.395
Score
base
0.590 0.465 0.315
Table 2: Effectiveness of the trick.
results illustrate that introducing the trick signif-
icantly improved the performance of Score
base
,
and so did multiplying Score
trick
and Score
base
,
which is our proposal Score.
(6) shows an example of Score
base
’s errors.
(6) gazou-sakusei-suru ? henkou-suru
image-making-do change-do
“make an image” “change”
This pair has only two shared nouns (f ? F
l
?F
r
),
and more than 99.99% of the pair’s similarity re-
flects only one of the two. Clearly, the trick would
have prevented the pair from being highly ranked.
4.3 Experiment 3: Pairs of Frequent Verbs
We found that the errors made by Lin and BInc
in Experiment 1 were mostly pairs of infrequent
verbs such as technical terms. Thus, we con-
ducted the acquisition of entailment pairs targeting
more frequent verbs to see how their performance
changed. The experimental conditions were the
same as in Experiment 1, except that the templates
(?p, v?) used were all Freq(?p, v?) ? 200.
Table 3 shows the accuracy figures for each
method with the changes in accuracy from those
of the original methods in parentheses. The re-
Method Acc-1 Acc-2 Acc-3
Score
0.690 0.520 0.335
(?0.080) (?0.140) (?0.125)
BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)
Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)
Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)
Table 3: Accuracy of frequent verb pairs.
sults show that the accuracies of Score and Pre-
cision (the two best methods in Experiment 1) de-
graded, while the other two improved a little. We
suspect that the performance difference between
these methods would get smaller if we further re-
stricted the target verbs to more frequent ones.
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Figure 2: Accuracy of verb entailment pairs ac-
quired by Score, Score
trick
, and Score
base
.
However, we believe that dealing with verbs com-
prehensively, including infrequent ones, is impor-
tant, since, in the era of information explosion, the
impact on applications is determined not only by
frequent verbs but also infrequent ones that consti-
tute the long tail of a verb-frequency graph. Thus,
this tendency does not matter for our purpose.
4.4 Experiment 4: Template Pairs
This section presents the entailment acquisition
accuracy for template pairs to show that our
method can also perform the entailment acqui-
sition of unary templates. We presented pairs
of unary templates, obtained by the procedure in
1178
§3.4, to the annotators. In doing so, we restricted
the correct entailment pairs to those for which en-
tailment always held regardless of what argument
filled the two unary slots, and the two slots had to
be filled with the same argument, as exemplified
in (2). We evaluated Score and Precision.
Table 4 shows the accuracy of the acquired pairs
of unary templates. Compared to verb entailment
Method Acc-1 Acc-2 Acc-3
Score
0.655 0.510 0.300
Scoring (?0.115) (?0.150) (?0.160)
Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)
Score
0.665 0.515 0.315
Nom (?0.105) (?0.145) (?0.145)
Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)
Table 4: Accuracy of entailment pairs of templates
whose slots were used for scoring.
acquisition, the accuracy of both methods dropped
by about 10%. This was mainly due to the evalua-
tion restriction exemplified in (2) which was not
introduced in the previous experiments; the an-
notators ignored the argument correspondence be-
tween the verb pairs in Experiment 1. Also note
that Score outperformed Precision in this experi-
ment, too.
(7) and (8) are examples of the Scoring Slots
template entailment pairs and (9) is that of the
Nom Slots acquired by our method.
(7) X-wo tatigui-suru ? X-wo taberu
X-ACC standing.up.eating-do X-ACC eat
“eat X standing up” “eat X”
(8) X-de marineedo-suru ? X-wo ireru
X-INS marinade-do X-ACC pour
“marinate with X” “pour X”
(9) X-ga NBA-iri-suru · · · (was X-de (INS))
X-NOM NBA-entering-do
‘X joins an NBA team’
? X-ga nyuudan-suru · · · (was X-de)
X-NOM enrollment-do
“X joins a team”
4.5 Experiment 5: Verb Pairs form the Top
100,000
Finally, we examined the accuracy of the top
100,000 verb pairs acquired by Score and Preci-
sion. As Table 5 shows, Score outperformed Pre-
Method Acc-1 Acc-2 Acc-3
Score 0.610 0.480 0.300
Precision 0.470 0.295 0.190
Table 5: Accuracy of the top 100,000 verb pairs.
cision. Note also that Score kept a reasonable ac-
curacy for the top 100,000 results (Acc-2: 48%).
The accuracy is encouraging enough to consider
human annotation for the top 100,000 results to
produce a language resource for verb entailment,
which we actually plan to do.
Below are correct verb entailment examples
from the top 100,000 results of our method.
(10) The 121th pair
kaado-kessai-suru ? siharau
card-payment-do pay
“pay by card” “pay”
(11) The 6,081th pair
saitei-suru ? sadameru
adjudicate-do settle
“adjudicate” “settle”
(12) The 15,464th pair
eraa-syuuryou-suru ? jikkou-suru
error-termination-do perform-do
“abend” “execute”
(13) The 30,044th pair
ribuuto-suru ? kidou-suru
reboot-do start-do
“reboot” “boot”
(14) The 57,653th pair
rinin-suru ? syuunin-suru
resignation-do accession-do
“resign” “accede”
(15) The 70,103th pair
sijou-tounyuu-suru ? happyou-suru
market-input-do publication-do
“bring to the market” “publicize”
Below are examples of erroneous pairs from our
results. (16) is a causal relation but not an entail-
ment. (17) is a contradictory pair.
(16) The 5,475th pair
juken-suru ? goukaku-suru
take.an.exam-do acceptance-do
“take an exam” “gain admission”
1179
(17) The 40,504th pair
ketujou-suru ? syutujou-suru
not.take.part-do take.part-do
“not take part” “take part”
5 Conclusion
This paper addressed verb entailment acquisition
from the Web, and proposed a novel directional
similarity measure Score. Through a series of ex-
periments, we showed i) that Score outperforms
the previously proposed measures, Lin, Precision,
and BInc in large scale verb entailment acquisi-
tion, ii) that our proposed trick implemented in
Score
trick
significantly improves the accuracy of
verb entailment acquisition despite its simplicity,
iii) that Score worked better than the others even
when we restricted the target verbs to more fre-
quent ones, iv) that our method is also moder-
ately successful at producing template-level en-
tailment pairs, and v) that our method maintained
reasonable accuracy (in terms of human annota-
tion) for the top 100,000 results. As examples of
the acquired verb entailment pairs illustrated, our
method can acquire from an ocean of information,
namely the Web, a variety of verb entailment pairs
ranging from those that are used in daily life to
those that are used in very specific fields.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL2005),
pages 597–604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003, pages 16–23.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP2007), pages 161–170.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of pred-
icate phrases. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING2008), pages 225–232.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL2005), pages 107–114.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing (IWP2003), pages
57–64.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources
and Evaluation (LREC-06), pages 1344–1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
Fully-Lexicalized Probabilistic Model for Japanese
Syntactic and Case Structure Analysis. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2006), pages 176–183.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343–360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL1998), pages
768–774.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL2006),
pages 49–56.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the 2nd international
Conference on Human Language Technology Re-
search (HLT2002), pages 313–318.
Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2004), pages 41–48.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary template. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING2008), pages 849–856.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556–560.
1180
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese cood-
inated sentences and noun-verb co-occurences. In
Proceedings of the Human Language Technology
Conference of the Norh American Chapter of the
ACL (HLT-NAACL2006), pages 57–64.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2003), pages 81–88.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics and 21th InternationalConference
on Computational Linguistics (COLING-ACL2006),
pages 849–856.
1181
