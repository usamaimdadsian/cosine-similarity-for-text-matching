Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1840–1850,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Collective Opinion Target Extraction in Chinese Microblogs 
 
 
 
Xinjie Zhou, Xiaojun Wan* and Jianguo Xiao 
Institute of Computer Science and Technology   
The MOE Key Laboratory of Computational Linguistics   
Peking University  
No. 5, Yiheyuan Road, Beijing, China 
{zhouxinjie,wanxiaojun,xiaojianguo}@pku.edu.cn 
 
 
Abstract 
Microblog messages pose severe challenges 
for current sentiment analysis techniques due 
to some inherent characteristics such as the 
length limit and informal writing style. In this 
paper, we study the problem of extracting 
opinion targets of Chinese microblog messag-
es. Such fine-grained word-level task has not 
been well investigated in microblogs yet. We 
propose an unsupervised label propagation al-
gorithm to address the problem. The opinion 
targets of all messages in a topic are collec-
tively extracted based on the assumption that 
similar messages may focus on similar opinion 
targets. Topics in microblogs are identified by 
hashtags or using clustering algorithms. Ex-
perimental results on Chinese microblogs 
show the effectiveness of our framework and 
algorithms. 
1 Introduction 
Microblogging services such as Twitter 1 , Sina 
Weibo2 and Tencent Weibo3 have swept across the 
globe in recent years. Users of microblogs range 
from celebrities to ordinary people, who usually 
express their emotions or attitudes towards a broad 
range of topics. It is reported that there are more 
than 340 million tweets per day on Twitter and 
more than 200 million on Sina Weibo. A tweet 
means a post on Twitter. Since we mainly focus 
on Chinese microblogs instead of Twitter in this 
paper, we will refer to a post as a message. Each 
message is limited to 140 Chinese characters and 
usually contains several sentences. 
                                                          
* Xiaojun Wan is the corresponding author. 
1 https://twitter.com 
2 http://weibo.com/ 
3 http://t.qq.com/ 
Currently, researches on microblog sentiment 
analysis have been conducted on polarity classifi-
cation (Barbosa and Feng, 2010; Jiang el al., 2011; 
Speriosu et al., 2011) and have been proved to be 
useful in many applications, such as opinion poll-
ing (Tang et al., 2012), election prediction 
(Tumasjan et al., 2010) and even stock market 
prediction (Bollen et al., 2011). However, classify-
ing microblog texts at the sentence level is often 
insufficient for applications because it does not 
identify the opinion targets. In this paper, we will 
study the task of opinion target extraction for Chi-
nese microblog messages.  
Opinion target extraction aims to find the object 
to which the opinion is expressed. For example, in 
the sentence “The sound quality is good!”, “sound 
quality” is the opinion target. This task is mostly 
studied in customer review texts in which opinion 
targets are often referred as features or aspects 
(Liu, 2012). Most of the opinion target extraction 
approaches rely on dependency parsing (Zhuang et 
al., 2006; Jakob and Gurevych, 2010; Qiu et al., 
2011) and are regarded as a domain-dependent 
task (Li et al., 2012a). However, such approaches 
are not suitable for microblogs because the natural 
language processing tools perform poorly on mi-
croblog texts due to their inherent characteristics. 
Studies show that one of the state-of-the-art part-
of-speech taggers - OpenNLP only achieves the 
accuracy of 74% on tweets (Liu et al. 2011). The 
syntactic analysis tool that generates dependency 
relation may perform even worse. Besides, mi-
croblog messages may express opinion in different 
ways and do not always contain opinion words, 
which lowers the performance of methods utiliz-
ing opinion words to find opinion targets.  
In this study, we propose an unsupervised 
method to collectively extract the opinion targets 
from opinionated sentences in the same topic. 
1840
Topics are directly identified by hashtags. We first 
present a dynamic programming based segmenta-
tion algorithm for Chinese hashtag segmentation. 
By leveraging the contents in a topic, our segmen-
tation algorithm can successfully identify out-of-
vocabulary words and achieve promising results. 
Afterwards, all the noun phrases in each sentence 
and the hashtag segments are extracted as opinion 
target candidates. We propose an unsupervised 
label propagation algorithm to collectively rank 
the candidates of all sentences based on the as-
sumption that similar sentences in a topic may 
share the same opinion targets. Finally, for each 
sentence, the candidate which gets the highest 
score after unsupervised label propagation is se-
lected as the opinion target. 
Our contributions in this study are summarized 
as follows: 1) our method considers not only the 
explicit opinion targets within the sentence but 
also the implicit opinion targets in the hashtag or 
mentioned in the previous sentence. 2) We devel-
op an efficient algorithm to segment Chinese 
hashtags. It can successfully identify out-of-
vocabulary words by leveraging contextual infor-
mation and help to improve the segmentation per-
formance of the messages in the topic. 3) We 
develop an unsupervised label propagation algo-
rithm for collective opinion target extraction. La-
bel propagation (Zhu and Ghahramani, 2002) aims 
to spread label distributions from a small training 
set throughout the graph.   However, our unsuper-
vised algorithm leverages the connection between 
two adjacent unlabeled nodes to find the correct 
labels for both of them. The proposed unsuper-
vised method does not need any training corpus 
which will cost much human labor especially for 
fine-grained annotation. 4) To the best of our 
knowledge, the task of opinion target extraction in 
microblogs has not been well studied yet. It is 
more challenging than microblog sentiment classi-
fication and opinion target extraction in review 
texts.  
2 Characteristics of Chinese Microblogs 
Most of previous microblog sentiment analysis 
researches focus on Twitter and especially in Eng-
lish. However, the analysis of Chinese microblogs 
has some differences with that of Twitter: 1) Chi-
nese word segmentation is a necessary step for 
Chinese sentiment analysis, but the existing seg-
mentation tool performs poorly on microblogs 
because the microblog texts are much different 
from regular texts. 2) Wang et al. (2011) find that 
hashtags in English tweets are used to highlight 
the sentiment information such as “ #love”, 
“#sucks” or serve as user-annotated coarse topics 
such as “#news”, “#sports”. But in Chinese mi-
croblogs, most of the hashtags are used to indicate 
fine-grained topics such as #NBA??????# 
(#NBAFinalG7#). Besides, hashtags in Twitter 
always appear within a sentence such as “I love 
#BarackObama!” while hashtags in Chinese mi-
croblogs are always isolated and are surrounded 
by two # symbols such as “#??????# ??
?!” (“#BarackObama# I love him?”). 
It is noteworthy that topics aggregated by the 
same hashtag play an important role in Chinese 
microblog websites. These websites often provide 
an individual webpage4 to list hot topics and invite 
people to participate in the discussion, where each 
topic consists of tens of thousands of messages 
with the same hashtag. The hot topics have a wide 
coverage of timely events and entities. Analyzing 
the opinion targets of these topics can help to get a 
deeper overview of the public attitudes towards 
the entities involved in the hot topics. 
3 Motivation 
As described above, #hashtags# in Chinese mi-
croblogs often indicate fine-grained topics. In this 
study, we aim to collectively extract the opinion 
targets of messages with the same hashtag, i.e. in 
the same topic. Opinion target of a sentence can be 
divided into two types, one of which called explic-
it target appears in the sentence such as “I love 
Obama”, and the other one called implicit target 
                                                          
4 http://huati.weibo.com/ 
Topic Sentence 
#??????# 
#Property publicity 
of government offic
-ials# 
1. ????? 
(Just for show?) 
2. ???????????? 
(Property publicity is just a show 
in China.) 
#???????# 
#Philippine navy 
vessel hits Chinese 
fishing boat# 
1. ????????? 
(The government is not tough 
enough.) 
2. ??????????? 
(Why cannot the government take 
a tougher line?) 
Table 1. Motivation Examples 
1841
may appear out of the sentence, for example, the 
sentence “Just for show!”  in Table 1 directly 
comments on the target in the hashtag “#Property 
publicity of government officials#” . Such implicit 
opinion targets are not considered in previous 
works and are more difficult to extract than explic-
it targets. However, we believe that the contextual 
information will help to locate both of the two 
kinds of opinion targets because similar sentences 
in a topic may share the same opinion target, 
which provides the possibility for collective ex-
traction. 
Table 1 shows the motivation examples of two 
topics and four sentences. The two sentences in 
each topic are considered to be similar because 
they share several Chinese words. In the topic #?
?????# (#Property publicity of government 
officials#), the first sentence omits the opinion 
target. However, the second one contains an ex-
plicit target “????” (“property publicity”) in 
the sentence. If we find the correct opinion target 
for sentence 2, we can infer that sentence 1 may 
have an implicit opinion target similar to the opin-
ion target in sentence 2. In the second topic, both 
sentences contain a noun word “??” (“govern-
ment”). The similarity between these two sentenc-
es may indicate that both of the two sentences are 
expressing opinion on “??”. 
Based on the above observation, we can assume 
that similar sentences in a topic may have the 
same opinion targets. Such assumption can help to 
locate both explicit and implicit opinion targets. 
Following this idea, we firstly extract all the noun 
phrases in each sentence as opinion target candi-
dates after applying Chinese word segmentation 
and part-of-speech tagging. Afterwards, an unsu-
pervised label propagation algorithm is proposed 
to rank these candidates for all sentences in the 
topic. 
In our methods, hashtags are used to find gold-
standard topics. For messages without hashtags, an 
alternative way is to generate pseudo topics by 
clustering microblogs messages and then apply the 
proposed algorithm to each pseudo topic. The de-
tailed discussion of such general circumstance is 
shown in Section 5.7. 
4 Methodology 
4.1 Context-Aware Hashtag Segmentation 
In our approach, the Chinese word segmentations 
of hashtags and topic contents are treated separate-
ly. Existing Chinese word segmentation tools 
work poorly on microblog texts. The segmentation 
errors especially on opinion target words will di-
rectly influence the results of part-of-speech tag-
ging and candidate extraction. However, some of 
the opinion target words in a topic are often in-
cluded in the hashtag. By finding the correct seg-
ments of a hashtag and adding them to the user 
dictionary of the Chinese word segmentation tool, 
we can remarkably improve the overall segmenta-
tion performance.  
The following example can help to understand 
the idea better. In the topic #90????# (means 
“A young man hits an old man”), “90?” (literally 
“90 later” and means a young man born in the 90s) 
is an important word because it is the opinion tar-
get of many sentences. However, existing Chinese 
word segmentation tools will regard it as two sep-
arate words “90” and “?” (“later”). Then in the 
part-of-speech tagging stage, “90” will be tagged 
as number and “?” will be tagged as localizer. As 
we only extract noun phrases as opinion target 
candidates, the wrong segmentation on “90 ?” 
makes it impossible to find the right opinion target. 
Such error may occur many times in sentences that 
mention the word “90?” and express opinion on 
it. In our method, the message texts of the topic 
are utilized to identify such out-of-vocabulary 
words based on its frequency in the topic. For ex-
ample, the high frequency of “90?” is a strong 
indication that it should be regard as a single word. 
After segmenting the hashtag correctly into “90?
/?/??”, we can add the hashtag segments to the 
user dictionary of the segmentation tool to further 
segment the message texts of the topic. 
The basic idea for our hashtag segmentation al-
gorithm is to regard strings that appear frequently 
in a topic as words. Formally, given a hashtag h 
that contains n Chinese characters c1c2...cn. We 
want to segment into several words w1w2...wm, 
where each word is formed by one of more charac-
ters. 
Firstly, we define the stickiness score for a Chi-
nese string c1c2...cn based on the Symmetrical 
Conditional Probability (SCP) (Silva and Lopes, 
1999): 
1842
2
1 2
1 2
1 1
1
Pr( ... )( ... ) 1 Pr( ... )Pr( ... )1
n
n n
i i n
i
c c cSCP c c c
c c c cn ??
?
? ?
 (1) 
and SCP(c1) = Pr(c1)
2 for string with only one 
character. Pr(c1c2...cn) is the occurrence frequency 
of the string in the topic.  
Following (Li et al., 2012b), we smooth the 
SCP value by taking logarithm calculation. Be-
sides, the length of the string is taken into consid-
eration, 
 
1 2 1 2( ... ) log ( ... )n nSCP c c c n SCP c c c? ? ? (2) 
where n is the number of characters in the string. 
Then the stickiness score is defined by the sig-
moid function as follows: 
 
1 21 2 ( ... )
2( ... ) 1 nn SCP c c cStickiness c c c e ??? ?
 (3) 
For the hashtag h = c1c2...cn, we want to seg-
ment it into m words w1w2...wm which maximize 
the following equation, 
 
1
max ( )m i
i
Stickness w
?
?
  (4) 
The optimization of Equation (4) can be solved 
efficiently by dynamic programming which itera-
tively segments a string into two substrings. Dif-
ferent from (Li et al., 2012b) which calculates the 
SCP value of each string based on Microsoft Web 
N-Gram, our hashtag segmentation algorithm only 
uses the topic content and do not need any addi-
tional corpus. 
4.2 Candidate Extraction 
After segmenting the hashtag, all the hashtag seg-
ments with length greater than one are added to 
the user dictionary of the Chinese word segmenta-
tion tool ICTCLAS5 to further segment the mes-
sage texts of the topic. It also assigns the part-of-
speech tag for each word after segmentation. The 
noun phrases in each sentence is extracted by the 
following regular expression:
( | )( ) .noun adj noun adj noun??That means a 
noun phrase can only include nouns, adjectives 
and the Chinese word “?” (“of”). It should begin 
with a noun or adjective and end with a noun. For 
                                                          
5 http://www.ictclas.org/ 
example, in the following sentence, “??/n ?/u 
??/n ??/n ?/v ??/n ?/w” (“Chinese edu-
cation system has problems.”), “???????” 
(“Chinese education system”) and “??” (“prob-
lem”) are extracted as noun phrases.  
The character number of a noun phrase is lim-
ited between two and seven Chinese characters. 
For each sentence, all phrases that match the regu-
lar expression and meet the length restriction are 
extracted as explicit opinion target candidates. The 
hashtag segments are regarded as implicit candi-
dates for all sentences. Besides, some opinionated 
sentences in microblogs do not contain any noun 
phase, such as “????? ” (“So boring!”). 
These sentences may express opinion on object 
that has been mentioned before. Therefore, the 
explicit candidates of the previous sentence in the 
same message are also taken as the implicit candi-
dates for such sentences.  
We do not use any syntactic parsing tool to ex-
tract noun phrases because the parsing results on 
microblogs are not reliable. A performance com-
parison of our rule based method and the state-of-
the-art syntactic parser will be shown in Section 5. 
4.3 Unsupervised Label Propagation for 
Candidate Ranking 
We simply assume that each opinionated sentence 
has one opinion target, which is consistent with 
Algorithm 1 Unsupervised Label Propagation 
Input: 
Graph:                              , ,G V E W?? ?  
Candidate Similarity:     M MS R ???  
Prior labeling:                 1 MvY R ???  for v?V  
Filtering Matrix:             M MvF R ??? for v?V 
Probability:                       pinj and pcont 
Output: 
 Label vector:                   1ˆ MvY R ???  
1: for all v?V do 
2:      
vˆ vY Y?  
3: end for 
4: repeat 
5:       for all v?V do 
6:         
? ?
,
ˆv uv u vu V u vD W Y S F? ?? ? ??
 
7:        ˆ inj contv v vY p Y p D? ?  
8:       end for 
9: until convergence 
 
1843
the statistical result of our dataset that over 93% 
sentences have only one opinion target and each 
sentence has an average of 1.09 targets. Therefore, 
the most confident candidate of each sentence will 
be selected as the opinion target. In this section, 
we introduce an unsupervised graph-based label 
propagation algorithm to collectively rank the 
candidates of all sentences in a topic.  
Label propagation (Zhu and Ghahramani, 2002; 
Talukdar and Crammer, 2009) is a semi-
supervised algorithm which spreads label distribu-
tions from a small set of nodes seeded with some 
initial label information throughout the graph. The 
basic idea is to use information from the labeled 
nodes to label the adjacent nodes in the graph. 
However, our idea is to use the connection be-
tween different nodes to find the correct labels for 
all of them. Our unsupervised label propagation 
algorithm is summarized in Algorithm 1. Sentenc-
es are regarded as nodes and candidates of each 
sentence are regarded as labels. The label vector 
for each node is initialized based on the results of 
the candidate extraction step, which means no 
manually-labeled instances are needed in our 
model. In each iteration, the label vector of one 
node is propagated to the adjacent nodes. Both the 
sentence (node) similarity and the candidate (label) 
similarity are considered during propagation. Fi-
nally, we select the candidate with the highest 
score in the label vector as the opinion target for 
each sentence. The details of Algorithm 1 are pre-
sented as follows. 
Formally, an undirected graph , ,G V E W?? ?  
is built for each topic. A node v?V represents a 
sentence in the topic and an edge e = (a, b) ?E 
indicates that the labels of the two vertices should 
be similar.  W  is the normalized weight matrix to 
reflect the strength of this similarity. The similari-
ty between two nodes Wab is simply calculated by 
using the cosine measure (Salton et al., 1975) of 
the two sentences. 
 
( , ) a bab a b
a b
T TW cos T T T T
?? ? ?
 (5) 
where Ta and Tb are the term vectors of sentences a 
and b represented by the standard vector space 
model and weighted by term frequency. After cal-
culating the similarity matrix W, we get the weight 
matrix W  by normalizing each row of W such that 
1abb W ??
. 
For each sentence (node) v, a candidate set Cv is 
extracted in the previous step. The candidate set 
CT for the whole topic is the union of all Cv, 
 
vCT C?
 (6) 
The total number of candidates in the topic is 
denoted by M = |CT|. We calculate the candidate 
similarity matrix M MS R ???  based on Jaccard In-
dex: 
 ( ) ( ) 1( ) ( )
i j
ij
i j
A CT A CTS i j MA CT A CT? ? ? ?
 (7) 
where A(CTi) and A(CTj) are the Chinese character 
sets of the i-th and j-th candidates in CT respec-
tively. 
Candidates are regarded as labels in our model 
and without loss of generality we assume that the 
possible labels for the whole topic are L = {1…M} 
and each label in L corresponds to a unique can-
didate in CT. For each node v?V, a label vector 
1 MvY R ???  is initialized as 
  
? ? 10 k vv k k v
w L CY k ML C
??? ? ?? ??
 (8) 
where w is the initial weight of the candidate. We 
set w = we if Lk is an explicit candidate (extracted 
noun phrase) of v and w = wi if Lk is an implicit 
candidate (hashtag segment or inherited from pre-
vious sentence) of v. If Lk is not a candidate of the 
current sentence, then the corresponding value in 
the label vector is 0. These values which are ini-
tialized as zero should always remain zero during 
the propagation algorithm because the correspond-
ing label does not belong to the candidate set Cv of 
node v. To reset the values on these positions, a 
diagonal matrix M MvF R ???  is created for all nodes 
v, 
 
? ? ? ?? ?
1 0 10 0
v k
v kk
v k
YF k MY
? ??? ? ?? ???
 (9) 
where the subscript kk denotes the k-th position in 
the diagonal of matrix Fv. We can right-multiply 
Yv by Fv to clear the values of the invalid candi-
1844
dates. Figure 1 shows an example of creating the 
filtering matrix for a label vector. 
The propagation process is formalized via two 
possible actions: inject and continue, with pre-
defined probabilities pinj and pcont. Their sum is 
unit: pinj + pcont = 1. In each iteration, every node is 
influenced by its adjacent nodes. The propagation 
influence for each node v is 
 ? ?
,
ˆv uv u vu V u vD W Y S F? ?? ? ??
 (10) 
where 
uˆY  is the label vector of node u at the previ-
ous iteration. By multiplying the candidate simi-
larity matrix S, we aim to propagate the score of 
the i-th candidate of node u not only to the i-th 
candidate of node v, but also to all the other can-
didates. Wuv measures the strength of such propa-
gation. The filtering matrix Fv is used to clear the 
values of the invalid candidates as described 
above. 
Then the label vector of node v is updated as 
follow, 
  ˆ inj contv v vY p Y p D? ?  (11) 
When the positions of the largest values in all 
label vectors keep unchanged in ten iterations, it is 
regarded that the algorithm has already converged.  
5 Experiments 
5.1 Dataset 
We use the dataset from the 2012 Chinese Mi-
croblog Sentiment Analysis Evaluation (CMSAE)6 
held by China Computer Federation (CCF). There 
are three tasks in the evaluation: subjectivity clas-
sification, polarity classification and opinion target 
extraction. The dataset contains 20 topics collect-
ed from Tencent Weibo, a popular Chinese mi-
croblogging website. All the messages in a topic 
contain the same hashtag. The dataset has a total 
                                                          
6 http://tcci.ccf.org.cn/conference/2012/pages/page04_eva.
html. The dataset can also be publicly accessed on the website. 
of 17518 messages and 31675 sentences. In each 
topic, 100 messages are manually annotated with 
subjectivity, polarity and opinion targets. A total 
of 2361opinion targets are annotated for 2152 
opinionated sentences.  
5.2 Evaluation Metric  
Precision, recall and F-measure are used in the 
evaluation. Since expression boundaries are hard 
to define exactly in annotation guidelines (Wiebe 
et al., 2005), both the strict evaluation metric and 
the soft evaluation metric are used in CMSAE. 
Strict Evaluation: For a proposed opinion tar-
get, it is regarded as correct only if it covers the 
same span with the annotation result. Note that, in 
CMSAE, an opinion target should be proposed 
along with its polarity. The correctness of the po-
larity is also necessary. 
Soft Evaluation: The soft evaluation metric 
presented in (Johansson and Moschitti, 2010) is 
adopted by CMSAE. The span coverage c be-
tween each pair of the proposed target span s and 
the gold standard span s’ is calculated as follows, 
 
? ?, s sc s s s
??? ? ?
 (12) 
In Equation 12, the operator |·| counts Chinese 
characters, and the intersection ? gives the set of 
characters that two spans have in common. 
Using the span coverage, the span set coverage 
C of a set of spans S with respect to another set S’ 
is 
? ?, ( , )
s S s S
C S S c s s
? ?? ?
?? ???
 (13) 
The soft precision P and recall R of a proposed 
set of spans Sˆ  with respect to a gold standard set 
S is defined as follows: 
 ˆ ˆ( , ) ( , )Precision Recallˆ | || |
C S S C S S
SS? ?
 (14) 
Note that the operator |·| counts spans in Equation 
14. The soft F-measure is the harmonic mean of 
soft precision and recall. 
5.3 Comparison Methods 
Our proposed approach is first compared with the 
CMSAE teams. 
CMSAE Teams: Sixteen teams participated in 
the opinion target extraction task of CMSAE. The 
methods of the top 3 teams are used as baselines 
? ?
1 0 0 0
0 1 0 0
1 1 0.5 0
0 0 1 0
0 0 0 0
v vY F
? ?
? ?
? ?? ? ?
? ?
? ?
? ?
 
Figure 1. Example of filtering matrix 
1845
here. They are denoted as Team-1, Team-2 and 
Team-3 respectively. The average result of all the 
sixteen teams is also included and is denoted as 
Team-Avg. We will briefly introduce the best 
team’s method. The most important component of 
their model is a topic-dependent opinion target 
lexicon which is called object sheet. If a word or 
phrase in the object sheet appears in a sentence or 
a hashtag, it is extracted as opinion target. The 
object sheet is manually built for each topic, 
which means their method cannot be applied to 
new topics. 
The following models are also used for compar-
ison. 
AssocMi: We implement the unsupervised 
method for opinion target extraction based on (Hu 
and Liu, 2004), which relies on association mining 
and a sentiment lexicon to extract frequent and 
infrequent product features. 
CRF: The CRF-based method used in (Jakob 
and Gurevych, 2010) is also used for comparison. 
We implement both the single-domain and cross-
domain models. Both models are evaluated using 
5-fold cross-validation. More specifically, the sin-
gle-domain model, denoted as CRF-S, trains dif-
ferent models for different topics. In each cross-
validation round, 80 percent of each topic is used 
for training and the other 20 percent is used for 
test. The cross-domain model, denoted as CRF-C, 
uses 16 topics for training and the rest 4 topics for 
test in each round.  
5.4 Comparison Results  
CMSAE requires all the teams to perform the sub-
jectivity and polarity classification task in advance. 
The opinion targets are extracted only for opinion-
ated sentences and should be proposed along with 
their polarity. To make a fair comparison, we di-
rectly use the subjectivity and polarity classifica-
tion results of Team-1. Then our unsupervised 
label propagation (ULP) method is used to extract 
the opinion targets for the proposed opinionated 
sentences. The parameters of our method are 
simply set as pinj = pcont = 0.5, we = 1 and wi = 0.5. 
Table 2 lists the comparison results with 
CMSAE teams. The average F-measure of all 
teams is 0.12 and 0.20 in strict and soft evaluation, 
respectively. It shows that opinion target extrac-
tion is a quite hard problem in Chinese microblogs. 
Our method performs better than all the teams. It 
increases by 10% and 13% in the two kinds of F-
measure compared to the best team. Besides, we 
do not need any prior information of the topics 
while Team-1 has to manually build an opinion 
target lexicon for each topic. 
To compare with the other opinion target ex-
traction methods, we only use gold-standard opin-
ionated sentences for evaluation and do not 
classify the polarity of the opinion targets. Table 3 
shows the experimental results of the four models. 
Our approach achieves the best result among them. 
AssocMi performs worst in strict evaluation but 
gets better results than the two CRF-based models 
in soft evaluation. The two CRF-based models 
achieve high precision but low recall. We can also 
observe that CRF-S is much more effective than 
CRF-C. It achieves high results because it has al-
ready seen the opinion targets in the training set. 
However, it is impossible to build such single-
domain model in practical applications because 
Method 
Strict Soft 
Precision Recall F-Measure Precision Recall F-Measure 
AssocMi 0.22 0.20 0.21 0.47 0.43 0.45 
CRF-C 0.59 0.15 0.24 0.70 0.18 0.28 
CRF-S 0.61 0.27 0.35 0.73 0.31 0.41 
ULP 0.43 0.39 0.41 0.61 0.55 0.58 
Table 3. Comparison results with baseline methods (only gold-standard opinionated sentences are used) 
Method. 
 
Strict Soft 
Precision Recall F-measure Precision Recall F-measure 
Team-Avg 0.17 0.09 0.12 0.29 0.15 0.20 
Team-3 0.26 0.16 0.20 0.40 0.25 0.31 
Team-2 0.31 0.18 0.23 0.40 0.22 0.29 
Team-1 0.30 0.27 0.29 0.39  0.36 0.37 
ULP 0.37 0.27 0.32 0.48 0.37 0.42 
Table 2. Comparison results with CMSAE teams (with subjectivity and polarity classification in advance) 
1846
labeled instances are not available for new topics. 
Our proposed method does not require any train-
ing data and gets an increase of 17% over CRF-S 
and 70% over CRF-C in strict evaluation. In terms 
of soft evaluation, we achieve an increase of 41% 
and 107% over the two CRF models.  
5.5 Parameter Sensitivity Study 
In this section, we study the parameter sensitivity. 
There are two major parameters in our algorithm: 
the initial weight w for both explicit and implicit 
candidates in Equation 8 and the injection proba-
bility pinj in Equation 11.  
The initial weights of explicit and implicit can-
didates are set differently because the explicit can-
didates are more likely to be the opinion targets. 
These two kinds of initial weights are denoted as 
we and wi for explicit and implicit candidate, re-
spectively. To study the impact of the initial 
weights, we fix we at 1 and tune wi because we 
only care about the relative contribution of them. 
The injection probability is fixed at 0.5. Figure 2(a) 
displays the opinion target extraction performance 
when wi varies from 0 to 1.5. Due to limited space, 
we only list the strict F-measure of opinion target 
extraction evaluated on opinioned sentences (same 
experimental setup as Table 3).  
In particular, when wi is equal to 0, only explicit 
candidates are considered. When wi becomes larg-
er than 1, the implicit candidates become more 
important than explicit candidates. From the curve 
in Figure 2(a), we can observe that the implicit 
candidates help to improve the performance sig-
nificantly when wi varies from 0 to 0.1. The per-
formance reaches the peak when wi = 0.7 and 
declines rapidly when wi gets larger than 1.  
To study the impact of injection probability pinj, 
we fix the initial weights for explicit and implicit 
candidates as 1 and 0.5, respectively. Figure 2(b) 
shows the results of opinion target extraction with 
respect to different values of the injection proba-
bility. We can observe that the performance keeps 
steady except for the two extreme values 0 and 1. 
From the above two figures, we can conclude that 
our proposed method performs well and robustly 
with a wide range of parameter values. 
5.6 Analysis of Candidate Extraction  
Candidate extraction is an important step in our 
proposed method. If the correct opinion target is 
not extracted as a candidate, the ranking step will 
be in vain. As described in Section 3, we develop 
a hashtag segmentation algorithm and use a rule 
based method to extract noun phrases from each 
sentence. We do not use any parsing tool because 
we believe the performance of these tools is not 
good enough when applied on microblogs. A 
quantitative comparison is shown in this section.  
We use one of the state-of-the-art syntactic 
analysis tools - Berkeley Parser (Petrov et al., 
2006) for comparison here. Noun phrases are di-
rectly extracted from the parsing results. Our 
method HS+Rule leverages the hashtag segments 
to enhance the segmentation result and extracts 
explicit candidate using a regular expression. To 
demonstrate the effectiveness of our hashtag seg-
mentation algorithm, the second comparison base-
line Rule directly uses ICTCLAS to segment the 
whole topic content and labels each word with its 
part-of-speech tag. The explicit candidates are ex-
tracted by using the same regular expression. 
The performance on candidate extraction is 
compared in Table 4. The second column shows 
the number of all extracted candidates for all the 
opinionated sentences by different methods. The 
third column shows the number of correct opinion 
targets among them. We can find that the two rule-
based models both outperform Berkeley Parser 
and our HS+Rule method finds 14% more correct 
opinion targets than Rule. It proves the effective-
ness of our hashtag segmentation algorithm. The 
Method Total Correct 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
Berkley 
Parser 
4554 877 0.36 0.56 
Rule 4105 918 0.37 0.56 
HS + Rule 4094 1042 0.41 0.58 
Table 4. Performance of candidate extraction and 
opinion target extraction 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1
Precision
Recall
F-Measure
pinj 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1 1.2 1.4
Precision
Recall
F-Measure
wi 
(a) Initial Candidate Weight        (b) Injection Probability 
Figure 2. Influence of the parameters 
 
1847
total number of candidates extracted by HS+Rule 
is also less than the other two methods. Therefore, 
the performance of label propagation will be im-
proved when there are fewer candidates to rank. It 
can be demonstrated by the F-measure of opinion 
target extraction in the fourth and fifth columns. 
The experiments are conducted on opinionated 
sentence only as above. By using HS+Rule to ex-
tract candidates, our label propagation algorithm 
gets the highest F-measure in both evaluation met-
rics.  
5.7 Performance on Pseudo Topics by Mes-
sage Clustering 
In our collective extraction algorithm, topics are 
directly identified by hashtags. For messages 
without hashtags, we can first employ clustering 
algorithms to obtain pseudo topics (clusters) and 
then exploiting the topic-oriented algorithm for 
collective opinion target extraction. To test the 
performance of the proposed method in such cir-
cumstance, we use the popular clustering algo-
rithm - Affinity Propagation (Frey and Dueck, 
2007) to generate topics. The experimental results 
are shown in Table 5. APCluster means that the 
messages are clustered after removing all the 
hashtags. APCluster+HS means that all the 
hashtags are retained as normal texts for calculat-
ing message similarity. Therefore, the clustering 
performance can be largely improved. The stand-
ard cosine similarity is used to measure the dis-
tance between microblog messages for Affinity 
Propagation in the above two methods. The last 
method denoted as GoldCluster directly uses 
hashtags to identify the gold-standard topics which 
shows the upper bound of the performance. After 
clustering microblogs, the opinion targets of mes-
sages in each cluster are collectively extracted by 
the proposed unsupervised label propagation algo-
rithm. The experiments are conducted on opinion-
ated sentences only. 
From the results, we can see that clustering mi-
croblogs without hashtags is a quite difficult job 
which only gets an F-Measure of 0.27. However, 
the corresponding opinion target extraction per-
formance is still promising, which outperforms the 
AssocMi and CRF-C methods in Table 3. With the 
help of hashtags, the clustering performance of 
APCluster+HS is largely improved and the opin-
ion target extraction performance is also increased. 
It outperforms all the baseline methods in Table 3. 
The above results reveal that our proposed unsu-
pervised label propagation algorithm works well 
in pseudo topics and the performance can be in-
creased with better clustering results. Therefore, 
we can try to incorporate other social network in-
formation to improve the message clustering per-
formance, which will be studied in our future 
work. 
6 Related Work 
Sentiment analysis, a.k.a. opinion mining, is the 
field of studying and analyzing people’s opinions, 
sentiments, evaluations, appraisals, attitudes, and 
emotions (Liu, 2012). Most of the previous senti-
ment analysis researches focus on customer re-
views (Pang et al., 2002; Hu and Liu, 2004) and 
some of them focus on news (Kim and Hovy, 
2006) and blogs (Draya et al., 2009). However, 
sentiment analysis on microblogs has recently at-
tracted much attention and has been proved to be 
very useful in many applications. 
Classification of opinion polarity is the most 
common task studied in microblogs. Go et.al 
(2009) follow the supervised machine learning 
approach of Pang et al. (2002) to classify the po-
larity of each tweet by distant supervision. The 
training dataset of their method is not manually 
labeled but automatically collected using the 
emoticons. Barbosa and Feng (2010) use the simi-
lar pseudo training data collected from three 
online websites which provide Twitter sentiment 
analysis services. Speriosu et al. (2009) explore 
the possibility of exploiting the Twitter follower 
graph to improve polarity classification.  
Opinion target extraction is a fine-grained 
word-level task of sentiment analysis. Currently, 
this task has not been well studied in microblogs 
yet. It is mostly performed on product reviews 
where opinion targets are always described as 
product features or aspects. The pioneering re-
search on this task is conducted by Hu and Liu 
Clustering Method 
F-Measure 
of Clustering 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
APCluster 0.27 0.35 0.50 
APCluster+HS 0.71 0.37 0.55 
GoldCluster 1.00 0.41 0.58 
Table 5.  Performance of clustering and opinion 
target extraction 
1848
(2004) who propose a method which extracts fre-
quent nouns and noun phrases as the opinion tar-
gets. Jakob and Gurevych (2010) model the 
problem as a sequence labeling task based on 
Conditional Random Fields (CRF). Qiu et al. 
(2011) propose a double propagation method to 
extract opinion word and opinion target simulta-
neously. Liu et al. (2012) use the word translation 
model in a monolingual scenario to mine the asso-
ciations between opinion targets and opinion 
words.  
7 Conclusion and Future Work  
In this paper, we study the problem of opinion 
target extraction in Chinese microblogs which has 
not been well investigated yet. We propose an un-
supervised label propagation algorithm to collec-
tively rank the opinion target candidates of all 
sentences in a topic. We also propose a dynamic 
programming based algorithm for segmenting 
Chinese hashtags. Experimental results show the 
effectiveness of our method. 
In future work, we will try to collect and anno-
tate data for microblogs in other languages to test 
the robustness of our method. The repost and reply 
messages can also be integrated into our graph 
model to help improve the results.  
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101).  
References  
Barbosa Luciano and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data. 
In Proceedings of the 23rd International Conference 
on Computational Linguistics: Posters. Association 
for Computational Linguistics, 2010. 
Johan Bollen, Huina Mao and Xiaojun Zeng. 2011. 
Twitter mood predicts the stock market. Journal of 
Computational Science 2.1 (2011): 1-8. 
Gérard Dray, Michel Plantié, Ali Harb, Pascal Poncelet, 
Mathieu Roche and François Trousset. 2009. Opin-
ion Mining from Blogs. In International Journal of 
Computer Informa-tion Systems and Industrial Man-
agement Applications. 
Brendan J. Frey and Delbert Dueck. 2007. "Clustering 
by passing messages between data points." Science 
315.5814 (2007): 972-976. 
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter 
sentiment classification using distant supervision. 
CS224N Project Report, Stanford (2009): 1-12. 
Minqing Hu and Bing Liu. Mining and summarizing 
customer reviews. 2004. In Proceedings of the tenth 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 168-177. 
ACM. 
Long Jiang , Mo Yu, Ming Zhou, Xiaohua Liu and 
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational 
Linguistics: Human Language Technologies, vol. 1, 
pp. 151-160. 
Niklas Jakob and Iryna Gurevych. Extracting opinion 
targets in a single-and cross-domain setting with 
conditional random fields. 2010. In Proceedings of 
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics. 
Richard Johansson and Alessandro Moschitti. 2010. 
Syntactic and semantic structure for opinion expres-
sion detection. Proceedings of the Fourteenth Con-
ference on Computational Natural Language 
Learning. Association for Computational Linguistics. 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders and Topics Expressed in 
Online News Media Text. In Proceedings of the 
ACL Workshop on Sentiment and Subjectivity in 
Text, 2006, pp. 1–8.  
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics, pages 410–419, Jeju, 
Republic of Korea, 8-14 July 2012. 
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, 
Anwitaman Datta, Aixin Sun and Bu-Sung Lee. 
2012b. Twiner: Named entity recognition in targeted 
twitter stream. In Proceedings of the 35th interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp. 721-730. 
ACM. 
Xiaohua Liu, Kuan Li, Ming Zhou and Zhongyang 
Xiong. 2011. Collective semantic role labeling for 
tweets with clustering. In Proceedings of the Twen-
ty-Second international joint conference on Artificial 
1849
Intelligence-Volume Volume Three, pp. 1832-1837. 
AAAI Press. 
Bing Liu. 2012. Sentiment analysis and opinion mining. 
Synthesis Lectures on Human Language Technolo-
gies 5.1 (2012): 1-167. 
Kang Liu, Liheng Xu and Jun Zhao. 2012. Opinion 
Target Extraction Using Word-Based Translation 
Model. In Proceedings of the 2012 Joint Conference 
on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings of the 
ACL-02 conference on Empirical methods in natural 
language processing-Volume 10, pp. 79-86. Associa-
tion for Computational Linguistics. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. Learning accurate, compact, and interpretable 
tree annotation. 2006. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics, pp. 433-440. 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion word expansion and target extraction 
through double propagation. Computational linguis-
tics 37, no. 1 (2011): 9-27. 
G. Salton, A. Wong and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing, Communica-
tions of the ACM, vol. 18, nr. 11, pages 613–620. 
J. F. da Silva and G. P. Lopes. 1999. A local maxima 
method and a fair dispersion normalization for ex-
tracting multi-word units from corpora. In Proc. of 
the 6th Meeting on Mathematics of Language . 
Michael Speriosu, Nikita Sudan, Sid Upadhyay and 
Jason Baldridge. 2011. Twitter polarity classification 
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First Workshop 
on Unsupervised Learning in NLP, pp. 53-63. Asso-
ciation for Computational Linguistics, 2011. 
Partha Talukdar and Koby Crammer. New regularized 
algorithms for transductive learning. 2009. Machine 
Learning and Knowledge Discovery in Databases 
(2009): 442-457. 
Jie Tang, Yuan Zhang, Jimeng Sun, Jinhai Rao, Wen-
jing Yu, Yiran Chen and A. C. M. Fong. 2012. 
Quantitative study of individual emotional states in 
social networks. Affective Computing, IEEE Trans-
actions on 3, no. 2 (2012): 132-144. 
Andranik Tumasjan, Timm O. Sprenger, Philipp G. 
Sandner and Isabell M. Welpe. 2010. Predicting 
elections with twitter: What 140 characters reveal 
about political sentiment. In Proceedings of the 
fourth international aaai conference on weblogs and 
social media, pp. 178-185. 
X. Zhu and Z. Ghahramani. 2002. Learning from la-
beled and unlabeled data with label propagation. 
Technical report, CMU CALD tech report. 
Li Zhuang, Feng Jing and Xiaoyan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the ACM 15th Conference on Information and 
Knowledge Management, pages 43–50, Arlington, 
Virginia, USA, November. 
 
1850
