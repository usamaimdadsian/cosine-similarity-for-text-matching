Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1319–1328,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Low-dimensional Embeddings for Interpretable
Anchor-based Topic Inference
Moontae Lee
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
moontae@cs.cornell.edu
David Mimno
Dept. of Information Science
Cornell University
Ithaca, NY, 14853
mimno@cornell.edu
Abstract
The anchor words algorithm performs
provably efficient topic model inference
by finding an approximate convex hull
in a high-dimensional word co-occurrence
space. However, the existing greedy al-
gorithm often selects poor anchor words,
reducing topic quality and interpretability.
Rather than finding an approximate con-
vex hull in a high-dimensional space, we
propose to find an exact convex hull in
a visualizable 2- or 3-dimensional space.
Such low-dimensional embeddings both
improve topics and clearly show users why
the algorithm selects certain words.
1 Introduction
Statistical topic modeling is useful in exploratory
data analysis (Blei et al., 2003), but model infer-
ence is known to be NP-hard even for the sim-
plest models with only two topics (Sontag and
Roy, 2011), and training often remains a black
box to users. Likelihood-based training requires
expensive approximate inference such as varia-
tional methods (Blei et al., 2003), which are deter-
ministic but sensitive to initialization, or Markov
chain Monte Carlo (MCMC) methods (Griffiths
and Steyvers, 2004), which have no finite conver-
gence guarantees. Recently Arora et al. proposed
the Anchor Words algorithm (Arora et al., 2013),
which casts topic inference as statistical recovery
using a separability assumption: each topic has
a specific anchor word that appears only in the
context of that single topic. Each anchor word
can be used as a unique pivot to disambiguate the
corresponding topic distribution. We then recon-
struct the word co-occurrence pattern of each non-
anchor words as a convex combination of the co-
occurrence patterns of the anchor words.
burgersaladpizza
chickengood
told
popcornstadiumviewstiremoviesscreen
sashimi
carcalledhotel yoga
bagels
shoppingdog movie
Figure 1: 2D t-SNE projection of a Yelp review
corpus and its convex hull. The words corre-
sponding to vertices are anchor words for topics,
whereas non-anchor words correspond to the inte-
rior points.
This algorithm is fast, requiring only one pass
through the training documents, and provides
provable guarantees, but results depend entirely on
selecting good anchor words. (Arora et al., 2013)
propose a greedy method that finds an approxi-
mate convex hull around a set of vectors corre-
sponding to the word co-occurrence patterns for
each vocabulary word. Although this method is
an improvement over previous work that used im-
practical linear programming methods (Arora et
al., 2012), serious problems remain. The method
greedily chooses the farthest point from the cur-
rent subspace until the given number of anchors
have been found. Particularly at the early stages
1319
of the algorithm, the words associated with the
farthest points are likely to be infrequent and id-
iosyncratic, and thus form poor bases for human
interpretation and topic recovery. This poor choice
of anchors noticeably affects topic quality: the an-
chor words algorithm tends to produce large num-
bers of nearly identical topics.
Besides providing a separability criterion, an-
chor words also have the potential to improve topic
interpretability. After learning topics for given text
collections, users often request a label that sum-
marizes each topic. Manually labeling topics is ar-
duous, and labels often do not carry over between
random initializations and models with differing
numbers of topics. Moreover, it is hard to con-
trol the subjectivity in labelings between annota-
tors, which is open to interpretive errors. There
has been considerable interest in automating the
labeling process (Mei et al., 2007; Lau et al., 2011;
Chuang et al., 2012). (Chuang et al., 2012) pro-
pose a measure of saliency: a good summary term
should be both distinctive specifically to one topic
and probable in that topic. Anchor words are by
definition optimally distinct, and therefore may
seem to be good candidates for topic labels, but
greedily selecting extreme words often results in
anchor words that have low probability.
In this work we explore the opposite of Arora et
al.’s method: rather than finding an approximate
convex hull for an exact set of vectors, we find an
exact convex hull for an approximate set of vec-
tors. We project the V × V word co-occurrence
matrix to visualizable 2- and 3-dimensional spaces
using methods such as t-SNE (van der Maaten and
Hinton, 2008), resulting in an input matrix up to
3600 times narrower than the original input for
our training corpora. Despite this radically low-
dimensional projection, the method not only finds
topics that are as good or better than the greedy
anchor method, it also finds highly salient, in-
terpretable anchor words and provides users with
a clear visual explanation for why the algorithm
chooses particular words, all while maintaining
the original algorithm’s computational benefits.
2 Related Work
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) models D documents with a vocabulary V
using a predefined number of topics by K. LDA
views both {A
k
}
K
k=1
, a set of K topic-word distri-
butions for each topic k, and {W
d
}
D
d=1
, a set of D
document-topic distributions for each document d,
and {z
d
}
D
d=1
, a set of topic-assignment vectors for
word tokens in the document d, as randomly gen-
erated from known stochastic processes. Merging
{A
k
} as k-th column vector of V ×K matrix A,
{W
d
} as d-th column vector of K ×D matrix W ,
the learning task is to estimate the posterior dis-
tribution of latent variables A, W , and {z
d
} given
V × D word-document matrix
ˆ
M , which is the
only observed variable where d-th column corre-
sponds to the empirical word frequencies in the
training documents d.
(Arora et al., 2013) recover word-topic matrix
A and topic-topic matrix R = E[WW
T
] instead
of W in the spirit of nonnegative matrix factoriza-
tion. Though the true underlying word distribu-
tion for each document is unknown and could be
far from the sample observation
ˆ
M , the empirical
word-word matrix
ˆ
Q converges to its expectation
AE[WW
T
]A
T
= ARA
T
as the number of docu-
ments increases. Thus the learning task is to ap-
proximately recover A and R pretending that the
empirical
ˆ
Q is close to the true second-order mo-
ment matrix Q.
The critical assumption for this method is to
suppose that every topic k has a specific anchor
word s
k
that occurs with non-negligible probabil-
ity (> 0) only in that topic. The anchor word s
k
need not always appear in every document about
the topic k, but we can be confident that the doc-
ument is at least to some degree about the topic k
if it contains s
k
. This assumption drastically im-
proves inference by guaranteeing the presence of
a diagonal sub-matrix inside the word-topic ma-
trix A. After constructing an estimate
ˆ
Q, the al-
gorithm in (Arora et al., 2013) first finds a set
S = {s
1
, ..., s
K
} of K anchor words (K is user-
specified), and recovers A and R subsequently
based on S. Due to this structure, overall perfor-
mance depends heavily on the quality of anchor
words.
In the matrix algebra literature this greedy
anchor finding method is called QR with row-
pivoting. Previous work classifies a matrix into
two sets of row (or column) vectors where the vec-
tors in one set can effectively reconstruct the vec-
tors in another set, called subset-selection algo-
rithms. (Gu and Eisenstat, 1996) suggest one im-
portant deterministic algorithm. A randomized al-
gorithm provided by (Boutsidis et al., 2009) is the
state-of-the art using a pre-stage that selects the
1320
candidates in addition to (Gu and Eisenstat, 1996).
We found no change in anchor selection using
these algorithms, verifying the difficulty of the an-
chor finding process. This difficulty is mostly be-
cause anchors must be nonnegative convex bases,
whereas the classified vectors from the subset se-
lection algorithms yield unconstrained bases.
The t-SNE model has previously been used to
display high-dimensional embeddings of words in
2D space by Turian.
1
Low-dimensional embed-
dings of topic spaces have also been used to sup-
port user interaction with models: (Eisenstein et
al., 2011) use a visual display of a topic embed-
ding to create a navigator interface. Although
t-SNE has been used to visualize the results of
topic models, for example by (Lacoste-Julien et
al., 2008) and (Zhu et al., 2009), we are not aware
of any use of the method as a fundamental compo-
nent of topic inference.
3 Low-dimensional Embeddings
Real text corpora typically involve vocabularies in
the tens of thousands of distinct words. As the
input matrix
ˆ
Q scales quadratically with V , the
Anchor Words algorithm must depend on a low-
dimensional projection of
ˆ
Q in order to be practi-
cal. Previous work (Arora et al., 2013) uses ran-
dom projections via either Gaussian random ma-
trices (Johnson and Lindenstrauss, 1984) or sparse
random matrices (Achlioptas, 2001), reducing the
representation of each word to around 1,000 di-
mensions. Since the dimensionality of the com-
pressed word co-occurrence space is an order of
magnitude larger than K, we must still approxi-
mate the convex hull by choosing extreme points
as before.
In this work we explore two projection meth-
ods: PCA and t-SNE (van der Maaten and Hinton,
2008). Principle Component Analysis (PCA) is a
commonly-used dimensionality reduction scheme
that linearly transforms the data to new coordi-
nates where the largest variances are orthogonally
captured for each dimension. By choosing only a
few such principle axes, we can represent the data
in a lower dimensional space. In contrast, t-SNE
embedding performs a non-linear dimensionality
reduction preserving the local structures. Given a
set of points {x
i
} in a high-dimensional space X ,
t-SNE allocates probability mass for each pair of
points so that pairs of similar (closer) points be-
1
http://metaoptimize.com/projects/wordreprs/
good
placegreatlove
chicken
store
riceshowerresponseto-gobroccoliteriyakiyogasalonlettuce
Figure 2: 2D PCA projections of a Yelp review
corpus and its convex hulls.
come more likely to co-occur than dissimilar (dis-
tant) points.
p
j|i
=
exp(?d(x
i
,x
j
)
2
/2?
2
i
)
?
k 6=i
exp(?d(x
i
,x
k
)
2
/2?
2
i
)
(1)
p
ij
=
p
j|i
+ p
i|j
2N
(symmetrized) (2)
Then it generates a set of new points {y
i
} in
low-dimensional space Y so that probability dis-
tribution over points in Y behaves similarly to the
distribution over points in X by minimizing KL-
divergence between two distributions:
q
ij
=
(1 + ?y
i
? y
j
?
2
)
?1
?
k 6=l
(1 + ?y
k
? y
l
?
2
)
?1
(3)
min KL(P ||Q) =
?
i 6=j
p
ij
log
p
ij
q
ij
(4)
Instead of approximating a convex hull in such
a high-dimensional space, we select the exact
vertices of the convex hull formed in a low-
dimensional projected space, which can be calcu-
lated efficiently. Figures 1 and 2 show the con-
vex hulls for 2D projections of
ˆ
Q using t-SNE and
PCA for a corpus of Yelp reviews. Figure 3 il-
lustrates the convex hulls for 3D t-SNE projection
for the same corpus. Anchor words correspond to
the vertices of these convex hulls. Note that we
present the 2D projections as illustrative examples
only; we find that three dimensional projections
perform better in practice.
1321
staffatmospherelovebarbeerhourchickenlocationfoodhighlywinemexicanyearskidsmusic screenshop
rice
roomsmanagercalled
bbq dimshowergroundgroup
cheesecake
tires
bagelssashimiwaiter
enchilada
specialshotelbeers
donuts
glass
yoga
peaks
cupcakes
divemoviedog
cookiechorizo
starbucks
shopping
hummus
play hair
bottleprompt
Figure 3: 3D t-SNE projection of a Yelp review
corpus and its convex hull. Vertices on the convex
hull correspond to anchor words.
In addition to the computational advantages,
this approach benefits anchor-based topic model-
ing in two aspects. First, as we now compute the
exact convex hull, the number of topics depends
on the dimensionality of the embedding, v. For
example in the figures, 2D projection has 21 ver-
tices, whereas 3D projection supports 69 vertices.
This implies users can easily tune the granularity
of topic clusters by varying v = 2, 3, 4, ... with-
out increasing the number of topics by one each
time. Second, we can effectively visualize the the-
matic relationships between topic anchors and the
rest of words in the vocabulary, enhancing both
interpretability and options for further vocabulary
curation.
4 Experimental Results
We find that radically low-dimensional t-SNE pro-
jections are effective at finding anchor words that
are much more salient than the greedy method, and
topics that are more distinctive, while maintain-
ing comparable held-out likelihood and semantic
coherence. As noted in Section 1, the previous
greedy anchor words algorithm tends to produce
many nearly identical topics. For example, 37 out
of 100 topics trained on a 2008 political blog cor-
pus have obama, mccain, bush, iraq or palin as
their most probable word, including 17 just for
obama. Only 66% of topics have a unique top
word. In contrast, the t-SNE model on the same
dataset has only one topic whose most probable
word is obama, and 86% of topics have a unique
top word (mccain is the most frequent top word,
with five topics).
We use three real datasets: business reviews
from the Yelp Academic Dataset,
2
political blogs
from the 2008 US presidential election (Eisen-
stein and Xing, 2010), and New York Times ar-
ticles from 2007.
3
Details are shown in Table
1. Documents with fewer than 10 word tokens
are discarded due to possible instability in con-
structing
ˆ
Q. We perform minimal vocabulary cu-
ration, eliminating a standard list of English stop-
words
4
and terms that occur below frequency cut-
offs: 100 times (Yelp, Blog) and 150 times (NYT).
We further restrict possible anchor words to words
that occur in more than three documents. As our
datasets are not artificially synthesized, we reserve
5% from each set of documents for held-out like-
lihood computation.
Name Documents Vocab. Avg. length
Yelp 20,000 1,606 40.6
Blog 13,000 4,447 161.3
NYT 41,000 10,713 277.8
Table 1: Statistics for datasets used in experiments
Unlike (Arora et al., 2013), which presents
results on synthetic datasets to compare perfor-
mance across different recovery methods given in-
creasing numbers of documents, we are are inter-
ested in comparing anchor finding methods, and
are mainly concerned with semantic quality. As
a result, although we have conducted experiments
on synthetic document collections,
5
we focus on
real datasets for this work. We also choose to com-
pare only anchor finding algorithms, so we do not
report comparisons to likelihood-based methods,
which can be found in (Arora et al., 2013).
For both PCA and t-SNE, we use three-
dimensional embeddings across all experiments.
This projection results in matrices that are 0.03%
as wide as the original V × V matrix for the
NYT dataset. Without low-dimensional embed-
ding, each word is represented by a V-dimensional
vector where only several terms are non-zero due
to the sparse co-occurrence patterns. Thus a ver-
2
https://www.yelp.com/academic dataset
3
http://catalog.ldc.upenn.edu/LDC2008T19
4
We used the list of 524 stop words included in the Mallet
library.
5
None of the algorithms are particularly effective at find-
ing synthetically introduced anchor words possibly because
there are other candidates around anchor vertices that approx-
imate the convex hull to almost the same degree.
1322
tex captured by the greedy anchor-finding method
is likely to be one of many eccentric vertices in
very high-dimensional space. In contrast, t-SNE
creates an effective dense representation where a
small number of pivotal vertices are more clearly
visible, improving both performance and inter-
pretability.
Note that since we can find an exact convex hull
in these spaces,
6
there is an upper bound to the
number of topics that can be found given a partic-
ular projection. If more topics are desired, one can
simply increase the dimensionality of the projec-
tion. For the greedy algorithm we use sparse ran-
dom projections with 1,000 dimensions with 5%
negative entries and 5% positive entries. PCA and
t-SNE choose (49, 32, 47) and (69, 77, 107) an-
chors, respectively for each of three Yelp, Blog,
and NYTimes datasets.
4.1 Anchor-word Selection
We begin by comparing the behavior of low-
dimensional embeddings to the behavior of the
standard greedy algorithm. Table 2 shows ordered
lists of the first 12 anchor words selected by three
algorithms: t-SNE embedding, PCA embedding,
and the original greedy algorithm. Anchor words
selected by t-SNE (police, business, court) are
more general than anchor words selected by the
greedy algorithm (cavalry, al-sadr, yiddish). Ad-
ditional examples of anchor words and their asso-
ciated topics are shown in Table 3 and discussed
in Section 4.2.
# t-SNE PCA Greedy
1 police beloved cavalry
2 bonds york biodiesel
3 business family h/w
4 day loving kingsley
5 initial late mourners
6 million president pcl
7 article people carlisle
8 wife article al-sadr
9 site funeral kaye
10 mother million abc’s
11 court board yiddish
12 percent percent great-grandmother
Table 2: The first 12 anchor words selected by
three algorithms for the NYT corpus.
The Gram-Schimdt process used by Arora et
al. greedily selects anchors in high-dimensional
space. As each word is represented within V -
6
In order to efficiently find an exact convex hull, we use
the Quickhull algorithm.
Type # HR Top Words (Yelp)
t-SNE 16 0 mexican good service great eat restaurant authentic delicious
PCA 15 0 mexican authentic eat chinese don’t restaurant fast salsa
Greedy 34 35 good great food place service restaurant it’s mexican
t-SNE 6 0 beer selection good pizza great wings tap nice
PCA 39 6 wine beer selection nice list glass wines bar
Greedy 99 11 beer selection great happy place wine good bar
t-SNE 3 0 prices great good service selection price nice quality
PCA 12 0 atmosphere prices drinks friendly selection nice beer ambiance
Greedy 34 35 good great food place service restaurant it’s mexican
t-SNE 10 0 chicken salad good lunch sauce ordered fried soup
PCA 10 0 chicken salad lunch fried pita time back sauce
Greedy 69 12 chicken rice sauce fried ordered i’m spicy soup
Type # HR Top Words (Blog)
t-SNE 10 0 hillary clinton campaign democratic bill party win race
PCA 4 0 hillary clinton campaign democratic party bill democrats vote
Greedy 45 19 obama hillary campaign clinton obama’s barack it’s democratic
t-SNE 3 0 iraq war troops iraqi mccain surge security american
PCA 9 1 iraq iraqi war troops military forces security american
Greedy 91 8 iraq mccain war bush troops withdrawal obama iraqi
t-SNE 9 0 allah muhammad qur verses unbelievers ibn muslims world
PCA 18 14 allah muhammad qur verses unbelievers story time update
Greedy 4 5 allah muhammad people qur verses unbelievers ibn sura
t-SNE 19 0 catholic abortion catholics life hagee time biden human
PCA 2 0 people it’s time don’t good make years palin
Greedy 40 1 abortion parenthood planned people time state life government
Type # HR Top Words (NYT)
t-SNE 0 0 police man yesterday officers shot officer year-old charged
PCA 6 0 people it’s police way those three back don’t
Greedy 50 198 police man yesterday officers officer people street city
t-SNE 19 0 senator republican senate democratic democrat state bill
PCA 33 2 state republican republicans senate senator house bill party
Greedy 85 33 senator republican president state campaign presidential people
t-SNE 2 0 business chief companies executive group yesterday billion
PCA 21 0 billion companies business deal group chief states united
Greedy 55 10 radio business companies percent day music article satellite
t-SNE 14 0 market sales stock companies prices billion investors price
PCA 11 0 percent market rate week state those increase high
Greedy 77 44 companies percent billion million group business chrysler people
Table 3: Example t-SNE topics and their most
similar topics across algorithms. The Greedy algo-
rithm can find similar topics, but the anchor words
are much less salient.
dimensions, finding the word that has the next
most distinctive co-occurrence pattern tends to
prefer overly eccentric words with only short, in-
tense bursts of co-occurring words. While the
bases corresponding to these anchor words could
be theoretically relevant for the original space in
high-dimension, they are less likely to be equally
important in low-dimensional space. Thus project-
ing down to low-dimensional space can rearrange
the points emphasizing not only uniqueness, but
also longevity, achieving the ability to form mea-
surably more specific topics.
Concretely, neither cavalry, al-sadr, yiddish nor
police, business, court are full representations of
New York Times articles, but the latter is a much
better basis than the former due to its greater gen-
erality. We see the effect of this difference in the
specificity of the resulting topics (for example in
17 obama topics). Most words in the vocabulary
have little connection to the word cavalry, so the
probability p(z|w) does not change much across
different w. When we convert these distributions
into P (w|z) using the Bayes’ rule, the resulting
topics are very close to the corpus distribution, a
1323
unigram distribution p(w).
p(w|z = k
cavalry
) ? p(z = k
cavalry
|w)p(w)
? p(w)
This lack of specificity results in the observed sim-
ilarity of topics.
4.2 Quantitative Results
In this section we compare PCA and t-SNE pro-
jections to the greedy algorithm along several
quantitative metrics. To show the effect of dif-
ferent values of K, we report results for varying
numbers of topics. As the anchor finding algo-
rithms are deterministic, the anchor words in a K-
dimensional model are identical to the first K an-
chor words in a (K + 1)-dimensional model. For
the greedy algorithm we select anchor words in
the order they are chosen. For the PCA and t-
SNE methods, which find anchors jointly, we sort
words in descending order by their distance from
their centroid.
Recovery Error. Each non-anchor word is ap-
proximated by a convex combination of the K
anchor words. The projected gradient algorithm
(Arora et al., 2013) determines these convex coef-
ficients so that the gap between the original word
vector and the approximation becomes minimized.
As choosing a good basis of K anchor words de-
creases this gap, Recovery Error (RE) is defined
by the average `
2
-residuals across all words.
RE =
1
V
V
?
i=1
?
¯
Q
i
?
K
?
k=1
p(z
1
= k|w
1
= i)
¯
Q
S
k
?
2
(5)
Recovery error decreases with the number of top-
Yelp Blog NYTimes
0.00
0.01
0.02
0.03
0.04
0.05
0 30 60 90 0 30 60 90 0 30 60 90Topics
Rec
ove
ry AlgorithmGreedy
PCA
tSNE
Figure 4: Recovery error is similar across algo-
rithms.
ics, and improves substantially after the first 10–15
anchor words for all methods. The t-SNE method
has slightly better performance than the greedy al-
gorithm, but they are similar. Results for recovery
with the original, unprojected matrix (not shown)
are much worse than the other algorithms, sug-
gesting that the initial anchor words chosen are es-
pecially likely to be uninformative.
Normalized Entropy. As shown previously, if
the probability of topics given a word is close to
uniform, the probability of that word in topics will
be close to the corpus distribution. Normalized
Entropy (NE) measures the entropy of this distri-
bution relative to the entropy of a K-dimensional
uniform distribution:
NE =
1
V
V
?
i=1
H(z|w = i)
logK
. (6)
The normalized entropy of topics given word dis-
Yelp Blog NYTimes
0.25
0.50
0.75
1.00
0 30 60 90 0 30 60 90 0 30 60 90Topics
Nor
ma
lize
dEn
trop
y
Algorithm
Greedy
PCA
tSNE
Figure 5: Words have higher topic entropy in the
greedy model, especially in NYT, resulting in less
specific topics.
tributions usually decreases as we add more top-
ics, although both t-SNE and PCA show a dip in
entropy for low numbers of topics. This result in-
dicates that words become more closely associated
with particular topics as we increase the number of
topics. The low-dimensional embedding methods
(t-SNE and PCA) have consistently lower entropy.
Topic Specificity and Topic Dissimilarity. We
want topics to be both specific (that is, not overly
general) and different from each other. When there
are insufficient number of topics, p(w|z) often re-
sembles the corpus distribution p(w), where high
frequency terms become the top words contribut-
ing to most topics. Topic Specificity (TS) is de-
fined by the average KL divergence from each
topic’s conditional distribution to the corpus dis-
tribution.
7
TS =
1
K
K
?
k=1
KL
(
p(w|z = k) || p(w)
)
(7)
7
We prefer specificity to (AlSumait et al., 2009)’s term
vacuousness because the metric increases as we move away
from the corpus distribution.
1324
One way to define the distance between multiple
points is the minimum radius of a ball that cov-
ers every point. Whereas this is simply the dis-
tance form the centroid to the farthest point in
the Euclidean space, it is an itself difficult opti-
mization problem to find such centroid of distri-
butions under metrics such as KL-divergence and
Jensen-Shannon divergence. To avoid this prob-
lem, we measure Topic Dissimilarity (TD) view-
ing each conditional distribution p(w|z) as a sim-
ple V -dimensional vector in R
V
. Recall a
ik
=
p(w = i|z = k),
TD = max
1?k?K
?
1
K
K
?
k
?
=1
a
?k
?
? a
?k
?
2
. (8)
Specificity and dissimilarity increase with the
Yelp Blog NYTimes
0.0
0.5
1.0
1.5
2.0
0 30 60 90 0 30 60 90 0 30 60 90Topics
Spe
cific
ity AlgorithmGreedy
PCA
tSNE
Yelp Blog NYTimes
0.0
0.2
0.4
0.6
0 30 60 90 0 30 60 90 0 30 60 90Topics
Dis
sim
ilar
ity AlgorithmGreedy
PCA
tSNE
Figure 6: Greedy topics look more like the corpus
distribution and more like each other.
number of topics, suggesting that with few anchor
words, the topic distributions are close to the over-
all corpus distribution and very similar to one an-
other. The t-SNE and PCA algorithms produce
consistently better specificity and dissimilarity, in-
dicating that they produce more useful topics early
with small numbers of topics. The greedy algo-
rithm produces topics that are closer to the corpus
distribution and less distinct from each other (17
obama topics).
Topic Coherence is known to correlate with the
semantic quality of topic judged by human anno-
tators (Mimno et al., 2011). LetW
(T )
k
be T most
probable words (i.e., top words) for the topic k.
TC =
?
w
1
6=w
2
?W
(T )
k
log
D(w
1
, w
2
) + 
D(w
1
)
(9)
Here D(w
1
, w
2
) is the co-document frequency,
which is the number of documents inD consisting
of two words w
1
and w
2
simultaneously. D(w)
is the simple document frequency with the word
w. The numerator contains smoothing count 
in order to avoid taking the logarithm of zero.
Coherence scores for t-SNE and PCA are worse
Yelp Blog NYTimes
?600
?550
?500
?450
?400
0 30 60 90 0 30 60 90 0 30 60 90Topics
Coh
ere
nce AlgorithmGreedy
PCA
tSNE
Figure 7: The greedy algorithm creates more co-
herent topics (higher is better), but at the cost of
many overly general or repetitive topics.
than those for the greedy method, but this result
must be understood in combination with the Speci-
ficity and Dissimilarity metrics. The most frequent
terms in the overall corpus distribution p(w) often
appear together in documents. Thus a model creat-
ing many topics similar to the corpus distribution
is likely to achieve high Coherence, but low Speci-
ficity by definition.
Saliency. (Chuang et al., 2012) define saliency
for topic words as a combination of distinctive-
ness and probability within a topic. Anchor words
are distinctive by construction, so we can increase
saliency by selecting more probable anchor words.
We measure the probability of anchor words in
two ways. First, we report the zero-based rank of
anchor words within their topics. Examples of this
metric, which we call “hard” rank are shown in Ta-
ble 3. The hard rank of the anchors in the PCA and
t-SNE models are close to zero, while the anchor
words for the greedy algorithm are much lower
ranked, well below the range usually displayed to
users. Second, while hard rank measures the per-
ceived difference in rank of contributing words,
position may not fully capture the relative impor-
tance of the anchor word. “Soft” rank quantifies
the average log ratio between probabilities of the
1325
prominent word w
?
k
and the anchor word s
k
.
SR =
1
K
K
?
k=1
log
p(w = w
?
k
|z = k)
p(w = s
k
|z = k)
(10)
Yelp Blog NYTimes
0
1
2
3
4
0 30 60 90 0 30 60 90 0 30 60 90Topics
Sof
tAn
cho
rRa
nk Algorithm
Greedy
PCA
tSNE
Figure 8: Anchor words have higher probability,
and therefore greater salience, in t-SNE and PCA
models (1 ? one third the probability of the top
ranked word).
Lower values of soft rank (Fig. 8 indicate that
the anchor word has greater relative probability to
occur within a topic. As we increase the num-
ber of topics, anchor words become more promi-
nent in topics learned by the greedy method, but
t-SNE anchor words remain relatively more prob-
able within their topics as measured by soft rank.
Held-out Probability. Given an estimate of
the topic-word matrix A, we can compute the
marginal probability of held-out documents under
that model. We use the left-to-right estimator in-
troduced by (Wallach et al., 2009), which uses a
sequential algorithm similar to a Gibbs sampler.
This method requires a smoothing parameter for
document-topic Dirichlet distributions, which we
set to ?
k
= 0.1. We note that the greedy algo-
Yelp Blog NYTimes
?6.65
?6.60
?6.55
?7.70
?7.65
?7.60
?7.55
?7.50
?7.45
?8.4
?8.3
?8.2
?8.1
0 25 50 75 100 0 25 50 75 100 0 30 60 90Topics
Hel
dOu
tLL AlgorithmGreedy
PCA
tSNE
Figure 9: t-SNE topics have better held-out prob-
ability than greedy topics.
rithm run on the original, unprojected matrix has
better held-out probability values than t-SNE for
the Yelp corpus, but as this method does not scale
to realistic vocabularies we compare here to the
sparse random projection method used in (Arora
et al., 2013). The t-SNE method appears to do
best, particularly in the NYT corpus, which has a
larger vocabulary and longer training documents.
The length of individual held-out documents has
no correlation with held-out probability.
We emphasize that Held-out Probability is sen-
sitive to smoothing parameters and should only be
used in combination with a range of other topic-
quality metrics. In initial experiments, we ob-
served significantly worse held-out performance
for the t-SNE algorithm. This phenomenon was
because setting the probability of anchor words to
zero for all but their own topics led to large neg-
ative values in held-out log probability for those
words. As t-SNE tends to choose more frequent
terms as anchor words, these “spikes” significantly
affected overall probability estimates. To make the
calculation more fair, we added 10
?5
to any zero
entries for anchor words in the topic-word matrix
A across all models and renormalized.
Because t-SNE is a stochastic model, different
initializations can result in different embeddings.
To evaluate how steady anchor word selection is,
we ran five random initializations for each dataset.
For the Yelp dataset, the number of anchor words
varies from 59 to 69, and 43 out of those are shared
across at least four trials. For the Blog dataset, the
number of anchor words varies from 80 to 95, with
56 shared across at least four trials. For the NYT
dataset, this number varies between 83 and 107,
with 51 shared across at least four models.
4.3 Qualitative Results
Table 3 shows topics trained by three methods (t-
SNE, PCA, and greedy) for all three datasets. For
each model, we select five topics at random from
the t-SNE model, and then find the closest topic
from each of the other models. If anchor words
present in the top eight words, they are shown in
boldface.
A fundamental difference between anchor-
based inference and traditional likelihood-based
inference is that we can give an order to top-
ics according to their contribution to word co-
occurrence convex hull. This order is intrinsic to
the original algorithm, and we heuristically give
orders to t-SNE and PCA based on their contri-
butions. This order is listed as # in the previous
table. For all but one topic, the closest topic from
the greedy model has a higher order number than
1326
the associated t-SNE topic. As shown above, the
standard algorithm tends to pick less useful anchor
words at the initial stage; only the later, higher or-
dered topics are specific.
The most clear distinction between models is
the rank of anchor words represented by Hard
Rank for each topic. Only one topic correspond-
ing to (initial) has the anchor word which does
not coincide with the top-ranked word. For the
greedy algorithm, anchor words are often tens of
words down the list in rank, indicating that they
are unlikely to find a connection to the topic’s se-
mantic core. In cases where the anchor word is
highly ranked (unbelievers, parenthood) the word
is a good indicator of the topic, but still less deci-
sive.
t-SNE and PCA are often consistent in their se-
lection of anchor words, which provides useful
validation that low-dimensional embeddings dis-
cern more relevant anchor words regardless of lin-
ear vs non-linear projections. Note that we are
only varying the anchor selection part of the An-
chor Words algorithm in these experiments, recov-
ering topic-word distributions in the same manner
given anchor words. As a result, any differences
between topics with the same anchor word (for ex-
ample chicken) are due to the difference in either
the number of topics or the rest of anchor words.
Since PCA suffers from a crowding problem in
lower-dimensional projection (see Figure 2) and
the problem could be severe in a dataset with a
large vocabulary, t-SNE is more likely to find the
proper number of anchors given a specified granu-
larity.
5 Conclusion
One of the main advantages of the anchor words
algorithm is that the running time is largely inde-
pendent of corpus size. Adding more documents
would not affect the size of the co-occurrence ma-
trix, requiring more times to construct the co-
occurrence matrix at the beginning. While the
inference is scalable depending only on the size
of the vocabulary, finding quality anchor words is
crucial for the performance of the inference.
(Arora et al., 2013) presents a greedy anchor
finding algorithm that improves over previous lin-
ear programming methods, but finding quality an-
chor words remains an open problem in spec-
tral topic inference. We have shown that previ-
ous approaches have several limitations. Exhaus-
tively finding anchor words by eliminating words
that are reproducible by other words (Arora et
al., 2012) is impractical. The anchor words se-
lected by the greedy algorithm are overly eccen-
tric, particularly at the early stages of the algo-
rithm, causing topics to be poorly differentiated.
We find that using low-dimensional embeddings
of word co-occurrence statistics allows us to ap-
proximate a better convex hull. The resulting
anchor words are highly salient, being both dis-
tinctive and probable. The models trained with
these words have better quantitative and qualita-
tive properties along various metrics. Most im-
portantly, using radically low-dimensional projec-
tions allows us to provide users with clear visual
explanations for the model’s anchor word selec-
tions.
An interesting property of using low-
dimensional embeddings is that the number
of topics depends only on the projecting dimen-
sion. Since we can efficiently find an exact convex
hull in low-dimensional space, users can achieve
topics with their preferred level of granularities
by changing the projection dimension. We do
not insist this is the “correct” number of topics
for a corpus, but this method, along with the
range of metrics described in this paper, provides
users with additional perspective when choosing a
dimensionality that is appropriate for their needs.
We find that the t-SNE method, besides its
well-known ability to produce high quality lay-
outs, provides the best overall anchor selection
performance. This method consistently selects
higher-frequency terms as anchor words, resulting
in greater clarity and interpretability. Embeddings
with PCA are also effective, but they result in less
well-formed spaces, being less effective in held-
out probability for sufficiently large corpora.
Anchor word finding methods based on low-
dimensional projections offer several important
advantages for topic model users. In addition to
producing more salient anchor words that can be
used effectively as topic labels, the relationship of
anchor words to a visualizable word co-occurrence
space offers significant potential. Users who can
see why the algorithm chose a particular model
will have greater confidence in the model and in
any findings that result from topic-based analy-
sis. Finally, visualizable spaces offer the poten-
tial to produce interactive environments for semi-
supervised topic reconstruction.
1327
Acknowledgments
We thank David Bindel and the anonymous re-
viewers for their valuable comments and sugges-
tions, and Laurens van der Maaten for providing
his t-SNE implementation.
References
Dimitris Achlioptas. 2001. Database-friendly random
projections. In SIGMOD, pages 274–281.
Loulwah AlSumait, Daniel Barbar, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance
ranking of lda generative models. In ECML.
S. Arora, R. Ge, and A. Moitra. 2012. Learning topic
models – going beyond svd. In FOCS.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
pages 993–1022. Preliminary version in NIPS 2001.
Christos Boutsidis, Michael W. Mahoney, and Petros
Drineas. 2009. An improved approximation algo-
rithm for the column subset selection problem. In
SODA, pages 968–977.
Jason Chuang, Christopher D. Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques
for assessing textual topic models. In International
Working Conference on Advanced Visual Interfaces
(AVI), pages 74–77.
Jacob Eisenstein and Eric Xing. 2010. The CMU
2008 political blog corpus. Technical report, CMU,
March.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and
Eric P. Xing. 2011. Topicviz: Semantic navigation
of document collections. CoRR, abs/1110.6200.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101:5228–5235.
Ming Gu and Stanley C. Eisenstat. 1996. Efficient
algorithms for computing a strong rank-revealing qr
factorization. In SIAM J. Sci Comput, pages 848–
869.
William B. Johnson and Joram Lindenstrauss. 1984.
Extensions of lipschitz mappings into a hilbert
space. Contemporary Mathematics, 26:189–206.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In NIPS.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In HLT, pages 1536–1545.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD, pages 490–499.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP.
D. Sontag and D. Roy. 2011. Complexity of inference
in latent dirichlet allocation. In NIPS, pages 1008–
1016.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visu-
alizing high-dimensional data using t-SNE. JMLR,
9:2579–2605, Nov.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum margin supervised topic mod-
els for regression and classication. In ICML.
1328
