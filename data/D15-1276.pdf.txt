Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2292–2297,
Lisbon, Portugal, 17-21 September 2015.
c©2015 Association for Computational Linguistics.
Morphological Analysis for Unsegmented Languages
using Recurrent Neural Network Language Model
Hajime Morita
1,2
Daisuke Kawahara
1
Sadao Kurohashi
1,2
1
Kyoto University
2
CREST, Japan Science and Technology Agency
{hmorita, dk, kuro}@i.kyoto-u.ac.jp
Abstract
We present a new morphological analy-
sis model that considers semantic plausi-
bility of word sequences by using a re-
current neural network language model
(RNNLM). In unsegmented languages,
since language models are learned from
automatically segmented texts and in-
evitably contain errors, it is not apparent
that conventional language models con-
tribute to morphological analysis. To solve
this problem, we do not use language mod-
els based on raw word sequences but use a
semantically generalized language model,
RNNLM, in morphological analysis. In
our experiments on two Japanese corpora,
our proposed model significantly outper-
formed baseline models. This result indi-
cates the effectiveness of RNNLM in mor-
phological analysis.
1 Introduction
In contrast to space-delimited languages like En-
glish, word segmentation is the first and most cru-
cial step for natural language processing (NLP)
in unsegmented languages like Japanese, Chinese,
and Thai (Kudo et al., 2004; Kaji and Kitsure-
gawa, 2014; Shen et al., 2014; Kruengkrai et al.,
2006). Word segmentation is usually performed
jointly with related analysis: POS tagging for Chi-
nese, and POS tagging and lemmatization (anal-
ysis of inflected words) for Japanese. Morpho-
logical analysis including word segmentation has
been widely and actively studied, and for exam-
ple, Japanese word segmentation accuracy is in the
high 90s. However, we often observe that strange
outputs of downstream NLP applications such as
machine translation and question answering come
from incorrect word segmentations.
For example, the state-of-the-art and popu-
lar Japanese morphological analyzers, JUMAN
(Kurohashi and Kawahara, 2009) and MeCab
(Kudo et al., 2004) both analyze “??????
(foreigner’s right to vote)” not into the correct seg-
mentation of (1a), but into the incorrect and awk-
ward segmentation of (1b).
(1) a. ?? /?
foreigner
/ ?? /?
right to vote
b. ??
foreign
/ ??
carrot
/ ??
regime
JUMAN is a rule-based morphological analyzer,
defining word-to-word (including inflection) con-
nectivities and their scores. MeCab is a supervised
morphological analyzer, learning the probabilities
of word/POS/inflection sequence from an anno-
tated corpus of tens of thousands of sentences.
Both systems, however, cannot realize semanti-
cally appropriate analysis, and often produce to-
tally strange outputs like the above.
This paper proposes a semantically appropriate
morphological analysis method for unsegmented
languages using a language model. For unseg-
mented languages, morphological analysis and
language modeling form a chicken-and-egg prob-
lem. That is, if high-quality morphological analy-
sis is available, we can learn a high-quality lan-
guage model from a morphologically analyzed
large corpus. On the other hand, if a high-quality
language model is available, we can achieve high-
quality morphological analysis by looking for a
segmented word sequence with a large language
model score. However, even if we learn a language
model from a corpus analyzed by a certain level
of morphological analyzer, the language model is
affected by the analysis errors of the morphologi-
cal analyzer and it is no practical use for the im-
provement of the morphological analyzer. A lan-
guage model trained by incorrectly segmented “?
? (foreign)/?? (carrot)/?? (regime)” just sup-
ports that incorrect segmentation.
The point of the paper is that we have tackled
the chicken-and-egg problem, not by using a lan-
2292
(foreign)	 (carrot)	 (regime)	
EOS	
?? 
 (a person name)	
?? (voting)	
? 
(particle)	
?
(discuss)	
noun	noun	
verb?past form	
noun	 noun	 noun	?? ?? ?? 
??? 
? ??? ? ? ? ? ? 
? ? ? ? ? ? 
(right)	(three)	(person)	(country)	(out)	 (politics)	
noun	 noun	 noun	 noun	 noun	 noun	BOS	
Input:	
Figure 1: An example of a word lattice.
guage model of raw word sequences, but by using
a semantically generalized language model based
on word embeddings, RNNLM (Recurrent Neural
Network Language Model) (Mikolov et al., 2010;
Mikolov et al., 2011). The RNNLM is trained on
an automatically analyzed corpus of ten million
sentences, which possibly includes incorrect seg-
mentations such as “?? (foreign)/?? (carrot)/
?? (regime).” However, on semantically gener-
alized level, it is an unnatural semantic sequence
like nation vegetable politics. Since the state-of-
the-art morphological analyzer achieves the high
accuracy, it does not often produce incorrect anal-
yses which support such a semantically strange se-
quence. This would prefer analysis toward seman-
tically appropriate word sequences. When a mor-
phological analyzer utilizes such a generalized and
reasonable language model, it can penalize strange
segmentations like “?? (foreign)/?? (carrot)/
?? (regime),” leading to better accuracy.
We furthermore retrain RNNLM using an an-
notated corpus of manually segmented 45k sen-
tences, which further improves morphological
analysis.
2 Related Work
There have been several studies that have inte-
grated language models into morphological anal-
ysis. Wang et al. (2011) improved Chinese word
segmentation and POS tagging by using N-gram
features learned from an automatically segmented
corpus. However, since the auto-segmented cor-
pus inevitably contains segmentation errors, fre-
quent N-grams are not always correct and thus
this problem might affect the performance of
morphological analysis. They also divided N-
gram frequencies into three binned features: high-
frequency, middle-frequency and low-frequency.
Such coarse features cannot express slight differ-
ences in the likelihood of language models.
Kaji and Kitsuregawa (2014) used a bigram lan-
guage model feature for Japanese word segmenta-
tion and POS tagging. Their objective of using a
language model is to normalize informally spelled
words in microblogs. Therefore, their objective is
different from ours.
Some studies have used character-based lan-
guage models for Chinese word segmentation and
POS tagging (Zheng et al., 2013; Liu et al., 2014).
Although their approaches have no drawbacks of
learning incorrect segmentations, they only cap-
ture more local information than word-based lan-
guage models.
Word embeddings have been also used for mor-
phological analysis. Neural network based models
have been proposed for Chinese word segmenta-
tion and POS tagging (Pei et al., 2014) or word
segmentation (Mansur et al., 2013). These meth-
ods acquire word embeddings from a corpus, and
then use them as the input of the neural networks.
Our proposed model learns word embeddings via
RNNLM, and these embeddings are used for scor-
ing word transitions in morphological analysis.
Our usage of word embeddings is different from
the previous studies.
3 Proposed Method
We propose a new morphological analysis model
that considers semantic plausibility of word se-
quences by using RNNLM. We integrate RNNLM
into morphological analysis (Figure 2). We train
the RNNLM using both an automatically analyzed
corpus and a manually labeled corpus.
3.1 Recurrent Neural Network Language
Model
RNNLM is a recurrent neural network language
model (Mikolov et al., 2010), which outputs a
probability distribution of the next word, given the
embedding of the last word and its context. We
2293
RNNLM	 RNNLM retrained	Re-training	
Training	 Training	 Base model	
Labeled corpusAuto segmented ? corpus	 Proposed model	
Figure 2: Workflow for training RNNLM and base
model.
employ the RNNME language model
1
proposed
by (Mikolov et al., 2011; Mikolov, 2012) as the
implementation of RNNLM. The RNNME lan-
guage model has direct connections from the input
layer of the recurrent neural network to the output
layer, which act as a maximum entropy model and
avoid to waste a lot of parameters to describe sim-
ple patterns. Hereafter, we refer to the RNNME
language model simply as RNNLM.
To train RNNLM, we use a raw corpus of 10
million sentences from the web corpus (Kawa-
hara and Kurohashi, 2006). These sentences are
automatically segmented by JUMAN (Kurohashi
and Kawahara, 2009). The training of RNNLM
is based on lemmatized word sequences without
POS tags.
The trained model contains errors caused by
an automatically analyzed corpus. We retrain
RNNLM using a manually labeled corpus after
training RNNLM using the automatically ana-
lyzed corpus as shown in Figure 2. The retraining
aims to cope with errors related to function word
sequences.
3.2 Base Model
For our base model, we adopt a model for su-
pervised morphological analysis, which performs
segmentation, lemmatization and POS tagging
jointly. We train this model using a tagged cor-
pus of tens of thousands of sentences that contain
gold segmentations, lemmas, inflection forms and
POS tags. To predict the most probable sequence
of words with lemmas and POS tags given an input
sentence, we execute the following procedure:
1. Look up the string of the input sentence using
a dictionary.
2. Make a word lattice.
3. Search for the path with the highest score
from the lattice.
1
RNNME is the abbreviation of Recurrent Neural Net-
work trained jointly with Maximum Entropy model.
Figure 1 illustrates the constructed lattice during
the procedure. At the dictionary lookup step, we
use the basic dictionary of JUMAN and an ad-
ditional dictionary comprising 0.8 million words,
both of which have lemma, POS and inflection in-
formation. The additional dictionary mainly con-
sists of itemizations in articles and article titles in
Japanese Wikipedia.
We define the scoring function as follows:
score
B
(y) = ?(y) · w?, (1)
where y is a tagged word sequence, ?(y) is a
feature vector for y, and w? is a weight vector.
Each element in w? gives a weight to its corre-
sponding feature in ?(y). We use the unigram
and the bigram features composed from word base
form, POS and inflection described in Kudo et al.
(2004). We also use additional lexical features
such as character type, and trigram features used
in Zhang and Clark (2008). To learn the weight
vector, we adopt exact soft confidence-weighted
learning (Wang et al., 2012).
To consider out-of-vocabulary (OOV) words
that are not found in the dictionary, we automat-
ically generate words at the lookup step by seg-
menting the input string by character types
2
. For
training, we regard words that are not found in the
dictionary but found in the training corpus as OOV
words to learn their weights.
3.3 RNNLM Integrated Model
Based on retrained RNNLM, we calculate an
RNNLM score (score
R
(y)) to be integrated into
the base model. The RNNLM score is defined as
the log probability of the next word given its con-
text (path). Here, the score for an OOV word is
given by the following formula:
?C
p
? L
p
· length(n), (2)
where C
p
is a constant penalty for OOV words,
L
p
is a factor for the character length penalty, and
length(n) returns the character length of the next
word n. This formula is defined to penalize longer
words, which are likely to produce segmentation
errors.
We then integrate the RNNLM score into the
base model using the following equation:
score
I
(y) = (1 ? ?)score
B
(y) + ? score
R
(y),
(3)
2
Japanese has three types of characters: Kanji, Hiragana
and Katakana.
2294
where ? is an interpolation parameter that is tuned
on development data.
For decoding, we employ beam search as used
in Zhang and Clark (2008). Since the possi-
ble context (paths in the word lattice) consid-
ered in RNNLM falls into combinatorial explosion
in morphological analysis, we keep only prob-
able context candidates inside the beam. That
is, each node keeps candidates inside the beam
width. Each candidate has a vector represent-
ing context, and two words of history. The re-
current model makes decoding harder than non-
recurrent neural network language models. How-
ever, we use RNNLM because the model outper-
forms other NNLMs (Mikolov, 2012) and the re-
sult suggests that the model is more likely to cap-
ture semantic plausibility. Since a sentence rarely
contains ambiguous and semantically appropriate
word sequences, we think that beam search with
enough beam size is able to keep the ambiguous
candidates of word sequences. In the case of non-
recurrent NNLMs and the base model, which uses
trigram features, we can conduct exact decoding
using the second-order Viterbi algorithm (Thede
and Harper, 1999).
4 Experiments
4.1 Experimental Settings
In our experiments, we used the Kyoto University
Text Corpus (Kawahara et al., 2002) and Kyoto
University Web Document Leads Corpus (Hangyo
et al., 2012) as manually tagged corpora. We ran-
domly chose 2,000 sentences from each corpus
for test data, and 500 sentences for development
data. We used the remaining part of the corpora
as training data to train our base model and retrain
RNNLM. In total, we used 45,000 sentences for
training.
For comparative purposes, we used the follow-
ing four baselines: the Japanese morphological an-
alyzer JUMAN, the supervised morphological an-
alyzer MeCab, the base model, and a model using
a conventional language model. For this language
model, we built a trigram language model with
Kneser-Ney smoothing using SRILM (Stolcke,
2002) from the same automatically segmented cor-
pus. The language model is modified to have an
interpolation parameter ? and length penalty for
OOV, L
p
.
We set the beam width to 5 by preliminary ex-
periments. We also set a constant penalty for OOV
words (C
p
) as 5, which is the default value in
the implementation of Mikolov et al. (2011). We
tuned the parameters of our proposed model and
the baseline model (? and L
p
) and the parameters
of language models using grid search on the de-
velopment data. We set ? = 0.3, L
p
=1.5 for the
proposed model (“ Base + RNNLM
retrain
”).
3
We measured the performance of the baseline
models and the proposed model by F-value of
word segmentation and F-value of joint evaluation
of word segmentation and POS tagging. We calcu-
lated F-value for the two corpora (news and web)
and the merged corpus (all).
We used the bootstrapping method (Zhang et
al., 2004) to test statistical significance between
proposed models and other models. Suppose we
have a test set T that includes N sentences. The
method repeatedly creates M new test sets by re-
sampling N sentences with replacement from T .
We calculate the F-value of each model on M + 1
test sets including T , and then we have M + 1
score differences. From the scores, we calculate
the 95% confidence interval. If the interval does
not overlap with zero, the two models are consid-
ered as statistically significantly different. In our
evaluation, M is set to 2,000.
4.2 Results and Discussions
Table 1 lists the results of our proposed model and
the baseline models. Our proposed model (“Base
+ RNNLM
retrain
”) significantly outperforms all the
baseline models and “Base + RNNLM,” which
does not use retraining. In particular, we achieved
a large improvement for segmentation. This can be
attributed to the use of RNNLM that was learned
based on lemmatized word sequence without POS
tags.
“Base + SRILM” segmented the example de-
scribed in Section 1 (“??????”) into the
incorrect segmentation “??/??/??” in the
same way as JUMAN. This segmentation error
was caused by errors in the automatically seg-
mented corpus that was used to train the language
model. Our proposed model can correctly seg-
ment this example if a proper context is available
by semantically capturing word transitions using
RNNLM.
The base model, JUMAN and “Base + SRILM”
incorrectly segmented “?? (healthy)/?? (etc.)/
3
We set ? = 0.1, L
p
= 2.0 for “Base + RNNLM”, and ? =
0.3, L
p
= 0.5 for “Base + SRILM.”
2295
Segmentation Seg + POS Segmentation Seg + POS Segmentation Seg + POS
(news) (news) (web) (web) (all) (all)
JUMAN 98.92 98.47 98.20 97.64 98.64 98.14
MeCab 99.07 98.58 98.22 97.51 98.74 98.16
Base model 98.94 98.46 97.71 96.90 98.46 97.85
Base + SRILM 98.94 98.40 98.13 97.33 98.62 97.98
Base + RNNLM 99.06 98.59 98.17 97.45 98.71 98.14
Base + RNNLM
retrain
99.15
?
98.70
?
98.37
?
97.68
?
98.84
?
98.30
?
Table 1: Results for test datasets. ? means the score of “Base + RNNLM
retrain
” is significantly improved
from that of all other models.
? (of)/? (point)/? (in)/……” (in terms of health
and so on) into “?? ?(healthy)/??(any)/?
(point)/? (in)/…….” Although this segmentation
can be grammatically accepted, it is difficult to
semantically interpret this word sequence. Our
proposed model can correctly segment this exam-
ple because RNNLM learns semantically plausible
word sequences.
5 Conclusion
In this paper, we proposed a new model for
morphological analysis that is integrated with
RNNLM. We trained RNNLM on an automati-
cally segmented corpus and tuned on a manually
tagged corpus. The proposed model was able to
significantly reduce errors in the base model by
capturing semantic plausibility of word sequences
using RNNLM. In the future, we will design fea-
tures derived from RNNLM models, and integrate
them into a unified learning framework. We also
intend to apply our method to unsegmented lan-
guages other than Japanese, such as Chinese and
Thai.
References
Masatsugu Hangyo, Daisuke Kawahara, and Sadao
Kurohashi. 2012. Building a diverse document
leads corpus annotated with semantic relations. In
Proceedings of the 26th Pacific Asia Conference
on Language, Information, and Computation, pages
535–544.
Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Ac-
curate word segmentation and POS tagging for
japanese microblogs: Corpus annotation and joint
modeling with lexical normalization. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
99–109, Doha, Qatar. Association for Computa-
tional Linguistics.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344–1347.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the
Third International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Palmas,
Canary Islands - Spain, May. European Language
Resources Association (ELRA). ACL Anthology
Identifier: L02-1302.
Canasai Kruengkrai, Virach Sornlertlamvanich, and
Hitoshi Isahara. 2006. A conditional random field
framework for Thai morphological analysis. In Pro-
ceedings of LREC, pages 2419–2424.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings
of the Conference on Emprical Methods in Natural
Language Processing (EMNLP 2004), volume 2004.
Sadao Kurohashi and Daisuke Kawahara, 2009.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Xiaodong Liu, Kevin Duh, Yuji Matsumoto, and To-
moya Iwakura. 2014. Learning character repre-
sentations for Chinese word segmentation. In NIPS
2014 Workshop on Modern Machine Learning and
Natural Language Processing.
Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
Chinese word segmentation. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1271–1277, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky. 2011. Strate-
gies for training large scale neural network language
2296
models. In Proceedings of ASRU 2011, pages 196–
201. IEEE Automatic Speech Recognition and Un-
derstanding Workshop.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Brno uni-
versity of technology.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for Chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 1: Long Papers, pages 293–303.
Mo Shen, Hongxiao Liu, Daisuke Kawahara, and
Sadao Kurohashi. 2014. Chinese Morphological
Analysis with Character-level POS Tagging. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 253–258, Baltimore, Maryland. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In John H L Hansen and
Bryan L Pellom, editors, 7th International Confer-
ence on Spoken Language Processing, ICSLP2002
- INTERSPEECH 2002, Denver, Colorado, USA,
September 16-20, 2002, pages 901–904. ISCA.
Scott M. Thede and Mary P. Harper. 1999. A second-
order Hidden Markov Model for part-of-speech tag-
ging. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 175–182, Morris-
town, NJ, USA, June. Association for Computa-
tional Linguistics.
Yiou Wang, Jun’ichi Kazama, Wenliang Chen, Yu-
jie Zhang, Kentaro Torisawa, and Yoshimasa Tsu-
ruoka. 2011. Improving chinese word segmenta-
tion and POS tagging with semi-supervised meth-
ods using large auto-analyzed data. In Proceedings
of the Fifth International Joint Conference on Nat-
ural Language Processing (IJCNLP-2011), pages
309–317, Chiang Mai, Thailand. Asian Federation
of Natural Language Processing.
Jialei Wang, Peilin Zhao, and Steven C.H. Hoi.
2012. Exact soft confidence-weighted learning. In
29th International Conference on Machine Learning
(ICML 2012), pages 121–128.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888–896,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores: How much improve-
ment do we need to have a better system? In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation (LREC-2004),
Lisbon, Portugal, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L04-1489.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmen-
tation and POS tagging. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2013, 18-21 Octo-
ber 2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL, pages 647–657.
2297
