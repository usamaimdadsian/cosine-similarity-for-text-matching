Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1465–1475,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Semi-Supervised Representation Learning for
Cross-Lingual Text Classification
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao,yuhong}@temple.edu
Abstract
Cross-lingual adaptation aims to learn a pre-
diction model in a label-scarce target lan-
guage by exploiting labeled data from a label-
rich source language. An effective cross-
lingual adaptation system can substantially re-
duce the manual annotation effort required in
many natural language processing tasks. In
this paper, we propose a new cross-lingual
adaptation approach for document classifica-
tion based on learning cross-lingual discrim-
inative distributed representations of words.
Specifically, we propose to maximize the log-
likelihood of the documents from both lan-
guage domains under a cross-lingual log-
bilinear document model, while minimizing
the prediction log-losses of labeled docu-
ments. We conduct extensive experiments on
cross-lingual sentiment classification tasks of
Amazon product reviews. Our experimental
results demonstrate the efficacy of the pro-
posed cross-lingual adaptation approach.
1 Introduction
With the rapid development of linguistic resources
in different languages, developing cross-lingual nat-
ural language processing (NLP) systems becomes
increasingly important (Bel et al., 2003; Shanahan
et al., 2004). Recently, cross-lingual adaptation
methods have been studied to exploit labeled infor-
mation from an existing source language domain
where labeled training data is abundant for use in
a target language domain where annotated training
data is scarce (Prettenhofer and Stein, 2010). Pre-
vious work has shown that cross-lingual adaptation
can greatly reduce labeling effort for a variety of
cross language NLP tasks such as document catego-
rization (Bel et al., 2003; Amini et al., 2009), genre
classification (Petrenz and Webber, 2012), and sen-
timent classification (Shanahan et al., 2004; Wei and
Pal, 2010; Prettenhofer and Stein, 2010).
The fundamental challenge of cross-lingual adap-
tation stems from a lack of overlap between the fea-
ture space of the source language data and that of
the target language data. To address this challenge,
previous work in the literature mainly relies on au-
tomatic machine translation tools. They first trans-
late all the text data from one language domain into
the other and then apply techniques such as domain
adaptation (Wan et al., 2011; Rigutini and Maggini,
2005; Ling et al., 2008) and multi-view learning
(Amini et al., 2009; Guo and Xiao, 2012b; Wan,
2009) to achieve cross-lingual adaptation. However,
machine translation tools may not be freely available
for all languages. Moreover, translating all the text
data in one language into the other language is too
time-consuming in reality. As an economic alter-
native solution, cross-lingual representation learn-
ing has recently been used in the literature to learn
language-independent representations of the data for
cross language text classification (Prettenhofer and
Stein, 2010; Petrenz and Webber, 2012).
In this paper, we propose to tackle cross language
text classification by inducing cross-lingual predic-
tive data representations with both labeled and un-
labeled documents from the two language domains.
Specifically, we propose a cross-lingual log-bilinear
document model to learn distributed representations
of words, which can capture both the semantic sim-
1465
ilarities of words across languages and the predic-
tive information with respect to the target classifi-
cation task. We conduct the representation learn-
ing by maximizing the log-likelihood of all docu-
ments from both language domains under the cross-
lingual log-bilinear document model and minimiz-
ing the prediction log-losses of labeled documents.
We formulate the learning problem as a joint non-
convex minimization problem and solve it using a
local optimization algorithm. To evaluate the effec-
tiveness of the proposed approach, we conduct ex-
periments on the task of cross language sentiment
classification of Amazon product reviews. The em-
pirical results show the proposed approach is very
effective for cross-lingual document classification,
and outperforms other comparison methods.
2 Related Work
Much work in the literature proposes to construct
cross-lingual representations by using aligned paral-
lel data. Basically, they first employ machine trans-
lation tools to translate documents from one lan-
guage domain to the other one and then induce low
dimensional latent representations as interlingual
representations (Littman et al., 1998; Vinokourov
et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo
and Xiao, 2012a). Littman et al. (1998) proposed
a cross-language latent semantic indexing method
to induce interlingual representations by perform-
ing latent semantic indexing over a dual-language
document-term matrix, where each dual-language
document contains its original words and the corre-
sponding translation text. Vinokourov et al. (2002)
proposed a cross-lingual kernel canonical corre-
lation analysis method, which learns two projec-
tions (one for each language) by conducting kernel
canonical correlation analysis over a paired bilin-
gual corpus and then uses the two projections to
project documents from language-specific feature
spaces to the shared multilingual semantic feature
space. Platt et al. (2010) employed oriented prin-
cipal component analysis (Diamantaras and Kung,
1996) over concatenated parallel documents, which
learns a multilingual projection by simultaneously
minimizing the projected distance between paral-
lel documents and maximizing the projected covari-
ance of documents across different languages. Pan
et al. (2011) proposed a bi-view non-negative matrix
tri-factorization method for cross-lingual sentiment
classification on the parallel training and test data.
Guo and Xiao (2012a) developed a transductive
subspace representation learning method for cross-
lingual text classification based on non-negative ma-
trix factorization. Some other works exploited par-
allel data by using multilingual topic models to ex-
tract cross-language latent topics as interlingual rep-
resentations (Mimno et al., 2009; Ni et al., 2011;
Platt et al., 2010; Smet et al., 2011) and using neu-
ral probabilistic language modes to learn word em-
beddings as cross-lingual distributed representations
(Klementiev et al., 2012). Most of them were de-
veloped by applying the latent Dirichlet allocation
(LDA) model (Blei et al., 2003) in a multilingual set-
ting, including the polylingual topic model (Mimno
et al., 2009), the bilingual LDA model (Smet et al.,
2011), and the multilingual LDA model (Ni et al.,
2011). Platt et al. (2010) extended the probabilis-
tic latent semantic analysis (PLSA) model (Hof-
mann, 1999) and presented two variants of multilin-
gual topic models: the joint PLSA model and the
coupled PLSA model. Recently, Klementiev et al.
(2012) extended the neural probabilistic language
model (Bengio et al., 2000) to induce cross-lingual
word distributed representations on a set of word-
level aligned parallel sentences. The applicability
of these approaches however is limited by the avail-
ability of parallel corpus. Translating the whole set
of documents to produce parallel corpus is too time-
consuming, expensive and even practically impossi-
ble for some language pairs. We thus do not evaluate
those approaches in our empirical study.
Another group of works propose to use bilin-
gual dictionaries to learn interlingual representa-
tions (Gliozzo, 2006; Prettenhofer and Stein, 2010).
Gliozzo (2006) first translated each term from one
language to the other using a bilingual dictionary
and used the translated terms to augment origi-
nal documents. Then they conducted latent se-
mantic analysis (LSA) over the document-term ma-
trix with concatenated vocabularies to obtain in-
terlingual representations. Prettenhofer and Stein
(2010) proposed a cross-language structural cor-
respondence learning (CL-SCL) method to induce
language-independent features by using word trans-
lation oracles. They first selected a subset of source
1466
language features, which have the highest mutual in-
formation with respect to the class labels in the la-
beled documents from the source language domain,
to translate them into the target language domain,
and then used these pivot pairs to induce cross-
lingual representations by modeling the correlations
between pivot features and non-pivot features. Our
proposed approach shares a similarity with the CL-
SCL method in (Prettenhofer and Stein, 2010) on
only requiring a small amount of word translations.
But our approach performs representation learning
in a semi-supervised manner by directly incorporat-
ing discriminative information with respect to the
target prediction task, while CL-SCL only exploits
labels when selecting pivot features and the struc-
tural correspondence learning process is conducted
in a fully unsupervised fashion.
Some other bilingual resources, such as multilin-
gual WordNet (Fellbaum, 1998) and universal part-
of-speech (POS) tags (Petrov et al., 2012), have also
been exploited in the literature for interlingual learn-
ing. Gliozzo (2006) proposed to use MultiWordNet
to map words from different languages to a common
synset-id as language-sharing terms. A similar work
was proposed in A.R. et al. (2012), which trans-
formed words from different languages to WordNet
synset identifiers as interlingual sense-based rep-
resentations. However, multilingual WordNet re-
sources are not always available for different lan-
guage pairs. Recently, Petrenz and Webber (2012)
used language-specific POS taggers to tag each word
and then mapped those language-specific POS tags
to twelve universal POS tags as interlingual features
for cross language fine-grained genre classification.
This approach requires a POS tagger for each lan-
guage and it may be adversely affected by the POS
tagging accuracy.
3 Semi-Supervised Representation
Learning for Cross-Lingual Text
Classification
In this section, we introduce a semi-supervised
cross-lingual representation learning method and
then use it for cross language text classification.
Assume we have ?s labeled and us unlabeled doc-
uments in the source language domain S and ?t la-
beled and ut unlabeled documents in the target lan-
guage domain T . We assume all the documents are
independent and identically distributed in each lan-
guage domain, and each document xi is represented
as a bag of words, xi = {wi1, wi2, . . . , wiNi}. We
use (x?i , yi) to denote the i-th labeled document and
its label, and consider exploiting the labeled docu-
ments in the source domain S for learning classifiers
in the target domain T .
To build connections between the two language
domains, we first construct a set of critical bilingual
word pairs M = {(wsi , wtj)}mi=1, where wsi is a crit-
ical word in the source language domain, wtj is its
translation in the target language domain, and m is
the number of word pairs. Here being critical means
the word should be discriminative for the prediction
task and occur frequently in both language domains.
Following the work (Prettenhofer and Stein, 2010),
we select bilingual word pairs in a heuristic way.
First we select a subset of words from the source lan-
guage domain, which have the highest mutual infor-
mation with the class labels in labeled source docu-
ments. The mutual information is computed based
on the empirical distributions of words and labels
in the labeled source documents. Then we translate
the selected words into the target language using a
translation tool to produce word pairs. Finally we
produce the M set by eliminating any candidate pair
(ws, wt), if either ws occurs less than a predefined
threshold value ? in all source language documents
or wt occurs less than ? in all target language docu-
ments. Given the constructed bilingual word pair set
M , the words appearing in the source language doc-
uments but not in M can be put together to form a
source specific vocabulary set Vs = {ws1, . . . , wsvs}.
Similarly, the words appearing in the target language
documents but not in M can be put together to form
a target specific vocabulary set Vt = {wt1, . . . , wtvt}.
An overall cross-lingual vocabulary set can then be
constructed as V = Vs ? Vt ? M , which has a total
of v = vs + vt + m entries. This cross-lingual vo-
cabulary set covers all words appearing in both do-
mains, while mapping each bilingual pair in M into
the same entry.
To tackle cross language text classification, we
then propose a cross-lingual log-bilinear document
model to learn a predictive cross-lingual represen-
tation of words, which maps each entry in the vo-
cabulary set V to one row vector in a word embed-
1467
ding matrix R ? Rv×k. Similar to the log-bilinear
language model (Mnih and Hinton, 2007) and the
log-bilinear document model (Maas et al., 2011),
our proposed model learns a dense feature vector for
each word to capture semantic similarities between
the vocabulary entries. But unlike the previous two
models which only work with a monolingual lan-
guage, our model also captures semantic similarities
across different languages. Moreover, we explicitly
incorporate the label information into our proposed
approach, rendering the induced word embeddings
more discriminative to the target prediction task.
3.1 Cross-Lingual Word Embeddings
As mentioned above, we assume a unified embed-
ding matrix R which contains the distributed vec-
tor representations of words in the two language
domains. However, even in a unified representa-
tion space, the distribution of words in the two do-
mains will be different. To capture the distribution
divergence of the two domains and facilitate cross-
lingual learning, we split the word embedding ma-
trix into three parts: source language specific part
Rs ? Rv×ks , common part Rc ? Rv×kc and tar-
get language specific part Rt ? Rv×kt , such that
k = ks + kc + kt. Intuitively, we assume that source
language words contain no target language specific
representations and target language words contain
no source language specific representations. Thus
for words in the two language domains, we retrieve
their distributed vector representations from the em-
bedding matrix R using two mapping functions, ?S
and ?T , one for each language domain. The two
mapping functions are defined as
?S(w) =[Rs(w), Rc(w),0t]T (1)
?T (w) =[0s, Rc(w), Rt(w)]T (2)
where 0t is a kt-dimensional row vector of zeros,
0s is a ks-dimensional row vector of zeros, Rs(w)
denotes the row vector of Rs matrix corresponding
to the word w, Rc(w) denotes the row vector of Rc
matrix corresponding to the word w, and Rt(w) de-
notes the row vector of Rt matrix corresponding to
the word w. It is easy to see that each pair of words
in M will share the same vector from Rc. To encode
more information into the common part of represen-
tation for better knowledge transfer from the source
language domain to the target language domain, we
assume kc ? ks and kc ? kt. The form of three part
feature representations has been exploited in previ-
ous work of domain adaptation with heterogeneous
feature spaces (Duan et al., 2012). However, their
approach simply duplicates the original features as
language-specific representations, while we will au-
tomatically learn those three part latent representa-
tions in our approach.
3.2 Semi-Supervised Cross-Lingual
Representation Learning
Given the word representation scheme above, we
conduct cross-lingual representation learning by si-
multaneously maximizing the log-likelihood of all
documents and the conditional likelihood of labeled
documents from the two language domains
max
?
?
L?{S,T }
?
xi?L
Ni
?
j=1
logPL(wij |?)+
?
?
L?{S,T }
?
x?i?L
logPL(yi|x?i , ?) (3)
where ? denotes the model parameters and ? is a
trade-off parameter. The first part of the objective
function captures the likelihood of the documents
being generated with the learned representation R.
PL(wij |?) is the probability of word wij appearing
in the document xi from the language domain L, and
is defined as
PL(wij |?) =
exp (?EL(wij , ?))
?
w??V exp (?EL(w?, ?))
(4)
The term EL(wij , ?) is a log-bilinear energy func-
tion, defined as
EL(wij , ?) = ?dTi ?L(wij) ? bwij (5)
where di is a k-dimensional weight vector for docu-
ment xi and bwij is the bias for word wij . Below we
will use b to denote a v-dimensional vector contain-
ing all words’ biases.
The second part of the objective function in (3)
takes the label information into account and aims
to render the latent word representations more task-
predictive. We use a logistic regression model to
1468
compute the conditional probability of the class la-
bel given the document with the induced word rep-
resentations, such that
PL(yi|x?i , ?) =
1
1 + exp
(
?yi
(
wT?L(x?i) + q
))
(6)
where w, q are model parameters of the logistic re-
gression model, ?L(xi) is the k-dimensional vector
representation of the document xi in the language
domain L. We compute ?L(xi) by taking average
over all words in the document xi such as
?L(xi) =
1
Ni
Ni
?
j=1
?L(wij) (7)
By summing over all descriptions above, we can
see that the proposed semi-supervised representa-
tion learning has a set of model parameters, ? =
{R, {di},b,w, q}. In order to avoid overfitting, we
add regularization terms for the parameters R, {di}
and w, which leads to the final optimization problem
below
max
?
?
L?{S,T }
?
xi?L
(
Ni
?
j=1
logPL(wij |?) ? ??di?22
)
+ ?
?
L?{S,T }
?
x?i?L
logPL(yi|x?i , ?)
? ??R?2F ? ??w?22 (8)
where ?, ?, ? are trade-off parameters, ? · ?F denote
the Frobenius norm and ? · ?2 denote the Euclidean-
norm. This objective function is not jointly convex
in all model parameters. We develop a gradient-
based iterative optimization procedure to seek a lo-
cal optimal solution. We first randomly initialize the
model parameters {di}, R,w and set b and q to ze-
ros. Then we iteratively make gradient-based up-
dates over the model parameters until reach a local
optimal solution.
3.3 Cross-Lingual Document Classification
After solving (8), we obtain a word embedding ma-
trix R. The distributed vector representation of any
given document can then be computed using Eq. (7)
based on Eq. (1) or Eq. (2). Under the distributed
vector representations of the documents in both lan-
guage domains, we perform cross-lingual document
classification by training a supervised classification
model using labeled data from both language do-
mains and then applying it to classify test documents
in the target language domain .
4 Experiments
We empirically evaluate the proposed approach us-
ing the cross language sentiment classification tasks
of Amazon product reviews in four languages. In
this section, we report our experimental results.
4.1 Dataset
We used the multilingual sentiment classification
dataset1 provided by Prettenhofer and Stein (2010),
which contains Amazon product reviews in four dif-
ferent languages, English (E), French (F), German
(G) and Japanese (J). The English product reviews
were sampled from previous cross-domain senti-
ment classification datasets (Blitzer et al., 2007),
while the other three language product reviews were
crawled from Amazon by the authors in November
2009. In the dataset, each language contains three
categories of product reviews, Books (B), DVD (D)
and Music (M). Each language-category pair con-
tains a balanced training set and test set, each of
which consists of 1000 positive reviews and 1000
negative reviews. Each review is represented as
a unigram bag-of-word feature vector with term-
frequency values. Following the work (Prettenhofer
and Stein, 2010), we used the original English re-
views as the source language while treating the other
three languages as target languages. Thus, we con-
struct nine cross language sentiment classification
tasks (GB, GD, GM, FB, FD, FM, JB, JD, JM), one
for each target language-category pair. For example,
the task GB means that the target language is Ger-
man and the training and test data are samples from
Books reviews.
4.2 Approaches
We compare our proposed semi-supervised cross-
lingual representation learning (CL-RL) approach
to the following approaches for cross-lingual doc-
ument classification.
1http://www.webis.de/research/corpora/
1469
Table 1: Average classification accuracies and standard deviations for the 9 cross-lingual sentiment classification tasks.
The bold format indicates that the difference between the results of CL-RL and MT is significant with p < 0.05 under
a McNemar paired test for labeling disagreements.
Task TB CL-Dict CLD-LSA CL-SCL MT CL-RL
GB 66.25±0.64 69.40±0.61 70.30±0.44 73.78±0.32 78.05±0.64 79.89±0.30
GD 63.16±0.66 66.37±0.63 66.85±0.46 71.99±0.25 75.75±0.58 77.14±0.16
GM 65.42±0.77 68.81±0.51 68.93±0.58 71.58±0.35 74.85±0.62 77.27±0.16
FB 65.98±0.51 69.35±0.48 69.98±0.51 73.89±0.16 78.00±0.49 78.25±0.32
FD 63.76±0.37 67.96±0.60 68.88±0.43 73.79±0.28 75.75±0.71 74.83±0.30
FM 65.94±0.56 67.98±0.69 68.42±0.60 71.20±0.28 74.85±0.49 78.71±0.32
JB 63.86±0.80 59.40±0.29 62.62±0.62 62.49±0.23 67.20±0.80 71.11±0.21
JD 63.59±0.74 62.13±0.26 63.87±0.72 65.54±0.29 67.70±0.57 73.12±0.23
JM 65.84±0.90 63.01±0.46 65.67±0.72 65.49±0.36 68.30±0.61 74.38±0.40
• TB: This is a target baseline method, which
trains a supervised monolingual classifier on
the labeled training data from the target lan-
guage domain without representation learning.
• CL-Dict: This is a simple baseline compar-
ison method, which uses the bilingual word
pairs directly to align features from different
language domains into a unified feature dictio-
nary and then trains a supervised classifier on
this aligned feature space with labeled training
data from both language domains.
• CLD-LSA: This is the cross-lingual represen-
tation learning method developed in (Gliozzo,
2006), which first translates each document
from one language into the other language via
a bilingual dictionary to produce augmenting
features, and then performs latent semantic
analysis (LSA) over the augmented bilingual
document-term matrix.
• CL-SCL: This is the cross language structural
correspondence learning method developed in
(Prettenhofer and Stein, 2010).
• MT: This is a machine translation based com-
parison method, which first uses an existing
machine translation tool (google translation) to
translate the target language documents into the
source language and then trains a monolingual
classifier with labeled training data from both
domains in the source language.
In all experiments, we used a linear support vec-
tor machine (SVM) for sentiment classification. For
implementation, we used the liblinear package (Fan
et al., 2008) with all of its default parameters. For
the CL-SCL method, we used the same parame-
ter setting as suggested in the paper (Prettenhofer
and Stein, 2010): the number of pivot features is
set as 450, the threshold value for selecting pivot
features is 30, and the reduced dimensionality af-
ter singular value decomposition is 100. For the
CLD-LSA method, we set the dimensionality of la-
tent representation as 1000. Similarly, for our pro-
posed approach, we built the cross-lingual vocabu-
lary M by setting m = 450 and ? = 30. For
our representation learning, we set ? = 1, ? =
? = ? = 1e?4, and set ks, kc, kt to be 25, 50,
25, respectively. The values of ?, ?, ? and ? are
selected using the first cross language classifica-
tion task GB. We selected the ? value from the
set {0.01, 0.1, 1, 10, 100} and selected ?, ?, ? values
from the set {1e?5, 1e?4, 1e?3, 1e?2, 1e?5} by re-
peating the experiment three times with random data
partitions and choosing the parameter values that led
to the best average classification accuracy.
4.3 Classification Accuracy
For each of the nine cross language sentiment classi-
fication tasks with different target language-category
pairs, we used the training set in the source language
domain (English) as labeled data while treating the
test set in the source language domain as unlabeled.
1470
100 200 300 400 500
65
70
75
80
GB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
GD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
GM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
80
FB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
80
FD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
65
70
75
80
FM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
55
60
65
70
75
JB
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
JD
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
100 200 300 400 500
60
65
70
75
JM
#Labeled target instances
A
cc
ur
ac
y
 
 
TB
CL?Dict
CLD?LSA
CL?SCL
MT
CL?RL
Figure 1: Average classification accuracies and standard deviations for 10 runs with respect to different numbers of
labeled training documents in the target language domain.
For target language domain, we used the test set as
test data while randomly selecting 100 documents
from the training set as labeled data and treating the
rest as unlabeled data. Thus, for each task, we have
2000 labeled documents and 2000 unlabeled docu-
ments from the source language domain, and 100
labeled and 1900 unlabeled documents from the tar-
get language domain for training. We have 2000 test
documents from the target language domain as test-
ing data. In each experiment, a classifier is produced
by each approach with the training data and tested
on the testing data. We repeated each experiment
10 times with different random selections of 100 la-
beled training documents from the target language
domain. The average classification accuracies and
standard deviations are reported in Table 1.
From Table 1, we can see that the proposed semi-
supervised cross-lingual representation learning ap-
proach, CL-RL, clearly outperforms all other com-
parison methods on eight out of the nine tasks. The
target baseline TB performs poorly on all the nine
tasks, which suggests that 100 labeled instances
from the target language is far from enough to ob-
tain an accurate sentiment classifier in the target lan-
1471
100 200 300 400
65
70
75
80
German
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
100 200 300 400
65
70
75
80
French
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
100 200 300 40060
65
70
75
80
Japanese
#Dimension
A
cc
ur
ac
y
 
 
Books
DVD
Music
Figure 2: Average classification accuracy and standard deviation results for the proposed approach over 10 runs with
respect to different dimensionality for the induced cross-lingual representations.
guage domain. By exploiting the large amount of
labeled training data from the source language do-
main, even the simple cross-lingual adaptation ap-
proach, CL-Dict, produces effective improvements
over TB. However, its performance is not consis-
tent across the nine tasks. It has inferior perfor-
mance than TB on the three tasks of adapting En-
glish to the Japanese language domain. This sug-
gests the simple bilingual word-pair based feature
space unification method is far from ideal for pro-
viding effective cross-lingual representations, espe-
cially when two languages (English, Japanese) are
very different. With a better designed representa-
tion learning, CLD-LSA outperforms CL-Dict on all
the nine tasks, but the improvements are very small
on some tasks (e.g., GM). CL-SCL not only out-
performs CL-Dict on all tasks, but also performs
much better than CLD-LSA on most tasks. Its per-
formance nevertheless is inferior to the method of
MT. Though MT can greatly increase the test accu-
racies comparing to the other four methods, TB, CL-
Dict, CLD-LSA, and CL-SCL, the benefit is obtained
at the cost of whole document translations. In con-
trast, our proposed approach does not require whole
document translations, but relies on the same sim-
ple word-pair translations used in CL-Dict. It how-
ever consistently and significantly outperforms TB,
CL-Dict, CLD-LSA, and CL-SCL on all tasks, and
outperforms MT on eight out of the nine tasks.
We also conduct significance tests for our pro-
posed approach and MT using a McNemar paired
test for labeling disagreements (Gillick and Cox,
1989). The results in bold format indicate that they
are significant with p < 0.05. All these results
demonstrate the efficacy of our cross-lingual repre-
sentation learning method.
4.4 Classification Accuracy vs the Number of
Labeled Target Documents
Next, we investigated the performance of the six ap-
proaches by varying the number of labeled train-
ing documents from the target language domain.
We maintained the same experimental setting as be-
fore, but investigated a range of different values,
?t = {100, 200, 300, 400, 500}, as the number of la-
beled training documents from the target language
domain. In each experiment, for a given value ?t,
we randomly selected ?t documents from the train-
ing set of the target language domain as labeled data
and used the rest as unlabeled data. We still per-
formed prediction on the same 2000 test documents
in the target language domain. We repeated each
experiment 10 times based on different random se-
lections of the labeled training data from the target
language domain. The average classification accura-
cies and standard deviations across different ?t val-
ues for all comparison methods on all the nine tasks
are plotted in Figure 1.
We can see when the number of labeled target
documents is small, TB performs poorly, especially
for the first six tasks (GB, GD, GM, FB, FD, FM).
By increasing the size of labeled target training data,
TB can greatly increase its prediction accuracies and
even outperform the CL-Dict method. The sim-
ple CL-Dict method has inconsistent performance
across the nine tasks. Its performance is better than
1472
TB when the labeled training data in the target lan-
guage domain is very limited and is poor than TB
when the labeled target data reaches 300 for the six
tasks using German and French as target languages.
Moreover, when adapting a system from English to
a much more different target language (Japanese),
CL-Dict produces much lower accuracies for all the
three tasks comparing with TB. These results show
that CL-Dict has very limited capacity on transfer-
ring labeled information from a related source lan-
guage domain. Similar performance is observed for
CLD-LSA. With a more sophisticated representation
learning, the CL-SCL method consistently outper-
forms CL-Dict. However, it produces inferior per-
formance than CLD-LSA on the tasks of JB and JM.
By using more translation resources, the MT method
outperforms TB, CL-Dict, CLD-LSA, CL-SCL in all
the nine tasks across almost all scenarios. Our pro-
posed method CL-RL significantly outperforms all
the other five comparison methods across all experi-
ments except on the task of FD, where MT produces
similar performance. Moreover, it is especially im-
portant to notice that CL-RL achieves high test ac-
curacies even when the number of labeled target in-
stances is small. This is important for transferring
knowledge from a source language to reduce the la-
beling effort in the target language.
4.5 Sensitivity Analysis
We also investigated the sensitivity of the proposed
approach over the dimensionality of the induced
cross-lingual representations. We used the same ex-
perimental setting as before, and conducted experi-
ments with a set of different dimensionality values,
k = {100, 200, 300, 400}. For each value k, we
set ks = 0.25k, kc = 0.5k, kt = 0.25k. We re-
peated each experiment for 10 times based on dif-
ferent random selections of labeled target training
data and plotted the average prediction accuracies
and standard deviations in Figure 2 for all the nine
cross-lingual sentiment classification tasks. We can
see the proposed approach produces stable accuracy
results across the range of different k values. This
suggests the proposed approach is not very sensitive
to the dimensionality of the cross-lingual embedding
features within the considered range of values, and
with a small dimensionality of 100, the induced rep-
resentation can already perform very well.
4.6 Cross-Lingual Word Representations
Finally, we used the first task GB, which adapts the
Books reviews from English to German, to gain in-
tuitive understandings over the learned cross-lingual
word representations. Given an English word as
seed word, we find its five closest neighboring En-
glish words and German words according to the Eu-
clidean distances calculated in the induced cross-
lingual representation space. We present a few re-
sults in Table 2. From Table 2, we can see that the re-
trieved words in both language domains are seman-
tically close to the seed words, which indicates that
our proposed method can capture semantic similar-
ities of words not only in a monolingual setting but
also in a multilingual setting.
5 Conclusion
In this paper, we proposed a semi-supervised cross-
lingual representation learning approach to address
cross-lingual text classification. The distributed
word representation induced by the proposed ap-
proach can capture semantic similarities of words
across languages while maintaining predictive infor-
mation with respect to the target classification tasks.
To evaluate the proposed approach, we conducted
experiments on nine cross language sentiment clas-
sification tasks constructed from the Amazon prod-
uct reviews in four languages, comparing to a num-
ber of comparison methods. The empirical results
showed that the proposed approach can produce
effective cross-lingual adaptation performance and
significantly outperform other comparison methods.
References
M. Amini, N. Usunier, and C. Goutte. Learning from
multiple partially observed views - an application
to multilingual text categorization. In Advances in
Neural Information Processing Systems (NIPS),
2009.
B. A.R., A. Joshi, and P. Bhattacharyya. Cross-
lingual sentiment analysis for indian languages
using linked wordnets. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING), 2012.
N. Bel, C. Koster, and M. Villegas. Cross-lingual
1473
Table 2: Examples of source seed words together with five closest English words and five closest German words
estimated using the Euclidean distance in the cross-lingual representation space on the task GB.
books absolutely love
English German English German English German
books buch absolutely absolut love liebe
book bu¨cher definitely absolute loved lieben
text text completely definitiv like wie
page blatt certainly komplett fond wieder
words wo¨rter totally sicher feel fu¨hlen
expensive good not
English German English German English German
expensive teuer good gut not nicht
expense ho¨her better besser no nie
overpriced ho¨chsten well nett cannot nein
costly hoch nice großartig non keine
price preis great gro¨ßten never keines
text categorization. In Proceedings of European
Conference on Digital Libraries (ECDL), 2003.
Y. Bengio, R. Ducharme, and P. Vincent. A neu-
ral probabilistic language model. In Advances in
Neural Information Processing Systems (NIPS),
2000.
D. Blei, A. Ng, and M. Jordan. Latent dirichlet al-
location. Journal of Machine Learning Research
(JMLR), 3:993–1022, 2003.
J. Blitzer, M. Dredze, and F. Pereira. Biographies,
bollywood, boomboxes and blenders: Domain
adaptation for sentiment classification. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2007.
K. Diamantaras and S. Kung. Principal component
neural networks: theory and applications. Wiley-
Interscience, 1996.
L. Duan, D. Xu, and I. Tsang. Learning with aug-
mented features for heterogeneous domain adap-
tation. In Proceedings of the International Con-
ference on Machine Learning (ICML), 2012.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
LIBLINEAR: A library for large linear classifi-
cation. Journal of Machine Learning Research
(JMLR), 9:1871–1874, 2008.
C. Fellbaum, editor. WordNet: an electronic lexical
database. MIT Press, 1998.
L. Gillick and S. Cox. Some statistical issues
in the comparison of speech recognition algo-
rithms. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing (ICASSP), 1989.
A. Gliozzo. Exploiting comparable corpora and
bilingual dictionaries for cross-language text cat-
egorization. In Proceedings of the International
Conference on Computational Linguistics and the
Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), 2006.
Y. Guo and M. Xiao. Transductive representation
learning for cross-lingual text classification. In
Proceedings of the IEEE International Confer-
ence on Data Mining (ICDM), 2012a.
Y. Guo and M. Xiao. Cross language text clas-
sification via subspace co-regularized multi-view
learning. In Proceedings of the International Con-
ference on Machine Learning (ICML), 2012b.
T. Hofmann. Probabilistic latent semantic analysis.
In Proceedings of Uncertainty in Artificial Intelli-
gence (UAI), 1999.
A. Klementiev, I. Titov, and B. Bhattarai. Inducing
crosslingual distributed representations of words.
1474
In Proceedings of the International Conference on
Computational Linguistics (COLING), 2012.
X. Ling, G. Xue, W. Dai, Y. Jiang, Q. Yang, and
Y. Yu. Can chinese web pages be classified with
english data source? In Proceedings of the Inter-
national Conference on World Wide Web (WWW),
2008.
M. Littman, S. Dumais, and T. Landauer. Automatic
Cross-Language Information Retrieval using La-
tent Semantic Indexing, chapter 5, pages 51–62.
Kluwer Academic Publishers, 1998.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. Learning word vectors for sentiment
analysis. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL), 2011.
D. Mimno, H. Wallach, J. Naradowsky, D. Smith,
and A. McCallum. Polylingual topic models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, 2009.
A. Mnih and G. Hinton. Three new graphical mod-
els for statistical language modelling. In Proceed-
ings of the International Conference on Machine
Learning (ICML), 2007.
X. Ni, J. Sun, J. Hu, and Z. Chen. Cross lingual
text classification by mining multilingual topics
from wikipedia. In Proceedings of the ACM In-
ternational Conference on Web Search and Data
Mining (WSDM), 2011.
J. Pan, G. Xue, Y. Yu, and Y. Wang. Cross-lingual
sentiment classification via bi-view non-negative
matrix tri-factorization. In Proceedings of the
Pacific-Asia conference on Advances in knowl-
edge discovery and data mining (PAKDD), 2011.
P. Petrenz and B. Webber. Label propagation for
fine-grained cross-lingual genre classification. In
Proceedings of the NIPS xLiTe workshop, 2012.
S. Petrov, D. Das, and R. McDonald. A universal
part-of-speech tagset. In Proceedings of the Inter-
national Conference on Language Resources and
Evaluation (LREC), 2012.
J. Platt, K. Toutanova, and W. Yih. Translingual doc-
ument representations from discriminative projec-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), 2010.
P. Prettenhofer and B. Stein. Cross-language
text classification using structural correspondence
learning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), 2010.
L. Rigutini and M. Maggini. An em based train-
ing algorithm for cross-language text categoriza-
tion. In Proceedings of the Web Intelligence Con-
ference, 2005.
J. Shanahan, G. Grefenstette, Y. Qu, and D. Evans.
Mining multilingual opinions through classifica-
tion and translation. In Proceedings of AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text, 2004.
W. Smet, J. Tang, and M. Moens. Knowledge trans-
fer across multilingual corpora via latent topics.
In Proceedings of the Pacific-Asia conference on
Advances in knowledge discovery and data min-
ing (PAKDD), 2011.
A. Vinokourov, J. Shawe-taylor, and N. Cristian-
ini. Inferring a semantic representation of text
via cross-language correlation analysis. In Ad-
vances in Neural Information Processing Systems
(NIPS), 2002.
C. Wan, R. Pan, and J. Li. Bi-weighting domain
adaptation for cross-language text classification.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI), 2011.
X. Wan. Co-training for cross-lingual sentiment
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), 2009.
B. Wei and C. Pal. Cross lingual adaptation: An
experiment on sentiment classifications. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2010.
1475
