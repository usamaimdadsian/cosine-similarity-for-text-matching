Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566,
Athens, Greece, 30 March – 3 April 2009. c©2009 Association for Computational Linguistics
Evaluating the Inferential Utility of Lexical-Semantic Resources
Shachar Mirkin, Ido Dagan, Eyal Shnarch
Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel
{mirkins,dagan,shey}@cs.biu.ac.il
Abstract
Lexical-semantic resources are used ex-
tensively for applied semantic inference,
yet a clear quantitative picture of their
current utility and limitations is largely
missing. We propose system- and
application-independent evaluation and
analysis methodologies for resources’ per-
formance, and systematically apply them
to seven prominent resources. Our find-
ings identify the currently limited recall of
available resources, and indicate the po-
tential to improve performance by exam-
ining non-standard relation types and by
distilling the output of distributional meth-
ods. Further, our results stress the need
to include auxiliary information regarding
the lexical and logical contexts in which
a lexical inference is valid, as well as its
prior validity likelihood.
1 Introduction
Lexical information plays a major role in seman-
tic inference, as the meaning of one term is of-
ten inferred form another. Lexical-semantic re-
sources, which provide the needed knowledge for
lexical inference, are commonly utilized by ap-
plied inference systems (Giampiccolo et al., 2007)
and applications such as Information Retrieval and
Question Answering (Shah and Croft, 2004; Pasca
and Harabagiu, 2001). Beyond WordNet (Fell-
baum, 1998), a wide range of resources has been
developed and utilized, including extensions to
WordNet (Moldovan and Rus, 2001; Snow et al.,
2006) and resources based on automatic distri-
butional similarity methods (Lin, 1998; Pantel
and Lin, 2002). Recently, Wikipedia is emerg-
ing as a source for extracting semantic relation-
ships (Suchanek et al., 2007; Kazama and Tori-
sawa, 2007).
As of today, only a partial comparative picture
is available regarding the actual utility and limi-
tations of available resources for lexical-semantic
inference. Works that do provide quantitative
information regarding resources utility have fo-
cused on few particular resources (Kouylekov and
Magnini, 2006; Roth and Sammons, 2007) and
evaluated their impact on a specific system. Most
often, works which utilized lexical resources do
not provide information about their isolated con-
tribution; rather, they only report overall per-
formance for systems in which lexical resources
serve as components.
Our paper provides a step towards clarify-
ing this picture. We propose a system- and
application-independent evaluation methodology
that isolates resources’ performance, and sys-
tematically apply it to seven prominent lexical-
semantic resources. The evaluation and analysis
methodology is specified within the Textual En-
tailment framework, which has become popular in
recent years for modeling practical semantic infer-
ence in a generic manner (Dagan and Glickman,
2004). To that end, we assume certain definitions
that extend the textual entailment paradigm to the
lexical level.
The findings of our work provide useful insights
and suggested directions for two research com-
munities: developers of applied inference systems
and researchers addressing lexical acquisition and
resource construction. Beyond the quantitative
mapping of resources’ performance, our analysis
points at issues concerning their effective utiliza-
tion and major characteristics. Even more impor-
tantly, the results highlight current gaps in exist-
ing resources and point at directions towards fill-
ing them. We show that the coverage of most
resources is quite limited, where a substantial
part of recall is attributable to semantic relations
that are typically not available to inference sys-
tems. Notably, distributional acquisition methods
558
are shown to provide many useful relationships
which are missing from other resources, but these
are embedded amongst many irrelevant ones. Ad-
ditionally, the results highlight the need to rep-
resent and inference over various aspects of con-
textual information, which affect the applicability
of lexical inferences. We suggest that these gaps
should be addressed by future research.
2 Sub-sentential Textual Entailment
Textual entailment captures the relation between a
text t and a textual statement (termed hypothesis)
h, such that a person reading t would infer that h
is most likely correct (Dagan et al., 2005).
The entailment relation has been defined insofar
in terms of truth values, assuming that h is a com-
plete sentence (proposition). However, there are
major aspects of inference that apply to the sub-
sentential level. First, in certain applications, the
target hypotheses are often sub-sentential. For ex-
ample, search queries in IR, which play the hy-
pothesis role from an entailment perspective, typ-
ically consist of a single term, like drug legaliza-
tion. Such sub-sentential hypotheses are not re-
garded naturally in terms of truth values and there-
fore do not fit well within the scope of the textual
entailment definition. Second, many entailment
models apply a compositional process, through
which they try to infer each sub-part of the hy-
pothesis from some parts of the text (Giampiccolo
et al., 2007).
Although inferences over sub-sentential ele-
ments are being applied in practice, so far there
are no standard definitions for entailment at sub-
sentential levels. To that end, and as a prerequisite
of our evaluation methodology and our analysis,
we first establish two relevant definitions for sub-
sentential entailment relations: (a) entailment of a
sub-sentential hypothesis by a text, and (b) entail-
ment of one lexical element by another.
2.1 Entailment of Sub-sentential Hypotheses
We first seek a definition that would capture the
entailment relationship between a text and a sub-
sentential hypothesis. A similar goal was ad-
dressed in (Glickman et al., 2006), who defined
the notion of lexical reference to model the fact
that in order to entail a hypothesis, the text has
to entail each non-compositional lexical element
within it. We suggest that a slight adaptation of
their definition is suitable to capture the notion of
entailment for any sub-sentential hypotheses, in-
cluding compositional ones:
Definition 1 A sub-sentential hypothesis h is en-
tailed by a text t if there is an explicit or implied
reference in t to a possible meaning of h.
For example, the sentence “crude steel output
is likely to fall in 2000” entails the sub-sentential
hypotheses production, steel production and steel
output decrease.
Glickman et al., achieving good inter-annotator
agreement, empirically found that almost all non-
compositional terms in an entailed sentential hy-
pothesis are indeed referenced in the entailing text.
This finding suggests that the above definition is
consistent with the original definition of textual
entailment for sentential hypotheses and can thus
model compositional entailment inferences.
We use this definition in our annotation method-
ology described in Section 3.
2.2 Entailment between Lexical Elements
In the majority of cases, the reference to an
“atomic” (non-compositional) lexical element e in
h stems from a particular lexical element e? in t,
as in the example above where the word output
implies the meaning of production.
To identify this relationship, an entailment sys-
tem needs a knowledge resource that would spec-
ify that the meaning of e? implies the meaning of
e, at least in some contexts. We thus suggest the
following definition to capture this relationship be-
tween e? and e:
Definition 2 A lexical element e’ entails another
lexical element e, denoted e’?e, if there exist
some natural (non-anecdotal) texts containing e’
which entail e, such that the reference to the mean-
ing of e can be implied solely from the meaning of
e’ in the text.
(Entailment of e by a text follows Definition 1).
We refer to this relation in this paper as lexical
entailment1, and call e’ ? e a lexical entailment
rule. e? is referred to as the rule’s left hand side
(LHS) and e as its right hand side (RHS).
Currently there are no knowledge resources de-
signed specifically for lexical entailment model-
ing. Hence, the types of relationships they cap-
ture do not fully coincide with entailment infer-
ence needs. Thus, the definition suggests a spec-
ification for the rules that should be provided by
1Section 6 discusses other definitions of lexical entailment
559
a lexical entailment resource, following an oper-
ative rationale: a rule e’ ? e should be included
in an entailment knowledge resource if it would be
needed, as part of a compositional process, to infer
the meaning of e from some natural texts. Based
on this definition, we perform an analysis of the re-
lationships included in lexical-semantic resources,
as described in Section 5.
A rule need not apply in all contexts, as long
as it is appropriate for some texts. Two contex-
tual aspects affect rule applicability. First is the
“lexical context” specifying the meanings of the
text’s words. A rules is applicable in a certain con-
text only when the intended sense of its LHS term
matches the sense of that term in the text. For ex-
ample, the application of the rule lay ? produce is
valid only in contexts where the producer is poul-
try and the products are eggs. This is a well known
issue observed, for instance, by Voorhees (1994).
A second contextual factor requiring validation
is the “logical context”. The logical context de-
termines the monotonicity of the LHS and is in-
duced by logical operators such as negation and
(explicit or implicit) quantifiers. For example, the
rule mammal ? whale may not be valid in most
cases, but is applicable in universally quantified
texts like “mammals are warm-blooded”. This is-
sue has been rarely addressed in applied inference
systems (de Marneffe et al., 2006). The above
mentioned rules both comply with Definition 2
and should therefore be included in a lexical en-
tailment resource.
3 Evaluating Entailment Resources
Our evaluation goal is to assess the utility of
lexical-semantic resources as sources for entail-
ment rules. An inference system applies a rule by
inferring the rule’s RHS from texts that match its
LHS. Thus, the utility of a resource depends on the
performance of its rule applications rather than on
the proportion of correct rules it contains. A rule,
whether correct or incorrect, has insignificant ef-
fect on the resource’s utility if it rarely matches
texts in real application settings. Additionally,
correct rules might produce incorrect applications
when applied in inappropriate contexts. There-
fore, we use an instance-based evaluation method-
ology, which simulates rule applications by col-
lecting texts that contain rules’ LHS and manually
assessing the correctness of their applications.
Systems typically handle lexical context either
implicitly or explicitly. Implicit context valida-
tion occurs when the different terms of a compos-
ite hypothesis disambiguate each other. For exam-
ple, the rule waterside ? bank is unlikely to be
applied when trying to infer the hypothesis bank
loans, since texts that match waterside are unlikely
to contain also the meaning of loan. Explicit meth-
ods, such as word-sense disambiguation or sense
matching, validate each rule application according
to the broader context in the text. Few systems
also address logical context validation by handling
quantifiers and negation. As we aim for a system-
independent comparison of resources, and explicit
approaches are not standardized yet within infer-
ence systems, our evaluation uses only implicit
context validation.
3.1 Evaluation Methodology
Figure 1: Evaluation methodology flow chart
The input for our evaluation methodology is a
lexical-semantic resource R, which contains lex-
ical entailment rules. We evaluate R’s utility by
testing how useful it is for inferring a sample of
test hypotheses H from a corpus. Each hypothesis
in H contains more than one lexical element in or-
der to provide implicit context validation for rule
applications, e.g. h: water pollution.
We next describe the steps of our evaluation
methodology, as illustrated in Figure 1. We refer
to the examples in the figure when needed:
1) Fetch rules: For each h ? H and each
lexical element e ? h (e.g. water), we fetch all
rules e’ ? e in R that might be applied to entail e
(e.g. lake ? water).
2) Generate intermediate hypotheses h’:
For each rule r: e’ ? e, we generate an intermedi-
ate hypothesis h? by replacing e in h with e? (e.g.
560
h?1: lake pollution). From a text t entailing h
?, h
can be further entailed by the single application of
r. We thus simulate the process by which an en-
tailment system would infer h from t using r.
3) Retrieve matching texts: For each h? we
retrieve from a corpus all texts that contain the
lemmatized words of h? (not necessarily as a sin-
gle phrase). These texts may entail h?. We dis-
card texts that also match h since entailing h from
them might not require the application of any rule
from the evaluated resource. In our example, the
retrieved texts contain lake and pollution but do
not contain water.
4) Annotation: A sample of the retrieved texts
is presented to human annotators. The annotators
are asked to answer the following two questions
for each text, simulating the typical inference pro-
cess of an entailment system:
a) Does t entail h’? If t does not entail h?
then the text would not provide a useful example
for the application of r. For instance, t1 (in Fig-
ure 1) does not entail h?1 and thus we cannot de-
duce h from it by applying the rule r. Such texts
are discarded from further evaluation.
b) Does t entail h? If t is annotated as en-
tailing h?, an entailment system would then infer
h from h? by applying r. If h is not entailed from
t even though h? is, the rule application is consid-
ered invalid. For instance, t2 does not entail h even
though it entails h?2. Indeed, the application of r2:
*soil ? water 2, from which h?2 was constructed,
yields incorrect inference. If the answer is ’yes’,
as in the case of t3, the application of r for t is
considered valid.
The above process yields a sample of annotated
rule applications for each test hypothesis, from
which we can measure resources performance, as
described in Section 5.
4 Experimental Setting
4.1 Dataset and Annotation
Current available state-of-the-art lexical-semantic
resources mainly deal with nouns. Therefore, we
used nominal hypotheses for our experiment3.
We chose TREC 1-8 (excluding 4) as our test
corpus and randomly sampled 25 ad-hoc queries
of two-word compounds as our hypotheses. We
did not use longer hypotheses to ensure that
2The asterisk marks an incorrect rule.
3We suggest that the definitions and methodologies can be
applied for other parts of speech as well.
enough texts containing the intermediate hypothe-
ses are found in the corpus. For annotation sim-
plicity, we retrieved single sentences as our texts.
For each rule applied for an hypothesis h, we
sampled 10 sentences from the sentences retrieved
for that rule. As a baseline, we also sampled 10
sentences for each original hypothesis h in which
both words of h are found. In total, 1550 unique
sentences were sampled and annotated by two an-
notators.
To assess the validity of our evaluation method-
ology, the annotators first judged a sample of 220
sentences. The Kappa scores for inter-annotator
agreement were 0.74 and 0.64 for judging h? and
h, respectively. These figures correspond to sub-
stantial agreement (Landis and Koch, 1997) and
are comparable with related semantic annotations
(Szpektor et al., 2007; Bhagat et al., 2007).
4.2 Lexical-Semantic Resources
We evaluated the following resources:
WordNet (WNd): There is no clear agreement
regarding which set of WordNet relations is use-
ful for entailment inference. We therefore took a
conservative approach using only synonymy and
hyponymy rules, which typically comply with the
lexical entailment relation and are commonly used
by textual entailment systems, e.g. (Herrera et al.,
2005; Bos and Markert, 2006). Given a term e,
we created a rule e’ ? e for each e? amongst the
synonyms or direct hyponyms for all senses of e
in WordNet 3.0.
Snow (Snow30k): Snow et al. (2006) pre-
sented a probabilistic model for taxonomy induc-
tion which considers as features paths in parse
trees between related taxonomy nodes. They show
that the best performing taxonomy was the one
adding 30,000 hyponyms to WordNet. We created
an entailment rule for each new hyponym added to
WordNet by their algorithm4.
LCC’s extended WordNet (XWN?): In
(Moldovan and Rus, 2001) WordNet glosses were
transformed into logical form axioms. From this
representation we created a rule e’ ? e for each e?
in the gloss which was tagged as referring to the
same entity as e.
CBC: A knowledgebase of labeled clusters gen-
erated by the statistical clustering and labeling al-
gorithms in (Pantel and Lin, 2002; Pantel and
4Available at http://ai.stanford.edu/˜ rion/swn
561
Ravichandran, 2004)5. Given a cluster label e, an
entailment rule e’ ? e is created for each member
e? of the cluster.
Lin Dependency Similarity (Lin-dep): A
distributional word similarity resource based on
syntactic-dependency features (Lin, 1998). Given
a term e and its list of similar terms, we construct
for each e? in the list the rule e’ ? e. This resource
was previously used in textual entailment engines,
e.g. (Roth and Sammons, 2007).
Lin Proximity Similarity (Lin-prox): A
knowledgebase of terms with their cooccurrence-
based distributionally similar terms. Rules are cre-
ated from this resource as from the previous one6.
Wikipedia first sentence (WikiFS): Kazama
and Torisawa (2007) used Wikipedia as an exter-
nal knowledge to improve Named Entity Recog-
nition. Using the first step of their algorithm, we
extracted from the first sentence of each page a
noun that appears in a is-a pattern referring to the
title. For each such pair we constructed a rule title
? noun (e.g. Michelle Pfeiffer ? actress).
The above resources represent various meth-
ods for detecting semantic relatedness between
words: Manually and semi-automatically con-
structed (WNd and XWN?, respectively), automat-
ically constructed based on a lexical-syntactic pat-
tern (WikiFS), distributional methods (Lin-dep and
Lin-prox) and combinations of pattern-based and
distributional methods (CBC and Snow30k).
5 Results and Analysis
The results and analysis described in this section
reveal new aspects concerning the utility of re-
sources for lexical entailment, and experimentally
quantify several intuitively-accepted notions re-
garding these resources and the lexical entailment
relation. Overall, our findings highlight where ef-
forts in developing future resources and inference
systems should be invested.
5.1 Resources Performance
Each resource was evaluated using two measures -
Precision and Recall-share, macro averaged over
all hypotheses. The results achieved for each re-
source are summarized in Table 1.
5Kindly provided to us by Patrick Pantel.
6Lin’s resources were downloaded from:
http://www.cs.ualberta.ca/˜ lindek/demos.htm
Resource Precision (%) Recall-share (%)
Snow30k 56 8
WNd 55 24
XWN? 51 9
WikiFS 45 7
CBC 33 9
Lin-dep 28 45
Lin-prox 24 36
Table 1: Lexical resources performance
5.1.1 Precision
The Precision of a resource R is the percentage of
valid rule applications for the resource. It is esti-
mated by the percentage of texts entailing h from
those that entail h?: countR(entailing h=yes)countR(entailing h?=yes) .
Not surprisingly, resources such as WNd, XWN?
or WikiFS achieved relatively high precision
scores, due to their accurate construction meth-
ods. In contrast, Lin’s distributional resources are
not designed to include lexical entailment relation-
ships. They provide pairs of contextually simi-
lar words, of which many have non-entailing rela-
tionships, such as co-hyponyms7 (e.g. *doctor ?
journalist) or topically-related words, such as *ra-
diotherapy ? outpatient. Hence their relatively
low precision.
One visible outcome is the large gap between
the perceived high accuracy of resources con-
structed by accurate methods, most notably WNd,
and their performance in practice. This finding
emphasizes the need for instance-based evalua-
tions, which capture the “real” contribution of a
resource. To better understand the reasons for
this gap we further assessed the three factors
that contribute to incorrect applications: incorrect
rules, lexical context and logical context (see Sec-
tion 2.2). This analysis is presented in Table 2.
From Table 2 we see that the gap for accurate
resources is mainly caused by applications of cor-
rect rules in inappropriate contexts. More inter-
estingly, the information in the table allows us to
asses the lexical “context-sensitivity” of resources.
When considering only the COR-LEX rules to re-
calculate resources precision, we find that Lin-dep
achieves precision of 71% ( 15%15%+6% ), while WN
d
yields only 56% ( 55%55%+44% ). This result indicates
that correct Lin-dep rules are less sensitive to lexi-
cal context, meaning that their prior likelihoods to
7a.k.a. sister terms or coordinate terms
562
(%)
Invalid Rule Applications Valid Rule Applications
INCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)
WNd 1 0 44 45 0 0 55 55
WikiFS 13 0 42 55 3 0 42 45
XWN? 19 0 30 49 0 0 51 51
Snow30k 23 0 21 44 0 0 56 56
CBC 51 12 4 67 14 0 19 33
Lin-prox 59 4 13 76 8 3 13 24
Lin-dep 61 5 6 72 9 4 15 28
Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring
“logical context” validation (COR-LOG), and correct rules requiring “lexical context” matching (COR-LEX). The numbers of each
resource’s valid applications add up to the resource’s precision.
be correct are higher. This is explained by the fact
that Lin-dep’s rules are calculated across multiple
contexts and therefore capture the more frequent
usages of words. WordNet, on the other hand, in-
cludes many anecdotal rules whose application is
rare, and thus is very sensitive to context. Simi-
larly, WikiFS turns out to be very context-sensitive.
This resource contains many rules for polysemous
proper nouns that are scarce in their proper noun
sense, e.g. Captive ? computer game. Snow30k,
when applied with the same calculation, reaches
73%, which explains how it achieved a compara-
ble result to WNd, even though it contains many
incorrect rules in comparison to WNd.
5.1.2 Recall
Absolute recall cannot be measured since the total
number of texts in the corpus that entail each hy-
pothesis is unknown. Instead, we measure recall-
share, the contribution of each resource to recall
relative to matching only the words of the origi-
nal hypothesis without any rules. We denote by
yield(h) the number of texts that match h directly
and are annotated as entailing h. This figure is es-
timated by the number of sampled texts annotated
as entailing h multiplied by the sampling propor-
tion. In the same fashion, for each resource R,
we estimate the number of texts entailing h ob-
tained through entailment rules of the resource R,
denoted yieldR(h). Recall-share of R for h is the
proportion of the yield obtained by the resource’s
rules relative to the overall yield with and without
the rules from R: yieldR(h)yield(h)+yieldR(h) .
From Table 1 we see that along with their rela-
tively low precision, Lin’s resources’ recall greatly
surpasses that of any other resource, including
WordNet8. The rest of the resources are even infe-
8A preliminary experiment we conducted showed that re-
rior to WNd in that respect, indicating their limited
utility for inference systems.
As expected, synonyms and hyponyms in Word-
Net contributed a noticeable portion to recall in all
resources. Additional correct rules correspond to
hyponyms and synonyms missing from WordNet,
many of them proper names and some slang ex-
pressions. These rules were mainly provided by
WikiFS and Snow30k, significantly supplementing
WordNet, whose HasInstance relation is quite par-
tial. However, there are other interesting types of
entailment relations contributing to recall. These
are discussed in Sections 5.2 and 5.3. Examples
for various rule types are found in Table 3.
5.1.3 Valid Applications of Incorrect Rules
We observed that many entailing sentences were
retrieved by inherently incorrect rules in the distri-
butional resources. Analysis of these rules reveals
they were matched in entailing texts when the LHS
has noticeable statistical correlation with another
term in the text that does entail the RHS. For ex-
ample, for the hypothesis wildlife extinction, the
rule *species ? extinction yielded valid applica-
tions in contexts about threatened or endangered
species. Has the resource included a rule between
the entailing term in the text and the RHS, the
entailing text would have been matched without
needing the incorrect rule.
These correlations accounted for nearly a third
of Lin resources’ recall. Nonetheless, in princi-
ple, we suggest that such rules, which do not con-
form with Definition 2, should not be included in a
lexical entailment resource, since they also cause
invalid rule applications, while the entailing texts
they retrieve will hopefully be matched by addi-
call does not dramatically improve when using the entire hy-
ponymy subtree from WordNet.
563
Type Correct Rules
HYPO Shevardnadze ? official Snow30k
ANT efficacy ? ineffectiveness Lin-dep
HOLO government ? official Lin-prox
HYPER arms ? gun Lin-prox
˜ childbirth ? motherhood Lin-dep
˜ mortgage ? bank Lin-prox
˜ Captive ? computer WikiFS
˜ negligence ? failure CBC
˜ beatification ? pope XWN?
Type Incorrect Rules
CO-HYP alcohol ? cigarette CBC
˜ radiotherapy ? outpatient Lin-dep
˜ teen-ager ? gun Snow30k
˜ basic ? paper WikiFS
˜ species ? extinction Lin-prox
Table 3: Examples of lexical resources rules by types.
HYPO: hyponymy, HYPER: hypernymy (class entailment of
its members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-
hyponymy. The non-categorized relations do not correspond
to any WordNet relation.
tional correct rules in a more comprehensive re-
source.
5.2 Non-standard Entailment Relations
An important finding of our analysis is that some
less standard entailment relationships have a con-
siderable impact on recall (see Table 3). These
rules, which comply with Definition 2 but do
not conform to any WordNet relation type, were
mainly contributed by Lin’s distributional re-
sources and to a smaller degree are also included
in XWN?. In Lin-dep, for example, they accounted
for approximately a third of the recall.
Among the finer grained relations we identi-
fied in this set are topical entailment (e.g. IBM
as the company entailing the topic computers),
consequential relationships (pregnancy?mother-
hood) and an entailment of inherent arguments by
a predicate, or of essential participants by a sce-
nario description, e.g. beatification ? pope. A
comprehensive typology of these relationships re-
quires further investigation, as well as the identi-
fication and development of additional resources
from which they can be extracted.
As opposed to hyponymy and synonymy rules,
these rules are typically non-substitutable, i.e. the
RHS of the rule is unlikely to have the exact same
role in the text as the LHS. Many inference sys-
tems perform rule-based transformations, substi-
tuting the LHS by the RHS. This finding suggests
that different methods may be required to utilize
such rules for inference.
5.3 Logical Context
WordNet relations other than synonyms and hy-
ponyms, including antonyms, holonyms and hy-
pernyms (see Table 3), contributed a noticeable
share of valid rule applications for some resources.
Following common practice, these relations are
missing by construction from the other resources.
As shown in Table 2 (COR-LOG columns), such
relations accounted for a seventh of Lin-dep’s
valid rule applications, as much as was the con-
tribution of hyponyms and synonyms to this re-
source’s recall. Yet, using these rules resulted with
more erroneous applications than correct ones. As
discussed in Section 2.2, the rules induced by
these relations do conform with our lexical entail-
ment definition. However, a valid application of
these rules requires certain logical conditions to
occur, which is not the common case. We thus
suggest that such rules are included in lexical en-
tailment resources, as long as they are marked
properly by their types, allowing inference sys-
tems to utilize them only when appropriate mech-
anisms for handling logical context are in place.
5.4 Rules Priors
In Section 5.1.1 we observed that some resources
are highly sensitive to context. Hence, when con-
sidering the validity of a rule’s application, two
factors should be regarded: the actual context in
which the rule is to be applied, as well as the rule’s
prior likelihood to be valid in an arbitrary con-
text. Somewhat indicative, yet mostly indirect, in-
formation about rules’ priors is contained in some
resources. This includes sense ranks in WordNet,
SemCor statistics (Miller et al., 1993), and similar-
ity scores and rankings in Lin’s resources. Infer-
ence systems often incorporated this information,
typically as top-k or threshold-based filters (Pan-
tel and Lin, 2003; Roth and Sammons, 2007). By
empirically assessing the effect of several such fil-
ters in our setting, we found that this type of data
is indeed informative in the sense that precision
increases as the threshold rises. Yet, no specific
filters were found to improve results in terms of
F1 score (where recall is measured relatively to
the yield of the unfiltered resource) due to a sig-
nificant drop in relative recall. For example, Lin-
564
prox loses more than 40% of its recall when only
the top-50 rules for each hypothesis are exploited,
and using only the first sense of WNd costs the re-
source over 60% of its recall. We thus suggest a
better strategy might be to combine the prior in-
formation with context matching scores in order
to obtain overall likelihood scores for rule appli-
cations, as in (Szpektor et al., 2008). Furthermore,
resources should include explicit information re-
garding the prior likelihoods of of their rules.
5.5 Operative Conclusions
Our findings highlight the currently limited re-
call of available resources for lexical inference.
The higher recall of Lin’s resources indicates
that many more entailment relationships can be
acquired, particularly when considering distribu-
tional evidence. Yet, available distributional ac-
quisition methods are not geared for lexical entail-
ment. This suggests the need to develop acqui-
sition methods for dedicated and more extensive
knowledge resources that would subsume the cor-
rect rules found by current distributional methods.
Furthermore, substantially better recall may be ob-
tained by acquiring non-standard lexical entail-
ment relationships, as discussed in Section 5.2, for
which a comprehensive typology is still needed.
At the same time, transformation-based inference
systems would need to handle these kinds of rules,
which are usually non-substitutable. Our results
also quantify and stress earlier findings regarding
the severe degradation in precision when rules are
applied in inappropriate contexts. This highlights
the need for resources to provide explicit informa-
tion about the suitable lexical and logical contexts
in which an entailment rule is applicable. In par-
allel, methods should be developed to utilize such
contextual information within inference systems.
Additional auxiliary information needed in lexical
resources is the prior likelihood for a given rule to
be correct in an arbitrary context.
6 Related Work
Several prior works defined lexical entailment.
WordNet’s lexical entailment is a relationship be-
tween verbs only, defined for propositions (Fell-
baum, 1998). Geffet and Dagan (2004) defined
substitutable lexical entailment as a relation be-
tween substitutable terms. We find this definition
too restrictive as non-substitutable rules may also
be useful for entailment inference. Examples are
breastfeeding ? baby and hospital ? medical.
Hence, Definition 2 is more broadly applicable for
defining the desired contents of lexical entailment
resources. We empirically observed that the rules
satisfying their definition are a proper subset of
the rules covered by our definition. Dagan and
Glickman (2004) referred to entailment at the sub-
sentential level by assigning truth values to sub-
propositional text fragments through their existen-
tial meaning. We find this criterion too permissive.
For instance, the existence of country implies the
existence of its flag. Yet, the meaning of flag is
typically not implied by country.
Previous works assessing rule application via
human annotation include (Pantel et al., 2007;
Szpektor et al., 2007), which evaluate acquisition
methods for lexical-syntactic rules. They posed an
additional question to the annotators asking them
to filter out invalid contexts. In our methodology
implicit context matching for the full hypothesis
was applied instead. Other related instance-based
evaluations (Giuliano and Gliozzo, 2007; Connor
and Roth, 2007) performed lexical substitutions,
but did not handle the non-substitutable cases.
7 Conclusions
This paper provides several methodological and
empirical contributions. We presented a novel
evaluation methodology for the utility of lexical-
semantic resources for semantic inference. To that
end we proposed definitions for entailment at sub-
sentential levels, addressing a gap in the textual
entailment framework. Our evaluation and analy-
sis provide a first quantitative comparative assess-
ment of the isolated utility of a range of prominent
potential resources for entailment rules. We have
shown various factors affecting rule applicability
and resources performance, while providing oper-
ative suggestions to address them in future infer-
ence systems and resources.
Acknowledgments
The authors would like to thank Naomi Frankel
and Iddo Greental for their excellent annotation
work, as well as Roy Bar-Haim and Idan Szpektor
for helpful discussion and advice. This work was
partially supported by the Negev Consortium of
the Israeli Ministry of Industry, Trade and Labor,
the PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1095/05.
565
References
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
J. Bos and K. Markert. 2006. When logical infer-
ence helps determining textual entailment (and when
it doesn’t). In Proceedings of the Second PASCAL
RTE Challenge.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan and Oren Glickman. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Joaquin Quinonero Candela, Ido Da-
gan, Bernardo Magnini, and Florence d’Alche´ Buc,
editors, MLCW, Lecture Notes in Computer Science.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D. Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of the Sec-
ond PASCAL RTE Challenge.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings
of COLING.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of EMNLP-CoNLL.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Jesu´s Herrera, Anselmo Pen˜as, and Felisa Verdejo.
2005. Textual entailment recognition based on de-
pendency analysis and wordnet. In Proceedings of
the First PASCAL RTE Challenge.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
Milen Kouylekov and Bernardo Magnini. 2006. Build-
ing a large-scale repository of textual entailment
rules. In Proceedings of LREC.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. In Bio-
metrics, pages 33:159–174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of ACM
SIGKDD.
Patrick Pantel and Dekang Lin. 2003. Automatically
discovering word senses. In Proceedings of NAACL.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT-NAACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of HLT.
Marius Pasca and Sanda M. Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of NAACL Workshop on
WordNet and Other Lexical Resources.
Dan Roth and Mark Sammons. 2007. Semantic and
logical inference model for textual entailment. In
Proceedings of ACL-WTEP Workshop.
Chirag Shah and Bruce W. Croft. 2004. Evaluating
high accuracy retrieval techniques. In Proceedings
of SIGIR.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
566
