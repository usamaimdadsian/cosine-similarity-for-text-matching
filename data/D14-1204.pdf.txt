Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1902–1911,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Automatic Inference of the Tense of Chinese Events Using Implicit
Linguistic Information
Yuchen Zhang
Brandeis University
415 South Street
Waltham, MA
yuchenz@brandeis.edu
Nianwen Xue
Brandeis University
415 South Street
Waltham, MA
xuen@brandeis.edu
Abstract
We address the problem of automatically
inferring the tense of events in Chinese
text. We use a new corpus annotated with
Chinese semantic tense information and
other implicit Chinese linguistic informa-
tion using a “distant annotation” method.
We propose three improvements over a rel-
atively strong baseline method – a statisti-
cal learning method with extensive feature
engineering. First, we add two sources
of implicit linguistic information as fea-
tures – eventuality type and modality of
an event, which are also inferred automat-
ically. Second, we perform joint learning
on semantic tense, eventuality type, and
modality of an event. Third, we train arti-
ficial neural network models for this prob-
lem and compare its performance with
feature-based approaches. Experimental
results show considerable improvements
on Chinese tense inference. Our best per-
formance reaches 68.6% in accuracy, out-
performing a strong baseline method.
1 Introduction
As a language with no grammatical tense, Chinese
does not encode the temporal location of an event
directly in a verb, while in English, the grammati-
cal tense of a verb is a strong indicator of the tem-
poral location of an event. In this paper we ad-
dress the problem of inferring the semantic tense,
or the temporal location of an event (e.g., present,
past, future) in Chinese text. The semantic tense is
defined relative to the utterance time or document
creation time, and it does not always agree with
the grammatical tense in languages like English
where there is grammatical tense. Inferring se-
mantic tense potentially benefits natural language
processing tasks such as Machine Translation and
Information Extraction (Xue, 2008; Reichart and
Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et
al., 2011), but previous work has shown that auto-
matic inference of the semantic tense of events in
Chinese is a very challenging task (Xue, 2008; Ye
et al., 2006; Liu et al., 2011).
There are at least two reasons why this is a dif-
ficult problem. First, since Chinese does not have
grammatical tense which could serve as an impor-
tant clue when annotating the semantic tense of
an event, generating consistent annotation for Chi-
nese semantic tense has proved to be a challenge.
Xue and Zhang (2014) use a “distant annotation”
method to address this problem. They take advan-
tage of an English-Chinese parallel corpus with
manual word alignments (Li et al., 2012) , and per-
form annotation on the English side, which pro-
vides more explicit information such as grammati-
cal tense that helps annotators decide the appropri-
ate semantic tense. The annotations are then pro-
jected to the Chinese side via the word alignments.
They show consistent annotation agreements on
semantic tense. Second, the lack of grammatical
tense also makes automatic inference of Chinese
semantic tense challenging since the grammatical
tense would be an important source of information
for predicting the semantic tense. Previous work
has shown that it is very difficult to achieve high
accuracy using standard machine learning tech-
niques such as Maximum Entropy and Conditional
Random Field classifiers combined with extensive
feature engineering.
We address these challenges in two ways. First
of all, we take advantage of the newly annotated
corpus described in (Xue and Zhang, 2014) in
which semantic tense is annotated together with
eventuality type and modality using the distant an-
notation method. This makes it possible to use
these two additional sources of information to help
predict tense. Eventuality type and modality are
intricately tied to tense. For example, Smith and
1902
Erbaugh (2005) show that states by default hold in
the present but (episodic) events occur by default
in the past. This means knowing the eventuality
type of an event would help determine the tense.
Eventuality type and modality are also annotated
on the English side and then projected onto the
Chinese side via manual word alignments, taking
advantage of the rich morphosyntactic clues in En-
glish. High inter-annotator agreement scores are
also reported on eventuality type and modality.
We experimented with two ways of using even-
tuality type and modality information. In the first
approach, we first train statistical machine learn-
ing models to predict eventuality type and modal-
ity and then use these two sources of information
as features to predict semantic tense. In the sec-
ond approach we trained joint learning models be-
tween semantic tense and eventuality type, and be-
tween semantic tense and modality. We show both
approaches improve the tense inference accuracy
over a baseline where these two sources of infor-
mation are not used. Second, in our statistical
machine learning experiments on tense inference
using feature engineering, we find that the design
of feature templates has great influence on the re-
sults. So in order to explore more possible feature
combinations and mitigate the feature engineering
work, we apply artificial neural network models to
this problem. This shows improvements on tense
inference accuracy as well in some of the experi-
ment settings.
The rest of the paper is organized as follows.
Section 2 discusses related work in automatic
tense inference. Section 3 briefly introduces the
distant annotation method. In section 4, we de-
scribe our experiments and analyze the experimen-
tal results. We conclude this paper in section 7.
2 Related Work
Inferring the semantic tense of events in Chinese
text is not a new topic. There have been several
attempts at it, yet high accuracy in this task has
proved to be elusive. Using a corpus with tense
annotated directly in Chinese text, Xue (2008) per-
formed extensive feature engineering in a machine
learning framework to address this problem. They
used both local lexical features and structured fea-
tures extracted from manually annotated syntactic
parsing trees. In our baseline method, we adopt
most of their features as the baseline, only on a
new corpus in which semantic tenses are not an-
notated directly on Chinese events but projected
from annotations from the English side of a par-
allel Chinese-English corpus. In our experiments,
we also use structural features extracted from au-
tomatic parse trees, so our experimental settings
are more realistic.
Ye et al. (2006) took a similar approach in
which they predict tense with feature engineering
in a statistical learning framework. They also used
a Chinese-English parallel corpus and projected
tense for English events onto Chinese events via
human alignments. The main difference between
their data and ours is that they used the gram-
matical tense of the English events, while we use
human-annotated semantic tense which we believe
are more “transferrable” across languages as it
is free of the language-specific idiosyncrasies of
grammatical tense. In addition, they also used hu-
man annotated linguistic information as “latent”
features in their work, which are similar to our
implicit linguistic features. However, the “latent”
features that they used in their system are human-
annotated, while the eventuality type and modality
features in our system are predicted automatically.
Another difference is that they ignored events that
are not verbs. For example, they excluded ver-
bal expressions in Chinese that are translated into
nominal phrases in English. In contrast, we kept
all events in our data, and they can be realized as
verbs, nouns, as well as words in other parts of
speech. We performed separate experiments on
events realized as verbal expressions and events
not in verbal expressions to investigate their im-
pact on semantic tense inference.
Liu et al. (2011) introduced more global fea-
tures in a machine learning framework, and on
top of that proposed an iterative learning algorithm
which better handles noisy data, but they also ig-
nored events that are not realized as verbal ex-
pressions, or events that are verbal expressions but
have more than one verb in them. They mainly
focused on events that are one-verb expressions.
In a similar work on inferring tense in English
text, Reichart and Rappoport (2010) aimed at in-
ferring fine-grained semantic tenses for events in
English. They introduced a fine-grained sense tax-
onomy for tense in a more general Tense Sense
Disambiguation (TSD) task to annotate and dis-
ambiguate semantic tenses. The underlying senses
include “things that are always true”, “general and
repeated actions and habits”, “plans, expectations
1903
and hopes”, etc., which encode a combination of
tense, eventuality type and modality. In the corpus
that we use, the same information is organized in
a more structured manner along three dimensions
– semantic tense, eventuality type, and modality.
3 Distant Annotation
Figure 1 shows the distant annotation procedure
from (Xue and Zhang, 2014). Starting with a
word-aligned parallel English-Chinese corpus, all
sentences are part-of-speech (POS) tagged first
and then all verb instances in the English text as
well as expressions aligned with verb instances
on the Chinese side are targeted for annotation.
As we will show in Section 4, these expressions
include verbs as well as nouns, prepositions and
word sequences “headed” by a verb. We consider
those expressions as events. Annotators work only
on the English side and tag every event with a
pre-defined semantic tense label. These labels are
then projected from the English side to the Chi-
nese side via word alignments. The resulting cor-
pus contains events annotated with semantic tense
labels in both languages. Categories for seman-
tic tense are “Past”, “Present”, “Future”, “Relative
Past”, “Relative Present”, “Relative Future”, and
“None”.
Events annotated with relative tenses are also
linked to another event that serves as the tempo-
ral reference for the event in question. In some
cases the relative tense can be resolved to an ab-
solute tense. For example, if an event is anno-
tated with a “relative past” tense to a reference
event that is annotated with a present tense, then
the semantic tense of that event can resolve to an
absolute “past” tense. In other cases, they can
not be resolved. For example, if an event is la-
beled with a “relative future” tense and the refer-
ence event has a past tense, then its tense cannot
be resolved to an absolute tense, which is defined
with regard to the utterance time or document cre-
ation time. In our work, where possible, we re-
solve these links and keep only absolute tense la-
bels. For events with relative tenses that can not
be resolved (i.e. events which are “Relative Fu-
ture” to “Past” events, or events which are “Rela-
tive Past” to “Future” events), we use “None” as
the default label.
Eventuality type and modality are labeled in the
same way as auxiliary annotation that can help
with the inference of tense. Labels for eventual-
ity type include “Episodic”, “Habitual”, “State”,
“Progressive”, “Completed”, and “None”. Labels
for modality are “Actual”, “Intended”, “Hypothet-
ical”, “Modalized”, and “None”. Readers are ref-
ered to (Xue and Zhang, 2014) for detailed expla-
nations of each label.
Figure 1: Distant annotation procedure.
As we mentioned in Section 2, in this corpus
not only verbs but also their counterparts on the
opposite language are considered as events, yield-
ing events that may not be verbs. For example,
in the following sentence pair (1), the Chinese
verb (VV) “??” is aligned with an English noun
(NN) “use”. In the sentence pair (2), the English
verb (VBG) “opening” is aligned with an Chinese
noun (NN) “??”.
(1) Statistics show that , in the past five years ,
Guangxi’s foreign trade and its use of foreign
investments has expanded rapidly.
?????????????????
???(li4yong4)?????????
(2) Beihai has already become a bright star aris-
ing from China’s policy of opening up to the
outside world.
???????????(kai1fang4) ?
????????
In this corpus, events could be either one verb,
or a verb compound, or a verb sequence “headed”
by a verb, or even nouns and words of other parts
of speech.
4 Experiments
4.1 Experimental Setting
Xue and Zhang (2014) annotated semantic tense,
eventuality type and modality on top of the Par-
allel Aligned Treebank (Li et al., 2012), a corpus
1904
of word-aligned Chinese-English sentences tree-
banked based on the Penn TreeBank (Marcus et
al., 1993) and the Chinese TreeBank (Xue et al.,
2005) standards. Human annotation of tense is
performed on the newswire and webblog sections
of this corpus. They report that the average pair-
wise agreement among three annotators consis-
tently stays above 80% and the average Kappa
score consistently exceeds 70%, indicating reli-
able annotation.
Apart from using the entire corpus, we also con-
ducted experiments on three different subsets of
the corpus. An examination of the data indicates
that newswire data is grammatically more for-
mal and complete than webblog data, so we also
conducted separate experiments on newswire data
only. Considering that the diversity of the parts
of speech of the events may affect the inference
accuracy and that most of our features extracted
from the parse trees assume that our events being
verbs, we also conducted experiments exclusively
on “v events”. “v events” consist of two parts.
One part is events that are realized as a single word
and the word is a verb; the other one is events
which have multiple words but there is only one
verb among them. In the latter case, we stripped
off words tagged with other parts of speech and
only keep the verbs as events. This makes it more
effective to use features from previous work that
are designed for single verbs. One such feature is
the aspect marker. Distinctions between newswire
and webblog data and between v events and other
events are further explored in Section 5.1 and Sec-
tion 5.2. Table 1 presents the statistics for each
subset of the experimental data.
dataset # of v events # of all events
nw 6,686 8,268
all 17,153 20,885
Table 1: Statistics of four subsets of the annotated
corpus (Chinese side). “nw” denotes the newswire
data. “v events” denotes events that consist of or
can be reduced to only a single verb.
For each subset, randomly selected 80% were
used as the training set, while 10% were used as
the development set and 10% were used as the test
set.
4.2 Baseline
Based on previous approaches on Chinese tense
inference, we used a Maximum Entropy model
with extensive feature engineering as our baseline
method. We use the implementation of the Maxi-
mum Entropy algorithm in Mallet
1
for our exper-
iments. The corpus is parsed using the Berkeley
Parser for the purpose of extracting structure fea-
tures. Since the Parallel Alignment TreeBank is
a subset of the Chinese TreeBank (CTB) 8.0, we
automatically parsed the CTB 8.0 by doing a 10-
fold cross validation. The bracketing F-score is
80.5%. Feature extractions are performed on the
automatic parse trees. Adopted features include
previous word and its POS tag, next word and
its POS tag, aspect marker following the event,
?following the event, the governing verb of the
event, the character string of the main verb in the
previous clause that is coordinated with the clause
the event is in, whether the event is in quote, and
left modifiers of the event including head of adver-
bial phrases, temporal noun phrases, prepositional
phrases, localizer phrases, as well as subordinat-
ing clauses. Readers are referred to (Xue, 2008)
for details of these features. Since in this corpus
an event can span over more than one verb, we
also use the character string and the POS string of
the entire event instead of one word and one POS
tag as features.
• The character string of an event – it could
be one or more words. In our corpus, only
69.7% events consist of single word (e.g. “?
?”, “live”), the other 30.3% of the events are
expressed with two or more words (e.g. “?
?+?”, “have caused”).
• The POS string of an event – it could be
verbs, nouns, or POS sequences of other
word sequences. Table 2 shows the top ten
POS tag or POS tag sequences with example
word or word sequences.
Other features that we used in the baseline sys-
tem are as follows.
• DEC – if the word immediately following an
event has the POS tag “DEC”, use its charac-
ter string as a feature. In most cases, “DEC”
is the POS tag for “?” when it used as a com-
plementizer marking the boundary in a rela-
tive clause. This feature implies that an event
1
http://mallet.cs.umass.edu/
1905
POS freq examples
VV 48.2% ??(live)
NN 5.8% ??(opening)
VC 5.2% ?(is)
VV+AS 5.2% ??+?(have caused)
VV+DEC 3.2% ??+?(isolated)
AD+VV 3.0% ??+??(is suggesting)
VA 3.0% ?(is big)
AD 2.0% ??(seemed)
VE 1.9% ?(there is)
P 1.8% ??(according to)
Table 2: Frequencies and examples of the ten most
frequent POS tag or POS tag sequences for events
in our corpus.
is inside a relative clause modifying a noun
phrase and it is more often stative than even-
tive.
• Determiners – we find the subject of an event
from its parse tree and extract the determiner
of the subject, if there is one, as a fea-
ture. This feature indicates different types
of agents, and different types of agents of-
ten signal different types of events. For ex-
ample, individual agents tend to perform one-
time episodic actions which are by default lo-
cated in the past or described by a state in
the present, while multiple agents tend to in-
volved in habitual actions that spans over a
long period of time.
Baseline results are reported in Table 5 and Ta-
ble 6, in MaxEnt b rows.
4.3 Eventuality Type and Modality as
Features
Xue and Zhang (2014) reports that gold eventual-
ity type and modality labels significantly help the
inference of tense in Chinese, improving the ac-
curacy by more than 20%. However, it is unreal-
istic to expect to have human annotated eventual-
ity type and modality labels in a random new data
set if we want to use these two sources of implicit
linguistic information in any Chinese text. So we
trained statistical learning models to automatically
extract these two labels. We trained Maximum En-
tropy models and ran a 10-fold cross validation on
the entire corpus in order to get automatic labels
for every event. Feature used for labeling modal-
ity are as follows. Table 3 shows the average ac-
curacies for automatic modality labeling.
• The character string of an event.
• The POS string of an event.
• The character string of an event’s governing
verb and its POS tag.
• Whether the event is in a conditional clause.
If an event is in a subtree with the func-
tional tag “CND”, return “True”; otherwise,
return “False”. This feature indicates that the
event’s modality label is “Hypothetical”.
• Whether the event is in a purpose or reason
clause. If an event is in a subtree with the
functional tag “PRP”, return “True”; other-
wise, return “False” as a feature. This feature
indicates the event’s modality label is “In-
tended”.
• Whether the event string is the start of a sen-
tence. If an event is the start of a sentence, re-
turn “True”; otherwise, return “False”. Sen-
tences that start with an event is often impera-
tive, and the event generally has “modalized”
modality label.
dataset v events all events
nw 81.1% 81.2%
all 75.4% 76.4%
Table 3: Average modality labeling accuracy, us-
ing a 10-fold cross validation.
Statistics show that the five labels for modality
have a skewed distribution in this corpus. Among
all events, 67.3% of them fall in the “Actual” cat-
egory, while the events of all the other categories
are around or less than 10%. Similar distributions
are found in all four subsets of the data. Still, com-
pared with always choosing the most frequent la-
bel (around 67% accuracy), we still get a big im-
provement from our statistical model, even though
only a very simple set of features are used.
Features used for labeling eventuality type are
as follows. Table 4 shows the average accuracies
for automatic eventuality type labeling.
• The character string of an event.
• The POS string of an event.
1906
• Adverbs on the left that modifies the event.
• Aspect marker following the event
• Whether the event is Inside a relative clause.
If an event is in a CP subtree with the word
“?” and POS tag “DEC” as its last node,
return “True”; otherwise, return “False”.
Events in relative clauses modifying a noun
phrase and tend to be more often stative than
eventive.
dataset v events all events
nw 68.7% 67.7%
all 65.3% 65.1%
Table 4: Average automatic eventuality type label-
ing accuracy using a 10-fold cross validation.
The six labels of eventuality type are also dis-
tributed unevenly. The first group of columns in
Figure 3 shows the distribution of all events. Over
65% of events are either “Episodic” or “State”,
while the other types of events are less than 15%.
There are two categories that are even less than
5%. However, even though we only use some sim-
ple features, our model still beats the most fre-
quent label baseline (around 35% accuracy) by a
big margin, as shown in Table 4.
Tense inference accuracies using automatic
eventuality type and/or modality features are re-
ported in Table 5 and Table 6, in MaxEnt e, Max-
Ent m, and MaxEnt em rows.
4.4 Joint Learning
Apart from using eventuality type and modality la-
bels as features, we also conducted joint learning
experiments on them. Joint learning are applied
on 1) tense and eventuality type, and 2) tense and
modality. Features used are the union of the two
sets of features in inferring each single label. Max-
Ent jle and MaxEnt jlm rows in Table 5 and Table
6 present the experimental results on joint learn-
ing.
4.5 Artificial Neural Network
For each of the experiments using the maximum
entropy algorithm, we conducted a neural network
experiment using the same setting in order to ex-
plore more possible feature combinations and mit-
igate the feature engineering work. We convert
the features in each of our tense inference meth-
ods into feature vectors. If a feature is not a word,
we use a one-hot representation for that feature (a
vector with all 0s except for a 1 at the place of the
feature’s index in our feature lexicon). If a feature
is a word, we convert it into a word embedding. To
get a dictionary of word embeddings, we use the
word2vec tool
2
(Mikolov et al., 2013) and train it
on the Chinese Gigaword corpus (LDC2003T09).
For each word embedding, a 300-dimensional vec-
tor is used. Artificial neural networks are built us-
ing the theano package
3
(Bergstra et al., 2010).
We use 5000 hidden units for all networks and set
the learning rate ? = 0.01. Experimental results
are presented in the ANN rows of Tables 5 and 6.
5 Results Analysis
A comparison of the baseline accuracy for the four
different subsets of the data shows that (1) tense
inference is slightly better on v events than on all
events, but the difference is not substantial; and
(2) tense inference on newswire data performs bet-
ter than on all data by around 8% on v events and
around 5% on all events, verifying our assump-
tion that automatic tense inference is easier on
newswire data than webblog data. Although our
experiments are performed on different data sets
from that of previous work, our baseline method
still shows strong results compared with previous
work (Xue, 2008; Ye et al., 2006; Liu et al., 2011).
Adding automatic eventuality type and modal-
ity labels as features for semantic tense inference
leads to improvements over the baseline on all four
data subsets. In fact they provide considerable
improvements (around 2% increase) on newswire
v events dataset. MaxEnt e rows report results
when only automatic eventuality type is added
as a feature, and MaxEnt m rows report results
when only automatic modality is added as a fea-
ture. They both outperform (or, in several datasets,
match) the baseline results on all datasets. Max-
Ent em rows report results when both automatic
linguistic labels are added as features, and they
show further improvements over when only one
source of information is used. Analysis of the
results shows again that tense inference accuracy
is higher than webblog data under this experi-
ment condition. The results also show that after
adding eventuality type and modality as features,
2
http://code.google.com/p/word2vec/
3
http://deeplearning.net/software/theano/
1907
method all data nw data
MaxEnt b 58.9% 66.8%
MaxEnt e 59.5% 67.9%
MaxEnt m 59.5% 67.1%
MaxEnt em 59.6% 68.6%
MaxEnt jle 59.6% 63.5%
MaxEnt jlm 60.5% 66.9%
MaxEnt ge 74.6% 77.4%
MaxEnt gm 66.6% 70.0%
MaxEnt gem 76.2% 76.9%
ANN b 63.4% 67.2%
ANN e 62.6% 66.1%
ANN m 63.4% 59.8%
ANN em 59.7% 68.3%
ANN jle 62.7% 64.5%
ANN jlm 62.0% 65.6%
Table 5: Accuracy of tense inference on v events.
Best performances for each group of methods are
in bold.
the improvements on v events (0.7% and 1.8%)
are much bigger than that on all events (0.2% and
0.4%), regardless of the data genre (newswire or
weblog).
In order to test the potential for these two new
features, we also conducted experiments using
gold eventuality type and/or modality labels as
features for the Maximum Entropy models (Table
5 and Table 6, MaxEnt ge, MaxEnt gm, and Max-
Ent gem rows.). They outperform our best Max-
Ent results by around 10% on newswire data and
around 15% on all data, indicating strong poten-
tials for more accurately classified automatic even-
tuality type and modality labels.
Results also show that joint learning with
modality proves to be working better than the
baseline (Table 5 and Table 6, MaxEnt jle, Max-
Ent jlm). In fact, on the datasets with all events,
joint learning with modality produces the highest
accuracy among all approaches. However, joint
learning with eventuality is even worse than the
baseline. One possible explanation is that the
lower eventuality type classification accuracy af-
fects the tense inference accuracy. We also believe
there is still room for improvement with features
tuned for the joint learning model. Simply adding
the features may not be the best strategy.
On the entire dataset, regardless of v events or
other events, results of the neural network models
show improvements over the maximum entropy
method all data nw data
MaxEnt b 59.7% 65.1%
MaxEnt e 59.9% 65.1%
MaxEnt m 59.9% 65.4%
MaxEnt em 59.9% 65.5%
MaxEnt jle 59.7% 62.7%
MaxEnt jlm 60.4% 65.6%
MaxEnt ge 75.3% 76.1%
MaxEnt gm 67.1% 69.0%
MaxEnt gem 76.2% 75.9%
ANN b 63.0% 64.0%
ANN e 63.2% 66.9%
ANN m 60.1% 64.7%
ANN em 57.8% 66.1%
ANN jle 61.4% 63.0%
ANN jlm 62.9% 63.5%
Table 6: Accuracy of tense inference on all events.
Best performances for each group of methods are
in bold.
models under most experimental conditions. A
clear trend is that artificial neural networks help
more on all data than on newswire data only, in-
dicating greater potentials of the neural network
models to select and combine features with care-
fully trained parameters, given noisier but larger
training sets.
Experimental results also show significant dif-
ferences in accuracy between newswire data and
webblog data, and smaller but still recognizable
difference between v events and all events. There-
fore, we specifically look into distinctions be-
tween these data sets.
5.1 Newswire Data vs. Webblog Data
Considering the big gap in accuracy between
newswire and webblog data in our baseline results,
we delve deeper into the data and found several
major distinctions between these two domains that
might have contributed to the rather significant dif-
ference in performance on tense inference. First,
we look into the word frequency distribution of the
two datasets. Here by “word” we mean the char-
acter string of an event. We find that both datasets
have a small portion of words with high frequen-
cies, but the webblog dataset contains much more
low-frequent words than the newswire dataset. In
Figure 2, the x-axis shows possible frequencies of
words and the y-axis shows the number of words at
a particular frequency. It can be seen that the num-
1908
ber of words that appear only once in the webblog
dataset is about three times as large as that in the
newswire dataset. The entire newswire dataset has
a vocabulary of only 2671 entries, while the web-
blog dataset has a vocabulary size of 6117. This
greatly reduces the coverage of features extracted
from the training dataset on the events in the test
dataset.
Second, webblog data contains more events
that are “inherently” ambiguous on temporal lo-
cation. Among four possible labels for tense
in this corpus, “None” is for events whose tem-
poral locations are not clear even to human an-
notators. Statistics show that in webblog data
about 13.4% of the events are tagged as “None”,
while in newswire data only around 6.7% are
“None”. Another piece of evidence showing web-
blog data is harder to process is the different inter-
annotator agreement scores for tense annotation
on newswire and webblog data reported by (Xue
and Zhang, 2014). Newswire data has a 89.0%
agreement score and a 84.9% kappa score, while
webblog data only has a 81.0% agreement score
and a 72.7% kappa. Third, automatic parse trees
for newswire data is also more accurate than that
for webblog data. The bracketing F-score of au-
tomatically parsed newswire data is 83.0% while
it is only 80.4% for weblog data. Moreover, sen-
tences in newswire data are more grammatically
complete. Analysis shows that webblog data has
more dropped constituents in sentences. There
are around 40.5% sentences in newswire data that
have nominal empty categories, while in webblog
data the number is 48.1%. Dropped constituents
affect the structures of parse trees and some of
the features, which can affect tense inference ac-
curacy.
0	  
500	  
1000	  
1500	  
2000	  
2500	  
3000	  
3500	  
4000	  
4500	  
5000	  
1	   2	   3	   4	   5	   6	   7	   8	   9	   10	  
wo
rd	  
cou
nt 
word	  frequency	  
 nw	  dataset	  
 wb	  dataset	  
Figure 2: Word frequency distribution in newswire
and webblog datasets.
5.2 V events vs. All Events
In our definition, v events are (1) events that are
single verbs (example 1, 3, 7, 9 in Table 2), and
(2) events that are multi-word sequences but only
one word among them is a verb and any non-verb
words are stripped off (verbs in example 4, 5, 6
in Table 2). Conversely, events that do not fall into
this definition include (1) events that have no verbs
in their surface form (example 2, 8, 10 in Table 2),
and (2) events that have more than one verb in their
surface form (e.g. “?+??(shi3+cheng2wei2)”,
VV+VV, “make it become”). So from the point
of view of a statistical learning algorithm, ev-
ery v event has one and only one verb. This
makes sure that all features that we used are ap-
plicable to v events. For other events, however,
some features may be not applicable. For ex-
ample, for an event which has a nominal expres-
sion, aspect marker, DER, and DEC features are
all “None” because these features are only appli-
cable to verbs. Another major distinction between
v events and “other events” is that the distributions
of eventuality type labels on them are very differ-
ent, presented in the second and third groups of
columns in Figure 3. There is a rather high per-
centage of “State” among “other events” and very
low percentage of “Completed” and “None”. The
highly uneven distribution of eventuality type la-
bels make it less effective as a feature for tense
inference.
0.0%	  
5.0%	  
10.0%	  
15.0%	  
20.0%	  
25.0%	  
30.0%	  
35.0%	  
40.0%	  
45.0%	  
50.0%	  
all	  events	   v_events	   other	  events	  
Episodic	  
Habitual	  
State	  
Progressive	  
Completed	  
None	  
Figure 3: Statistics of eventuality types on differ-
ent events.
We also find that, on newswire datasets, max-
imum entropy models and neural network mod-
els do not show much difference in performance.
To understand this result better, we plot learn-
ing curves of the artificial neural network model,
trained and tested on newswire v events dataset.
In Figure 4, the black line represents the error rate
on training set, and the grey line represents the er-
1909
ror rate on test set. As the size of training data
grows, the error rate on the training set gets larger
because with more training examples the training
set becomes noisier and it gets harder to model all
samples with the same number of features; and the
error rate on the test set gets smaller because a big-
ger training set reduces the data sparsity and trains
the parameters better. Both lines end at a rather
high error rate (around 30%, i.e. only around 70%
in accuracy) which means the current network is
general enough to cover most cases in the test set,
but it is under-fitting the training data. The cur-
rent model is not specific enough to better cap-
ture the fine distinctions between the tense cat-
egories. The black line being not very smooth
is also understandable, given that there are only
around 6000 training examples in the newswire
v events dataset.
0.0%	  
5.0%	  
10.0%	  
15.0%	  
20.0%	  
25.0%	  
30.0%	  
35.0%	  
40.0%	  
10%
	  
20%
	  
30%
	  
40%
	  
50%
	  
60%
	  
70%
	  
80%
	  
90%
	  
100
%	  
Err
or
	  ra
te
 
Percentage	  of	  training	  data 
 train	  
 test	  
Figure 4: Learning curves of the artificial neural
network model, trained and tested on newswire
v events dataset.
6 Error Analysis
In order to get a better understanding of the use
of eventuality type and modality, we look into the
error rates for each error type in greater detail.
In Table 7, “Pa” stands for “Past”, “Pre” is short
for “Present”, “Fu” is for “Future”, and “No” is
“None”. For each error type, the left-hand side
is the gold-standard tense, and the right-hand side
is the wrongly assigned label. Statistics are col-
lected on the newswire v events data test set. Ta-
ble 7 compares the different error types between
the baseline method and the MaxEnt em method,
the best approach for this dataset. We can see that
(1) “Present” and “Past” is the most frequently
confused tense pair, and (2) eventuality type and
modality information help disambiguate “Present”
and “Past” events greatly, and reduce the errors
due to mis-classifying “Past” as “Future”, or “Fu-
ture” as “Present”, or “None” as “Present”.
error type MaxEnt b MaxEnt em
Pre? Pa 11.7% 11.2%
Pa? Pre 9.8% 9.2%
Pa? Fu 2.5% 2.3%
No? Pa 2.0% 2.0%
Fu? Pre 1.9% 1.6%
Pre? Fu 1.4% 1.4%
Fu? Pa 1.4% 1.4%
No? Pre 1.6% 1.2%
No? Fu 0.5% 0.5%
Pa? No 0.3% 0.3%
Pre? No 0.2% 0.2%
Fu? No 0.0% 0.0%
Table 7: Tense inference error rates for different
error types on newswire v events test set.
A closer examination of the sentences in which
events are assigned the wrong tense reveals that
“Pre ? Pa” error is prone to occur on events
in relative clauses. The Chinese verb implies a
past episodic event, while the event is actually a
present state or habitual event. As a good example,
the “??(sheng1chan3)” event in Sentence (3) is
wrongly labeled as “Past” by MaxEnt b but cor-
rectly classified as “Present” by MaxEnt em with
eventuality type “Habitual” and modality tag “Ac-
tual” (the underlined part in the Chinese sentence
is the relative clause). It is also found that most
“Pa ? Pre” errors occur on events that are more
stative. It is reasonable since classifiers tend to
assign “Present” to states and “Past” to episodic
events. MaxEnt em managed to correct some with
“episodic” as their correct eventuality type.
(3) ??????(sheng1chan3)???? ?
?????????????????
??????????????????
??
At present , the Pu Kang Company
, which produces the vaccine in this zone ,
has already formed a production scale of 5
million doses per year , which has great sig-
nificance in effectively controlling the hepati-
tis A epidemic .
We are also surprised to see that over 2% “Past”
events are classified as “Future” events, ranking
1910
third among all error types. This mistake seems
very unlikely, but it is still possible when per-
forming tense inference on a language with no
grammatical tense at all. Take the following sen-
tence pair (4) as an example. In the Chinese sen-
tence, MaxEnt b classifies “??(tao3lun4)” as
“Future” because there is no grammatical indica-
tor in the Chinese sentence implying that the “dis-
cussion” has already happened and it is reason-
able to assume the “discussion” is in the near fu-
ture. However, with eventuality type “Episodic”
and modality label “Actual”, MaxEnt em classi-
fies it as “Past” correctly, because episodic events
tend to occur in the past and future events tend to
get “Intended” or “Hypothetical” modality labels.
(4) ????????“?????????
?????????(tao3lun4) ????
??????????”?
He also said, the French government “even
directed its representative not to vote Yes
when the Security Council discussed the res-
olution on sanctions on Cuba”.
7 Conclusion and Future Work
In this paper, we address the problem of automatic
inference of Chinese semantic tense. We took ad-
vantage of a new corpus annotated with rich lin-
guistic information, and experimented with three
approaches. In the first approach, we use two
sources of implicit linguistic information, even-
tuality type and modality, automatically derived,
as features in tense inference. We then conducted
joint learning on tense and each of these two infor-
mation types. Finally, we experimented with using
artificial neural networks to train models for tense
prediction. All three approaches outperformed a
strong baseline, a maximum entropy model with
extensive engineering. Our future work will in-
clude exploring ways to improve automatic even-
tuality type and modality labeling accuracy to fur-
ther improve tense inference accuracy.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their suggestions and comments. This
work is supported by the National Science Foun-
dation via Grant No. 0910532 entitled “Richer
Representations for Machine Translation”. All
views expressed in this paper are those of the au-
thors and do not necessarily represent the view of
the National Science Foundation.
References
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Xuansong Li, Stephanie Strassel, Stephen Grimes, Safa
Ismael, Mohamed Maamouri, Ann Bies, and Nian-
wen Xue. 2012. Parallel Aligned Treebanks at
LDC: New Challenges Interfacing Existing Infras-
tructures. In Proceedings of LREC-2012, Istanbul,
Turkey.
Feifan Liu, Fei Liu, and Yang Liu. 2011. Learning
from chinese-english parallel data for chinese tense
prediction. In Proceedings of the 5th International
Conference on Natural Language Processing, pages
1116–1124, November.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Roi Reichart and Ari Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325–334, Cambridge, MA, October. Association for
Computational Linguistics.
Carlota S. Smith and Mary Erbaugh. 2005. Tempo-
ral interpretation in Mandarin Chinese. Linguistics,
43(4):713–756.
Nianwen Xue and Yuchen Zhang. 2014. Buy one get
one free: Distant annotation of chinese tense, event
type, and modality. In Proceedings of LREC-2014,
Reykjavik, Iceland.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207–238.
Nianwen Xue. 2008. Automatic Inference of the Tem-
poral Location of Situations in Chinese Text. In
EMNLP-2008, Honolulu, Hawaii.
Yang Ye, Victoria Li Fossum, and Steven Abney. 2006.
Latent features in automatic tense translation be-
tween Chinese and English. In The Proceedings
of the 5th SIGHAN Workshop on Chinese Language
Processing, Sydney, Australia.
Yang Ye. 2007. Automatic Tense and Aspect Trans-
lation between Chinese and English. Ph.D. thesis,
University of Michigan.
1911
