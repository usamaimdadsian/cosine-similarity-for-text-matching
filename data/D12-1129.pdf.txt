Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1411–1422, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
A New Minimally-Supervised Framework
for Domain Word Sense Disambiguation
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
We present a new minimally-supervised
framework for performing domain-driven
Word Sense Disambiguation (WSD). Glos-
saries for several domains are iteratively ac-
quired from the Web by means of a boot-
strapping technique. The acquired glosses are
then used as the sense inventory for fully-
unsupervised domain WSD. Our experiments,
on new and gold-standard datasets, show that
our wide-coverage framework enables high-
performance results on dozens of domains at
a coarse and fine-grained level.
1 Introduction
Domain information pervades most of the text we
read every day. If we just think of the Web, the vast
majority of its textual content is domain oriented.
A case in point is Wikipedia, which provides ency-
clopedic coverage for a huge number of knowledge
domains (Medelyan et al., 2009), but most blogs,
Web sites and newspapers also provide a great deal
of information focused on specific areas of knowl-
edge. When it comes to automatic text understand-
ing, then, it is crucial to take into account the domain
specificity of a piece of text, so as to perform a fo-
cused and as-precise-as-possible analysis which, in
its turn, can enable domain-aware applications such
as question answering and information extraction.
Domain knowledge also has the potential to improve
open-text applications such as summarization (Cey-
lan et al., 2010) and machine translation (Foster et
al., 2010).
Research in Word Sense Disambiguation (Nav-
igli, 2009, WSD), the task aimed at the automatic
labeling of text with word senses, has been ori-
ented towards domain text understanding for sev-
eral years now. Many approaches have been devised,
including the identification of domain-specific pre-
dominant senses (McCarthy et al., 2007; Lapata and
Keller, 2007), the development of domain resources
(Magnini and Cavaglia`, 2000; Magnini et al., 2002),
their application to WSD (Gliozzo et al., 2004), and
the effective use of link analysis algorithms such as
Personalized PageRank (Agirre et al., 2009; Nav-
igli et al., 2011). More recently, semi-supervised ap-
proaches to domain WSD have been proposed which
aim at decreasing the amount of supervision needed
to carry out the task (Khapra et al., 2010).
High-performance domain WSD, however, has
been hampered by the widespread use of a general-
purpose sense inventory, i.e., WordNet (Miller et
al., 1990; Fellbaum, 1998). Unfortunately WordNet
does not contain many specialized terms, making
it difficult to use it in work on arbitrary special-
ized domains. While Wikipedia has recently been
considered a valid alternative (Mihalcea, 2007), it
is mainly focused on covering named entities and,
strictly speaking, does not contain a formal wide-
coverage sense inventory (not even in disambigua-
tion pages, which are often incomplete, especially
in the lexicographic sense).
In this paper we provide three main contributions:
• We tackle the above issues by introducing
a new framework based on the minimally-
supervised acquisition of specialized glossaries
for dozens of domains.
1411
• In turn, we use the acquired domain glossaries
as a sense inventory for domain WSD. As a re-
sult, we redefine the domain WSD task as one
of picking out the most appropriate gloss (fine-
grained setting) or domain (coarse-grained set-
ting) from a multi-domain glossary.
• We show that our framework represents a con-
siderable departure from the common usage
of a general-purpose sense inventory such as
WordNet, in that, thanks to the wide cov-
erage of domain meanings, it enables high-
performance unsupervised WSD on many do-
mains in the range of 69-80% F1.
Furthermore, our approach can be customized to
any set of domains of interest, and new senses, i.e.,
glosses, can be added at any time (either manually or
automatically) to the multi-domain sense inventory.
2 Related Work
Domain WSD has been the focus of much interest
in the last few years. An important research direc-
tion identifies distributionally similar neighbors in
raw text as cues for determining the predominant
sense of a target word by means of a semantic simi-
larity measure (McCarthy et al., 2004; Koeling et al.,
2005; McCarthy et al., 2007). Other distributional
methods include the use of a word-category cooccur-
rence matrix, where categories are coarse senses ob-
tained from an existing thesaurus (Mohammad and
Hirst, 2006), and synonym-based word occurrence
counts (Lapata and Keller, 2007). Domain-informed
methods have also been proposed which make use of
domain labels as cues for disambiguation purposes
(Gliozzo et al., 2004).
Domain-driven approaches have been shown to
obtain the best performance among the unsupervised
alternatives (Strapparava et al., 2004), especially
when domain kernels are coupled with a syntag-
matic one (Gliozzo et al., 2005). However, their per-
formance is typically lower than supervised systems.
On the other hand, supervised systems fall short
of carrying out high-performance WSD within do-
mains, the main reason being the need for retraining
on each new specific knowledge domain. Nonethe-
less, the knowledge acquisition bottleneck can be
relieved by means of domain adaptation (Chan and
Ng, 2006; Chan and Ng, 2007; Agirre and de La-
calle, 2009) or by effectively injecting a general-
purpose corpus into a smaller domain-specific train-
ing set (Khapra et al., 2010).
However, as mentioned above, most work on
domain WSD uses WordNet as a sense inven-
tory. But even if WordNet senses have been en-
riched with topically-distinctive words and concepts
(Agirre and de Lacalle, 2004; Cuadros and Rigau,
2008), manually-developed domain labels (Magnini
et al., 2002), and disambiguated semantic relations
(Navigli, 2005), the main obstacle of being stuck
with an open-ended fine-grained sense inventory re-
mains. Recent results on the SPORTS and FINANCE
gold standard dataset (Koeling et al., 2005) show
that domain WSD can achieve accuracy in the 50-
60% ballpark when a state-of-the-art algorithm such
as Personalized PageRank is paired with a distribu-
tional approach (Agirre et al., 2009) or with seman-
tic model vectors acquired for many domains (Nav-
igli et al., 2011).
In this paper, we take domain WSD to the next
level by proposing a new framework based on
the minimally-supervised acquisition of large do-
main sense inventories thanks to which high per-
formance can be attained on virtually any domain
using unsupervised algorithms. Glossary acquisi-
tion approaches in the literature are mostly fo-
cused on pattern-based definition extraction (Fujii
and Ishikawa, 2000; Hovy et al., 2003; Fahmi and
Bouma, 2006, among others) and lattice-based su-
pervised models (Navigli and Velardi, 2010) start-
ing from an initial terminology, while we jointly
bootstrap the lexicon and the definitions for sev-
eral domains with minimal supervision and without
the requirement of domain-specific corpora. To do
so, we adapt bootstrapping techniques (Brin, 1998;
Agichtein and Gravano, 2000; Pasca et al., 2006) to
the novel task of domain glossary acquisition from
the Web.
Approaches to domain sense modeling have al-
ready been proposed which go beyond the WordNet
sense inventory (Duan and Yates, 2010). Distinc-
tive collocations are extracted from corpora and used
as features to bootstrap a supervised WSD system.
Experiments in the biomedical domain show good
performance, however only in-domain ambiguity is
addressed. In contrast, our approach tackles cross-
1412
Figure 1: The bootstrapping process for glossary acquisition.
domain ambiguity, by working with virtually any set
of domains and minimizing the requirements by har-
vesting domain terms and definitions from the Web,
bootstrapped using a small number of seeds.
The existing approach closest to ours is that of
Huang and Riloff (2010), who devised a bootstrap-
ping approach to induce semantic class taggers from
domain text. The semantic classes are associated
with arbitrary NPs and must be established before-
hand. Our objective, instead, is to perform domain
disambiguation at the word level. To do this, we re-
define the domain WSD problem as one of selecting
the most suitable gloss from those available in our
full-fledged multi-domain glossary.
3 A Minimally-Supervised Framework for
Domain WSD
In this section we present our new framework for
performing domain WSD. The framework consists
of two phases: glossary bootstrapping (Section 3.1)
and domain WSD (Section 3.2).
3.1 Phase 1: Bootstrapping Domain Glossaries
The objective of the first phase is to acquire a multi-
domain glossary from the Web with minimal super-
vision. We initially select a set D of domains of
interest. For each individual domain d ? D we start
with an empty set of HTML patterns Pd (i.e., Pd :=
?), used for gloss harvesting. During this phase we
iteratively populate the pattern set by means of six
steps, described in the next six subsections and de-
picted in Figure 1. The final output of this phase will
be a glossary Gd consisting of domain terms and
their automatically-harvested glosses.
3.1.1 Step 1: Initial seed selection
First, given the domain d, we manually
pick out K hypernymy relation seeds Sd =
{(t1, h1), . . . , (tK , hK)}, where the pair (ti, hi)
contains a domain term ti and its generalization hi
(e.g., (firewall, security system)). The only con-
straint we impose is that the selected relations must
be distinctive for the domain d of interest. The cho-
sen hypernymy relations have to be as topical and
representative as possible for the given domain (e.g.,
(compiler, computer program) is an appropriate pair
for computer science, while (byte, unit of measure-
ment) is not, as it might cause the extraction of sev-
eral glossaries of various units and measures). Note
that this is the only human intervention in the entire
glossary acquisition process.
We now set the iteration counter k to 1 and start
the first iteration of the process (steps 2-5). After
each iteration k, we keep track of the set of glosses
Gkd, acquired during iteration k.
3.1.2 Step 2: Seed queries
For each seed pair (ti, hi), we submit the follow-
ing three queries to a Web search engine: “ti” “hi”
glossary1, “ti” “hi” definition, “ti” “hi”
dictionary and collect the 64 top-ranking results
for each query2. Each resulting page is a candidate
glossary for the domain d identified by our relation
seeds Sd.
3.1.3 Step 3: Pattern and glossary extraction
We initialize the glossary for iteration k as fol-
lows: Gkd := ?. Next, from each resulting page,
we harvest all the text snippets s starting with
ti and ending with hi (e.g., firewall</b> -- a
<i>security system), i.e., s = ti . . . hi. For each
such text snippet s, we perform five substeps:
a) extraction of the term/gloss separator: we
1In what follows, we use the typewriter font for key-
words and term/gloss separators.
2We use the Google AJAX API, which returns 64 results.
1413
Term Gloss Hypernym # seeds Gloss score
dynamic
packet filter
A firewall facility that monitors the state of connections and uses this
information to determine which network packets to allow through the firewall
firewall 2 0.75
peripheral Hardware that extends the capabilities of the computer, such as a printer,
modem, or scanner.
hardware 1 0.83
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and initialise its data method 0 1.00
schema In database terminology, a schema is the organization of the tables, the fields in
each table, and the relationships between fields and tables.
database 0 0.78
Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., “</b> --” between “firewall”
and “a” in the above example);
b) gloss extraction: we expand the snippet s to
the right of hi in search of the entire gloss of
ti, i.e., until we reach a non-formatting tag el-
ement (e.g., <span>, <p>, <div>), while ig-
noring formatting elements such as <b>, <i>
and <a> which are typically included within a
definition sentence. As a result, we obtain the
sequence ti pM glosss(ti) pR, where glosss(ti)
is our gloss for seed term ti in snippet s (which
includes hi by construction) and pR is the non-
formatting HTML tag element to the right of
the extracted gloss. For example, we extend the
above definition for firewall to: “a <i>security
system</i> for protecting against illegal entry
to a local area network.”.
c) pattern instance extraction: we extract the fol-
lowing pattern instance:
pL ti pM glosss(ti) pR,
where pL and pR are, respectively, the left bound-
ary of ti and the right boundary of glosss(ti), and
pM is the term/gloss separator extracted at step
3(a). The two boundaries pL and pR are obtained
by extracting the longest sequence of HTML
tags and non-alphanumeric characters obtained
when moving to the left of ti and the right of
glosss(ti), respectively3. For the above exam-
ple, we extract the following pattern instance:
3The minimum and maximum length of both pL and pR are
set to 4 and 50 characters, respectively, as a result of a tuning
phase described in Section 4.1.
pL = “<p><b>”, ti = “firewall”, pM = “</b>
--”, glosss(ti) = “a <i>security system</i>
for protecting against illegal entry to a local area
network.”, pR =“</p>”.
d) pattern extraction: we generalize the above pat-
tern instance to the following pattern:
pL ? pM ? pR,
i.e., we replace ti and glosss(ti) with *. In the
above example, we obtain the following pattern:
<p><b> ? </b> -- ? </p>.
Finally, we add the generalized pattern to the set
of patterns Pd, i.e., we set Pd := Pd ? {pL ?
pM ? pR}. We also add the first sentence of
the retrieved definition glosss(ti) to our glossary
Gkd, i.e., G
k
d := G
k
d ? {(ti, first(glosss(ti)))},
where first(g) returns the first sentence of gloss
g.
e) pattern matching: we look for additional pairs
of terms/glosses in the Web page containing the
snippet s by matching the page against the gen-
eralized pattern pL ? pM ? pR. We then add
to Gkd the new (term, gloss) pairs matching the
generalized pattern.
As a result of this step, we obtain a glossary Gkd
for the terms discovered at iteration k.
3.1.4 Step 4: Gloss ranking and filtering
Importantly not all the extracted definitions per-
tain to the domain of interest. In order to rank by
domain pertinence the glosses obtained at iteration
k, we define the terminology T k?11 of the terms
accumulated up until iteration k ? 1 as follows:
T k?11 :=
?k?1
i=1 T
i, where T i := {t : ?(t, g) ? Gid}.
1414
Gloss Domain
Measures undertaken to return a degraded ecosystem’s functions and values, including its hydrology, plant and. . . BIOLOGY
The renewing or repairing of a natural system so that its functions and qualities are comparable to its original. . . GEOGRAPHY
The reign of Charles II in England. ROYALTY
A goal of criminal sentencing that attempts to make the victim ”whole again.” LAW
The process and work of improving the degraded quality of the sound or image in terms of video and audio preservation. MEDIA
A process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by. . . PHYSICS
Table 2: Examples of glosses harvested for the term restoration.
For the base step k = 1, we define T 01 := T
1, i.e.,
we use the first-iteration terminology itself. To rank
the glosses, we first transform each acquired gloss
g to its bag-of-words representation Bag(g), which
contains all the single- and multi-word expressions
in g. We then score each gloss g by the ratio of do-
main terms found in its bag of words:
score(g) =
|Bag(g) ? T k?11 |
|Bag(g)|
. (1)
In Table 1 we show some glosses in the computer
science domain (second column, domain terms are
underlined) together with their score (last column).
Next, we use a threshold ? (tuned on a held-out do-
main, described in Section 4.1) to remove from Gkd
those glosses g whose score(g) < ?.
3.1.5 Step 5: Seed selection for next iteration
We now aim at selecting the new set of hyper-
nymy relation seeds to be used to start the next it-
eration. We perform three substeps:
a) Hypernym extraction: for each newly-acquired
term/gloss pair (t, g) ? Gkd, we automatically ex-
tract a candidate hypernym h from the textual
gloss g. To do this we use a simple unsupervised
heuristic which just selects the first term in the
gloss. More sophisticated, supervised approaches
could have been used for hypernym extraction
from glosses (Navigli and Velardi, 2010). How-
ever, note that, for the purposes of our glossary
extraction task, it is not crucial to extract ac-
curate hypernyms, but rather to harvest terms h
which are very likely to occur in the glosses of t.
We show an example of hypernym extraction for
some terms in Table 1 (we report the term in col-
umn 1, the gloss in column 2 and the hypernyms
extracted by our hypernym extraction technique
in column 3).
b) (Term, Hypernym)-ranking: we sort all the
glosses in Gkd by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score shown in Formula 1. We
show the number of seed terms and the scores
for some glosses in Table 1 (columns 4 and 5,
respectively), where seed terms are in bold and
domain terms (i.e., in T k?11 ) are underlined.
c) New seed selection: as new seeds we select the
(term, hypernym) pairs corresponding to the K
top-ranking glosses.
If k equals the maximum number of iterations, we
stop. Else, we increment the iteration counter (i.e.,
k := k + 1) and jump to step (2) of our glossary
bootstrapping algorithm after replacing Sd with the
new set of seeds.
The output of the glossary bootstrapping phase is
a domain glossary Gd :=
?
i=1,...,maxG
i
d, where
max is the total number of iterations.
3.1.6 Step 6: Increasing Coverage
Given the nature of Web domain glossaries one
can rarely find terms and definitions for general
terms (e.g., jurisprudence for the LAW domain). In
order to cover this gap, we apply domain filtering
(see Section 3.1.4) to all the glosses contained in a
general-purpose dictionary (we use WordNet). We
then add the surviving term/gloss pairs to Gd.
3.2 Phase 2: Domain WSD
Now that we have acquired a glossary for each do-
main in our set D, we can create a multi-domain
glossary G := {((t, g), d) : d ? D, (t, g) ? Gd}.
Our glossary G is thus a set of term/gloss pairs
for many domains. Note that one pair might indi-
vidually belong to more than one domain, as glos-
sary bootstrapping is performed separately for each
domain. In Table 2 we show an example of the
1415
glosses acquired for the term restoration. We ob-
serve that 5 out of 6 senses are not available in Word-
Net (namely: the BIOLOGY, GEOGRAPHY, LAW, ME-
DIA and PHYSICS senses). Many of them are domain-
specific meanings for the general concept of “the
act of restoring”, with the BIOLOGY and GEOGRA-
PHY senses being very similar. However, this is a
perfectly acceptable phenomenon as any of the two
senses, i.e., glosses, would be equally valid when
disambiguating a domain text dealing with ecosys-
tem restoration.
3.2.1 Gloss-driven WSD
We redefine the task of domain WSD as one of
selecting the most suitable gloss, if one exists, for
an input term t. For instance, consider the sentence:
“He performed the restoration of heavily corrupted
images”. An appropriate option for this occurrence
would be the MEDIA sense of restoration in Table 2.
Our gloss-driven WSD paradigm has the desir-
able property of automatically providing two levels
of sense granularity: a domain, coarse-grained level,
similar in spirit to Word Domain Disambiguation
(Sanfilippo et al., 2006), in which the sense inven-
tory of a term t is just the set of domains for which t
is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW,
MEDIA, PHYSICS in the example of Table 2), and a
fine-grained level, which requires the selection of
the gloss which best describes the sense denoted
by the given word occurrence. A second desirable
property of our gloss-driven WSD paradigm is that
it relies on a flexible framework, which allows for
the bootstrapping of new domain glossaries or the
expansion of existing ones. However, while these
two properties – i.e., double level of granularity dis-
tinctions and flexibility – are naturally inherent in
the gloss-driven paradigm, the same cannot be said
for mainstream open-text WSD in which general-
purpose static dictionaries are typically used.
In order to evaluate our framework for domain
WSD, we propose two fully unsupervised algo-
rithms for gloss-driven domain WSD. Ideally, high
performance could be obtained using state-of-the-art
supervised WSD systems. However, in order to train
such systems, a wide-coverage sense-labeled corpus
should be available for each domain, a heavy task
which we leave to future work. Instead, our objec-
tive is to show that high-performance domain WSD
can be enabled with little effort by our framework.
3.2.2 Algorithm 1: WSD with Personalized
PageRank
Domain Glossaries as Graphs For each domain
d ? D, we create an undirected graph Nd =
(Vd, Ed) as follows: Vd is the set of concepts identi-
fied by term/gloss pairs in the domain glossary Gd,
i.e., Vd := Gd; Ed is the set of edges between pairs
of concepts, where an edge {(t, g), (t?, g?)} exists if
and only if t? is such that t? 6= t and t? occurs in the
bag of words of the gloss g of t. In other words, t is
connected to all the domain senses of words used in
its definition g.
Graph-based WSD Given an input text, for each
domain d ? D, we produce its bag of domain con-
tent words Cd = {w1, w2, . . . , wn} by perform-
ing tokenization, lemmatization and compounding
based on the lexicon of domain d. Then, given a
target word t, we use Cd \ {t} as the context to dis-
ambiguate t within the domain d. In order to carry
out domain WSD, i.e., to pick out the most suit-
able sense of t across domains, we apply a state-of-
the-art graph-based algorithm, namely Personalized
PageRank (Haveliwala, 2002, PPR), to each domain
graph Nd. PPR is a variant of the popular PageRank
algorithm (Brin and Page, 1998) in which the damp-
ing probability mass is concentrated on a selected
number of graph nodes, instead of being uniformly
distributed across all nodes. Specifically, following
Agirre and Soroa (2009) we concentrate the proba-
bility mass on the nodes (t?, g?) ? Vd for which the
term t? is a context word, i.e., t? ? Cd. Next, for each
domain d ? D, we run PPR for a given number of
iterations and obtain as output a probability distribu-
tion PPVd over the graph nodes. Finally, we select
the most suitable gloss of t as follows:
SensePPR(t) = argmax
g:?d?D,(t,g)?Vd
PPVd(t, g) (2)
where PPVd(t, g) is the PPR probability for the
term/gloss pair (t, g) and SensePPR(t) contains the
best interpretation of t across all the domains D.
3.2.3 Algorithm 2: PPR Boosted with Domain
Distribution Information
The words in a given text do not typically deal
with a single domain. Instead, they touch different
1416
ART BIOLOGY BUSINESS CHEMISTRY COMPUTING EDUCATION ENGINEERING ENVIRONMENT FOOD & DRINK GEOGRAPHY
GEOLOGY HEALTH HISTORY LANGUAGE LAW LITERATURE MATHS MEDIA METEOROLOGY MUSIC
PHILOSOPHY PHYSICS POLITICS PSYCHOLOGY RELIGION ROYALTY SPORTS TOURISM VIDEOGAMES WARFARE
Table 3: List of the 30 domains used in our experiments.
COMPUTING FOOD ENVIRONMENT BUSINESS
chip circuit timbale dish sewage waste eurobond bond
destructor method brioche bread acid rain rain asset play stock
compiler program macaroni pasta ecosystem system income stock security
html language pizza dish air monitoring sampling financial intermediary institution
firewall security system ice cream dessert global warming temperature derivative financial product
remote lan access process pasteurized milk milk fermentation decomposition arbitrage pricing theory economic theory
relational database tabular database salted butter butter attainment area area banker’s draft bill of exchange
admin console user interface prosecco wine fugitive dust matter working capital cash
Table 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.
areas of knowledge which are intertwined with each
other within the discourse. For example, a text deal-
ing with VIDEOGAMES will often concern domains
such as BUSINESS, COMPUTING, SPORTS, etc. Given an
input text, we can capture its relevance for each do-
main by calculating the following domain score:
?d =
|Cd|
?
d??D |Cd? |
(3)
where, as above, Cd is the set of content words from
the input text which are covered by domain d. We
thus propose a second algorithm which synergisti-
cally combines the spreading effect of PPR with the
domain distribution information. The best sense for
a given term t is calculated as follows:
SenseDomPPR(t) = argmax
g:?d?D,(t,g)?Vd
?dPPVd(t, g)
(4)
that is, we select as the most suitable gloss for t the
one which maximizes the product of its domain rel-
evance score by its domain PPVd value. Note that
the same gloss can occur in multiple domains and
that it might obtain different scores depending on the
domain. Again, since the approach is gloss-driven,
we do not see this as a problem, but rather as a natu-
ral characteristic of our framework.
4 Experimental Setup
4.1 Domains
We selected 30 domains starting from the Wikipedia
featured articles4. We show the domain labels in Ta-
4http://en.wikipedia.org/wiki/Wikipedia:Featured articles
Table 5: Statistics on the multi-domain acquired glossary.
From the Web From WordNet From both Total
Terms 74,295 83,904 18,313 176,512
Glosses 153,920 68,731 596 223,247
ble 3 (some labels have been conveniently short-
ened, e.g., PHYSICS should read PHYSICS & ASTRON-
OMY). We manually identified 8 hypernym/hyponym
seeds for each domain, totalizing 240 seeds. We
used two criteria for selecting a seed: i) it covers a
separate segment of the domain, and ii) it has to be
specialized enough to avoid ambiguity. We show the
seeds used in four of our domains in Table 4. We
bootstrapped our glossary acquisition technique (cf.
Section 3.1) on each domain and performed 5 itera-
tions. For increasing the coverage of domain terms
we used WordNet glosses (see Section 3.1.6). As a
result, we obtained 30 domain glossaries. We also
kept aside a 31st domain, namely FASHION, which
we employed for tuning the minimum and maximum
length of both pL and pR in Section 3.1.3 and the
threshold ? used to filter out non-domain glosses in
Section 3.1.4.
In Table 5 we show the statistics for the ac-
quired multi-domain glossary by distinguishing
Web-derived and WordNet terms and glosses.
4.2 Sense Inventory
Our sense inventory is given by the 30-domain
glossary obtained as a result of our glossary boot-
strapping phase. Overall we collected 176,512 and
223,247 distinct terms and glosses, respectively,
with an important contribution from both the Web
1417
and WordNet (see Table 5). The average num-
ber of glosses per term in our inventory is 1.9 (3.6
glosses on polysemous terms). However, note that
a monosemous word in our domain sense inventory
does not necessarily make disambiguation easier,
as i) we might have missed other domain-specific
senses, ii) an uncovered, non-domain sense might fit
a word occurrence (in this case, the domain WSD
algorithms might be (wrongly) biased towards re-
turning the only possible choice if a non-zero dis-
ambiguation score is calculated for it).
In order to determine the suitability of our multi-
domain sense inventory, we compared it with the
latest version of WordNet Domains (Magnini et al.,
2002, WND 3.2), a well-known resource which
provides domain labels for almost 65,000 nomi-
nal WordNet synsets (we removed all the synsets
tagged with the FACTOTUM label, which indicates no
domain specificity). Since WND uses about 160
finer-grained domain labels, we manually mapped
them to our 30 labels when possible (e.g. SOCCER
and SWIMMING were mapped to SPORTS), totalizing
62,100 domain-labeled synsets.
We calculated the coverage of our sense inventory
against WND at the synset and the sense level, for
each non-FACTOTUM synset. Given a WordNet synset
S, let d =
?
s?S ds be the union of the domains ds
provided for each synonym s ? S by our sense in-
ventory (ds = ? if not present), and let d? be the do-
main labels assigned to S by WND. A synset is cov-
ered if d and d? intersect. At the sense level, instead,
we consider a synonym s ? S to be covered if ds and
d? intersect. Our synset and sense coverage is 65.9%
(40,969/62,100) and 63.7% (71,950/112,875), re-
spectively. We also calculated an extra-coverage of
203.2% (229,384/112,875), that is the fraction of do-
main senses which are not available in WND, but
we are able to provide in our sense inventory (see
e.g. the example in Table 2) over the total number of
senses in WND. While coverage and extra-coverage
provide a good indicator of the completeness of our
sense inventory, we need to calculate its precision to
determine its correctness. To do so, we randomly
sampled 500 domain glosses of terms for which no
WordNet sense was tagged with the same domain in
WND. A manual validation of this sample resulted
in an 87.0% (435/500) estimate of the precision of
our sense inventory.
4.3 Datasets
A dataset for 30 domains We used the Giga-
word corpus (Graff and Cieri, 2003) to extract a 6-
paragraph text snippet for each of the 30 domains.
As a result, we obtained a domain dataset made up
of 180 paragraphs to which we applied tokeniza-
tion, lemmatization and compounding, totaling 1432
domain content words overall (47.7 content words
per domain on average). The average polysemy of
the words in the dataset was of 9.7 glosses and 4.4
domains per word. Each content word was manu-
ally tagged with the most suitable glosses from our
multi-domain glossary (3.9 glosses, i.e., senses per
word were assigned on average). The annotation
task was performed by two annotators with adjudi-
cation.
Sports and Finance We also experimented with
the gold standard produced by Koeling et al. (2005).
The dataset covers two domains: SPORTS and FI-
NANCE. The dataset comprises 41 ambiguous words
(with an average polysemy of 6.7 senses), many
of which express different meanings in the two do-
mains. In each domain, and for each word, around
100 sentences were sense-annotated with WordNet.
Environment Finally, we also carried out an ex-
periment on the ENVIRONMENT dataset from the
Semeval-2010 domain WSD task (Agirre et al.,
2010). The dataset includes 1,398 content words (of
which 1,032 content nouns) tagged with WordNet
senses.
4.4 Systems
We applied the two algorithms proposed in Section
3.2, namely vanilla PPR and domain-boosted PPR.
For both versions of PPR we employed UKB, a
readily-available implementation of PPR for WSD5,
successfully experimented by Agirre and Soroa
(2009) and Agirre et al. (2009).
4.5 Baselines
Random baseline We compared our algorithms
with the random baseline, which associates a ran-
dom gloss among those available for each word oc-
currence according to a uniform distribution.
5http://ixa2.si.ehu.es/ukb/
1418
Predominant domain We also compared our al-
gorithms with a predominant sense baseline which
assigns to each word occurrence the domain label
with the highest domain score ?d among those avail-
able for the word (cf. Formula 3). Note that this is
a strong baseline, because it aims at identifying the
domain covered by the majority of terms in the input
text, however it can disambiguate only at a coarse-
grained level, i.e., at the domain level.
5 Experimental Results
30 domains We ran our WSD systems and the
baselines on our 30-domain dataset, on a sentence-
by-sentence basis. We calculated results at the two
levels of granularity enabled by our WSD frame-
work: a coarse-grained setting where systems out-
put the most appropriate domain label for each word
item to be disambiguated; a fine-grained setting
where systems are required to output the most suit-
able gloss for the input word. The results are shown
in Table 6. Domain PPR outperforms Vanilla PPR
by some points in precision, recall and F1 in both the
coarse-grained and the fine-grained setting, achiev-
ing an F1 around 80% and 69%, respectively (dif-
ferences in recall performance are statistically sig-
nificant using a ?2 test). The predominant domain
baseline, available only in the coarse-grained set-
ting, lags behind Domain PPR by more than 3 points
in precision and 2 in recall. While these differences
are not statistically significant, the variance across
domains is much higher, thus suggesting lower reli-
ability of the method.
These results were obtained in a fully unsuper-
vised setting in which no structured knowledge was
provided, unlike previous applications of PPR to
WSD (Agirre et al., 2009; Agirre and Soroa, 2009)
which relied on the underlying WordNet graph, a
manually created resource. Furthermore, our graph
contains “noisy” semantic relations, as we connect
each gloss to all the senses of its gloss words (cf.
Section 3.2.2). Finally, we note that the results
shown in Table 6 could never have been obtained
with WordNet. In fact, drawing on our domain map-
ping, we calculated that the correct domain sense is
not in WordNet for about 68% of the words in the
dataset. Instead, the results in Table 6 show that our
framework enables high-performance unsupervised
Coarse-grained Fine-grained
P R F1 P R F1
Vanilla PPR 76.7 74.3† 75.5 66.1 64.1† 65.1
Domain PPR 81.2 78.7† 79.9 69.7 67.6† 68.6
Predom. domain 77.9 76.8 77.3 - - -
Random baseline 42.5 42.5 42.5 44.1 44.1 44.1
Table 6: Performance results on the 30-domain dataset
(† differences between the two systems are statistically
significant using a ?2 test, p < 0.05).
WSD thanks to the wide coverage of domain mean-
ings.
As regards the random baseline, this performs
42.5% and 44.1% in the two settings. Despite the
higher polysemy of glosses (9.7 glosses vs. 4.4 do-
mains per word in the dataset), the performance is
higher in the fine-grained setting because often there
is more than one gloss covering the same meaning of
a domain word.
Sports, Finance and Environment For the
SPORTS, FINANCE and ENVIRONMENT datasets (cf. Sec-
tion 4.3) we did not have gloss-based sense annota-
tions, so we could not perform a fine-grained evalu-
ation. Therefore, we first studied the different sys-
tems at a coarse level on the basis of the domain dis-
tribution of the senses returned for the word items
in the dataset. We show the 3 most frequent domain
labels for each system and each dataset in Figure 2.
The figure seems to confirm our results showing Do-
main PPR as being more robust than its Vanilla ver-
sion. Next, to get a more accurate evaluation, we
randomly sampled 200 sentences from each dataset
and manually validated the coarse-grained senses,
i.e., domain assignments, output by each system on
this set of sentences. We remark that several words
in the datasets did not pertain to the domain of inter-
est. For instance, will and share do not have any
sports sense in WordNet, while the same applies
to half and chip for the business domain. Table 7
shows the results of our validation, where a domain
output by a system was considered correct if a suit-
able gloss existed for that domain in our inventory.
The results show that our framework enables
coarse-grained recall in the 70-80% ballpark even
on difficult gold standard datasets for which fine-
grained recall with WordNet struggles to surpass the
50-60% range. For instance, the best performance
1419
Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom.
FINANCE SPORTS ENVIRONMENT
Figure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets.
FINANCE SPORTS ENVIRONMENT
P R F1 P R F1 P R F1
Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7
Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2
Predom. domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6
Table 7: Coarse-grained performance results on gold-standard domain datasets.
on the ENVIRONMENT dataset was around 60% re-
call (Kulkarni et al., 2010) using a semi-supervised
WSD system, trained on the domain. Similarly, both
the FINANCE and SPORTS datasets are notoriously dif-
ficult gold standards on which state-of-the-art recall
using WordNet is lower than 60% (Navigli et al.,
2011).
Interestingly, the predominant domain baseline
shows a bias towards BUSINESS, thus performing best
on the FINANCE dataset. This is because of the large
number of terms covered in our domain glossary,
and consequently the high overlap with cue words
in context. On the other two domains, we observe
performance in line with our 30-domain experiment.
6 Conclusion
We have here presented a new framework for do-
main Word Sense Disambiguation. We depart from
the use of general-purpose sense inventories like
WordNet and propose a bootstrapping approach to
the acquisition of sense inventories for virtually any
domain. While we selected 30 domains for this
study, nothing would prevent us from using a smaller
or larger set of these domains, or a set of completely
different domains.
Our work provides three main contributions:
i) we propose a new, flexible approach to glossary
bootstrapping which harvests hundreds of thou-
sands of term/gloss pairs; the resulting multi-
domain glossary is shown to have wide cov-
erage across domains and to include a large
amount of terms not available in WordNet;
ii) we propose a novel framework for fully-
unsupervised domain WSD which uses the
multi-domain glossary as our sense inventory;
iii) we show that high performance can be achieved
by means of simple, unsupervised WSD algo-
rithms (around 80% and 69% in a coarse- and
fine-grained setting, respectively).
Note that our aim here has not been to determine
which system performs best, but rather to show that
a reliable, full-fledged framework for domain WSD
can be set up with minimal supervision. Addition-
ally, our framework can be applied to any language
of interest, provided enough glossaries are available
online, by simply translating the keywords used for
our queries.
The multi-domain glossary (and sense inven-
tory) together with the seeds used for bootstrapping
are available from http://lcl.uniroma1.
it/dwsd.
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
1420
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digi-
tal Libraries (DL 2000), pages 85–94, San Antonio,
Texas, United States.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC 2004, pages 1123–1126, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for WSD. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, EACL
2009, pages 42–50, Athens, Greece.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL 2009, pages 33–41, Athens, Greece.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pages 1501–1506,
Pasadena, California.
Eneko Agirre, Oier Lo´pez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words word sense disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 75–80, Uppsala, Sweden.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual web search engine. In Pro-
ceedings of the 7th Conference on World Wide Web,
WWW 2007, pages 107–117, Brisbane, Australia.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proceedings of the In-
ternational Workshop on The World Wide Web and
Databases (WebDB 1998), pages 172–183, London,
UK.
Hakan Ceylan, Rada Mihalcea, Umut O¨zertem, Elena
Lloret, and Manuel Palomar. 2010. Quantifying the
limits and success of extractive summarization sys-
tems across domains. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 903–911, Los Angeles, California.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL 2006, pages 89–96, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, ACL 2007,
pages 49–56, Prague, Czech Republic.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING 2008, pages
161–168, Manchester, U.K.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, NAACL 2010, pages 627–635,
Los Angeles, California, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 64–71, Trento, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 451–
459, Cambridge, Massachusetts.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the world wide web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, ACL 2000, pages 488–
495, Hong Kong.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2004.
Unsupervised and supervised exploitation of semantic
domains in lexical disambiguation. Computer Speech
and Language, 18(3):275–299.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL 2005, pages
403–410, Ann Arbor, Michigan.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW 2002, pages 517–526, Hon-
olulu, Hawaii.
1421
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending meta-
data definitions by automatically extracting and orga-
nizing glossary definitions. In Proceedings of the 2003
Annual National Conference on Digital Government
Research, pages 1–6, Boston, MA.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 275–285, Uppsala, Sweden.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. All words domain
adapted WSD: Finding a middle ground between su-
pervision and unsupervision. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1532–1541, Sweden.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 419–426, Vancouver, B.C., Canada.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource con-
scious approaches for all-words domain specific WSD.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (Semeval-2010), pages 421–426,
Stroudsburg, PA, USA.
Mirella Lapata and Frank Keller. 2007. An information
retrieval approach to sense ranking. In Proceedings of
Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL 2007, pages
348–355, Rochester, USA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. In-
tegrating subject field codes into WordNet. In Pro-
ceedings of the 2nd Conference on Language Re-
sources and Evaluation, LREC 2000, pages 1413–
1418, Athens, Greece.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8:359–373.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2004, pages 280–287, Barcelona, Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716–
754.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL, pages 196–
203, Rochester, N.Y.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235–244.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2006, pages 121–128, Trento, Italy.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 1318–1327, Uppsala, Sweden.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de La-
calle, and Eneko Agirre. 2011. Two birds with one
stone: Learning semantic models for text categoriza-
tion and Word Sense Disambiguation. In Proceed-
ings of the 20th ACM Conference on Information and
Knowledge Management, CIKM 2011, pages 2317–
2320, Glasgow, UK.
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research Sym-
posium Conference (FLAIRS), 15–17 May 2005, pages
548–553, Clearwater Beach, Florida.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1–69.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial intelligence (AAAI
2006), pages 1400–1405, Boston, MA.
Antonio Sanfilippo, Stephen Tratz, and Michelle Gre-
gory. 2006. Word domain disambiguation via word
sense disambiguation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL 2006,
pages 141–144, New York, USA.
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.
2004. Pattern abstraction and term similarity for
Word Sense Disambiguation: IRST at Senseval-3.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analy-
sis of Text (SENSEVAL-3), pages 229–234, Barcelona,
Spain.
1422
