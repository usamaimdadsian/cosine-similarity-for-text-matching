Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11–20,
Singapore, 6-7 August 2009.
c©2009 ACL and AFNLP
Graph Alignment for Semi-Supervised Semantic Role Labeling
Hagen F¨urstenau
Dept. of Computational Linguistics
Saarland University
Saarbr¨ucken, Germany
hagenf@coli.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Unknown lexical items present a major
obstacle to the development of broad-
coverage semantic role labeling systems.
We address this problem with a semi-
supervised learning approach which ac-
quires training instances for unseen verbs
from an unlabeled corpus. Our method re-
lies on the hypothesis that unknown lexical
items will be structurally and semantically
similar to known items for which annota-
tions are available. Accordingly, we rep-
resent known and unknown sentences as
graphs, formalize the search for the most
similar verb as a graph alignment prob-
lem and solve the optimization using inte-
ger linear programming. Experimental re-
sults show that role labeling performance
for unknown lexical items improves with
training data produced automatically by
our method.
1 Introduction
Semantic role labeling, the task of automatically
identifying the semantic roles conveyed by sen-
tential constituents, has recently attracted much at-
tention in the literature. The ability to express the
relations between predicates and their arguments
while abstracting over surface syntactic configu-
rations holds promise for many applications that
require broad coverage semantic processing. Ex-
amples include information extraction (Surdeanu
et al., 2003), question answering (Narayanan
and Harabagiu, 2004), machine translation (Boas,
2005), and summarization (Melli et al., 2005).
Much progress in the area of semantic role la-
beling is due to the creation of resources like
FrameNet (Fillmore et al., 2003), which document
the surface realization of semantic roles in real
world corpora. Such data is paramount for de-
veloping semantic role labelers which are usually
based on supervised learning techniques and thus
require training on role-annotated data. Examples
of the training instances provided in FrameNet are
given below:
(1) a. If [you]
Agent
[carelessly]
Manner
chance going back there, you
deserve what you get.
b. Only [one winner]
Buyer
purchased
[the paintings]
Goods
c. [Rachel]
Agent
injured [her
friend]
Victim
[by closing the car
door on his left hand]
Means
.
Each verb in the example sentences evokes a frame
which is situation-specific. For instance, chance
evokes the Daring frame, purchased the Com-
merce buy frame, and injured the Cause harm
frame. In addition, frames are associated with
semantic roles corresponding to salient entities
present in the situation evoked by the predicate.
The semantic roles for the frame Daring are Agent
and Manner, whereas for Commerce buy these are
Buyer and Goods. A system trained on large
amounts of such hand-annotated sentences typi-
cally learns to identify the boundaries of the argu-
ments of the verb predicate (argument identifica-
tion) and label themwith semantic roles (argument
classification).
A variety of methods have been developed for
semantic role labeling with reasonably good per-
formance (F
1
measures in the low 80s on standard
test collections for English; we refer the interested
reader to the proceedings of the SemEval-2007
shared task (Baker et al., 2007) for an overview
of the state-of-the-art). Unfortunately, the reliance
on training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text gen-
res. The English FrameNet (version 1.3) is not
11
a small resource — it contains 502 frames cov-
ering 5,866 lexical entries and 135,000 annotated
sentences. Nevertheless, by virtue of being un-
der development it is incomplete. Lexical items
(i.e., predicates evoking existing frames) are miss-
ing as well as frames and annotated sentences
(their number varies greatly across lexical items).
Considering how the performance of supervised
systems degrades on out-of-domain data (Baker
et al., 2007), not to mention unseen events, semi-
supervised or unsupervised methods seem to offer
the primary near-term hope for broad coverage se-
mantic role labeling.
In this work, we develop a semi-supervised
method for enhancing FrameNet with additional
annotations which could then be used for clas-
sifier training. We assume that an initial set of
labeled examples is available. Then, faced with
an unknown predicate, i.e., a predicate that does
not evoke any frame according to the FrameNet
database, we must decide (a) which frames it be-
longs to and (b) how to automatically annotate
example sentences containing the predicate. We
solve both problems jointly, using a graph align-
ment algorithm. Specifically, we view the task
of inferring annotations for new verbs as an in-
stance of a structural matching problem and fol-
low a graph-based formulation for pairwise global
network alignment (Klau, 2009). Labeled and un-
labeled sentences are represented as dependency-
graphs; we formulate the search for an optimal
alignment as an integer linear program where dif-
ferent graph alignments are scored using a func-
tion based on semantic and structural similarity.
We evaluate our algorithm in two ways. We assess
how accurate it is in predicting the frame for an
unknown verb and also evaluate whether the an-
notations we produce are useful for semantic role
labeling.
In the following section we provide an overview
of related work. Next, we describe our graph-
alignment model in more detail (Section 3) and
present the resources and evaluation methodology
used in our experiments (Section 4). We conclude
the paper by presenting and discussing our results.
2 Related Work
Much previous work has focused on creating
FrameNet-style annotations for languages other
than English. A common strategy is to exploit
parallel corpora and transfer annotations from
English sentences onto their translations (Pad´o
and Lapata, 2006; Johansson and Nugues, 2006).
Other work attempts to automatically augment the
English FrameNet in a monolingual setting either
by extending its coverage or by creating additional
training data.
There has been growing interest recently in
determining the frame membership for unknown
predicates. This is a challenging task, FrameNet
currently lists 502 frames with example sentences
which are simply too many (potentially related)
classes to consider for a hypothetical system.
Moreover, predicates may have to be assigned to
multiple frames, on account of lexical ambiguity.
Previous work has mainly used WordNet (Fell-
baum, 1998) to extend FrameNet. For example,
Burchardt et al. (2005) apply a word sense dis-
ambiguation system to annotate predicates with
a WordNet sense and hyponyms of these predi-
cates are then assumed to evoke the same frame.
Johansson and Nugues (2007) treat this problem
as an instance of supervised classification. Using
a feature representation based also on WordNet,
they learn a classifier for each frame which decides
whether an unseen word belongs to the frame or
not. Pennacchiotti et al. (2008) create “distribu-
tional profiles” for frames. Each frame is repre-
sented as a vector, the (weighted) centroid of the
vectors representing the meaning of the predicates
it evokes. Unknown predicates are then assigned
to the most similar frame. They also propose a
WordNet-based model that computes the similar-
ity between the synsets representing an unknown
predicate and those activated by the predicates of
a frame.
All the approaches described above are type-
based. They place more emphasis on extending
the lexicon rather than the annotations that come
with it. In our earlier work (F¨urstenau and Lapata,
2009) we acquire new training instances, by pro-
jecting annotations from existing FrameNet sen-
tences to new unseen ones. The proposed method
is token-based, however, it only produces annota-
tions for known verbs, i.e., verbs that FrameNet
lists as evoking a given frame.
In this paper we generalize the proposals of
Pennacchiotti et al. (2008) and F¨urstenau and Lap-
ata (2009) in a unified framework. We create train-
ing data for semantic role labeling of unknown
predicates by projection of annotations from la-
beled onto unlabeled data. This projection is con-
12
ceptualized as a graph alignment problem where
we seek to find a globally optimal alignment sub-
ject to semantic and structural constraints. Instead
of predicting the same frame for each occurence of
an unknown predicate, we consider a set of candi-
date frames and allow projection from any labeled
predicate that can evoke one of these frames. This
allows us to make instance-based decisions and
thus account for predicate ambiguity.
3 Graph Alignment Method
Our approach acquires annotations for an un-
known frame evoking verb by selecting sen-
tences featuring this verb from a large unlabeled
corpus (the expansion corpus). The choice is
based upon a measure of similarity between the
predicate-argument structure of the unknown verb
and those of similar verbs in a manually labeled
corpus (the seed corpus). We formulate the prob-
lem of finding the most similar verbs as the search
for an optimal graph alignment (we represent
labeled and unlabeled sentences as dependency
graphs). Conveniently, this allows us to create la-
beled training instances for the unknown verb by
projecting role labels from the most similar seed
instance. The annotations can be subsequently
used for training a semantic role labeler.
Given an unknown verb, the first step is to nar-
row down the number of frames it could poten-
tially evoke. FrameNet provides definitions for
more than 500 frames, of which we entertain only
a small number. This is done using a method sim-
ilar to Pennacchiotti et al. (2008). Each frame
is represented in a semantic space as the cen-
troid of the vectors of all its known frame evoking
verbs. For an unknown verb we then consider as
frame candidates the k closest frames according to
a measure of distributional similarity (which we
compute between the unknown verb’s vector and
the frame centroid vector). We provide details of
the semantic space we used in our experiments in
Section 4.
Next, we compare each sentence featuring the
unknown verb in question to labeled sentences fea-
turing known verbs which according to FrameNet
evoke any of the k candidate frames. If sufficiently
similar seeds exist, the unlabeled sentence is anno-
tated by projecting role labels from the most sim-
ilar one. The similarity score of this best match is
recorded as a measure of the quality (or reliability)
of the new instance. After carrying out this pro-
Body movement
FEE
??
~
~
~
~
~
~
~
~
~
Agent

_
e
i
k
m
p
r
u
y



Body part






~
|
}

and
SUBJ
xxq
q
q
q
q
q
q
q
q
q
CONJ

CONJ
''
O
O
O
O
O
O
O
O
O
O
O
O
O
Herkimer
MOD

blink
DOBJ

nod
MOD

Old
eye
DET

wisely
his
Figure 1: Annotated dependency graph for the
sentenceOld Herkimer blinked his eye and nodded
wisely. The alignment domain is indicated in bold
face. Labels in italics denote frame roles, whereas
grammatical roles are rendered in small capitals.
The verb blink evokes the frame Body Movement.
cedure for all sentences in the expansion corpus
featuring an unknown verb, we collect the highest
scoring new instances and add them back to our
seed corpus as new training items. In the follow-
ing we discuss in more detail how the similarity of
predicate-argument structures is assessed.
3.1 Alignment Scoring
Let s be a semantically labeled dependency graph
in which node n
FEE
represents the frame evoking
verb. Here, we use the term “labeled” to indi-
cate that the graph contains semantic role labels
in addition to grammatical role labels (e.g., sub-
ject or object). Let g be an unlabeled graph
and n
target
a verbal node in it. The “unlabeled”
graph contains grammatical roles but no semantic
roles. We wish to find an alignment between the
predicate-argument structures of n
FEE
and n
target
,
respectively. Such an alignment takes the form of
a function ? from a set M of nodes of s (the align-
ment domain) to a set N of nodes of g (the align-
ment range). These two sets represent the rele-
vant predicate-argument structures within the two
graphs; nodes that are not members of these sets
are excluded from any further computations.
If there were no mismatches between (frame)
semantic arguments and syntactic arguments, we
would expect all roles in s to be instantiated by
syntactic dependents in n
FEE
. This is usually the
case but not always. We cannot therefore sim-
13
ply define M as the set of direct dependents of
the predicate, but also have to consider complex
paths between n
FEE
and role bearing nodes. An
example is given in Figure 1, where the role Agent
is filled by a node which is not dominated by the
frame evoking verb blink ; instead, it is connected
to blink by the complex path (CONJ
?1
, SUBJ). For
a given seed s we build a list of all such complex
paths and also include all nodes of s connected
to n
FEE
by one of these paths. We thus define the
alignment domain M as:
1. the predicate node n
FEE
2. all direct dependents of n
FEE
, except auxil-
iaries
3. all nodes on complex paths originating
in n
FEE
4. single direct dependents of any preposition or
conjunction node which is in (2) or end-point
of a complex path covered in (3)
The last rule ensures that the semantic heads
of prepositional phrases and conjunctions are in-
cluded in the alignment domain.
The alignment range N is defined in a similar
way. However, we cannot extract complex paths
from the unlabeled graph g, as it does not con-
tain semantic role information. Therefore, we use
the same list of complex paths extracted from s.
Note that this introduces an unavoidable asymme-
try into our similarity computation.
An alignment is a function ? : M ? N?{?}
which is injective for all values except ?,
i.e., ?(n
1
) = ?(n
2
) 6= ? ? n
1
= n
2
. We score the
similarity of two subgraphs expressed by an align-
ment function ? by the following term:
?
n?M
?(n)6=?
sem(n,?(n))+? ·
?
(n
1
,n
2
)?E(M)
(?(n
1
),?(n
2
))?E(N)
syn
(
r
n
1
n
2
,r
?(n
1
)
?(n
2
)
)
(2)
Here, sem represents a semantic similarity mea-
sure between graph nodes and syn a syntactic sim-
ilarity measure between the grammatical role la-
bels of graph edges. E(M) and E(N) are the sets
of all graph edges between nodes of M and nodes
of N, respectively, and r
n
1
n
2
denotes the grammati-
cal relation between nodes n
1
and n
2
.
Equation (2) expresses the similarity between
two predicate-argument structures in terms of the
sum of semantic similarity scores of aligned graph
nodes and the sum of syntactic similarity scores of
aligned graph edges. The relative weight of these
two sums is determined by the parameter ?. Fig-
ure 2 shows an example of an alignment between
two dependency graphs. Here, the aligned node
pairs thud and thump, back and rest, against and
against, as well as wall and front contribute se-
mantic similarity scores, while the three edge pairs
SUBJ and SUBJ, IOBJ and IOBJ, as well as DOBJ
and DOBJ contribute syntactic similarity scores.
We normalize the resulting score so that it al-
ways falls within the interval [0,1]. To take into
account unaligned nodes in both the alignment do-
main and the alignment range, we divide Equa-
tion (2) by:
?
|M| · |N|+?
?
|E(M)| · |E(N)| (3)
A trivial alignment of a seed with itself where all
semantic and syntactic scores are 1 will thus re-
ceive a score of:
|M| ·1+? · |E(M)| ·1
?
|M|
2
+?
?
E(M)
2
= 1 (4)
which is the largest possible similarity score. The
lowest possible score is obviously 0, assuming that
the semantic and syntactic scores cannot be nega-
tive.
Considerable latitude is available in selecting
the semantic and syntactic similarity measures.
With regard to semantic similarity, WordNet is a
prime contender and indeed has been previously
used to acquire new predicates in FrameNet (Pen-
nacchiotti et al., 2008; Burchardt et al., 2005; Jo-
hansson and Nugues, 2007). Syntactic similarity
may be operationalized in many ways, for exam-
ple by taking account a hierarchy of grammatical
relations (Keenan and Comrie, 1977). Our experi-
ments employed relatively simple instantiations of
these measures. We did not make use of Word-
Net, as we were interested in exploring the set-
ting where WordNet is not available or has limited
coverage. Therefore, we approximate the seman-
tic similarity between two nodes via distributional
similarity. We present the details of the semantic
space model we used in Section 4.
If n and n
?
are both nouns, verbs or adjectives,
we set:
sem(n,n
?
) := cos(~v
n
,~v
n
?
) (5)
where ~v
n
and ~v
n
?
are the vectors representing the
lemmas of n and n
?
respectively. If n and n
?
14
Impact
FEE
OO


Impactor

_
i
w
	





Impactee

_
V
J
9
.
(
$
!

thud
((
SUBJ
zzv
v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
K
thump
SUBJ
{{v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
back
DET

''
against
DOBJ

66
rest
DET

IOBJ
$$
H
H
H
H
H
H
H
H
H
H
against
DOBJ

his wall
DET

77
the of
DOBJ

front
DET

IOBJ
$$
I
I
I
I
I
I
I
I
I
I
the
body
DET

the of
DOBJ

his
cage
DET

the
Figure 2: The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
against the wall. and The rest of his body thumped against the front of the cage. (Graph edges are also
aligned to each other.) The alignment domain and alignment range are indicated in bold face. The verb
thud evokes the frame Impact.
are identical prepositions or conjunctions we set
sem(n,n
?
) := 1. In all other cases sem(n,n
?
) := 0.
As far as syntactic similarity is concerned, we
chose the simplest metric possible and set:
syn
(
r,r
?
)
:=
{
1 if r = r
?
0 otherwise
(6)
3.2 Alignment Search
The problem of finding the best alignment ac-
cording to the scoring function presented in Equa-
tion (2) can be formulated as an integer linear pro-
gram. Let the binary variables x
ik
indicate whether
node n
i
of graph s is aligned to node n
k
of graph g.
Since it is not only nodes but also graph edges
that must be aligned we further introduce binary
variables y
i jkl
, where y
i jkl
= 1 indicates that the
edge between nodes n
i
and n
j
of graph s is aligned
to the edge between nodes n
k
and n
l
of graph g.
This follows a general formulation of the graph
alignment problem based on maximum structural
matching (Klau, 2009). In order for the x
ik
and
y
i jkl
variables to represent a valid alignment, the
following constraints must hold:
1. Each node of s is aligned to at most one node
of g:
?
k
x
ik
? 1
2. Each node of g is aligned to at most one node
of s:
?
i
x
ik
? 1
3. Two edges may only be aligned if their
adjacent nodes are aligned: y
i jkl
? x
ik
and
y
i jkl
? x
jl
The scoring function then becomes:
?
i,k
sem(n
i
,n
k
)x
ik
+? ·
?
i, j,k,l
syn
(
r
n
i
n
j
,r
n
k
n
l
)
y
i jkl
(7)
We solve this optimization problem with a ver-
sion of the branch-and-bound algorithm (Land
and Doig, 1960). In general, this graph align-
ment problem is NP-hard (Klau, 2009) and usually
solved approximately following a procedure simi-
lar to beam search. However, the special structure
of constraints 1 to 3, originating from the required
injectivity of the alignment function, allows us to
solve the optimization exactly. Our implementa-
tion of the branch-and-bound algorithm does not
generally run in polynomial time, however, we
found that in practice we could efficiently com-
pute optimal alignments in almost all cases (less
than 0.1% of alignment pairs in our data could not
be solved in reasonable time). This relatively be-
nign behavior depends crucially on the fact that
we do not have to consider alignments between
15
full graphs, and the number of nodes in the aligned
subgraphs is limited.
4 Experimental Design
In this section we present our experimental set-up
for assessing the performance of our method. We
give details on the data sets we used, describe the
baselines we adopted for comparison with our ap-
proach, and explain how our system output was
evaluated.
Data Our experiments used annotated sentences
from FrameNet as a seed corpus. These were
augmented with automatically labeled sentences
from the BNC which we used as our expan-
sion corpus. FrameNet sentences were parsed
with RASP (Briscoe et al., 2006). In addi-
tion to phrase structure trees, RASP delivers a
dependency-based representation of the sentence
which we used in our experiments. FrameNet role
annotations were mapped onto those dependency
graph nodes that corresponded most closely to the
annotated substring (see F¨urstenau (2008) for a de-
tailed description of the mapping algorithm). BNC
sentences were also parsed with RASP (Andersen
et al., 2008).
We randomly split the FrameNet corpus
1
into 80% training set, 10% test set, and 10% de-
velopment set. Next, all frame evoking verbs in
the training set were ordered by their number of
occurrence and split into two groups, seen and un-
seen. Every other verb from the ordered list was
considered unseen. This quasi-random split covers
a broad range of predicates with a varying number
of annotations. Accordingly, the FrameNet sen-
tences in the training and test sets were divided
into the sets train seen, train unseen, test seen,
and test unseen. As we explain below, this was
necessary for evaluation purposes.
The train seen dataset consisted of 24,220 sen-
tences, with 1,238 distinct frame evoking verbs,
whereas train unseen contained 24,315 sentences
with the same number of frame evoking verbs.
Analogously, test seen had 2,990 sentences and
817 unique frame evoking verbs; the number
of sentences in test unseen was 3,064 (with
847 unique frame evoking verbs).
Model Parameters The alignment model pre-
sented in Section 3 crucially relies on the similar-
1
Here, we consider only FrameNet example sentences
featuring verbal predicates.
ity function that scores potential alignments (see
Equation (2)). This function has a free parameter,
the weight ? for determining the relative contri-
bution of semantic and syntactic similarity. We
tuned ? using leave-one-out cross-validation on
the development set. For each annotated sentence
in this set we found its most similar other sentence
and determined the best alignment between the
two dependency graphs representing them. Since
the true annotations for each sentence were avail-
able, it was possible to evaluate the accuracy of our
method for any ? value. We did this by compar-
ing the true annotation of a sentence to the anno-
tation its nearest neighbor would have induced by
projection. Following this procedure, we obtained
best results with ? = 0.2.
The semantic similarity measure relies on a se-
mantic space model which we built on a lemma-
tized version of the BNC. Our implementation fol-
lowed closely the model presented in F¨urstenau
and Lapata (2009) as it was used in a similar
task and obtained good results. Specifically, we
used a context window of five words on either
side of the target word, and 2,000 vector dimen-
sions. These were the common context words in
the BNC. Their values were set to the ratio of the
probability of the context word given the target
word to the probability of the context word over-
all. Semantic similarity was measured using the
cosine of the angle between the vectors represent-
ing any two words. The same semantic space was
used to create the distributional profile of a frame
(which is the centroid of the vectors of its verbs).
For each unknown verb, we consider the k most
similar frame candidates (again similarity is mea-
sured via cosine). Our experiments explored dif-
ferent values of k ranging from 1 to 10.
Evaluation Our evaluation assessed the perfor-
mance of a semantic frame and role labeler with
and without the annotations produced by our
method. The labeler followed closely the im-
plementation described in Johansson and Nugues
(2008). We extracted features from dependency
parses corresponding to those routinely used in
the semantic role labeling literature (see Baker
et al. (2007) for an overview). SVM classifiers
were trained
2
with the LIBLINEAR library (Fan
et al., 2008) and learned to predict the frame
name, role spans, and role labels. We followed
2
The regularization parameterC was set to 0.1.
16
Figure 3: Frame labeling accuracy on high,
medium and low frequency verbs, before and af-
ter applying our expansion method; the labeler de-
cides among k = 1, . . . ,10 candidate frames.
the one-versus-one strategy for multi-class classi-
fication (Friedman, 1996).
Specifically, the labeler was trained on the
train seen data set without any access to training
instances representative of the “unknown” verbs in
test unseen. We then trained the labeler on a larger
set containing train seen and new training exam-
ples obtained with our method. To do this, we used
train seen as the seed corpus and the BNC as the
expansion corpus. For each “unknown” verb in
train unseen we obtained BNC sentences with an-
notations projected from their most similar seeds.
The quality of these sentences as training instances
varies depending on their similarity to the seed.
In our experiments we added to the training set
the 20 highest scoring BNC sentences per verb
(adding less or more instances led to worse per-
formance).
The average number of frames which can be
evoked by a verb token in the set test unseen
was 1.96. About half of them (1,522 instances)
can evoke only one frame, 22% can evoke two
frames, and 14 instances can evoke up to 11 differ-
ent frames. Finally, there are 120 instances (4%)
in test unseen for which the correct frame is not
annotated on any sentence in train seen.
Figure 4: Role labeling F
1
for high, medium, and
low frequency verbs (roles of mislabeled frames
are counted as wrong); the labeler decides among
k = 1, . . . ,10 candidate frames.
5 Results
We first examine how well our method performs
at frame labeling. We partitioned the frame evok-
ing verbs in our data set into three bands (High,
Medium, and Low) based on an equal division
of the range of their occurrence frequency in the
BNC. As frequency is strongly correlated with
polysemy, the division allows us to assess how
well our method is performing at different degrees
of ambiguity. Figure 3 summarizes our results for
High, Medium, and Low frequency verbs. The
number of verbs in each band are 282, 282, and
283, respectively. We compare the frame accuracy
of a labeler trained solely on the annotations avail-
able in FrameNet (Without expansion) against a
labeler that also uses annotations created with our
method (After expansion). Both classifiers were
employed in a setting where they had to decide
among k candidate frames. These were the k most
similar frames to the unknown verb in question.
We also show the accuracy of a simple baseline
labeler, which randomly chooses one of the k can-
didate frames.
The graphs in Figure 3 show that for verbs in the
Medium and Low frequency bands, both classi-
fiers (with and without expansion) outperform the
baseline of randomly choosing among k candidate
frames. Interestingly, rather than defaulting to the
most similar frame (k = 1), we observe that ac-
17
Figure 5: Hybrid frame labeling accuracy (k = 1
for High frequency verbs).
curacy improves when frame selection is viewed
as a classification task. The classifier trained on
the expanded training set consistently outperforms
the one trained on the original training set. While
this is also true for the verbs in the High frequency
band, labeling accuracy peaks at k = 1 and does
not improve when more candidate frames are con-
sidered. This is presumably due to the skewed
sense distributions of high frequency verbs, and
defaulting to the most likely sense achieves rela-
tively good performance.
Next, we evaluated our method on role label-
ing, again by comparing the performance of our
role labeler on the expanded and original train-
ing set. Since role and frame labeling are inter-
dependent, we count all predicted roles of an in-
correctly predicted frame as wrong. This unavoid-
ably results in low role labeling scores, but allows
us to directly compare performance across differ-
ent settings (e.g., different number of candidate
frames, with or without expansion). Figure 4 re-
ports labeled F
1
for verbs in the High, Medium
and Low frequency bands. The results are simi-
lar to those obtained for frame labeling; the role
labeler trained on the the expanded training set
consistently outperforms the labeler trained on the
unexpanded one. (There is no obvious baseline
for role labeling, which is a complex task involv-
ing the prediction of frame labels, identification of
the role bearing elements, and assignment of role
labels.) Again, for High frequency verbs simply
defaulting to k = 1 performs best.
Taken together, our results on frame and role
labeling indicate that our method is not very effec-
tive for High frequency verbs (which in practice
should be still annotated manually). We there-
Figure 6: Hybrid role labeling F
1
(k = 1 for High
frequency verbs).
fore also experimented with a hybrid approach
that lets the classifier choose among k candi-
dates for Medium and Low frequency verbs and
defaults to the most similar candidate for High
frequency verbs. Results for this approach are
shown in Figures 5 and 6. All differences be-
tween the expanded and the unexpanded classi-
fier when choosing between the same k > 1 can-
didates are significant according to McNemar’s
test (p < .05). The best frame labeling accu-
racy (26.3%) is achieved by the expanded classi-
fier when deciding among k = 6 candidate frames.
This is significantly better (p < .01) than the best
performance of the unexpanded classifier (25.0%),
which is achieved at k = 2. Role labeling results
follow a similar pattern. The best expanded classi-
fier (F
1
=14.9% at k = 6) outperforms the best un-
expanded one (F
1
=14.1% at k = 2). The difference
in performance as significant at p < 0.05, using
stratified shuffling (Noreen, 1989).
6 Conclusions
This paper presents a novel semi-supervised ap-
proach for reducing the annotation effort involved
in creating resources for semantic role labeling.
Our method acquires training instances for un-
known verbs (i.e., verbs that are not evoked by
existing FrameNet frames) from an unlabeled cor-
pus. A key assumption underlying our work is
that verbs with similar meanings will have sim-
ilar argument structures. Our task then amounts
to finding the seen instances that resemble the un-
seen instances most, and projecting their annota-
tions. We represent this task as a graph alignment
problem, and formalize the search for an optimal
alignment as an integer linear program under an
18
objective function that takes semantic and struc-
tural similarity into account.
Experimental results show that our method im-
proves frame and role labeling accuracy, espe-
cially for Medium and Low frequency verbs. The
overall frame labeling accuracy may seem low.
There are at least two reasons for this. Firstly, the
unknown verb might have a frame for which no
manual annotation exists. And secondly, many er-
rors are due to near-misses, i.e., we assign the un-
known verb a wrong frame which is nevertheless
very similar to the right one. In this case, accuracy
will not give us any credit.
An obvious direction for future work concerns
improving our scoring function. Pennacchiotti
et al. (2008) show that WordNet-based similarity
measures outperform their simpler distributional
alternatives. An interesting question is whether the
incorporation of WordNet-based similarity would
lead to similar improvements in our case. Also
note that currently our method assigns unknown
lexical items to existing frames. A better alterna-
tive would be to decide first whether the unknown
item can be classified at all (because it evokes a
known frame) or whether it represents a genuinely
novel frame for which manual annotation must be
provided.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC (grant
GR/T04540/01). We are grateful to Richard Jo-
hansson for his help with the re-implementation of
his semantic role labeler. Special thanks to Man-
fred Pinkal for valuable feedback on this work.
References
Øistein E. Andersen, Julien Nioche, Ted Briscoe,
and John Carroll. 2008. The BNC Parsed with
RASP4UIMA. In Proceedings of the 6th Interna-
tional Language Resources and Evaluation Confer-
ence, pages 865–869, Marrakech, Morocco.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99–104, Prague, Czech Republic.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases.
International Journal of Lexicography, 18(4):445–
478.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77–80, Sydney, Australia.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In Proceed-
ings of the GLDV 200Workshop GermaNet II, Bonn,
Germany.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. Journal of
Machine Learning Research, 9:1871–1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235–250.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Hagen F¨urstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 220–228, Athens, Greece.
Hagen F¨urstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, pages 1478–1484, Marrakech, Morocco.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436–443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Richard
Johansson and Pierre Nugues, editors, FRAME
2007: Building Frame Semantics Resources for
Scandinavian and Baltic Languages, pages 27–30,
Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393–400, Manchester, UK.
E. Keenan and B. Comrie. 1977. Noun phrase acces-
sibility and universal grammar. Linguistic Inquiry,
8:62–100.
Gunnar W. Klau. 2009. A new graph-based method
for pairwise global network alignment. BMC Bioin-
formatics, 10 (Suppl 1).
A.H. Land and A.G. Doig. 1960. An automatic
method for solving discrete programming problems.
Econometrica, 28:497–520.
19
Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the duc-2005 summarization task. In
Proceedings of the HLT/EMNLP Document Under-
standing Workshop, Vancouver, Canada.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693–701, Geneva,
Switzerland.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161–1168, Sydney,
Australia.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457–465, Honolulu,
Hawaii.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8–15, Sap-
poro, Japan.
20
