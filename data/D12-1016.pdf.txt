Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Aligning Predicates across Monolingual Comparable Texts
using Graph-based Clustering
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Generating coherent discourse is an important
aspect in natural language generation. Our
aim is to learn factors that constitute coherent
discourse from data, with a focus on how to re-
alize predicate-argument structures in a model
that exceeds the sentence level. We present
an important subtask for this overall goal, in
which we align predicates across compara-
ble texts, admitting partial argument struc-
ture correspondence. The contribution of this
work is two-fold: We first construct a large
corpus resource of comparable texts, includ-
ing an evaluation set with manual predicate
alignments. Secondly, we present a novel ap-
proach for aligning predicates across compa-
rable texts using graph-based clustering with
Mincuts. Our method significantly outper-
forms other alignment techniques when ap-
plied to this novel alignment task, by a margin
of at least 6.5 percentage points in F1-score.
1 Introduction
Discourse coherence is an important aspect in natu-
ral language generation (NLG) applications. A num-
ber of theories have investigated coherence inducing
factors. A prominent example is Centering Theory
(Grosz et al., 1995), which models local coherence
by relating the choice of referring expressions to the
importance of an entity at a certain stage of a dis-
course. A data-driven model based on this theory
is the entity-based approach by Barzilay and Lap-
ata (2008), which models coherence phenomena by
observing sentence-to-sentence transitions of entity
occurrences.
Barzilay and Lapata show that their approach can
discriminate between a coherent and a non-coherent
set of ordered sentences. However, their model is
not able to generate alternative entity realizations by
itself. Furthermore, the entity-based approach only
investigates realization patterns for individual enti-
ties in discourse in terms of core grammatical func-
tions. It does not investigate the interplay between
entity transitions and realization patterns for full-
fledged semantic structures. This interplay, how-
ever, is an important factor for a semantics-based,
generative model of discourse coherence.
The main hypothesis of our work is that we can
automatically learn context-specific realization pat-
terns for predicate argument structures (PAS) from a
semantically parsed corpus of comparable text pairs.
Our assumption builds on the success of previous
research, where comparable and parallel texts have
been exploited for a range of related learning tasks,
e.g., unsupervised discourse segmentation (Barzilay
and Lee, 2004) and bootstrapping semantic analyz-
ers (Titov and Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can inves-
tigate the factors that determine discourse coher-
ence in the realization patterns for the involved argu-
ments. These include the specific forms of argument
realization, as a pronoun or a specific type of refer-
ential expression, as studied in prior work in NLG
(Belz et al., 2009, inter alia). The specific set-up
we examine, however, allows us to further investi-
171
gate the factors that govern the non-realization of
an argument position, as a special form of coher-
ence inducing element in discourse. Example (1),
extracted from our corpus of aligned texts,illustrates
this point: Both texts report on the same event of
locating victims in an avalanche. While (1.a) explic-
itly talks about the location of this event, the role re-
mains implicit in the second sentence of (1.b), given
that it can be recovered from the preceding sentence.
In fact, realization of this argument role would im-
pede the fluency of discourse by being overly repet-
itive.
(1) a. . . . The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
This phenomenon clearly relates to the problem
of discourse-linking of implicit roles, a very chal-
lenging task in discourse processing.1 In our work,
we consider this problem from a content-based gen-
eration perspective, concentrating on the discourse
factors that allow for the omission of a role.
Thus, our aim is to identify comparable predica-
tions across aligned texts, and to study the discourse
coherence factors that determine the realization pat-
terns of arguments in the respective discourses. This
can be achieved by considering the full set of argu-
ments that can be recovered from the aligned pred-
ications. This paper focuses on the first of these
tasks, henceforth called predicate alignment.2
In line with data-driven approaches in NLP, we
automatically align predicates in a suitable corpus of
paired texts. The induced alignments will (i) serve to
identify events described in both comparable texts,
and (ii) provide information about the underlying ar-
gument structures and how they are realized in each
context to establish a coherent discourse. We in-
vestigate a graph-based clustering method for induc-
1See the recent SemEval 2010 task: Linking Events and
their Participants in Discourse, (Ruppenhofer et al., 2010).
2Note that we provide details regarding the construction of
a suitable data set and further examples involving non-realized
arguments in a complementary paper (Roth and Frank, 2012).
ing such alignments as clustering provides a suitable
framework to implicitly relate alignment decisions
to one another, by exploiting global information en-
coded in a graph.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 describes our task and a suit-
able data set. Section 4 introduces a graph-based
clustering model using Mincuts for the alignment of
predicates. Section 5 outlines the experiments and
presents evaluation results. Finally, we conclude in
Section 6 and discuss future work.
2 Related Work
The task of aligning words in general has been stud-
ied extensively in previous work, for example as part
of research in statistical machine translation (SMT).
Typically, alignment models in SMT are trained by
observing and (re-)estimating co-occurrence counts
of word pairs in parallel sentences (Brown et al.,
1993). The same methods have also been applied
in monolingual settings, for example to align words
in paraphrases (Cohn et al., 2008). In contrast to
traditional word alignment tasks, our focus is not on
pairs of isolated sentences but on aligning predicates
within the discourse contexts in which they are sit-
uated. Furthermore, text pairs for our task should
not be strictly parallel as we are specifically inter-
ested in the impact of different discourse contexts.
In Section 5, we will show that this particular set-
ting indeed constitutes a more challenging task com-
pared to traditional word alignment in parallel or
paraphrasing sentences.
Another set of related tasks is found in the area of
textual inference. Since 2006, there have been reg-
ular challenges on the task of Recognizing Textual
Entailment (RTE). In the original task description,
Dagan et al. (2006) define textual entailment “as a
directional relationship between pairs of text expres-
sions, denoted by T - the entailing ‘Text’ -, and H
- the entailed ‘Hypothesis’. (. . . ) T entails H if the
meaning of H can be inferred from the meaning of
T, as would typically be interpreted by people.” Al-
though this relation does not necessarily require the
presence of corresponding predicates, previous work
by MacCartney et al. (2008) shows that word align-
ments can serve as a good indicator of entailment.
172
As a matter of fact, the same holds true for the task
of detecting paraphrases. In contrast to RTE, this lat-
ter task requires bi-directional entailments, i.e., each
of the two phrases must entail the other. Wan et al.
(2006) show that a simple approach solely based on
word (and lemmatized n-gram) overlap can already
achieve an F1-score of up to 83% for detecting para-
phrases in the Microsoft Research Paraphrase Cor-
pus (Dolan and Brockett, 2005, MSRPC). In fact,
this is just 0.6% points below the state-of-the-art re-
sults recently reported by Socher et al. (2011).
The MSRPC and data sets from the first RTE
challenges only consisted of isolated pairs of sen-
tences. The Fifth PASCAL Recognizing Textual En-
tailment Challenge (Bentivogli et al., 2009) intro-
duced a “Search Task”, where entailing sentences
for a hypothesis have to be found in a set of full
documents. This new task first opened the doors for
assessing the role of discourse (Mirkin et al., 2010a;
Mirkin et al., 2010b) in RTE. However, this setting is
still limited as discourse contexts are only provided
for the entailing part (T ) of each text pair but not for
the hypothesis H .
A further task related to ours is the detection
of event coreference. The goal of this task is to
identify all mentions of the same event within a
document and, in some settings, also across docu-
ments. However, the task setting is typically more
restricted than ours in that its focus lies on iden-
tical events/references (cf. Walker et al. (2006),
Weischedel et al. (2011), inter alia). In particular,
verbalizations of different aspects of an event (e.g.,
‘buy’–‘sell’, ‘kill’–‘die’, ‘recover’–‘find’) are gen-
erally not linked in this paradigm. In contrast to co-
reference methods that identify chains of events, we
are interested in pairs of corresponding predicates
(and their argument structure), for which we can ob-
serve alternative realizations in discourse.
3 Aligning Predicates Across Texts
This section summarizes how we built a large cor-
pus of comparable texts, as a basis for the predicate
alignment task. We motivate the choice of the cor-
pus and present a strategy for extracting comparable
text pairs. Subsequently, we report on the prepara-
tion of an evaluation data set with manual predicate
alignments across the paired texts. We conclude this
section with an example that showcases the poten-
tial of using aligned predicates for the study of co-
herence phenomena. More detailed information re-
garding corpus creation, annotation guidelines and
additional examples illustrating the potential of this
corpus can be found in Roth and Frank (2012).
3.1 Corpus Creation
The goal of our work is to investigate coherence fac-
tors for argument structure realization, using com-
parable texts that describe the same events, but that
include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources that describe
the same underlying events. The English Gigaword
Fifth Edition (Parker et al., 2011) corpus (henceforth
just Gigaword) is one of the largest corpus collec-
tions for English. It comprises a total of 9.8 million
newswire articles from seven distinct sources.
In previous work (Roth and Frank, 2012), we in-
troduced GigaPairs, a sub-corpus extracted from Gi-
gaword that includes over 160,000 pairs of newswire
articles from distinct sources. GigaPairs has been
derived from Gigaword using the pairwise similar-
ity method on headlines presented by Wubben et al.
(2009). In addition to calculating the similarity of
news titles, we impose an additional date constraint
to further increase the precision of extracted pairs of
texts. Random inspection of about 100 documents
revealed only two texts describing different events.
Overall, we extracted 167,728 document pairs con-
taining a total of 50 million word tokens. Each doc-
ument in this corpus consists of up to 7.564 words
with a mean and median of 301 and 213 words, re-
spectively. All texts have been pre-processed us-
ing MATE tools (Bjo¨rkelund et al., 2010; Bohnet,
2010), a pipeline of NLP modules including a state-
of-the-art semantic role labeler that computes Prop-
Bank/NomBank annotations (Palmer et al., 2005;
Meyers et al., 2008).
3.2 Gold Standard Annotation
We selected 70 text pairs from the GigaPairs cor-
pus for manual predicate alignment. All document
pairs were randomly chosen with the constraint that
each text consists of 100 to 300 words.3 Predi-
3This constraint is satisfied by 75.3% of all documents in
GigaPairs.
173
cates identified by the semantic parser are provided
as pre-labeled annotations for alignment. We asked
two students4 to tag corresponding predicates across
each text pair. Following standard practice in word
alignment tasks (cf. Cohn et al. (2008)) the annota-
tors were instructed to distinguish between sure and
possible alignments, depending on how certainly, in
their opinion, two predicates describe verbalizations
of the same event. The following examples show
predicate pairings marked as sure (2) and as possi-
ble alignments (3).
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [. . . ]
b. The authority [. . . ] had already approved a sim-
ilar application by Nasdaq.
(3) a. Myanmar’s military government said earlier this
year it has released some 220 political prisoners
[. . . ]
b. The government has been regularly releasing
members of Suu Kyi’s National League for
Democracy party [. . . ]
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86.5 For the construction of a
gold standard, we merged the alignments from both
annotators by taking the union of all possible align-
ments and the intersection of all sure alignments.
Cases which involved a sure alignment on which the
annotators disagreed were resolved in a group dis-
cussion with the first author.
We split the final corpus into a development set
of 10 document pairs and a test set of 60 document
pairs. The test set contains a total of 3,453 predicates
(1,531 nouns and 1,922 verbs). Its gold standard an-
notation consists of 446 sure and 361 possible align-
ments, which corresponds to an average of 7.4 sure
(6.0 possible) alignments per document pair. Most
of the gold alignments (82.4%) are between predi-
cates of the same part-of-speech (242 noun and 423
verb pairs). A total of 383 gold alignments (47.5%)
have been annotated between predicates with iden-
tical lemma form. Diverging numbers of realized
arguments can be observed in 320 pairs (39.7%).
4Both annotators are students in computational linguistics,
one undergraduate (A) and one postgraduate (B) student.
5Following Brockett (2007), we computed agreement on la-
beled annotations, including unaligned predicate pairs as an ad-
ditional null category.
3.3 Potential for Discourse Coherence
This section presents an example of an aligned
predicate pair from our development set that il-
lustrates the potential of aggregating corresponding
PAS across comparable texts. The example repre-
sents one of eleven cases involving unrealized argu-
ments that can be found in our development set of
only ten document pairs.
(4) a. The Chadians said theyArg0 had fled in fear of
their lives.
b. The United Nations says some 20,000
refugeesArg0 have fled into CameroonArg1.
In both sentences, the Arg0 role of the predicate flee
is filled, but Arg1 (here: the goal) has not been real-
ized in (4.a). However, sentence (4.a) is still part of a
coherent discourse, as a role filler for the omitted ar-
gument can be inferred from the preceding context.
For the goal of our work, we are interested in factors
that license such omissions of an argument. Poten-
tial factors on the discourse level include the infor-
mation status of the entity filling an argument posi-
tion, and its salience at the corresponding point in
discourse. Roth and Frank (2012) discuss additional
examples that demonstrate the importance of fac-
tors on further linguistic levels, e.g., lexical choice
of predicates and their syntactic realization.
In the example above, the aggregation of aligned
PAS presents an effective means to identify appro-
priate fillers for unrealized roles. Hence, we can uti-
lize each such pair as one positive and one negative
training instance for a model of discourse coherence
that controls the omissibility of arguments. In what
follows, we introduce an alignment approach that
can be used to automatically acquire more training
data using the entire GigaPairs corpus.
4 Model
For the automatic induction of predicate alignments
across texts, we opt for an unsupervised graph-based
clustering method. In this section, we first define a
graph representation for pairs of documents. In par-
ticular, predicates are represented as nodes in such a
graph and similarities between predicates as edges.
We then proceed to describe various similarity mea-
sures that can be used to identify similar predicate
instances. Finally, we introduce the clustering algo-
rithm that we apply to graphs (representing pairs of
174
documents) in order to induce alignments between
corresponding predicates.
4.1 Graph representation
We build a bipartite graph representation for each
pair of texts, using as vertices the predicate argu-
ment structures assigned in pre-processing (cf. Sec-
tion 3.1). We represent each predicate as a node and
integrate information about arguments only implic-
itly. Given the sets of predicates P1 and P2 of two
comparable texts T1 and T2, respectively, we for-
mally define an undirected graph GP1,P2 as follows:
GP1,P2 = ?V,E? where
V = P1 ? P2
E = P1 × P2
(1)
Edge weights. We specify the edge weight be-
tween two nodes representing predicates p1 ? P1
and p2 ? P2 as a weighted linear combination of
four similarity measures described in the next sec-
tion: WordNet and VerbNet similarity, Distributional
similarity and Argument similarity.
wp1p2 = ?1 ? simWN(p1, p2)
+ ?2 ? simVN(p1, p2)
+ ?3 ? simDist(p1, p2)
+ ?4 ? simArg(p1, p2)
(2)
Initially we set all weighting parameters ?1 . . . ?4 to
have uniform weights by default. In Section 5, we
define an optimized weighting setting for the indi-
vidual similarity measures.
4.2 Similarity Measures
We employ a number of similarity measures
that make use of complementary information
that is type-based (simWN/VN/Dist) or token-based
(simArg).6 Given two lemmatized predicates p1, p2
and their set of arguments A1 = args(p1), A2 =
args(p2), we define the following measures.
WordNet similarity. Given all pairs of synsets s1,
s2 that contain the predicates p1, p2, respectively,
we compute the maximal similarity using the infor-
mation theoretic measure described in Lin (1998).
Our implementation exploits the WordNet hierarchy
6All token-based frequency counts (i.e., freq() and idf())
are computed over all documents from the AFP and APW parts
of the English Gigaword Fifth Edition.
(Fellbaum, 1998) to find the synset of the least com-
mon subsumer (lcs) and uses the pre-computed In-
formation Content (IC) files from Pedersen et al.
(2004) to compute Lin’s measure:
simWN(p1, p2) =
IC(lcs(s1, s2))
IC(s1) ? IC(s2)
(3)
In order to compute similarities between verbal and
nominal predicates, we further use derivation infor-
mation from NomBank (Meyers et al., 2008): if a
noun represents a nominalization of a verbal pred-
icate, we resort to the corresponding verb synset.
If no relation can be found between two predicates,
we set a default value of simWN = 0. This applies
in particular to all cases that involve a predicate not
present in WordNet.
VerbNet similarity. To overcome systematic
problems with the WordNet verb hierarchy (cf.
Richens (2008)), we further compute similarity
between verbal predicates using VerbNet (Kipper
et al., 2008). Verbs in VerbNet are categorized into
semantic classes according to their syntactic behav-
ior. A class C can recursively embed sub-classes
Cs ? sub(C) that represent finer semantic and
syntactic distinctions. We define a simple similarity
function that defines fixed similarity scores between
0 and 1 for pairs of predicates p1, p2 depending on
their relatedness within the VerbNet class hierarchy:
simVN(p1, p2) =
?
????
????
1.0 if ?C : p1, p2 ? C
0.8 if ?C,Cs : Cs ? sub(C)
? p1, p2 ? C ? Cs
0.0 else
(4)
Distributional similarity. As some predicates
may not be covered by the WordNet and VerbNet hi-
erarchies, we additionally calculate similarity based
on distributional meaning in a semantic space (Lan-
dauer and Dumais, 1997). Following the traditional
bag-of-words approach that has been applied in re-
lated tasks (Guo and Diab, 2011; Mitchell and La-
pata, 2010), we consider the 2,000 most frequent
context words c1, . . . , c2000 ? C as dimensions of
a vector space and define predicates as vectors using
their Pointwise Mutual Information (PMI):
~p = (PMI(p, c1), . . . ,PMI(p, c2000) (5)
175
with PMI(x, y) =
freq(x, y)
freq(x) ? freq(y)
Given the vector representations of two predicates,
we calculate their similarity as the cosine of the an-
gle between the two vectors:
simDist(p1, p2) =
~p1 · ~p2
|~p1| ? |~p2|
(6)
Argument similarity. While the previous similar-
ity measures are purely type-based, argument simi-
larity integrates token-based, i.e., discourse-specific,
similarity information about predications by taking
into account the similarity of their arguments. This
measure calculates the association between the ar-
guments A1 of the first and the arguments A2 of the
second predicate by determining the ratio of over-
lapping words in both argument sets.
simArg(p1, p2) =
?
w?A1?A2 idf(w)?
w?A1 idf(w) +
?
w?A2 idf(w)
(7)
In order to give higher weight to (rare) content
words, we weight each word by its Inverse Docu-
ment Frequency (IDF), which we calculate over all
documents d from the AFP and APW sections of the
Gigaword corpus:
idf(w) = log
|D|
|{d : w ? D|}
(8)
Normalization. In order to make the outputs of all
similarity measures comparable, we normalize their
value ranges on the development set to have a mean
and standard deviation of 1.0.
4.3 Mincut-based Clustering
Our graph clustering method uses minimum cuts (or
Mincut) in order to partition the bipartite text graph
into clusters of aligned predicates. A Mincut op-
eration divides a given graph into two disjoint sub-
graphs. Each minimum cut is performed as a cut
between some source node s and some target node
t, such that (i) each of the two nodes will be in a
different sub-graph and (ii) the sum of weights of all
removed edges will be as small as possible. Our sys-
tem determines each Mincut using an implementa-
tion of the method by Goldberg and Tarjan (1986).7
7Basic graph operations are performed using the freely
available Java library JGraph, cf. http://jgrapht.org/.
function CLUSTER(G)
clusters? ?
E ? GETEDGES(G) . Step 1
e? GETEDGEWITHLOWESTWEIGHT(E)
s? GETSOURCENODE(e)
t? GETTARGETNODE(e)
G? ? MINCUT(G, s, t) . Step 2
C ? GETCONNECTEDCOMPONENTS(G?)
for all Gs ? C do . Step 3
if SIZE(Gs) <= 2 then
clusters? clusters ?Gs
else
clusters? clusters ? CLUSTER(Gs)
end if
end for
return clusters;
end function
Figure 2: Pseudo code of our clustering algorithm
As our goal is to induce clusters that correspond to
pairs of similar predicates, we set a maximum num-
ber of two nodes per cluster as stopping criterion.
Given an input graph G, our algorithm recursively
applies Mincuts in three steps as described in Figure
2. Step 1 identifies the edge e with lowest weight in
the given graph G. Step 2 performs the actual Min-
cut operation on G. Finally, the stopping criterion
and recursion are applied in Step 3. An example of
a clustered graph is illustrated in Figure 1.
The advantage of our method compared to off-
the-shelf clustering techniques is two-fold: On the
one hand, the clustering algorithm is free of any pa-
rameters, such as the number of clusters or a clus-
tering threshold, that require fine-tuning. On the
other hand, the approach makes use of a termina-
tion criterion that very well represents the nature of
the goal of our task, namely to align pairs of predi-
cates across comparable texts. The next section pro-
vides empirical evidence for the advantage of this
approach.
5 Experiments
This section evaluates our graph-clustering model
on the task of aligning predicates across compara-
ble texts. For comparison to related tasks and meth-
ods, we describe different evaluation settings, vari-
176
Figure 1: The predicates of two sentences (white: “The company has said it plans to restate its earnings for 2000
through 2002.”; grey: “The company had announced in January that it would have to restate earnings (. . . )”) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
ous baselines, as well as results for these baselines
and the model presented above.
5.1 Settings
In order to benchmark our model against tradi-
tional methods for word alignment, we first apply
our graph-based alignment model (Full) on three
sentence-based paraphrase corpora. This model uses
the similarity measures defined in Section 4.2 and
the clustering algorithm introduced in Section 4.3.
In a second experiment, we evaluate Full on our
novel task of inducing predicate alignments across
comparable monolingual texts, using the GigaPairs
data set described in Section 3. We evaluate against
the manually annotated gold alignments in the test
data set described in Section 3.2. To gain more in-
sight into the performance of the various similar-
ity measures included in the Full model, we eval-
uate simplified versions that omit individual similar-
ity measures (Full–[measure name]).
The relative differences in performance against
various baselines will help us quantify the differ-
ences and difficulties between a traditional sentence-
based word alignment setting and our novel align-
ment task that operates on full texts.
5.1.1 Sentence-level Alignment Setting
For sentence-based predicate alignment we make
use of the following three corpora that are word-
aligned subsets of the paraphrase collections de-
scribed in (Cohn et al., 2008): MTC consists of 100
sentence pairs from the Multiple-Translation Chi-
nese Corpus (Huang et al., 2002), Leagues contains
100 sentential paraphrases from two translations of
Jules Verne’s “Twenty Thousand Leagues Under
the Sea”, and MSR is a sub-set of the Microsoft
Research Paraphrase Corpus (Dolan and Brockett,
2005), consisting of 130 sentence pairs. All three
paraphrase collections are in English.
Results for these experiments are reported in Sec-
tion 5.3.1. Note that in order to determine alignment
candidates, we apply the same pre-processing steps
as used for the annotation of our corpus. The se-
mantic parser identified an average number of 3.8,
5.1 and 4.7 predicates per text (i.e., per paraphrase
sentence) in MTC, Leagues and MSR, respectively.
All models are evaluated against the subset of gold
standard alignments (cf. Cohn et al. (2008)) between
pairs of words marked as predicates.
5.1.2 Text-level Alignment Setting
Results for our own data set, GigaPairs, are reported
in Section 5.3.2. In this setting, models are evaluated
against the annotated gold standard alignments be-
tween predicates as described in Section 3.2. Since
all text pairs in GigaPairs comprise multiple sen-
tences each, the average number of predicates per
text to consider (27.5) is much higher than in the
paraphrase settings. As the full graph representa-
tion becomes rather inefficient to handle (by default,
edges are inserted between all predicate pairs), we
use the development set of 10 text pairs to estimate
177
MTC Leagues MSR
Precision Recall F1 Precision Recall F1 Precision Recall F1
LemmaId 25.1** 74.9 37.6** 31.5** 67.2 42.9** 42.3** 90.8 57.7**
Greedy 74.8** 88.3** 81.0 75.0** 86.0** 80.1 80.7** 97.0** 88.1
WordAlign 99.3 86.6 92.5 98.7 78.5 87.4 99.5 96.0* 97.7*
Full 92.3 72.2 81.1 92.7 69.4 79.4 94.5 88.3 91.3
Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all
numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01).
a threshold on predicate similarity for adding edges.
We tested all thresholds from 1.5 to 4.0 with a step-
size of 0.25 and found 2.5 to perform best. This
threshold is applied in the evaluation of all graph-
based models.
5.2 Baselines
A simple baseline for both settings is to align all
predicates whose lemmas are identical. This base-
line, henceforth called LemmaId, is computed as a
lower bound for all settings. In order to assess the
benefits of the clustering step, we propose a second
baseline that uses the same similarity measures and
thresholds as our Full model, but omits the cluster-
ing step described in Section 4.3. Instead, it greed-
ily computes as many 1-to-1 alignments as possible,
starting from the highest similarity to the learned
threshold (Greedy).
As a more sophisticated baseline, we make
use of alignment tools commonly used in sta-
tistical machine translation (SMT). For the three
sentence-based paraphrase settings MTC, Leagues
and MSR, Cohn et al. (2008) readily provide
GIZA++ (Och and Ney, 2003) alignments as part
of their word-aligned paraphrase corpus. For the
experiments in the GigaPairs setting, we train our
own word alignment model using the state-of-the-
art word alignment tool Berkeley Aligner (Liang et
al., 2006). As word alignment tools require pairs of
sentences as input, we first extract paraphrases in the
latter setting using a re-implementation of the para-
phrase detection system by Wan et al. (2006).8 In
the following section, we abbreviate both baselines
using SMT alignment tools as WordAlign.
8Note that the performance of this system lies slightly be-
low the state-of-the-art results reported by Socher et al. (2011)
However, we were not able to reproduce the results of Socher et
al. using the publicly available release of their software.
5.3 Results
We measure precision as the number of predicted
alignments that are annotated in the gold standard
divided by the total number of predictions. Recall
is measured as the number of correctly predicted
sure alignments divided by the total number of sure
alignments in the gold standard. This conforms to
evaluation measures used for word alignment mod-
els in SMT (Och and Ney, 2003). Following Cohn
et al. (2008), we subsequently compute the F1-score
as the harmonic mean between precision and recall.
We compute statistical significance of result dif-
ferences with a paired t-test (Cohen, 1995) over the
affected test set documents and provide correspond-
ing significance levels where appropriate.
5.3.1 Sentence-level Predicate Alignment
The results for MTC, Leagues and MSR are pre-
sented in Table 1. The numbers indicate that
WordAlign consistently outperforms all other mod-
els on the three data sets in terms of F1-score. Sta-
tistical significance of result differences between
WordAlign and Full can only be observed for recall
and F1-score on the MSR data set (p<0.05). Other
differences are not significant due to high variance
of results compared to data set sizes.
The overall performance of WordAlign does not
come much as a surprise, seeing that all three data
sets consist of highly parallel sentence pairs. In
fact, the results for LemmaId show that by align-
ing all predicates with identical lemmas, most of the
sure alignments in the three settings are already cov-
ered. The reason for the low precision lies in the
fact that the same lemma can occur multiple times
in the same paraphrase, a phenomenon that is bet-
ter handled by WordAlign, Greedy and Full. In-
terestingly, the Greedy model achieves the highest
recall in all settings but it performs below our Full
178
model in terms of precision and F1-score. The per-
formance differences between Greedy and Full are
statistically significant (p<0.01) regarding precision
and recall.
5.3.2 Text-level Predicate Alignment
We now turn to the experiments on our own data
set, GigaPairs, which comprises full documents
of unequal lengths instead of pairs of single sen-
tences. Table 2 presents the results for our full model
and the three baselines. From all four approaches,
WordAlign yields lowest performance. We observe
two main reasons for this: On the one hand, sen-
tence paraphrase detection does not perform per-
fectly. Hence, the extracted sentence pairs do not
always contain gold alignments. On the other hand,
even sentence pairs that contain gold alignments are
generally less parallel than in the previous settings,
which make them harder to align. The increased dif-
ficulty can also be seen in the results for the Greedy
baseline, which only achieves an F1-score of 20.1%
in this setting. In contrast, we observe that the ma-
jority of all sure alignments (60.3%) can be retrieved
by applying the LemmaId model.
The Full model achieves a recall of 46.6%, but
it significantly outperforms LemmaId (p<0.01) in
terms of precision (58.7%, +18.4 percentage points).
This is an important factor for us, as we plan to use
the alignments in subsequent tasks. With 52.0%,
Full achieves the best overall F1-score.
Ablating similarity measures. All aforemen-
tioned results were conducted in experiments with
a uniform weighting scheme of similarity measures
as introduced in Section 4.3. Table 3 shows the per-
formance impact of individual similarity measures
by removing them completely (i.e., setting their
weight to 0.0). The numbers indicate that not all
measures contribute positively to the overall perfor-
mance when using equal weights. However, a signif-
icant difference can only be observed when remov-
ing the argument similarity measure, which drasti-
cally reduces the results. This clearly highlights the
importance of incorporating the context of individ-
ual predications in this task.
Tuning weights. Subsequently, we tested various
combinations of weights on our development set in
order to estimate a good overall weighting scheme.
Precision Recall F1
LemmaId 40.3** 60.3** 48.3
Greedy 19.6** 20.6** 20.1**
WordAlign 19.7** 15.2** 17.2**
Full 58.7 46.6 52.0
Table 2: Results for GigaPairs (all numbers in %); re-
sults that significantly differ from Full are marked with
asterisks (* p<0.05; ** p<0.01).
Precision Recall F1
Full–WN 58.9 48.0 52.9
Full–VN 57.3 48.7 52.6
Full–Dist 54.3 42.8 47.9
Full–Args 40.1** 24.0** 30.0**
Full 58.7 46.6 52.0
Full+tuned 59.7** 50.7** 54.8**
Table 3: Impact of removing individual measures and us-
ing a tuned weighting scheme (all numbers in %); results
that significantly differ from Full are marked with aster-
isks (* p<0.05; ** p<0.01).
This tuning procedure is implemented as a brute-
force technique, in which we fix the weight of one
similarity measure and allow all other measures to
receive a weight assignment between 0.25 to 5.0
times the fixed weight. Finally, the resulting weights
are normalized to sum to 1.0. We found the best per-
forming weighting scheme to be 0.09, 0.48, 0.24 and
0.19 for ?1, . . . , ?4, respectively (cf. Eq. (2), Section
4). The performance gains of the resulting model
(Full+tuned) can be seen in Table 3. Comput-
ing statistical significance of the result differences
between Full+tuned and all baseline models con-
firmed significant improvements (p<0.01) for both
precision and F1-score.
5.4 Error Analysis
We perform an error analysis on the output of
Full+tuned on the development set of GigaPairs
in order to determine re-occurring problems. In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors).
Six Type I errors (46%) occurred when the lemma
of an affected predicate occurred more than once in a
text and the model missed a correct link. Vice versa,
identical predicates that refer to different events have
179
been the source of 8 Type II errors (35%). We ob-
serve that these errors are frequently related to pred-
icates, such as “say” and “appear”, that often occur
in news texts. Altogether, we find 15 Type II errors
(65%) that are due to high predicate similarity de-
spite low argument overlap (cf. Example (5)).
(5) a. The US alert (. . . ) followed intelligence reports
that . . .
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious . . .
We observe that argument overlap itself can be low
even for correct alignments. This clearly indicates
that a better integration of context is needed. Ex-
ample (6.a) illustrates a case in which the agent of
a warning event is not realized. Here, contextual in-
formation is required to correctly align it to the first
warning event in (6.b). This involves inference be-
yond the local PAS.
(6) a. The US alert (. . . ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.
b. Japan has issued a travel alert . . . (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (. . . ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning . . .
6 Conclusion
We presented a novel task for predicate alignment
across comparable monolingual texts, which we ad-
dress using graph-based clustering with Mincuts.
The motivation for this task is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization.
As a first step, we constructed a data set of com-
parable texts that provide full discourse contexts
for alternative verbalizations of the same underlying
events. The data set is derived from all newswire
pairs found in the English Gigaword Fifth Edition
and contains a total of more than 160,000 paired
documents.
A subset of these pairs forms an evaluation set,
annotated with gold alignments that relate predica-
tions, which exhibit a (possibly partial) correspond-
ing argument structure. We established that the an-
notation task, while difficult, can be performed with
good inter-annotator agreement (? at 0.86).
Our main contribution is a novel clustering ap-
proach using Mincuts for aligning predications
across comparable texts. Our experiments estab-
lished that recursive clustering improves on greedy
selection methods by profiting from global infor-
mation encoded in the graph representation. While
the Mincut-based method is in itself unsupervised, a
small amount of development data is needed to tune
parameters for the construction of particularly suit-
able input graphs.
We tested our full model against two additional
baselines: simple heuristic alignment based on iden-
tical lemma forms and a combination of techniques
from SMT and paraphrase detection. The evalua-
tion for our novel task was complemented by a tra-
ditional word alignment task using established para-
phrase data sets. We determined clear differences in
performance for all models for the two types of task
settings. While word alignment methods from SMT
outperform the competing models in the sentence-
based alignment tasks, they perform poorly in the
discourse setting.
In future work, we will enhance our model by
incorporating more refined similarity measures in-
cluding discourse-based criteria. We will further ex-
plore tuning techniques, e.g., a more suitable pre-
selection method for edges in graph construction, in
order to increase either precision or recall. The deci-
sion of optimizing towards one measure or another
is clearly task-dependent. In our case, high preci-
sion is favorable as we plan to learn accurate dis-
course model parameters from the computed align-
ments. Even though such an optimization will result
in an overall lower recall, application of the align-
ment model on the entire GigaPairs corpus can still
provide us with a large amount of precise predicate
alignments. Using this set of alignments, we will
then proceed to exploit contextual information in or-
der to learn a semantic model for discourse coher-
ence in argument structure realization.
Acknowledgements
We are grateful to the Landesgraduiertenfo¨rderung
Baden-Wu¨rttemberg for funding within the research
initiative “Coherence in language processing” at
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation.
180
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tions, Montreal, Canada, June. to appear.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2–7 May 2004,
pages 113–120.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79–87.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of TAC.
Anders Bjo¨rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33–36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89–97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Peter F. Brown, Vincent J. Della Pietra, Stephan A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37–46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In J. Quin˜onero-Candela, I. Dagan, and
B. Magnini, editors, Machine Learning Challenges,
pages 177–190. Springer, Heidelberg, Germany.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Adrew V. Goldberg and Robert E. Tarjan. 1986. A
new approach to the maximum flow problem. In Pro-
ceedings of the eighteenth annual ACM symposium on
Theory of computing, pages 136–146, New York, NY,
USA.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 552–561, July.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. 42(1):21–40.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato’s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and represen-
tation of knowledge. Psychological Review, 104:211–
240.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104–111.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madison,
Wisc., 24–27 July 1998, pages 296–304.
Bill MacCartney, Michael Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-
27 October 2008.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal
Shnarch. 2010a. Recognising entailment within dis-
181
course. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Shachar Mirkin, Ido Dagan, and Sebastian Pado´. 2010b.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11–16 July 2010.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. 34(8):1388–
1429.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
29(1):19–51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity – Measuring the re-
latedness of concepts. In Companion Volume to the
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, Boston, Mass.,
2–7 May 2004, pages 267–270.
Tom Richens. 2008. Anomalies in the wordnet verb hier-
archy. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 729–736. Association for Computational Lin-
guistics.
Michael Roth and Anette Frank. 2012. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal, Canada, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45–50, Up-
psala, Sweden, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems (NIPS 2011).
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11–16 July 2010, pages 958–967.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
”Para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131–138.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122–
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
182
