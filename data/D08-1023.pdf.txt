Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223,
Honolulu, October 2008. c©2008 Association for Computational Linguistics
Probabilistic Inference for Machine Translation
Phil Blunsom and Miles Osborne
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
{pblunsom,miles}@inf.ed.ac.uk
Abstract
We advance the state-of-the-art for discrimi-
natively trained machine translation systems
by presenting novel probabilistic inference
and search methods for synchronous gram-
mars. By approximating the intractable space
of all candidate translations produced by inter-
secting an ngram language model with a
synchronous grammar, we are able to train
and decode models incorporating millions of
sparse, heterogeneous features. Further, we
demonstrate the power of the discriminative
training paradigm by extracting structured
syntactic features, and achieving increases in
translation performance.
1 Introduction
The goal of creating statistical machine translation
(SMT) systems incorporating rich, sparse, features
over syntax and morphology has consumed much
recent research attention. Discriminative approaches
are widely seen as a promising technique, poten-
tially allowing us to further the state-of-the-art.
Most work on discriminative training for SMT has
focussed on linear models, often with margin based
algorithms (Liang et al., 2006; Watanabe et al.,
2006), or rescaling a product of sub-models (Och,
2003; Ittycheriah and Roukos, 2007).
Recent work by Blunsom et al. (2008) has shown
how translation can be framed as a probabilistic
log-linear model, where the distribution over trans-
lations is modelled in terms of a latent variable
on derivations. Their approach was globally opti-
mised and discriminative trained. However, a lan-
guage model, an information source known to be
crucial for obtaining good performance in SMT, was
notably omitted. This was because adding a lan-
guage model would mean that the normalising parti-
tion function could no longer be exactly calculated,
thereby preventing efficient parameter estimation.
Here, we show how language models can be
incorporated into large-scale discriminative transla-
tion models, without losing the probabilistic inter-
pretation of the model. The key insight is that we
can use Monte-Carlo methods to approximate the
partition function, thereby allowing us to tackle the
extra computational burden associated with adding
the language model. This approach is theoreti-
cally justified and means that the model contin-
ues to be both probabilistic and globally optimised.
As expected, using a language model dramatically
increases translation performance.
Our second major contribution is an exploita-
tion of syntactic features. By encoding source syn-
tax as features allows the model to use, or ignore,
this information as it sees fit, thereby avoiding the
problems of coverage and sparsity associated with
directly incorporating the syntax into the grammar
(Huang et al., 2006; Mi et al., 2008). We report on
translation gains using this approach.
We begin by introducing the synchronous gram-
mar approach to SMT in Section 2. In Section
3 we define the parametric form of our model
and describe techniques for approximating the
intractable space of all translations for a given
source sentence. In Section 4 we evaluate the abil-
ity of our model to effectively estimate the highly
dependent weights for the sparse features and real-
valued language model. In addition we describe how
215
X?
 ? ??? , Brown?
X
?
 ? ?? X
? 
?? X
? 
, arrived in X
?
 from X
?
?
X
?
 ? ??? , Shanghai?
X
?
 ? ??? , Beijing?
X
?
 ? ?X
? 
? ?? ?? X
? 
, X
? 
X
? 
late last night?
S ? ?X
? 
?
 
? , X
?
 .?
Figure 1. An example SCFG derivation from a Chi-
nese source sentence which yields the English sentence:
“Brown arrived in Shanghai from Beijing late last night.”
our model can easily integrate rich features over
source syntax trees and compare our training meth-
ods to a state-of-the-art benchmark.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) describes the gener-
ation of pairs of strings. A string pair is generated
by applying a series of paired context-free rewrite
rules of the form,X ? ??, ?,??, whereX is a non-
terminal, ? and ? are strings of terminals and non-
terminals and ? specifies a one-to-one alignment
between non-terminals in ? and ?. In the context of
SMT, by assigning the source and target languages
to the respective sides of a SCFG it is possible to
describe translation as the process of parsing the
source sentence, while generating the target trans-
lation (Chiang, 2007).
In this paper we only consider grammars
extracted using the heuristics described for the Hiero
SMT system (Chiang, 2007). Note however that our
approach is general and could be used with other
synchronous grammar transducers (e.g., (Galley et
al., 2006)). SCFG productions can specify that the
order of the child non-terminals is the same in both
languages (a monotone production), or is reversed (a
reordering production). Without loss of generality,
here we add the restriction that non-terminals on the
source and target sides of the grammar must have the
same category. Figure 1 shows an example deriva-
tion for Chinese to English translation.
3 Model
We start by defining a log-linear model for the con-
ditional probability distribution over target transla-
tions of a given source sentence. A sequence of
SCFG rule applications which produce a translation
from a source sentence is referred to as a derivation,
and each translation may be produced by many dif-
ferent derivations. As the training data only provides
source and target sentences, the derivations are mod-
elled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r, q(r,d)) (2)
Using Equation (1), the conditional probability of
a target translation given the source is the sum over
all of its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f)
where ?(e, f) is the set of all derivations of the
target sentence e from the source f.
Here k ranges over the model’s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The function q(r,d)
returns the target ngram context, for a language
model with order m, of rule r in derivation d.
For a rule which spans the target words (i, j) and
target yield(d) = {t0, · · · , tl}:
q(r,d) =
{
ti···ti+m?2?tj?m+2···tj if j ? i > m
ti···tj otherwise
The feature functions hk are real-valued functions
over the source and target sentences, and can include
overlapping and non-independent features of the
data. The features must decompose with the deriva-
tion and the ngram context defined by the function q,
as shown in Equation (2). The features can reference
the entire source sentence coupled with each rule, r,
and its target context, in a derivation.
By directly incorporating the language model
context q into the model formulation, we will not
216
be able to exactly compute the partition function
Z?(f), which sums over all possible derivations.
Even though a dynamic program over this space
would still run in polynomial time, as shown by Chi-
ang (2007), a packed chart representation of the par-
tition function for the binary Hiero grammars used
in this work would require O(n3|T |4(m?1)) space,1
which is far too large to be practical.
Instead we approximate the partition function
using a sum over a large subset of the possible
derivations (?(e, f)):
Z?(f) ?
?
e
?
d?{??(e,f)}
exp
?
k
?kHk(d, e, f)
= Z˜?(f)
This model formulation raises the questions of
what an appropriate large subset of derivations for
training is, and how to efficiently calculate the sum
over all derivations in decoding. In the following
sections we elucidate and evaluate our solutions to
these problems.
3.1 Sampling Derivations
The training and decoding algorithms presented in
the following sections rely upon Monte-Carlo tech-
niques, which in turn require the ability to draw
derivation samples from the probability distribution
defined by our log-linear model. Here we adapt
previously presented algorithms for sampling from
a PCFG (Goodman, 1998) for use with our syn-
chronous grammar model. Algorithm 1 describes the
algorithm for sampling derivations. The sampling
algorithm assumes the pre-existance of a packed
chart representation of all derivations for a given
source sentence. The inside algorithm is then used
to calculate the scores needed to define a multino-
mial distribution over all partial derivations associ-
ated with expanding a given child rule. These ini-
tial steps are performed once and then an unlim-
ited number of samples can be drawn by calling the
recursive SAMPLE procedure. MULTI draws a sample
from the distribution over rules for a given chart cell,
CHILDREN enumerates the chart cells connected to
a rule as variables, and DERIVATION is a recursive
tree data structure for derivations. The algorithm is
1where |T | is the size of the terminal alphabet, i.e. the num-
ber of unique English words.
Algorithm 1 Top-down recursive derivation sam-
pling algorithm.
1: procedure SAMPLE(X, i, k)
2: rule? MULTI(inside chart(X, i, k))
3: c = ?
4: for (child category, x, y) ? CHILDREN(rule)
do
5: c? c ? SAMPLE(child category, x, y)
6: end for
7: return DERIVATION(X, children)
8: end procedure
first called on a category and chart cell spanning the
entire chart, and then proceeds top down by using
the function MULTI to draw the next rule to expand
from the distribution defined by the inside scores.
3.2 Approximate Inference
Approximating the partition function with Z˜?(f)
could introduce biases into inference and in the fol-
lowing discussion we describe measures taken to
minimise the effects of the approximation bias.
An obvious approach to approximating the parti-
tion function, and the feature expectations required
for calculating the derivative in training, is to use
the packed chart of derivations produced by running
the cube pruning beam search algorithm of Chiang
(2007) on the source sentence. In this case Z˜?(f)
includes all the derivations that fall within the cube
pruning beam, hopefully representing the majority
of the probability mass. We denote the partition
function estimated with this cube beam approxima-
tion as Z˜cb? (f). This approach has the advantage of
using the same beam search dynamic program dur-
ing training as is used for decoding. As the approxi-
mated partition function does not contain all deriva-
tions, it is possible that some, or all, of the deriva-
tions of the reference translation from the parallel
corpus may be excluded. We must therefore intersect
the packed chart built from the cube beam with that
of the reference derivations to ensure consistency.
Although, as would be done using cube-pruning,
it would seem intuitively sensible to approximate
the partition function using only high probability
derivations, it is possible that doing so will bias
our model in odd ways. The space of derivations
contained within the beam will be tightly clustered
about a maximum, and thus a model trained with
such an approximation will only see a very small
217
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
possible
X
X
X
S
Alles / 
Everything
und / 
and
jedes / 
everyone
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
X
[1,2]
 
Everything
X
[3,4]
 
anything
X
[1,4]
 
Everything 
and
 s
X
[4,5]
 
is
X
[5,6]
 
possible
X
[1,5]
 
Everything * 
is
S
[1,6]
 
Everything * 
possible
X
[5,6]
 
conceivable
X
[2,3]
and
X
[1,3]
 
Everything * 
anything
X
[1,3]
 
Everything * 
everyone
X
[3,4]
 
everyone
S
[1,6]
 
Everything * 
conceivable
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
(a)
(c)
(b)
2
1
3
4
5
6
2
1
3
4
5
6
2
1
3
4
5
6
Figure 2. A German-English translation example of building Z˜sam? (f) from samples. (a) Two sample derivations are
drawn from the model, (b) these samples are then combined into a packed representation, here represented by a
hypergraph with target translations elided for a bigram language model. The derivation in (c) is contained within the
hypergraph even though it was never explicitly inserted.
part of the overall distribution, possibly leading it
astray. Consider the example of a language model
feature: as this is a very strong indicator of transla-
tion quality, we would expect all derivations within
the beam to have a similar (high) language model
score, thereby robbing this feature of its discriminat-
ing power. However if our model could also see the
low probability derivations it would be clear that this
feature is indeed very strongly correlated with good
translations. Thus a good approximation of the space
of derivations is one that includes both good and bad
examples, not just a cluster around the maximum.
A principled solution to the problem of approx-
imating the partition function would be to use a
Markov Chain Monte Carlo (MCMC) sampler to
estimate the sum with a large number of samples.
Most of the sampled derivations would be in the
high probability region of the distribution, however
there would also be a number of samples drawn
from the rest of the space, giving the model a more
global view of the distribution, avoiding the pit-
falls of the narrow view obtained by a beam search.
Although effective, the computational cost of such
an approach is prohibitive as we would need to draw
hundreds of thousands of samples to obtain conver-
gence, for every training iteration.
Here we mediate between the computational
advantages of a beam and the broad view of the dis-
tribution provided by sampling. Using the algorithm
outlined in Section 3.1 we draw samples from the
distribution of derivations and then insert these sam-
ples into a packed chart representation. This process
is illustrated in Figure 2. The packed chart created
by intersecting the sample derivations represents a
space of derivations much greater than the original
samples. In Figure 2 the chart is built from the first
two sampled derivations, while the third derivation
can be extracted from the chart even though it was
never explicitly entered. This approximation of the
partition function (denoted Z˜sam? (f)) allows us to
build an efficient packed chart representation of a
large number of derivations, centred on those with
high probability while still including a significant
representation of the low probability space. Deriva-
tions corresponding to the reference can be detected
during sampling and thus we can build the chart
for the reference derivations at the same time as
the one approximating the partition function. This
could lead to some, or none of, the possible ref-
erence derivations being included, as they may not
have been sampled. Although we could intersect all
of the reference derivations with the sampled chart,
this could distort the distribution over derivations,
218
and we believe it to be advantageous to keep the
distributions between the partition function and ref-
erence charts consistent.
Both of the approximations proposed above,
Z˜cb? (f) and Z˜
sam
? (f), rely on the pre-existence of a
trained translation model in order to either guide the
cube-pruning beam, or define the probability distri-
bution from which we draw samples. We solve this
chicken and egg problem by first training an exact
translation model without a language model, and
then use this model to create the partition function
approximations for training with a language model.
We denote the distribution without a language model
as p?LM? (e|f) and that with as p
+LM
? (e|f).
A final training problem that we need to address
is the appropriate initialisation of the model param-
eters. In theory we could simply randomly initialise
? for p+LM? (e|f), however in practice we found that
this resulted in poor performance on the develop-
ment data. This is due to the complex non-convex
optimisation function, and the fact that many fea-
tures will fall outside the approximated charts result-
ing in random, or zero, weights in testing. We intro-
duce a novel solution in which we use the Gaus-
sian prior over model weights to tie the exact model
trained without a language model, which assigns
sensible values to all rule features, with the approx-
imated model. The prior over model parameters for
p+LM? (e|f) is defined as:
p+LM(?k) ? e
?
??k??
?LM
k ?
2
2?2
Here we have set the mean parameters of the Gaus-
sian distribution for the approximated model to
those learnt for the exact one. This has the effect
that any features that fall outside the approximated
model will simply retain the weight assigned by the
exact model. While for other feature weights the
prior will penalise substantial deviations away from
??LM , essentially encoding the intuition that the
rule rule parameters should not change substantially
with the inclusion of language model features.
This results in the following log-likelihood objec-
tive and corresponding gradient:
L =
?
(ei,fi)?D
log p+LM? (ei|fi) +
?
k
log p+LM0 (?k)
?L
??k
= Ep+LM? (d|ei,fi)
[hk]? Ep+LM? (e|fi)
[hk]
?
?+LMk ? ?
?LM
k
?2
3.3 Decoding
As stated in Equation 3 the probability of a given
translation string is calculated as the sum of the
probabilities of all the derivations that yield that
string. In decoding, where the reference translation
is not known, the exact calculation of this summa-
tion is NP-Hard. This problem also arises in mono-
lingual parsing with probabilistic tree substitution
grammars and has been tackled in the literature
using Monte-Carlo sampling methods (Chappelier
and Rajman, 2000). Their approach is directly appli-
cable to our SCFG decoding problem and we can use
Algorithm 1 to draw sample translation derivations
for the source sentence. The probability of a trans-
lation can be calculated simply from the number of
times a derivation that yields it was sampled, divided
by the total number of samples. For the p?LM? (e|f)
model we can build the full chart of all possible
derivations and thus sample from the true distribu-
tion over derivations. For the p+LM? (e|f) model we
suffer the same problem as in training and cannot
build the full chart. Instead a chart is built using
the cube-pruning algorithm with a wide beam and
we then draw samples from this chart. Although
sampling from a reduced chart will result in biased
samples, in Section 4 we show this approach to be
effective in practice.2 In Section 4 we compare our
sampling approach to the heuristic beam search pro-
posed by Blunsom et al. (2008).
It is of interest to compare our proposed decoding
algorithms to minimum Bayes risk (MBR) decoding
(Kumar and Byrne, 2004), a commonly used decod-
ing method. From a theoretical standpoint, the sum-
ming of derivations for a given translation is exactly
2We have experimented with using a Metropolis Hastings
sampler, with p?LM? (e|f) as the proposal distribution, to sam-
ple from the true distribution with the language model. Unfor-
tunately the sample rejection rate was very high such that this
method proved infeasibly slow.
219
equivalent to performing MBR with a 0/1 loss func-
tion over derivations. From a practical perspective,
MBR is normally performed with BLEU as the loss
and approximated using n-best lists. These n-best
lists are produced using algorithms tuned to remove
multiple derivations of the same translation (which
have previously been seen as undesirable). However,
it would be simple to extend our sampling based
decoding algorithm to calculate the MBR estimate
using BLEU , in theory providing a lower variance
estimate than attained with n-best lists.
4 Evaluation
We evaluate our model on the IWSLT 2005 Chinese
to English translation task (Eck and Hori, 2005),
using the 2004 test set as development data for
tuning the hyperparameters and MERT training the
benchmark systems. The statistics for this data are
presented in Table 1.3 The training data made avail-
able for this task consisted of 40k pairs of tran-
scribed utterances, drawn from the travel domain.
The development and test data for this task are some-
what unusual in that each sentence has a single
human translated reference, and fifteen paraphrases
of this reference, provided by monolingual anno-
tators. Model performance is evaluated using the
standard BLEU metric (Papineni et al., 2002) which
measures average n-gram precision, n ? 4, and we
use the NIST definition of the brevity penalty for
multiple reference test sets. We provide evaluation
against both the entire multi-reference sets, and the
single human translation.
Our translation grammar is induced using the
standard alignment and rule extraction heuristics
used in hierarchical translation models (Chiang,
2007).4 As these heuristics aren’t based on a genera-
tive model, and don’t guarantee that the target trans-
lation will be reachable from the source, we discard
those sentence pairs for which we cannot produce a
derivation, leaving 38,405 sentences for training.
Our base model contains a single feature for each
rule which counts the number of times it appeared in
a particular derivation. For models which include a
3Development and test set statistics are for the single human
translation reference.
4With the exception that we allow unaligned words at the
boundary of rules. This improves training set coverage.
language model, we train a standard Kneser-Ney tri-
gram model on the target side of the training corpus.
We also include a word penalty feature to compen-
sate for the shortening effect of the language model.
In total our model contains 2.9M features.
The aims of our evaluation are: (1) to deter-
mine that our proposed training regimes are able to
realise performance increase when training sparse
rule features and a real valued language model fea-
ture together, (2) that the model is able to effectively
use rich features over the source sentence, and (3)
to confirm that our model obtains performance com-
petitive with the current state-of-the-art.
4.1 Inference and Decoding
We have described a number of modelling choices
which aim to compensate for the training biases
introduced by incorporating a language model fea-
ture through approximate inference. Our a priori
knowledge from other SMT systems suggests that
incorporating a language model should lead to large
increases in BLEU score. In this evaluation we aim
to determine whether our training regimes are able
to realises these expected gains.
Table 2 shows a matrix of development BLEU
scores achieved by varying the approximation of the
partition function in training, and varying the decod-
ing algorithm. If we consider the vertical axis we
can see that the sampling method for approximat-
ing the partition function has a small but consistent
advantage over using the cube-pruning beam. The
charts produced by the sampling approach occupy
roughly half the disc space as those produced by
the beam search, so in subsequent experiments we
present results using the Z˜sam? (f) approximation.
Comparing the decoding algorithms on the hori-
zontal axis we can reconfirm the findings of Blun-
som et al. (2008) that the max-translation decod-
ing outperforms the Viterbi max-derivation approx-
imation. It is also of note that this BLEU increase
is robust to the introduction of the language model
feature, assuaging fears that the max-translation
approach may have been doing the job of the lan-
guage model. We also compare using Monte-Carlo
sampling for decoding with the previously pro-
posed heuristic beam search algorithm. The differ-
ence between the two algorithms is small, however
220
Training Development Test
Chinese English Chinese English Chinese English
Utterances 38405 500 506
Segments/Words 317621 353116 3464 3752 3784 3823
Av. Utterances Length 8 9 6 7 7 7
Longest Utterance 55 68 58 62 61 56
Table 1. IWSLT Chinese to English translation corpus statistics.
Model Max-derivation Max-translation(Beam) Max-translation(Sampling)
p?LM? (e|f) 31.0 32.5 32.6
p+LM? (e|f) (Z˜
cb
? (f)) 39.1 39.8 39.8
p+LM? (e|f) (Z˜
sam
? (f)) 39.9 40.5 40.6
Table 2. Development set results for varying the approximation of the partition function in training, Z˜cb? (f) vs. Z˜sam? (f),
and decoding using the Viterbi max-derivation algorithm, or the max-translation algorithm with either a beam approxi-
mation or Monte-Carlo sampling.
we feeling the sampling approach is more theoreti-
cally justified and adopt it for our later experiments.
The most important result from this evaluation
is that both our training regimes realise substantial
gains from the introduction of the language model
feature. Thus we can be confident that our model
is capable of modelling the distribution over trans-
lations even when the space over all derivations is
intractable to dynamically program exactly.
4.2 A Discriminative Syntactic Translation
Model
In the previous sections we’ve described and evalu-
ated a statistical model of translation that is able to
estimate a probability distribution over translations
using millions of sparse features. A prime motiva-
tion for such a model is the ability to define com-
plex features over more than just the surface forms
of the source and target strings. There are limit-
less options for such features, and previous work
has focused on defining token based features such
as part-of-speech and morphology (Ittycheriah and
Roukos, 2007). Although such features are applica-
ble to our model, here we attempt to test the model’s
ability to incorporate complex features over source-
side syntax trees, essentially subsuming and extend-
ing previous work on tree-to-string translation mod-
els (Huang et al., 2006; Mi et al., 2008).
We first parse the source side of our training,
development and test corpora using the Stanford
parser.5 Next, while building the synchronous charts
5http://nlp.stanford.edu/software/lex-parser.shtml
required for training, whenever a rule is used in a
derivation a feature is activated which captures: (1)
the constituent spanning the rule’s source side in the
syntax tree (if any) (2) constituents spanning any
variables in the rule, and (3) the rule’s target side
surface form. Figure 3 illustrates this process.
These syntactic features are equivalent to the
grammar rules extracted for tree-to-string translation
systems. The key difference in our model is that the
source syntax tree is treated as conditioning context
and it’s information encoded as features, thus this
information can be used or ignored as the model sees
fit. This avoids the problems associated with explic-
itly encoding the source syntax in the grammar, such
as sparsity and overly constraining the model. In
addition we could easily incorporate features over
multiple source trees, for example mixing labelled
syntax trees with dependency graphs.
We limit the extraction of syntactic features to
those that appear in at least two training derivations,
giving a total of 4.2M syntactic features, for an over-
all total of 7.1M features.
4.3 Discussion
Table 3 shows the results from applying our
described models to the test set. We benchmark our
results against a model (Hiero) which was directly
trained to optimise BLEUNIST using the standard
MERT algorithm (Och, 2003) and the full set of
translation and lexical weight features described
for the Hiero model (Chiang, 2007). As well as
221
Model BLEUNIST BLEUIBM BLEUHumanRef
p?LM? (e|f) 33.5 35.2 25.2
p+LM? (e|f) 44.6 44.6 31.2
p+LM? (e|f) + syntax 45.3 45.2 31.8
MERT (BLEUNIST ) 46.2 44.5 30.2
Table 3. Test set results.
?? 
???
? ?? ?
V WH ?NN
NP
VP
SQ
Where is the currency exchange office ?
(Step 2) X
2
 -> < [X
1
] ? ?? ?, Where is the [X
1
] ?>
(Step 1) X
1
 -> <?? ???, currency exchange office>
NP
SQ
? ?? ?
Where is the [X
1
] ? Example Syntax feature =
for Step 2
Example Derivation:
X
1
(a) (b)
(c)
Figure 3. Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b), we
extract the syntax feature in (c) by combining the grammar rule with the source syntax of the constituents contained
within that rule.
Source ???????????????????
p?LM? (e|f) don ’t have enough bag on me change please go purchase a new by plane .
p+LM? (e|f) i have enough money to buy a new one by air .
p+LM? (e|f) + syntax i don ’t have enough money to buy a new airline ticket .
MERT (BLEUNIST ) i don ’t have enough money to buy a new ticket .
Reference i do n’t have enough money with me to buy a new airplane ticket .
Table 4. Example test set output produced when: not using a language model, using a language model, also using
syntax, output optimised using MERT and finally the reference
BLEUNIST (brevity penalty uses the shortest ref-
erence), we also include results from the IBM
(BLEUIBM ) metric (brevity penalty uses the closest
reference), and using only the actual human transla-
tion in the test set, not the monolingual paraphrase
multiple references (BLEUHumanRef ).
The first result of interest is that we see an
increase in performance through the incorporation
of the source syntax features. This is an encourag-
ing result as the transcribed speech source sentences
are well out of the domain of the data on which the
parser was trained, suggesting that our model is able
to sift the good information from the noisy in the
unreliable source syntax trees. Table 4 shows illus-
trative system output on the test set.
On the BLEUNIST metric we see that our mod-
els under-perform the MERT trained system. We
hypothesise that this is predominately due to the
interaction of the brevity penalty with the unusual
nature of the multiple paraphrase reference train-
ing and development data. Their performance is
however quite consistent across the different inter-
pretations of the brevity penalty (NIST vs. IBM).
This contrasts with the MERT trained model, which
clearly over-fits to the NIST metric that it was
trained on and underperforms our models when eval-
uated on the single human test translations. If we
directly compare the brevity penalties of the MERT
model (0.868) and our discriminative model incor-
porating source syntax (0.942), on the these single
222
references, we can see that the MERT training has
optimised to the shortest paraphrase reference.
From these results it is difficult to draw any hard
conclusions on the relative performance of the dif-
ferent training regimes. However we feel confident
in claiming that we have achieved our goal of train-
ing a probabilistic model on millions of sparse fea-
tures which obtains performance competitive with
the current state-of-the-art training algorithm.
5 Conclusion
In this paper we have shown that statistical machine
translation can be effectively modelled as a well
posed machine learning task. In doing so we have
described a model capable of estimating a probabil-
ity distribution over translations using sparse com-
plex features, and achieving performance compara-
ble to the state-of-the-art on standard metrics. With
further work on scaling these models to large data
sets, and engineering high performance features, we
believe this research has the potential to provide sig-
nificant increases in translation quality.
Acknowledgements
The authors acknowledge the support of the EPSRC
grant EP/D074959/1.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of the 46th Annual Con-
ference of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08:HLT),
pages 200–208, Columbus, Ohio, June.
Jean-Ce´dric Chappelier and Martin Rajman. 2000.
Monte-carlo sampling for np-hard maximization prob-
lems in the framework of weighted parsing. In NLP
’00: Proceedings of the Second International Confer-
ence on Natural Language Processing, pages 106–
117, London, UK.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Pittsburgh, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 961–968, Sydney,
Australia, July.
Joshua T. Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Cambridge, MA, USA. Adviser-Stuart Shieber.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In In Proceedings of the 7th Bien-
nial Conference of the Association for Machine Trans-
lation in the Americas (AMTA), Boston, MA.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57–64, Rochester, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. of the 4th International Conference on Human
Language Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 169–
176.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465–488.
Percy Liang, Alexandre Bouchard-Coˆte´, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of the
44th Annual Meeting of the ACL and 21st Inter-
national Conference on Computational Linguistics
(COLING/ACL-2006), pages 761–768, Sydney, Aus-
tralia, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of the 46th Annual Confer-
ence of the Association for Computational Linguistics:
Human Language Technologies (ACL-08:HLT), pages
192–199, Columbus, Ohio, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160–
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311–318, Philadelphia,
Pennsylvania.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of the 44th Annual
Meeting of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
pages 777–784, Sydney, Australia.
223
