Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 665–675,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
 
 
Biography-Dependent Collaborative Entity Archiving for Slot Filling 
 
Yu Hong† 
Soochow University  
& Rensselaer Polytechnic Institute 
tianxianer@gmail.com 
Xiaobin Wang†, Yadong Chen, Jian Wang 
Soochow University 
Suzhou, CHN (215006) 
czwangxiaobin@gmail.com 
Tongtao Zhang, Heng Ji† 
Rensselaer Polytechnic Institute 
NY, USA (12180) 
jih@rpi.edu 
  
  
Abstract 
Knowledge Base Population (KBP) tasks, such 
as slot filling, show the particular importance 
of entity-oriented automatic relevant document 
acquisition. Rich, diverse and reliable relevant 
documents satisfy the fundamental require-
ment that a KBP system explores the nature of 
an entity. Towards the bottleneck problem be-
tween comprehensiveness and definiteness of 
acquisition, we propose a collaborative archiv-
ing method. In particular we introduce topic 
modeling methodologies into entity biography 
profiling, so as to build a bridge between 
fuzzy and exact matching. On one side, we 
employ the topics in a small-scale high-quality 
relevant documents (i.e., exact matching re-
sults) to summarize the life slices of a target 
entity (i.e., biography), and on the other side, 
we use the biography as a reliable reference 
material to detect new truly relevant docu-
ments from a large-scale partially complete 
pseudo-feedback (i.e., fuzzy matching results). 
We leverage the archiving method to enhance 
slot filling systems. Experiments on KBP cor-
pus show significant improvement over state-
of-the-art. 
1 Introduction 
Entity archiving is an entity-oriented document 
retrieval task. Towards a target entity of a specif-
ic type, such as the ones discussed in this paper, 
a person or an organization, the goal of entity 
archiving is to search and collect all relevant 
documents from large-scale data sets under lim-
ited prior knowledge of the entity. We limit our 
study to the regular English entity archiving, in 
which the prior knowledge contains the com-
monly used full name (formatted by English enti-
ty naming criteria) along with a gold-standard 
reference document, such as a news story on 
President “George W. Bush”. 
Entity archiving plays a fundamental role in 
KBP tasks. It narrows down the range of the data 
source for knowledge discovery to small-scale 
closely related documents. Such documents, on 
one hand, contain informative content on a target 
entity, which is extremely favorable for back-
ground knowledge extraction. On the other hand, 
the documents provide definitive evidence for 
verifying the claimed identity of the entity. 
As for KBP slot filling and verification tasks 
(Surdeanu and Ji, 2014), the archived relevant 
documents to an entity provide sufficient con-
texts (provenances) of the concrete instances 
(fillers) of the entity attributes (slots). See Figure 
1, in which both the provenances support filler 
extraction, meanwhile the provenance 1 addi-
tionally provides the evidence to verify the fillers 
(e.g., is episcopalism the true religion of Bush?). 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Use of entity archiving in slot filling 
The main challenges of entity archiving are as 
 Global data (e.g., all web docu-
ments) on PERSON entities 
KBP Slot Filling task 
Target entity: George W. Bush 
Slots: Title, Religion, Nation, etc. 
Filler Extraction 
Provenance 1-George Walker 
Bush is an American politician 
and businessman. 
Title: Politician & Businessman 
Nation: American 
Provenance 2-Bush left his fami-
ly’s Episcopal Church to join his 
wife’s United Methodist Church. 
Religion: United Methodist (True) 
Religion: Episcopal (False) 
 
Relevant docs. to 
George W. Bush 
Relevant docs. to 
George H.W. Bush 
Others. 
665
 follows: 1) it is difficult to retrieve all relevant 
documents through exact matching at the level of 
entity name, because an entity can be mentioned 
in various forms, such as alternate names and 
abbreviations; 2) in contrast, fuzzy matching in-
troduces a large amount of noise into retrieval 
results (see the examples in Figure 1), although it 
is capable of recalling an overwhelming majority 
of relevant documents; 3) inadequate prior 
knowledge makes it difficult to generate a full 
profile of an entity; 4) although pseudo-feedback 
is helpful to enrich the prior knowledge, tradi-
tional entity profiling (e.g., bag-of-words) meth-
ods establish vague boundaries among different 
life slices of an entity. For example, they are in-
capable of distinguishing the slice of the 
“Church Scientologist in Sea Organization” of 
Mark Fisher1  from the freelance career as the 
“Corporate liaison to Miscavige”. As a result it 
is difficult to enhance the independent effects of 
different slices on the entity-document relevance 
determination. 
To solve these problems, we propose a collab-
orative entity archiving (CEA) method. It em-
ploys the exact-matching based document re-
trieval to obtain a few high-quality reference 
documents, and leverages fuzzy matching for 
high-speed acquisition of adequate candidate 
documents (section 3). In addition, CEA uses the 
reference documents as prior knowledge to mod-
el the topic-level biography of an entity, and 
identifies the truly relevant documents from the 
candidates based on biography-document rele-
vance (section 4). Experiments show that CEA 
has substantial advantages over traditional re-
trieval methods (section 5.1). We apply CEA to 
state-of-the-art slot filling systems. Experimental 
results show that CEA provides consistent gains 
(section 5.2). 
2 Related Work 
? Entity Search 
One research topic similar to entity archiving is 
entity search. Entity search aims to seek, collect, 
and rank entities associated with specific infor-
mation needs (Balog et al, 2011). The TREC En-
terprise track featured an expert finding task (Ba-
log et al, 2008a): given a topic, return a ranked 
list of experts on the topic. In response to this 
problem, there have been considerable efforts on 
content based retrieval models, as well as feature 
                                                 
1 Mark Fisher: a PERSON query name in Slot Filing evalu-
ation of 2014, ID=SF14_ENG_031 
selection, such as proximity (Petkova and Croft, 
2007), document priors (Hu et al, 2006; Zhu et al, 
2010), expert-document associations (Balog and 
De-Rijke, 2008) and external evidence (Serdyu-
kov and Hiemstra, 2008).  
Since INEX was launched in 2002, which is 
an entity ranking task specific to structured data 
and multimedia (Demartini et al, 2010), struc-
tured features have been widely used in entity 
search, such as the most recent studies on Wik-
ipedia links and categories (Vercoustre et al, 
2008; Zhu et al, 2008; Jiang et al, 2009; We-
erkamp et al, 2009; Kaptein and Kamps, 2009; 
Balog et al, 2011) and web link structure (Balog 
et al, 2008b; You et al, 2011; Blanco et al, 2011; 
Neumayer et al, 2012; Bron et al, 2013). 
? Slot Filling 
The goal of slot filling is to seek and extract the 
concrete instances (fillers) specific to multiple 
entity attributes (slots) from a large-scale textual 
data set (Ji et al., 2010 and 2011; Surdeanu, 2013; 
Surdeanu and Ji, 2014).  The quality of the fillers 
largely depends on the performance of entity ar-
chiving and information extraction. 
Related studies on archiving mainly employed 
traditional retrieval techniques, including query 
expansion and string matching (Ji and Grishman, 
2011). A few studies involved document ranking 
and prioritizing by using probability model (Byr-
ne and Dunnion, 2010; Roth et al, 2014) and sta-
tistical language model (Chrupala et al, 2010). 
For filler extraction, great efforts were made 
to generate effective patterns and structure per-
ceptrons by supervised learning and reasoning 
(Chen et al, 2010; Grishman and Min, 2010; Gao 
et al, 2010; Surdeanu et al, 2011; Louis et al, 
2011; Kisiel et al, 2013). And effective feature 
selection and distant-supervision based classifi-
ers have been explored (Lehmann et al, 2010; 
Artiles et al, 2011; Sun et al, 2011; Roth and 
Klakow, 2013; Roth et al, 2014). Recently active 
learning (Angeli et al, 2014), truth-finding (Yu et 
al,   2014) as well, scanning (Yu et al., 2015) and 
ensemble learning (Viswanathan et al., 2015) 
were introduced to this field. 
? Brief Summary 
In all, entity search concentrates on the analysis 
of a single specific aspect of an entity, which is 
of interest or related to a domain. In the expert 
finding task, only academic careers of person 
entities (potential experts) are of concern in enti-
ty-document relevance determination. By con-
trast, for the sake of comprehensive understand-
ing of an entity, entity archiving necessarily 
666
 takes multiple and diverse aspects into account, 
such as a person’s career, family, religion, social-
ity, academics, etc. Due to the difference in goals, 
entity search techniques cannot be used directly 
to solve entity archiving problems. 
The performance of conventional retrieval 
techniques was generally limited due to the lack 
of precise modeling of the characteristics of an 
entity. Sparse prior knowledge and absence of 
effective profiling methods cause difficulties in 
characterizing the entity. The rest of the paper 
will be about knowledge acquisition and partition, 
as well as the collaborative method, along with a 
topic-level biographical profiling method. 
3 Prior Knowledge Acquisition 
We use string matching based retrieval methods 
to acquire relevant documents. It is worth con-
sidering that the acquired documents are not 
straightforwardly defined as the final entity ar-
chiving results. As we will show in this section, 
some of them are reliable, while others are full of 
noise. Instead, we regard them as the prerequisite 
knowledge for a coarse-to-fine processing. 
In the retrieval phase, a query Q is formulated 
as the full name of the target entity, while a doc-
ument D is represented as a string of words. D is 
determined to be relevant only if it contains some 
words that match Q. Accordingly we name such 
words as entity mentions. Both Q and D are pre-
processed by tokenization and stop-word filter-
ing. Other commonly used preprocessing steps 
(stemming and lemmatization) are disabled be-
cause they may cause confusion between entity 
mentions and common words. Table 1 shows the 
examples where the underlined words in <1> 
denotes an entity mention but <2> does not. 
Mark Fisher (PER)                  <ID: SF14_ENG_031 > 
<1> Mark Fisher, Sea Org member 
After stemming: mark fish 
<2>How to mark fishing landmarks? 
After stemming: mark fish 
3
rd
 Guard Division (ORG)      <ID: SF14_ENG_085 > 
<1>The 3
rd
 Guard Division of People’s Liberation 
Army of China. 
After lemmatization: guard divide 
<2>The 24 guards divide up into 2 groups. 
After lemmatization: guard divide 
Table 1: Inappropriate use of preprocessing 
We employ two matching methods for the rel-
evance determination: exact and fuzzy matching. 
Exact matching (EM) requires that a sequence 
of successive words in D exactly matches Q. By 
EM, entity archiving regards a full entity name 
as an indivisible word-order-fixed unit. Accord-
ingly it only acquires the documents which con-
tain the entity mentions in the form of complete-
ly-preserved full name.  
Fuzzy matching (FM) relaxes the conditions to 
a large extent, allowing Q to be split into nonad-
jacent words. In particular, it supports the change 
in word order as well as partial match. By FM, 
entity archiving is able to retrieve documents that 
contain the entity mentions in the form of sepa-
rated, pruned and/or reordered names. Table 2 
shows some examples of using these matching 
methods, where the mark “•” denotes the availa-
ble methods for a certain form of entity mention. 
Mark Fisher (PER)                   <ID: SF14_ENG_031 > 
Mark Fisher, Sea Org member.                           (exact) 
Availability: EM (•) FM (•)  
Mark, husband of Julie Fisher.                    (separation) 
Fisher had been Miscavige’s aide for 7 years.(pruning) 
Fisher’s first name, Mark, is impressive due to his in-
conceivable career change.                           (reordering) 
Availability: EM (  ) FM (•)  
Table 2: Examples of string matching results 
EM and FM have substantially different ad-
vantages and disadvantages in entity archiving. 
Table 3 shows the performance of EM and FM 
based entity archiving on KBP corpus (Surdeanu, 
2013). We will introduce the corpus in details in 
Section 5. EM yields precise archiving results 
because the constraint conditions are helpful to 
reduce uncertainty in string matching. In contrast 
FM-based archiving is able to match entity name 
mentions with various forms, and thus it achieves 
higher recall. 
 Precision Recall F-measure 
EM 72.5 52.6 61.0 
FM 10.8 86.8 19.2 
Table 3: Effects of EM and FM on archiving 
However FM generally introduces much noise, 
namely those mistakenly retrieved irrelevant 
documents. The documents are recalled because 
some pseudo entity mentions they contain can 
easily satisfy the constraints of fuzzy matching. 
See examples of the pseudo mentions in Table 4. 
As a result, FM yields a very low precision score. 
Mark Fisher (PER)                  <ID: SF14_ENG_031 > 
PlantWeb is a mark of the Fisher-Rosemount group of 
companies.                                                                                                         (separation) 
Deutsche Mark was the currency in Germany  (pruning) 
Iconic Fisher-Price mark.                                                                                  (reordering) 
Table 4: Examples of pseudo entity mentions 
obtained by using FM 
667
 Undoubtedly it is helpful for global optimiza-
tion of entity archiving to make full use of the 
advantages of EM and FM. In view of the above-
mentioned investigation, we partition the string 
matching results into two parts, exact and fuzzy 
ones, which are used as reliable prior knowledge 
(named reference source) and unrefined prior 
knowledge (candidate source) respectively. Most 
documents in the reference are truly related to 
the target entity but the scale is not big (see Re-
call of EM in Table 3), while the candidate is 
full of both true answers and noise (see Precision 
and Recall of FM in Table 3), respectively. As 
we will show in the next section, the final archiv-
ing results are generated by synthesizing the 
sources in a collaborative coarse-to-fine way. 
4 Collaborative Entity Archiving (CEA) 
We propose a Collaborative Entity Archiving 
approach (CEA for short). CEA synthesizes the 
reference source and candidate source in a col-
laborative manner (section 4.1) through a biog-
raphy-document relevance determination method 
(section 4.2 and 4.3). In addition, CEA involves 
mention disambiguation and query expansion in 
pre-processing to optimize the quality of both 
sources (section 4.4) 
4.1 Overall Framework of CEA 
CEA models the biography of an entity by using 
the topics in the reference source, in which, each 
topic serves as the description of a slice of life of 
the target entity (life slice for short), as shown in 
Figure 2. The Life slice means an episode in the 
whole story of the entity, which may represent an 
event, state or scenario at a certain moment, such 
as a person’s birth or an organization’s estab-
lishment. 
 
 
 
 
 
 
 
 
 
 
Figure 2: Framework of collaborative archiving 
CEA pulls out a document from the candidate 
source, one by one, and measures the biography-
document relevance at the topic level. By using a 
relevance threshold as the discrimination factor, 
CEA either preserves the document if it is rele-
vant, or filters otherwise. Meanwhile, CEA adds 
the newly found relevant documents to the refer-
ence source, and updates the biography by re-
shaping life slices (i.e., topics). CEA iteratively 
goes through the process of biography formula-
tion, biography-document relevance measure-
ment and determination until a condition is satis-
fied. Finally CEA selects all the preserved doc-
uments in the reference source as the final output. 
Figure 2 shows the framework. 
4.2 Biography-Document Relevance Models 
We design a generative approach to estimate the 
biography-document relevance r, which calcu-
lates the conditional probability that a candidate 
document D generates the biography B: 
? ( | )r P B D                             (1) 
In total we leverage three probabilistic models 
for modeling B and D, including relevance model, 
topic model and context-level topic model. Then 
we introduce Hellinger Distance (Lindsay, 1994) 
into relevance measurement. 
? Relevance Model (RM) 
Generally, Relevance Model (RM) (Huang and 
Croft, 2009) refers to the probability distribution 
over all words conditioned on their occurrences 
in a set of previously-known relevant documents 
(or high-quality pseudo-relevant documents), i.e., 
, ( | )w V P w R? ? , where V is the vocabulary, R is 
the document set, and P(w|R) can be estimated by 
TF-IDF. RM is often used in combination with 
Document Model (DM). Similar to RM, DM re-
fers to the probability distribution over words in 
a particular document, i.e., , ( | )w V P w D? ? . The 
relevance between R and D is normally deter-
mined by the agreement of RM and DM. The 
agreement can be estimated with Hellinger Dis-
tance between the models:  
2( | ) ( ( | ) ( | ))
w V
H RM DM P w R P w D
?
? ??
 (2) 
RM is a widely-used probabilistic model for 
information retrieval. It determines the relevance 
of a document to an object in accordance with 
homogeneousness in content between the docu-
ment and the relevant documents of the object.  
For an entity, in our case, we generate RM on 
the reference source, and regard it as the proba-
bilistic model of a macro-level all-embracing 
biography B over the prior knowledge R. For a 
candidate document D, the biography-document 
relevance r is measured with Hellinger Distance 
between RM and DM: P(B|D)=H(RM|DM). We 
D D D D D 
T T T T T T 
T T T 
D 
Documents in 
Reference Source 
A document in 
Candidate Source 
 
Relevance 
Topic 
Topic 
 Biography 
Topic-level 
Biography-Document 
Relevance Model 
EM 
Biography Modeling 
FM 
Reference Source 
Candidate Source 
Document Modeling 
D 
D 
Expand refer-
ence source  
668
 will demonstrate the effect of RM heavily relies 
on the quality of reference source in experiments. 
? Topic Model (TM) 
Empirically, RM is coarse-grained. It mixes up 
different, separate and incoherent life slices of an 
entity. A more serious problem is that RM as-
signs uneven weights to life slices, giving exces-
sive weights to the words about the popular slic-
es, but low or even zeroth weights to the unpopu-
lar ones. A popular slice is defined as the slice of 
greater concern, which is normally frequently 
mentioned in the reference source, such as the 
slice of “the career of George W. Bush as the 
President” (high weight) versus “his childhood” 
(low weight). As a result, the RM based biog-
raphy-document relevance is only helpful to 
identify and recall the documents relevant to the 
popular slices but not to the unpopular ones. 
As a modification, we employ Topic Model 
(TM) for biography modeling. We define a topic 
in the reference source as an independent fine-
grained representation for a microscopic life slice. 
Accordingly we treat the biography as a bucket 
of topics. We leverage Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003; Wei and Croft, 
2006) for topic discovery and modeling in the 
reference source. A topic is modeled as a proba-
bility distribution over all words in lexicon con-
ditioned on the association of the words with the 
topic, denoted as w V? ? , P(w|tR), in which tR 
refers to a topic in the reference source, repre-
senting a life slice s. Table 5 shows partial topic 
models (slices) in the reference source of Mark 
Fisher, where the highlighted probability values 
by a box reveal the words that well characterize a 
topic (slice). In the same way, we survey the top-
ics tC in the candidate source, modeled as 
w V? ? , P(w|tC). It is worth noting that those top-
ics (tC) may represent anything, namely the slices 
of the target entity or namesakes, related or unre-
lated events, etc. It means they are full of noise. 
Mark Fisher (Slice Modeling) <ID: SF14_ENG_031> 
Slice 1(s1), topic ts1: political career  
 ( | ) 0.003   ( | ) 8 5 ( | ) 8 5   ( | ) 4 51 1 1 1 ..." " " "      " " "  "
P w t P w t E P w t E P w t Es s s s
Parliament screenwriter film Bear Fisher
? ? ? ? ? ? ?? ?? ?
? ?
 
Slice 2(s2), topic ts2: artistic career 
 ( | ) 6 -5   ( | ) 0.003 ( | ) 0.001  ( | ) 3 52 2 2 2 ..." " " "      " " "  "
P w t E P w t P w t P w t Es s s s
Parliament screenwriter film Bear Fisher
? ? ? ? ?? ?? ?
? ?
 
Slice 3(s3), topic ts3: family 
 ( | ) 9 -5   ( | ) 1 -5 ( | ) 5 -6   ( | ) 0.0013 3 3 3 ..." " " "      " " "  "
P w t E P w t E P w t E P w ts s s s
Parliament screenwriter film Bear Fisher
? ? ? ?? ?? ?
? ?
 
Table 5: Example of life slice modeling by TM  
In practice, given a target entity, its reference 
source (exact matching results) is a subset of the 
candidate source (fuzzy matching results). We 
picked the reference source out of the candidate 
to parse topics independently, forming the set of 
tR. Meanwhile, we parse topics in the candidate 
source to form the set of tC. Benefitting from the 
separate treatment, some of the truly related top-
ics (tR) to the entity (correct slices) can be col-
lected along with less noise. Using the topics as 
references, we detect the relevant documents in 
the candidate source in terms of the topic-level 
biography-document relevance P(B|D). 
Given a document D in the candidate source, 
we transform P(B|D) into the combination of 
topic-document relevance of all topics in the ref-
erence source. We measure the topic-document 
relevance with the conditional probability P(tR|D) 
that the topic tR occurs in the document D. Ac-
cordingly, P(B|D) is estimated by: 
( | ) ( | )
log( ) log ( | )
R
R
R
t
R
t
r P B D P t D
r P t D
? ?
?
?
?
               (3) 
where, we incorporate the log likelihood into the 
numerical calculation for the sake of nonzero 
joint probability.  
Due to the separate topic modeling procedures 
for the reference and candidate sources, the 
probability P(tR|D)  ?  a topic tR in the reference 
source occurs in a candidate document D  ?  can-
not be obtained directly. To solve the problem, 
we introduce the joint probability of topic-topic 
relevance between topics (tR) in reference and 
topics (tC) in candidate (see the mode in Figure 2) 
into the probability calculation: 
,  ( | ) ( | ) ( | )
C C
R R R R C Ct T
t T P t D P t t P t D
?
? ? ? ?
 (4) 
where TR is the set of all topics in the reference 
source while TC is the candidate. The topic-topic 
relevance P(tR|tC) is approximated by Hellinger 
distance estimation between the topic models of 
tR and tC. As a whole, we measure the biography-
document relevance as: 
  
  
2
log( ) log ( | ) ( | )
log ( | ) ( | )
log ( | ) ( | )
log( | ) ( | )) ( | )
R R C C
R R C C
R R C C
R R C C
R C C
t T t T
R C C
t T t T
R C C
t T t T
R C C
t T t T w V
r P t t P t D
P t t P t D
H t t P t D
t P w t P t D
? ?
? ?
? ?
? ? ?
?
?
?
? ?
? ?
? ?
? ?
? ? ?
 
(5) 
669
 We employ the toolkit GibbsLDA++2 in topic 
modeling, which is an implementation of LDA 
using Gibbs sampling (Porteous et al, 2008). 
GibbsLDA++ makes it easy to parse the topics in 
a document set as well as estimate topic models 
P(w|t). Besides, GibbsLDA++ offers the proba-
bility over topics in generating a specific docu-
ment, facilitating the estimation of P(tC|D) in 
equation (5). Table 6 shows the operating param-
eters what we set in experiments, where the ones 
{?, ?} were set as the default values while the 
iterative number num is an empirical value. 
?= 1 num= 200 
?= 0.1 Nt?HDP 
Table 6: Operating parameters of GibbsLDA++ 
The necessary precondition for GibbsLDA++ 
in topic partition is to define the number Nt of 
potential topics in a set of documents. We exe-
cute the Hierarchical Dirichlet Processes (Teh et 
al, 2005), abbr., HDP, to predict Nt. HDP is simi-
lar to current hierarchical information organiza-
tion methods, such as the hierarchical clustering 
(Kummamuru et al, 2004), unsupervised and 
coarse-to-fine grained. Hence HDP is useful in 
exploring the basic rules of topic partition in an 
automatic way, such as number and granularity. 
We employ HDP to estimate the number (Nt) of 
all possible topics in reference source and candi-
date separately, acquiring two Nt for each target 
entity, one per source. 
? Context-level Topic Model (CTM) 
In consideration of the reliability of contexts in 
representing closely-related life slices to the enti-
ty, we use the contexts around entity mentions to 
improve the slice-oriented topic modeling. 
SEN: A sentence where an entity mention occurs 
NEB: Left and right neighbor sentences of SEN 
DEP: Words dependent on entity mention 
SYN: Words in maximum syntactic subtree in SEN 
Context 1: SEN 
Context 2: DEP + SYN 
Context 3: SEN + left NEB + Right NEB 
Table 7: Instructions of various types of contexts 
A context consists of the words co-occurring 
with an entity mention in a radius-fixed text span 
or syntactic or dependent structure (see instruc-
tions in Table 7). Given a target entity, the entity 
mention in the reference source is its full name. 
The union of all contexts in the source defines 
the vocabulary VR that most probably represent 
                                                 
2 http://gibbslda.sourceforge.net/ 
the slices of the entity. In the candidate source, 
on the contrary, the entity mention can be a reor-
dered, separated or pruned entity name, as well 
as abbreviation or alias, such as GWB (abbr.) and 
Dubya (alias) of George W. Bush. Different from 
the cases in reference, the vocabulary VC ob-
tained from the contexts in candidate are closely 
related to diverse entities or other objects with 
the same name (see Table 2&4).  
CTM measures the biography-document rele-
vance in the same way with TM, estimating the 
topic-level P(B|D) by equation (5). The only dif-
ference lies in the available words in topic model 
P(w|t). For the ones not included in VR and VC, 
CTM assigns a weight zero in topic model no 
matter what GibbsLDA++ does. 
4.3 Unsupervised Threshold Estimation 
For each target entity, CEA measures the biog-
raphy-document relevance for all documents in 
the candidate source. In the light of the relevance 
scores, CEA ranks the documents and sets a clear 
threshold ? to cut off the long tail in the ranking 
list, in other words, filtering the documents that 
have a relevance score lower than ?. The pre-
served documents will be added to the reference 
source for both biography reformulation and ar-
chiving result output. 
We estimate the threshold by learning density 
distribution of documents over relevance scores 
(Arampatzis et al, 2009). Density means the 
number of documents that have similar relevance 
scores. The distribution is produced by densities 
within all interval ranges of relevance score. Our 
empirical findings show that the density distribu-
tion fits a mixture of two Gaussians, where the 
highly relevant documents and the irrelevant 
ones distribute in two separate Gaussian peaks 
respectively. Accordingly we define the thresh-
old as the range of relevance score at the extreme 
point between the peaks, as shown in Figure 3. 
 
Figure 3: Extremum detection for threshold se-
lection. (Note: Y-axis indicates the density in a 
specific interval range of relevance score) 
In order to detect the extreme point, we firstly 
use a cubic polynomial function to approximate 
670
 the density distribution. Second, we go through 
the integral solution of the function in every fine-
grained interval range of relevance score (inter-
val is set as max(r)/100). We finally detect the 
extremum between peaks. The threshold is ini-
tialized during runtime exclusively for each tar-
get entity, without training. It is re-estimated 
every time when the biography is reformulated.  
4.4 Termination Criterion for Iteration 
CEA identifies relevant documents in candidate 
source and moves them to reference. Then CEA 
reshapes statistical models (RM, TM or CTM) 
over the updated sources. In terms of the re-
formed models, CEA starts a new round of rele-
vance determination, data movement, and statis-
tical modeling. CEA keeps it going until meeting 
any of the following termination criterions: 
? T1: No more new topic occurs in the refer-
ence source (Nt doesn’t change). 
? T2: The number of the documents in Peak1 
(Figure 3) begins to increase continuously. 
T2 is triggered if T1 loses its efficacy. The in-
validation happens when some general slices (i.e., 
general topics) are mistakenly introduced into the 
reference source, causing large-scale irrelevant 
document to be recalled and moved to reference. 
It will dramatically increase the number (Nt) of 
topics in a long term in the iterative procedure, 
driving CEA to capture more irrelevant docu-
ments. Thus Peak1 will be enlarged continuously. 
However, if as expected, Peak1 should be nar-
rowed with increasing the iteration times because: 
? Fewer new related slices appear. 
? The number of documents related to the 
slices is less than that in previous iterations.  
4.5 Optimization of EM and FM 
In the preprocessing phase, we improve the pre-
cision of EM because higher-quality EM results 
can offer more reliable reference documents for 
biography modeling. In addition, we expand que-
ries for FM to recall a larger number of relevant 
documents. It is helpful to minimize the loss of 
relevant documents before proceeding to CEA.  
To improve EM, we focus on identifying the 
common words that completely match the full 
name of the target entity. The words normally 
are elusive and easily treated as a correct entity 
name, called deceptive name, see that in (1). 
(1) Countrywide Financial  <ORG> 
True: Countrywide Financial Corporation. 
Deceptive: Bank of America purchased the fail-
ing countrywide financial for $4.1 billion. 
To reduce EM errors caused by deceptive names, 
we use name tagging (Miller et al, 2004) to dis-
tinguish deceptive names and true names. Fur-
ther, we filter the documents that are mistakenly 
retrieved based on the match between a decep-
tive name and the full-entity-name based query Q.  
We leverage an Alternate Name Table (ANT) 
for query expansion. ANT is a mapping table 
between entity name and alternate name. An al-
ternate name is either generated according to the 
naming conventions (Burman et al, 2011), such 
as abbreviation, suffixation and revivification. 
Some alternative names were extracted from 
knowledge base through redirect links (Nia et al, 
2014), such as nicknames in Wikipedia dumps. 
For an entity, we reformulate query Q by the 
combination of the pre-assigned full entity name 
and all possible alternate names in ANT, see (2). 
(2) Initial Q: Countrywide Financial Corporation. 
Expanded: Countrywide Financial+Corporation 
+Corp. +Company +Co. +Ltd. +Co Ltd. +CFC. 
We use the expanded query for FM to increase 
the number of relevant documents in the candi-
date source, regardless of whether or not it will 
introduce a larger scale of new noises. 
5 Experiments 
We evaluate our methods on KBP 2013 corpus. 
The corpus contains 2.1M texts collected from 
web pages, newswires and discussion forums. 
From this corpus, a slot filling system is required 
to find fillers for 41 types of slots that represent 
the attributes of the target entities. There are 25 
slot types of person and 16 slot types of organi-
zation, such as a Person’s birth date and an Or-
ganization’s founder (Ji et al., 2010 and 2011). 
KBP 2013 includes 100 target entities and 
ground-truth fillers and provenances, where the 
ground-truth data was obtained by manual verifi-
cation and annotation on the pool of system out-
puts. The provenances contain the IDs of docu-
ments relevant to target entities and fine-grained 
text spans which illustrate the eligibility of fillers.  
In total there are 1,851 gold standard relevant 
documents available for the evaluation of entity 
archiving. However the data is far from enough 
because it only covers a small portion of all rele-
vant documents in the pool. Most are excluded 
since KBP annotators ignore relevant documents 
in which there isn’t any filler for the assigned 
slots or, although exists, the fillers were inexact-
ly identified by Slot Filling systems. Therefore, 
we manually went over the pool and extracted 
4,405 relevant documents as our ground-truth. 
671
  
 
Sources 
 
Archiving 
Before Optimization of EM & FM (%) After Optimization of EM & FM (%) 
Micro-Average Macro-Average Micro-Average Macro-Average 
P R F P R F P R F P R F 
EM 65.4 40.1 49.7 72.5 52.6 61.0 78.8 24.6 37.5 81.1 46.9 59.4 
FM   9.4 74.6 16.7 10.8 86.8 19.2   6.3 92.9 11.8  6.9 94.2 12.9 
Baseline 29.1 50.6 36.9 36.1 62.9 45.9 22.6 33.3 26.9 25.1 52.6 34.0 
CEA(RM) 49.7 86.5 63.1 62.3 85.8 72.2 59.5 84.3 69.7 64.9 87.4 74.5 
CEA(TM) 52.6 87.7 65.8 63.2 86.1 72.9 60.6 82.9 70.0 65.7 85.8 74.4 
CEA(CTM1) 63.9 81.7 71.7 69.7 82.7 75.7 65.1 75.6 70.0 69.8 84.4 76.4 
CEA(CTM2) 63.9 81.7 71.7 69.7 82.7 75.7 65.1 75.6 70.0 69.8 84.4 76.4 
CEA(CTM3) 61.8 84.3 71.4 68.1 83.8 75.1 66.2 71.2 68.6 70.6 77.7 74.0 
Table 8: Archiving results (CTM1, 2 &3 are CTM using different types of contexts, context1, 2 &3 in Table 7) 
5.1 Archiving Results and Analysis 
We evaluate the entity archiving methods by mi-
cro and macro Precision (P), Recall (R) and F 
metrics. Table 8 shows the main results. 
? Overall Archiving Results 
Overall, the proposed CEA methods perform 
much better than the string matching based entity 
archiving methods (i.e., EM and FM).  
In addition the methods outperform a random-
sampling based CEA (baseline), which randomly 
selects a certain number of documents (candi-
dates) from the candidate source to combine with 
reference source straightforwardly (for final ar-
chiving results generation). The sampling num-
ber is set to be the same as the number of candi-
dates eventually archived by RM-based CEA. 
Random RM TM CTM1 CTM2 CTM3 
5,901 5,901 5,578 3,866 3.866 4,243 
5,158 5,158 4,943 4,032 4,032 3,655 
Table 9: The number of candidates added to ref-
erence source for archiving results generation 
(the second row indicates the number before op-
timizing EM and FM, while the third row indi-
cates after optimization) 
Table 9 shows the number of candidates ar-
chived from the candidate source by all kinds of 
CEA methods. It demonstrates that the biog-
raphy-based CEAs yield higher precision (Table 
8) after introducing the same or smaller number 
of candidates in the reference source, revealing 
the positive effect of biography modeling on en-
tity-oriented document relevance determination. 
? Reliability versus Comprehensiveness 
CEA achieves higher precision by using the op-
timized EM results as reference source. It 
demonstrates the importance of reliable prior 
knowledge for entity understanding as well as 
detecting relevant documents. However, the ref-
erence source causes lower recall scores of all 
CEA methods. The reason lies in reduction of 
prior knowledge. As shown in Table 8, the re-
fined reference source (i.e., optimized EM results) 
covers only 24.6% of all relevant documents, 
which is far less than the coverage before opti-
mization (nearly 40%).  
The reduced prior knowledge provides fewer 
available life slices of an entity for constructing 
an informative biography, inevitably resulting in 
missing some relevant documents. In order to 
confirm this, we regard the 41 KBP slot types as 
some readily-made visible life slices, and use the 
manual annotations of the slot fillers to verify 
whether a life slice appears in a relevant docu-
ment. For example, the filler “Corporate liaison 
of the slot Title reveals the slice of freelance ca-
reer of Mark Fisher. Then we figure out the cov-
erage rate of life slices for both the original ref-
erence source and the refined. 
Figure 4 exhibits the coverage rates for 5 most 
frequently occurred life slices. The coverage rate 
is calculated by the number of reference sources 
that contain a specific life slice versus 100, i.e., 
the number of reference sources for the 100 KBP 
entities (one per entity). It can be found that the 
refined reference sources miss lots of life slices. 
 
 
 
 
Figure 4: Coverage rates of life slices (for top 5) 
? Comparison among Biography Models 
RM is biased towards the popular life slices in 
biography modeling. The reasons are as follow-
ing: 1) RM gives greater weights to the high-
frequency words, and 2) popular slices are of 
much greater public interest and hence frequently 
mentioned in relevant documents. However, 
some entities not only share similar names but 
similar popular slices, such as the religious voca-
tion of different church scientologists. Therefore 
672
 RM is extremely likely to acquire the documents 
related to the namesakes if they have similar 
popular background as the target entity, causing 
a greater loss of precision.  
Table 10 shows the top highly-weighted words 
in RM for the target Mark Fisher (a church sci-
entologist), along with 2 namesakes who occur 
most frequently in the incorrect archiving results. 
Keywords in RM The most similar entities 
church 
committee 
religious 
Sea Org 
policy 
Miles Mark Fisher 
Church historian, Educa-
tor, Baptist minister and 
writer 
Mark Fisher 
Senior Pastor 
Table 10: Entities of similar background 
By contrast, TM independently represents dif-
ferent life slices and combines the effects of the 
slices on biography-document relevance deter-
mination, evenly and exhaustively. Comprehen-
sive and unbiased measurement of every known 
life slices is helpful in disambiguating entities 
that have similar backgrounds (definitely not the 
same in all). As a result, TM improves the preci-
sion. And the context-based TM goes further.  
5.2 Slot Filling Results and Analysis 
We apply our entity archiving methods to two 
top-ranked slot filling systems in the evaluation 
of KBP 2013, including LSV (Roth and Klakow, 
2013) and Blender (Yu et al 2013).  
The LSV incorporates a string matching based 
entity archiving and a SVM classifier based filler 
extraction. LSV’s archiving model expands que-
ries by using suffixes and Wikipedia anchor texts, 
and uses mutual information based relevance 
measure in document ranking and filtering.  
Blender employs a hybrid retrieval model for 
archiving relevant documents. It combines Bool-
ean and VSM models and expands query by an 
alternate name table similar to ours. For filler 
extraction, Blender implements truth finding 
over conflicting claims from multiple rule-based 
extraction systems. 
Methods P R F 
LSV’s archiving 53.0 88.2 66.2 
Blender’s archiving 54.6 71.7 62.0 
Table 11: Archiving performance of LSV &BLD 
Table 11 shows entity archiving performances 
of LSV and Blender (Macro-Average P, R and F). 
All CEA methods perform better than the both. 
With the aim to optimize provenances of fillers, 
we modify the slot filling systems by substituting 
their archiving methods with ours. Table 12 ex-
hibits the performance gains after replacement. 
               Slot Filling 
Archiving 
LSV (%) Blender (%) 
P R F P R F 
Original system 40.8 30.0 34.6 34.1 22.1 26.8 
Mod. (RM)CEA 42.1 30.0 35.0 35.6 23.9 28.6 
Mod. (TM)CEA 42.2 30.6 35.5 35.6 23.9 28.6 
Mod. (CTM3)CEA 42.7 29.3 34.7 36.0 23.8 28.7 
Table 12: Slot filling performance gains 
Both LSV and Blender achieve significant 
gains. The most interesting finding is on the dif-
ferent performance gains. It should reveal the 
fact that the well-supervised classification based 
filler extraction of LSV has a better capability of 
noise resistance, while by contrast, the truth-
finding in Blender is capable of identifying valid 
fillers if the quality of archiving results is high, 
otherwise easily makes mistake.  
6 Conclusion 
We doubt that it is easy to maintain the stability 
of current entity-oriented knowledge acquisition 
methods, including ours, in dealing with ordinary 
entities. Most target entities now in use for the 
evaluation are made to stand as “out of the ordi-
nary”, such as well-known enterprises, celebri-
ties or domain experts. As a result, a corpus con-
tains abundant relevant documents of the entities 
but less about the little-known namesakes. It 
greatly reduces the interference of namesakes 
and thus the difficulty of the task. 
In future work, we will make the task critical 
for success by employing the little known name-
sakes as targets. In addition to verifying the ro-
bustness of the CEA method, we will work on 
the relationship among entities (ACE entity rela-
tion types, Doddington et al, 2004) and related 
events (e.g., causal, temporal and sub-event rela-
tions), by which to build graph-based biography. 
7 Acknowledgment 
This work was supported by the U.S. DARPA 
DEFT Program No. FA8750-13-2-0041, ARL 
NS-CTA No. W911NF-09-2-0053, NSF CA-
REER Award IIS-1523198, AFRL DREAM pro-
ject, gift awards from IBM, Google, Disney and 
Bosch. It was also supported by Natural Science 
Foundation of China (NSFC) No. K111818713, 
K111818612. The views and conclusions con-
tained in this document are those of the authors 
and should not be interpreted as representing the 
official policies, either expressed or implied, of 
the U.S. and CHN Governments. The U.S. and 
CHN Governments are authorized to reproduce 
and distribute reprints for Government purposes 
notwithstanding any copyright notation here on.  
673
 References 
Gabor Angeli, Sonal Gupta, Melvin Jose, Christopher 
D. Manning, Christopher Ré, Julie Tibshirani, Jean 
Y. Wu, Sen Wu, and Ce Zhang. 2014. Stanford’s 
2014 slot filling systems. In Proceedings of the 7th 
TAC. 
Avi Arampatzis, Jaap Kamps, and Stephen Robertson. 
2009. Where to stop reading a ranked list: thresh-
old optimization using truncated score distributions. 
In Proceedings of the 32nd SIGIR, pages 524-531, 
Boston, Massachusetts, USA, July. 
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne Ta-
mang, and Heng Ji. 2011. CUNY BLENDER 
TAC-KBP2011 temporal slot filling system de-
scription. In Proceedings of the 4th TAC. 
Bruce G. Lindsay. 1994. Efficiency versus robustness: 
the case for minimum hellinger distance and relat-
ed methods. Annals of Statistics, 22(2): 1081-1114. 
Krisztian Balog, Lan Soboroff, Paul Thomas, Peter 
Bailey, Nick Craswell, and Arjen P. De Vries. 
2008a. Overview of the TREC 2008 enterprise 
track. In Proceedings of the 17th TREC, NIST. 
Krisztian Balog, Wouter Weerkamp, and Maarten De 
Rijke. 2008b. A few examples go a long way: con-
structing query models from elaborate query for-
mulations. In Proceedings of the 31st SIGIR, pages 
371-378, Singapore, July. 
Krisztian Balog and Maarten De Rijke. 2008. Associ-
ating people and documents. Advances in Infor-
mation Retrieval: 296-308 
Krisztian Balog, Marc Bron and Maarten De Rijke. 
2011. Query modeling for entity search based on 
terms, categories, and examples. ACM Transaction 
on Information System, 29(4): 22:1-22:31. 
Krishna Kummamuru, Rohit Lotlikar, Shourya Roy, 
Karan Singal, and Raghu Krishnapuram. 2004. A 
hierarchical monothetic document clustering algo-
rithm for summarization and browsing search re-
sults. In proceedings of the 13rd WWW, pages 658-
665. 
Roi Blanco, Peter Mika, Sebastiano Vigna. 2011. Ef-
fective and efficient entity search in RDF data. The 
Semantic Web-ISWC: 83-97. 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research 3: 993-1022. 
Marc Bron, Krisztian Balog, and Maarten De Rijke. 
2013. Example based entity search in the web of 
data. Advances in Information Retrieval: 392-403. 
Amev Burman, Arun Jayapal, Sathish Kannan, 
Madhu Kavilikatta, Ayman Alhelbawy, Leon Der-
czynski, and Robert Gaizauskas. 2011. USFD at 
KBP 2011: entity linking, slot filling and temporal 
bounding. In Proceedings of the 4th TAC. 
Lorna Byrne and John Dunnion. 2010. UCD IIRG at 
TAT 2010 KBP slot filling task. In Proceedings of 
the 3rd TAC, Maryland, USA, November. 
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, 
Marissa Passantino, and Heng Ji. 2010. Top-down 
and bottom-up: a combined approach to slot filling. 
In Proceedings of the 6th AIRS, pages 300-309, 
Taipei, Taiwan, December. 
Grzegorz Chrupala, Saeedeh Momtazi, Michael Wie-
gand, and Stefan Kazalski. 2010. Saaland universi-
ty spoken language systems at the slot filling task 
of TAC KBP 2010. In Proceedings of the 3rd TAC. 
Gianluca Demartini, Tereza Iofciu, and Arjen P. De 
Vries. 2010. Overview of the INEX 2009 entity 
ranking track. In Proceedings of the 8th Interna-
tional workshop of the Initiative for the Evaluation 
of XML Retrieval, pages 254-264. 
George Doddington, Alexis Mitchell, Mark Przybocki, 
Lance Ramshaw, Stephanie Strassel, and Ralph 
Weischedel. 2004. The ACE program-task, data, 
and evaluation. LREC. 
Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, 
Jingyi Guan, Yan Li, Hao Zhang, Weiran Xu, and 
Jun Guo. 2010. PRIS at TAC2010 KBP track. In 
Proceedings of the 3rd TAC. 
Ralph Grishman and Bonan Min. 2010. New York 
University KBP 2010 slot filling system. In Pro-
ceedings of the 3rd TAC. 
Guoping Hu, Jingjiang Liu, Hang Li, Yunbo Cao, 
Jian-Yun Nie, and Jianfeng Gao. 2006. A super-
vised learning approach to entity search. In Pro-
ceedings of the 3rd AIRS, pages 54-66, Singapore, 
October. 
Xuanjing Huang and W. Bruce Croft. 2009. A unified 
relevance model for opinion retrieval. In Proceed-
ings of the 18th CIKM, pages 947-956, Hong Kong, 
China, November. 
Jiepu Jiang, Wei Lu, Xianqian Rong, and Yangyan 
Gao. 2009. Adapting language modeling methods 
for expert search to rank Wikipedia entities. Lec-
ture Notes in Computer Science, 5631: 264-272. 
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira 
Griffitt, and Joe Ellis. 2010. Overview of the 
TAC2010 knowledge base population track. In 
Proceedings of the 3rd TAC, Maryland, USA, No-
vember. 
Heng Ji and Ralph Grishman. 2011. Knowledge base 
population: successful approaches and challenges. 
In Proceedings of the 49th ACL, pages 1148-1158, 
Portland, Oregon, USA, June. 
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011. 
Overview of the TAC2011 knowledge base popu-
lation track. In Proceedings of the 4th TAC. 
674
 Rianne Kaptein and Jaap Kamps. 2009. Finding enti-
ties in Wikipedia using links and categories. Lec-
ture Notes in Computer Science, 5631: 273-279. 
Bryan Kisiel, Justin Betteridge, Matt Gardner, Jayant 
Krishnamurthy, Ndapa Nakashole, Mehdi Samadi, 
Partha Talukdar, Drry Wijaya, and Tom Mitchell. 
2013. CMUML system for KBP 2013 slot filling. 
In Proceedings of the 6th TAC. 
John Lehmann, Sean Monahan, Luke Nezda, Arnold 
Jung, and Ying Shi. 2010. LCC approaches to 
knowledge base population at TAC 2010. In Pro-
ceedings of the 3rd TAC. 
Ludovic Jean-Louis, Romaric Besançon, Olivier Fer-
ret, and Wei Wang. 2011. Using a weakly super-
vised approach and lexical patterns for the KBP 
slot filling task. In Proceedings of the 4th TAC. 
Scott Miller, Jethran Guinness, and Alex Zamanian. 
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT-
NAACL 2004, pages 337-342, Boston, Massachu-
setts, USA, May. 
Robert Neumayer, Krisztian Balog, and Kjetil Nørvåg. 
2012. On the modeling of entities for ad-hoc entity 
search in the web of data. Advances in Information 
Retrieval: 133-145. 
Morteza Shahriari Nia, Christan Grant, Milenko Pe-
trovic, Yang Peng, and Daisy Zhe Wang. 2014. In 
proceedings of the 27th FLAIRS, pages 467-472, 
Pensacola Beach, Florida, USA, May. 
Desislava Petkova and W. Bruce Croft. 2007. Proxim-
ity-based document representation for named entity 
retrieval. In Proceedings of the 16th CIKM, pages 
731-740, Lisboa, Portugal, November. 
Lan Porteous, David Newman, Alexander Ihler, Ar-
thur Asuncion, Padhraic Smyth, and Max Welling. 
2008. Fast collapsed gibbs sampling for latent di-
richlet allocation. In Proceedings of the 14th 
SIGKDD, pages 24-27, Las Vegas, USA, August. 
Benjamin Roth and Dietrich Klakow. 2013. Combin-
ing generative and discriminative model scores for 
distant supervision. In Proceedings of the EMNLP, 
pages 24-29, Seattle, Washington, USA, October. 
Benjamin Roth, Tassilo Barth, Michael Wiegand, 
Mittul Singh and Dietrich Klakow. 2014a. Effec-
tive slot filling based on shallow distant supervi-
sion methods. Arxiv preprint arxiv: 1401-1158. 
Pavel Serdyukov and Djoerd Hiemstra. 2008. Being 
omnipresent to be almighty: the importance of the 
global web evidence for organizational expert find-
ing. In Proceedings of the 31s SIGIR, pages 17-24, 
Singapore, July. 
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 
2011. New York University 2011 system for KBP 
slot filling. In Proceedings of the 4th TAC. 
Mihai Surdeanu, Sonal Gupta, John Bauer, and David 
McClosky. 2011. Stanford’s distantly-supervised 
slot filling system. In Proceedings of the 4th TAC. 
Mihai Surdeanu. 2013. Overview of the TAC2013 
knowledge base population evaluation: English slot 
filling and temporal slot filling. In Proceedings of 
the 6th TAC. 
Mihai Surdeanu and Heng Ji. 2014. Overview of the 
English slot filling track at the TAC2014 
knowledge base population evaluation. In Proceed-
ings of the 7th TAC. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, 
and David M. Blei. 2005. Hierarchival Dirichlet 
Processes. Journal of the American Statistical As-
sociation 101(476): 1-29. 
Anne-Marie Vercoustre, James, A. Thom, and Jovan 
Pehcevski. 2008a. Entity ranking in Wikipedia. In 
Proceedings of the 23rd SAC, pages 1101-1106, 
Fortaleza, Ceara, Brazil, March. 
Vidhoon Viswanathan; Nazneen Fatema Rajani; Yi-
non Bentor; Raymond Mooney. 2015. Stacked En-
sembles of Information Extractors for Knowledge-
Base Population. In Proceedings of ACL 2015. 
Wouter Weerkamp, Krisztian Balog, and Edgar Meij. 
2009. A generative language modeling approach 
for ranking entiies. Lecture Notes in Computer Sci-
ence, 5631: 292-299. 
Xing Wei and W. Bruce Croft. 2006. LDA-based 
document models for ad-hoc retrieval. In Proceed-
ings of the 29th SIGIR, pages 569-577, Seattle, 
Washington, USA, August. 
Gae-won You, Seung-won Hwang, Zaiqing Nie, and 
Ji-Rong Wen. 2011. Social search: enhancing enti-
ty search with social network matching. In Pro-
ceedings of the 14th EDBT, pages 515-519, Uppsa-
la, Sweden, March. 
Dian Yu, Hongzhao Huang, Taylor Cassidy, Heng Ji, 
Chi Wang, Shi Zhi, Jiawei Han, Clare Voss, and 
Malik Magdon-Ismail. 2014. The wisdom of mi-
nority: unsupervised slot filling validation based on 
multi-dimensional truth-finding. In Proceedings of 
the 25th COLING, pages 1567-1578, Dublin, Ire-
land, August. 
Dian Yu, Heng Ji, Sujian Li and Chin-Yew Lin. 2015. 
Why Read if You can Scan: Scoping Strategy for 
Biographical Fact Extraction. In Proceedings of 
NAACL-HLT 2015. 
Jianhan Zhu, Dawei Song, and Stefan R?ger. 2008. 
Integrating document features for entity ranking. 
Lecture Notes in Computer Science, 4862: 336-347. 
Jianhan Zhu, Xiangji Huang, Dawei Song, and Stefan 
R?ger. 2010. Integrating multiple document fea-
tures in language models for expert finding. 
Knowledge and Information System, 23(1):29-54. 
675
