Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 368–378, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics
Why Question Answering using Sentiment Analysis and Word Classes
Jong-Hoon Oh? Kentaro Torisawa† Chikara Hashimoto ‡
Takuya Kawada§ Stijn De Saeger¶ Jun’ichi Kazama? Yiou Wang??
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? rovellia,† torisawa,‡ ch,§ tkawada,¶stijn,? kazama,??wangyiou}@nict.go.jp
Abstract
In this paper we explore the utility of sen-
timent analysis and semantic word classes
for improving why-question answering on a
large-scale web corpus. Our work is moti-
vated by the observation that a why-question
and its answer often follow the pattern that if
something undesirable happens, the reason is
also often something undesirable, and if some-
thing desirable happens, the reason is also of-
ten something desirable. To the best of our
knowledge, this is the first work that intro-
duces sentiment analysis to non-factoid ques-
tion answering. We combine this simple idea
with semantic word classes for ranking an-
swers to why-questions and show that on a set
of 850 why-questions our method gains 15.2%
improvement in precision at the top-1 answer
over a baseline state-of-the-art QA system that
achieved the best performance in a shared task
of Japanese non-factoid QA in NTCIR-6.
1 Introduction
Question Answering (QA) research for factoid ques-
tions has recently achieved great success as demon-
strated by IBM’s Watson at Jeopardy: its accuracy
has been reported to be around 85% on factoid ques-
tions (Ferrucci et al., 2010). Although recent shared
QA tasks (Voorhees, 2004; Peñas et al., 2011; Fuku-
moto et al., 2007) have stimulated the research com-
munity to move beyond factoid QA, comparatively
little attention has been paid to QA for non-factoid
questions such as why questions and how to ques-
tions, and the performance of the state-of-art non-
factoid QA systems reported in the literature (Mu-
rata et al., 2007; Surdeanu et al., 2011; Verberne et
al., 2010) remains considerably lower than that of
factoid QA (i.e., 34% in MRR at top-150 results on
why-questions (Verberne et al., 2010)).
In this paper we explore the utility of sentiment
analysis (Pang et al., 2002; Turney, 2002; Nakagawa
et al., 2010) and semantic word classes for improv-
ing why-question answering (why-QA) on a large-
scale web corpus. The inspiration behind this work
is the observation that why-questions and their an-
swers often have the following tendency:
• if something undesirable happens, the reason is
often also something undesirable, and
• if something desirable happens, its reason is of-
ten also desirable.
Consider the following question Q1, and its an-
swer candidates A1-1 and A1-2.
• Q1: Why does cancer occur?
• A1-1: Carcinogens such as nitrosamine and
benzopyrene may increase the risk of cancer by
altering DNA in cells.
• A1-2: Maintaining a healthy weight may lower
the risk of various types of cancer.
Here A1-1 describes an undesirable event related to
cancer, while A1-2 suggests a desirable action for
its prevention. Our hypothesis suggests that A1-1
is more appropriate for answering Q1. If this hy-
pothesis holds, we can obtain a significant improve-
ment in performance on why-QA tasks by exploiting
the sentiment orientation1 of expressions obtainable
1 In this paper we denote the desirable/undesirable polar-
ity of an expression by the term “sentiment orientation” instead
of “semantic orientation” to avoid confusion with our different
notion of “semantic word classes.”
368
by automatic sentiment analysis of questions and an-
swers.
A second observation motivating this work is that
there are often significant associations between the
lexico-semantic classes of words in a question and
those in its answer sentence. For instance, questions
concerning diseases like Q1 often have answers that
include references to specific semantic word classes
such as chemicals (like A1-1), viruses, body parts,
and so on. Capturing such statistical correlations be-
tween diseases and harmful substances may lead to
higher why-QA performance. For this purpose we
use classes of semantically similar words that were
automatically acquired from a large web corpus us-
ing an EM-based clustering method (Kazama and
Torisawa, 2008).
Another issue is that simply introducing the sen-
timent orientation of words or phrases in question
and answer sentences in a naive way is insufficient,
since answer candidate sentences may contain mul-
tiple sentiment expressions with different polarities
in answer candidates (i.e., about 33% of correct an-
swers had such multiple sentiment expressions with
different polarities in our test set). For example, if
A1-2 contained a second sentiment expression with
negative polarity like the example below,
“Trusting a specific food is not effective
for preventing cancer, but maintaining a
healthy weight may help lower the risk of
various types of cancer.”
both A1-1 and A1-2 would contain sentiment ex-
pressions with the same polarity as that of Q1. Thus,
it is difficult to expect that the sentiment orientation
alone will work well for recognizing A1-1 as a cor-
rect answer to Q1. To address this problem, we con-
sider the combination of sentiment polarity and the
contents of sentiment expressions associated with
the polarity in questions and their answer candidates
as well. To deal with the data sparseness problem
arising in using the content of sentiment expressions,
we developed a feature set that combines the polar-
ity and the semantic word classes effectively.
We exploit these two main ideas (concerned with
the sentiment orientation and the semantic classes
described so far) for training a supervised classi-
fier to rank answer candidates to why-questions.
Through a series of experiments on 850 Japanese
why-questions, we showed that the proposed seman-
tic features were effective in identifying correct an-
swers, and our proposed method obtained more than
15% improvement in precision of its top answer
(P@1) over our baseline, which achieved the best
performance in the non-factoid QA task in NTCIR-
6 (Murata et al., 2007). We also show that our
method can potentially perform with high precision
(64.8% in P@1) when answer candidates containing
at least one correct answer are given to our re-ranker.
2 Approach
Our proposed method is composed of answer re-
trieval and answer re-ranking. The first step, an-
swer retrieval, extracts a set of answer candidates to
a why-question from 600 million Japanese Web cor-
pus. The answer retrieval is our implementation of
the state-of-art method that has shown the best per-
formance in the shared task of Japanese non-factoid
QA in NTCIR-6 (Murata et al., 2007; Fukumoto et
al., 2007). The second step, answer re-ranking, is
the focus of this work.
2.1 Answer Retrieval
We use Solr2 to retrieve documents from a 600 mil-
lion Japanese Web page corpus3for a given why-
question. Let a set of content words in a why-
question be T = {t1, · · · , tn}. Two boolean queries
for a why-question, “t1 AND · · · AND tn” and “t1
OR · · · OR tn,” are given to Solr and top-300 doc-
uments for each query are retrieved. Note that re-
trieved documents by each query have different cov-
erage and relevance to a given why-question. To
keep balance between the coverage and relevance of
retrieved documents, we use a set of retrieved doc-
uments by these two queries for obtaining answer
candidates. Each document in the result of docu-
ment retrieval is split into a set of answer candi-
dates consisting of five subsequent sentences4. Sub-
sequent answer candidates can share up to two sen-
tences to avoid errors due to wrong document seg-
mentation.
2 http://lucene.apache.org/solr
3 To the best of our knowledge, few Japanese non-factoid
QA systems in the literature have used such a large-scale cor-
pus.
4 The length of acceptable answer candidates for why-
QA in the literature ranges from one sentence to two para-
graphs (Fukumoto et al., 2007; Murata et al., 2007; Higashinaka
and Isozaki, 2008; Verberne et al., 2007; Verberne et al., 2010).
369
Answer candidate ac for question q is ranked
according to scoring function S(q, ac) given in
Eq. (1) (Murata et al., 2007). Murata et al. (2007)’s
method uses text search to look for answer candi-
dates containing terms from the question with ad-
ditional clue terms referring to “reason” or “cause.”
Following the original method we used riyuu (rea-
son), genin (cause) and youin (cause) as clue terms.
The top-20 answer candidates for each question are
passed on to the next step, which is answer re-
ranking. S(q, ac) assigns a score to answer candi-
dates like tf -idf , where 1/dist(t1, t2) functions like
tf and 1/df(t2) is idf for given terms t1 and t2 that
are shared by q and ac.
S(q, ac) = maxt1?T
?
t2?T
?× log(ts(t1, t2)) (1)
ts(t1, t2) =
N
2× dist(t1, t2)× df(t2)
Here T is a set of terms including nouns, verbs, and
adjectives in question q that appear in answer can-
didate ac. Note that the clue terms are added to T
if they exist in ac. N is the total number of docu-
ments (600 million), dist(t1, t2) represents the dis-
tance (the number of characters) between t1 and t2
in answer candidate ac, df(t) is the document fre-
quency of term t, and ? ? {0, 1} is an indicator,
where ? = 1 if ts(t1, t2) > 1, ? = 0 otherwise.
2.2 Answer Re-ranking
Our re-ranker is a supervised classifier (SVMs)
(Vapnik, 1995) that uses three types of feature
sets: features expressing morphological and syn-
tactic analysis of questions and answer candidates,
features representing semantic word classes appear-
ing in questions and answer candidates, and features
from sentiment analysis. All answer candidates of a
question are ranked in a descending order of their
score given by SVMs. We trained and tested the
re-ranker using 10-fold cross validation on a cor-
pus composed of 850 why-questions and their top-
20 answer candidates provided by the answer re-
trieval procedure in Section 2.1. The answer candi-
dates were manually annotated by three human an-
notators (not by the authors). Our corpus construc-
tion method is described in more detail in Section 4.
3 Features for Answer Re-ranking
This section describes our feature sets for answer
re-ranking: features expressing morphological and
syntactic analysis (MSA), features representing se-
mantic word class (SWC), and features indicat-
ing sentiment analysis (SA). MSA, which has been
widely used for re-ranking answers in the literature,
is used to identify associations between questions
and answers at the morpheme, word phrase, and syn-
tactic dependency levels. The other two feature sets
are proposed in this paper. SWC is devised for iden-
tifying semantic word class associations between
questions and answers. SA is used for identify-
ing sentiment orientation associations between ques-
tions and answers as well as expressing the combi-
nation of each sentiment expression and its polarity.
Table 1 summarizes the respective feature sets, each
of which is described in detail below.
3.1 Morphological and Syntactic Analysis
MSA including n-grams of morphemes, words, and
syntactic dependencies has been widely used for re-
ranking answers in non-factoid QA (Higashinaka
and Isozaki, 2008; Surdeanu et al., 2011; Verberne
et al., 2007; Verberne et al., 2010). We use MSA as
a baseline feature set in this work.
We represent all sentences in a question and
its answer candidate in three ways: morphemes,
word phrases (bunsetsu5) and syntactic dependency
chains. These are obtained using a morphological
analyzer6 and a dependency parser7. From each
question and answer candidate we extract n-grams
of morphemes, word phrases, and syntactic depen-
dencies, where n ranges from 1 to 3. Syntactic de-
pendency n-grams are defined as a syntactic depen-
dency chain containing n word phrases. Syntactic
dependency 1-grams coincide with word phrase 1-
grams, so they are ignored.
Table 1 defines four types of MSA (MSA1 to
MSA4). MSA1 is n-gram features from all sen-
tences in a question and its answer candidates and
distinguishes an n-gram feature found in a ques-
tion from that same feature found in answer candi-
dates. MSA2 contains n-grams found in the answer
5 A bunsetsu is a syntactic constituent composed of a content
word and several function words such as post-positions and case
markers. It is the smallest unit of syntactic analysis in Japanese.
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
370
MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n ranges
from 1 to 3. n-grams in a question and those in an answer candidate are distinguished.
MSA2 MSA1’s n-grams in an answer candidate that contain a question term.
MSA3 MSA1’s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause). These n-grams in a question and
those in an answer candidate are distinguished.
MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.
SWC1 Word class n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are distin-
guished.
SWC2 SWC1’s n-grams in an answer candidate whose original MSA1’s n-grams contain any question term.
SA@W1 Word polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W2 SA@W1’s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.
SA@W3 Joint class-polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W4 SA@W3’s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.
SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair of
such sentiment phrases has polarity in agreement, 0 otherwise.
SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.
SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,
where n ranges from 1 to 3. These n-grams in a question and those in an answer candidate are distinguished.
SA@P4 SA@P3’s n-grams in an answer candidates that contain a question term.
SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of question
terms.
SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity. These n-grams in a question and those in an answer candidate
are distinguished.
SA@P7 SA@P6’s n-grams in an answer candidates, whose original MSA1’s n-grams include any question term.
SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentiment
phrases. These n-grams in a question and those in an answer candidate are distinguished.
SA@P9 SA@P8’s n-grams in an answer candidates, whose original MSA1’s n-grams include any question term.
SA@P10 A pair of SA@P6’s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the two
sentiment phrases should have the same sentiment orientation.
Table 1: Features used in training our re-ranker
candidates that themselves contain a term from the
question (e.g., “types of cancer” in example A1-2).
MSA3 is the n-gram feature that contains one of the
clue terms used for answer retrieval (riyuu (reason),
genin (cause) or youin (cause)). Here too, n-grams
obtained from the questions and answer candidates
are distinguished. Finally, MSA4 is the percentage
of the question terms found in an answer candidate.
3.2 Semantic Word Class
Semantic word classes are sets of semantically simi-
lar words. We construct these semantic word classes
by using the noun clustering algorithm proposed in
Kazama and Torisawa (2008). The algorithm fol-
lows the distributional hypothesis, which states that
semantically similar words tend to appear in simi-
lar contexts (Harris, 1954). By treating syntactic de-
pendency relations between words as “contexts,” the
clustering method defines a probabilistic model of
noun-verb dependencies with hidden classes as:
p(n, v, r) =
?
c
p(n|c)p(?v, r?|c)p(c) (2)
Here, n is a noun, v is a verb or noun on which n de-
pends via a grammatical relation r (post-positions in
Japanese), and c is a hidden class. Dependency rela-
tion frequencies were obtained from our 600-million
page web corpus, and model parameters p(n|c),
p(?v, r?|c) and p(c) were estimated using the EM
algorithm (Hofmann, 1999). We successfully clus-
tered 5.5 million nouns into 500 classes. For each
noun n, EM clustering estimates a probability dis-
tribution over hidden variables representing seman-
tic classes. From this distribution we obtained dis-
crete semantic word classes by assigning each noun
n to semantic class c = argmaxc? p(c?|n). The
resulting classes actually form clean semantic cat-
egories such as chemicals, nutrients, diseases and
conditions, in our examples of Q1 and Q2. The fol-
lowing are the top-10 words (English translation) ac-
cording to p(c|n) for these classes.
chemicals: acetylene, hydrogenation product,
phosphoric monoester, methacrylate, levoglu-
cosan, ammonium salt, halogenated organic
compound, benzonitrile, alkyne, nitrosamine
371
nutrients: glucide, carbonhydrate, mineral, salt,
sugar, water, fat, vitamin, nutrients, protein
diseases: pneumonia, neuritis, cancer, oral leuko-
plakia, pachymeningitis, acidosis, encephalitis,
abdominal injury, valvulitis, gingivitis
conditions: proficiency, decrepitude, deficiency,
impurity, abnormalities, floated, crisis, dis-
placement, condition, shortage
Semantic word class (SWC) features are used to
capture associations between semantic classes of
words in the question and those in the answer candi-
dates. For example:
• Q2: Why does rickets (Wdisease) occur in chil-
dren?
• A2: Deficiency (Wcondition) of vitamin D
(Wnutrients) can cause rickets (Wdisease).
Wcondition, Wdisease and Wnutrients represent se-
mantic word classes of conditions, diseases and nu-
trients, respectively. If this question-answer pair is
given to the classifier as a positive training sample,
we expect it to learn that if a disease name appears
in a question then, everything else being equal, an-
swers including nutrient names are more likely to be
correct. Note that in principle the same association
could be learned between word pairs, i.e., rickets and
vitamin D. However, we found that word level asso-
ciations are often too specific, and because of data
sparseness this knowledge cannot easily be general-
ized to unseen questions. This is our main motiva-
tion for introducing broad coverage semantic word
classes into the feature set.
We call the feature set with the word classes SWC
and use two types of SWC, as shown in Table 1. To
obtain the first type (SWC1), we convert all nouns
in the MSA1 n-grams into their respective word
classes, and keep all n-grams that contain at least
one word class. We call these features word class
n-grams. Again, word class n-grams obtained from
questions are distinguished from the ones in answer
candidates. For example, we extract “Wdisease oc-
cur” as a word class 2-gram from Q2.
The second type of SWC, SWC2, represents word
class n-grams in an answer candidate, in which
question terms are replaced by their respective se-
mantic word classes. For example, Wdisease in word
class 2-gram “cause Wdisease” from A2 is the se-
mantic word class of rickets, one of the question
terms. These features capture the correspondence
between semantic word classes in the question and
answer candidates.
3.3 Sentiment Analysis
Sentiment analysis (SA) features are classified into
word-polarity and phrase-polarity features. We use
opinion extraction tool8 and sentiment orientation
lexicon in the tool for these features.
3.3.1 Opinion Extraction Tool
Opinion extraction tool is a software, the im-
plementation of Nakagawa et al. (2010). It ex-
tracts linguistic expressions representing opinions
(henceforth, we call them sentiment phrases) from
a Japanese sentence and then identifies the polarity
of these sentiment phrases using machine learning
techniques. For example, rickets occur in Q2 and
Deficiency of vitamin D can cause rickets in A2 can
be identified as sentiment phrases with a negative
polarity. The tool identifies sentiment phrases and
their polarity by using polarities of words and de-
pendency subtrees as evidence, where these polari-
ties are given in a word polarity dictionary.
In this paper, we use a trained model and a word
polarity dictionary (containing about 35,000 entries)
distributed via the ALAGIN forum9 for our sen-
timent analysis. Table 2 shows the performance
of opinion extraction tool, precision (P), recall (R)
and F-value (F), in this setting (reported in the
Japanese homepage of this tool). In the evaluation of
sentiment-phrase extraction, an extracted sentiment
phrase is determined as correct if its head word is
the same as one in the gold standard. Polarity clas-
sification is evaluated under the condition that all of
the sentiment phrases are correctly extracted.
P R F
Sentiment-phrase extraction 0.602 0.408 0.486
Polarity classification (pos.) 0.873 0.893 0.883
Polarity classification (neg.) 0.866 0.842 0.854
Table 2: The performance of opinion extraction tool
3.3.2 Word Polarity (SA@W)
Polarities of words are identified by simply look-
ing up the word polarity dictionary of opinion ex-
8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html
9 http://www.alagin.jp/index-e.html. Only the members of
the ALAGIN forum can access these resources.
372
traction tool. Word polarity features are used
for identifying associations between the polarity of
words in a question and that in a correct answer. For
example:
• Q2: Why does rickets (W?) occur in children?
• A2: Deficiency (W?) of vitamin D can cause
rickets (W?).
Here, W? represents negative word polarities. We
expect our classifier to learn from this question and
answer pair that if a word with negative polarity ap-
pears in a question then its correct answer is likely
to contain a negative polarity word as well.
SA@W1 and SA@W2 in Table 1 are sentiment
analysis features from word polarity n-grams, which
contain at least one word that has word polarities.
We obtain these n-grams by converting all nouns in
MSA1 n-grams into their word polarities through
dictionary lookup. For example, from Q2 in the
above example we extract “W? occur” as a word
polarity 2-gram. SA@W1 is concerned with all
word polarity n-grams in questions and answer can-
didates. For SA@W2, we restrict word polarity
n-grams from SA@W1 to those whose original n-
gram include a question term.
Furthermore, word polarities are coupled with se-
mantic word classes so that our classifier can iden-
tify meaningful combinations of both. For example,
deficiency in A2 can be represented asW?condition by
its respective semantic word class and word polar-
ity, which allows for the representation of undesir-
able conditions. This in turn lets our system learn
meaningful correlations between words expressing
these kind of negative conditions and their connec-
tion to questions asking about diseases. SA@W3
and SA@W4 are features from this combination.
They are defined in the same way as SA@W1 and
SA@W2 except that word polarities are replaced
with the combination of semantic word classes and
word polarities. We call n-grams in SA@W3 and
SA@W4 joint (word) class-polarity n-grams.
3.3.3 Phrase Polarity (SA@P)
Opinion extraction tool is applied to question and
its answer candidate to identify sentiment phrases
and their phrase-polarities. In preliminary tests we
found that sentiment phrases do not help to iden-
tify correct answers if answer sentences including
the sentiment phrases do not have any term from the
question. So we restrict the target sentiment phrases
to those acquired from sentences containing at least
one question term. From these sentiment phrases we
extract three categories of features.
First, SA@P1 and SA@P2 are features concerned
with phrase-polarity agreement between sentiment
phrases in a question and its answer candidate. We
consider all possible pairs of sentiment phrases from
the question and answer. If any such pair agrees
in phrase-polarity, an indicator for the agreement
and its polarity in the agreement become features
SA@P1 and SA@P2, respectively.
Secondly, following the original hypothesis un-
derlying this paper, we assume that sentiment
phrases often represent the core part of the cor-
rect answer (e.g., A2 above) and it is important
to express the content of the sentiment phrases in
features. SA@P3 and SA@P4 were devised for
this purpose. SA@P3 represents this sentiment
phrase contents as n-grams of morphemes, words,
and syntactic dependencies of sentiment phrases,
together with their phrase-polarity. Furthermore,
SA@P4 is the subset of SA@P3 n-grams restricted
to those that include terms found in the question,
and SA@P5 indicates the percentage of sentiment
n-grams from the question that are found in a given
answer candidate.
Finally, features SA@P6 through SA@P9 use se-
mantic word classes to generalize the content fea-
tures mentioned above. These features consist of
word class n-grams and joint class-polarity n-grams
taken from sentiment phrases, together with their
phrase polarity. Similar to the definition of SA@P4,
for SA@P7 and SA@P9 we restrict ourselves to n-
grams containing a question term. SA@P10 repre-
sents the semantic content of two sentiment phrases
with the same sentiment orientation (one from a
question and the other from an answer candidate)
using word class n-grams, together with the phrase-
polarity in agreement.
4 Test Set
We prepared three sets of why-questions (QS1, QS2
and QS3) and used these questions to build two test
sets for our experiments.
Why-questions in QS1 are taken from the
Japanese version of Yahoo! Answers (called Ya-
hoo! Chiebukuro)10. We automatically extracted
10 We used “Yahoo! Chiebukuro Data (2nd edition)” which is
373
questions consisting of a single sentence and con-
taining the interrogative naze (why), and our anno-
tators verified that these questions are meaningful
without further context. For example, they discarded
questions like “Why doesn’t the WBC (world box-
ing council) make an objection to the WBC (World
baseball classic)?” (the object of the objection is
unclear) and “Why do minors trade at the auction
even though it is disallowed by the rules” (informa-
tion about which auction is not provided).
Because questions in Yahoo! Answers are aimed
at human readers, users often “set the stage” by giv-
ing lots of background information about their ques-
tion. This often leads to large stylistic differences
between the questions in Yahoo! Answers and those
typically posed to a QA system. We therefore cre-
ated a second set of why-questions, QS2, whose
style should be more appropriate for a QA system
(examples showing these differences are given in the
supplementary materials of this paper). Six human
annotators (not the authors) were asked to create
why-questions in their own words, keeping in mind
that the questions they create are for a QA system. In
addition, the annotators were asked to verify on the
Web that the questions they created ask about some
real event or phenomena. For example, a question
like “Why does Mars appear blue?” is disallowed in
QS2 because “Mars appears blue” is false. Note that
the correct answer to these questions does not have
to be either in our target corpus or in real-world Web
texts. These two sets of why-questions, QS1 and
QS2, are used to build a test set for evaluating our
proposed method.
Finally, QS3 contains why-questions that have at
least one answer in our target corpus (600 million
Japanese Web page corpus). For creating such why-
questions, four human annotators (not the authors)
were given a text passage composed of three contin-
uous sentences and asked to locate the reasons for
some event as described in this passage. Then they
created a why-question for which the description is a
correct answer. Because randomly selected passages
from our target corpus have little chance of generat-
ing good why-questions we extracted passages from
our target corpus that include at least one of the clue
terms used in our answer retrieval step (i.e. riyuu
(reason), genin (cause), or youin (cause)). This set-
provided by Yahoo Japan Corporation and contains 16 million
questions asked from April, 2004 to April 2009.
ting may not necessarily reflect a “real world” dis-
tribution of why-questions, in which ideally a wide
range of people ask questions that may or may not
have an answer in our corpus. However, QS3 al-
lows us to evaluate our method under the idealized
conditions where we have a perfect answer retrieval
module whose answer candidates always contain at
least one correct answer (the source passage used
for creating the why-question). This setting allows
us to estimate the ideal-case performance of our
method. Under these circumstances we found that
our method achieves almost 65% precision in P@1,
which suggests that it can potentially perform with
high precision if the answer candidates given by the
answer retrieval module contain at least one correct
answer. This is the main purpose of QS3. Addition-
ally, we use QS3 for building training data, to check
whether questions that do not reflect the real-world
distribution of why-questions are useful for improv-
ing the system’s performance on “real-world” ques-
tions (see Section 5.1).
In addition, we checked QS1, QS2 and QS3 for
questions having the same topic, to avoid the pos-
sibility that the distribution of questions is biased
towards certain topics. We manually extracted the
questions’ topic words and randomly selected a sin-
gle representative question from all questions with
the same topic. For example, “Why does Twitter
only allow 140 characters?” and “Why is Twitter
so popular?” both have as topic Twitter. In the end
we obtained 250 questions in QS1, 250 questions in
QS2 and 350 questions in QS3.
For evaluation we prepared two test sets, Set1 and
Set2. Set1 contains question-answer pairs whose
questions are taken from QS1 and QS2. In our ex-
periment, we evaluate systems with 10-fold cross
validation on Set1. Set2 has question-answer pairs
whose questions are from QS3. Set2 is mainly used
for estimating estimate the ideal-case performance
of our method with a perfect answer retrieval mod-
ule. Furthermore Set2 is used as additional training
data in evaluating systems with 10-fold cross vali-
dation on Set1. We used our answer retrieval sys-
tem to obtain the top-20 answer candidates for each
question, and all question-answer (candidate) pairs
were checked by three annotators, where their inter-
rater agreement (Fleiss’ kappa) was 0.634, indicat-
ing substantial agreement. Finally, correct answers
to each question were determined by majority vote.
374
Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?)
A1: .. ????????????????????????????????????????????????????????
??????????????????????????????????????? ... ???????????????????
???????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb more
thermal radiation. As a result, Earth’s average surface temperature increases. This is global warming. ... There are warnings that the increase of sea
water and melting of polar ice due to the global warming may cause sea-surface height to rise by 9–88 cm on average.
Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)
A2:... ????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????..
(... Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to be
dispensed from the organism. If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen delivered
throughout the body decreases, causing oxygen deficiency. ... )
Table 3: Correct question-answer pairs in our test set
Table 3 shows a sample of correct question-answer
pairs in our test set. Please see the supplementary
materials of this paper for more examples.
Note that word and phrase polarities are not con-
sidered by the annotators in building our test sets
and these polarities are automatically identified us-
ing a word polarity dictionary and opinion extraction
tool. We confirmed that about 35% of questions and
40% of answer candidates had at least one sentiment
phrase by opinion extraction tool, and about 45% of
questions and 85% of answer candidates contained
at least one word having polarity by a word polarity
dictionary.
5 Experiments
We use TinySVM11 with a linear kernel for training
our re-ranker. Evaluation was done by P@1 (Pre-
cision of the top answer) and MAP (Mean Average
Precision). P@1 measures how many questions have
a correct top answer candidate. MAP, widely used in
evaluation of IR systems, measures the overall qual-
ity of the top-n answer candidates (n=20 in this ex-
periment) using the formula:
MAP =
1
|Q|
?
q?Q
?n
k=1(Prec(k)× rel(k))
|Aq|
(3)
Here Q is a set of why-questions, Aq is a set of cor-
rect answers to why-question q ? Q, Prec(k) is the
precision at cut-off k in the top-n answer candidates,
rel(k) is an indicator, 1 if the item at rank k is a cor-
rect answer in Aq, 0 otherwise.
We evaluated all systems using 10-fold cross val-
idation in two ways. In the first setting we per-
formed 10-fold cross validation on Set1. Set1 con-
11 http://chasen.org/?taku/software/TinySVM/
sists of 10,000 question-answer pairs (500 questions
with their 20 answer candidates), and was parti-
tioned into 10 subsamples such that the questions
in one subsample do not overlap with those of the
other subsamples. 9 subsamples (9,000 question-
answer pairs) were used as training data and the
remaining subsample (1,000 question-answer pairs)
was retained as test data. This experiment is called
CV(Set1). It shows the effect of answer re-ranking
when evaluating our proposed method with train-
ing data built with real world why-questions alone.
In the second setting, we used the same 10 sub-
samples of Set1 as in CV(Set1) and exploited Set2
(composed of 7,000 question-answer pairs) as ad-
ditional training data for 10-fold cross validation.
As a result, in each fold 16,000 question-answer
pairs (9,000 from Set1 and 7,000 from Set2) were
used as training data for re-rankers, and all systems
were evaluated on the remaining 1,000 question-
answer pair subsample from Set1. We call this set-
ting CV(Set1+Set2). It verifies whether training
data that does not necessarily reflect a real-world
distribution of why-questions can improve why-QA
performance on real-world questions.
5.1 Results
Table 4 shows the evaluation results of six different
systems. For each system, we represent the perfor-
mance in P@1 and MAP. B-QA is a system of our
answer retrieval and the other five re-rank top-20 an-
swer candidates using their own re-ranker.
B-QA: our answer retrieval system, our implemen-
tation of Murata et al. (2007).
B-Ranker: a system that has a re-ranker trained
with morphological and syntactic analysis
(MSA) features alone.
375
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
B-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)
B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)
B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)
B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)
Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)
UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)
Table 4: Comparison of systems
B-Ranker+CR: a system has a re-ranker trained
with our MSA features and the causal relation
(CR) features used in Higashinaka and Isozaki
(2008). The CR features include binary fea-
tures indicating whether an answer candidate
contains a causal relation pattern, which causal
relation pattern the answer candidate has, and
whether the question-answer pair contains a
causal relation instance — cause in the answer,
effect in the question). We acquired causal
relation instances from our target corpus us-
ing the method from (De Saeger et al., 2009),
and exploited the top-100,000 causal relation
instances and the patterns that extracted them
for CR features. Note that these CR features
are introduced only for comparing our semantic
features with ones in Higashinaka and Isozaki
(2008) and they are not a part of our method.
B-Ranker+WN: its re-ranker is trained with our
MSA features and the WordNet features in Ver-
berne et al. (2010). The WordNet features in-
clude the percentage of the question terms and
their synonyms in WordNet synsets found in
an answer candidate and the semantic related-
ness score between a question and its answer
candidate, the average of the concept similar-
ity between each question term and all of the
answer terms by WordNet::Similarity (Peder-
sen et al., 2004). We used the Japanese Word-
Net 1.1 (Bond et al., 2009) for these WordNet
features. Note that the Japanese WordNet 1.1
has 93,834 Japanese words linked to 57,238
WordNet synsets, while the English WordNet
3.0 covers 155,287 words linked to 117,659
synsets. Due to this lower coverage, the Word-
Net features in Japanese may have a less power
for finding a correct answer than those in En-
glish used in Verberne et al. (2010).
Proposed: our proposed method. All of the MSA,
SWC and SA features are used for training our
re-ranker.
UpperBound: a system that ranks all n correct an-
swers as the top n results of the 20 answer can-
didates if there are any. This indicates the per-
formance upperbound in this experiment. The
relative performance of each system compared
to UpperBound is shown in parentheses.
The proposed method achieved the best perfor-
mance both in CV(Set1) and CV(Set1+Set2). Our
method shows a significant improvement (11.4–
15.2% in P@1 and 10.7–12.1% in MAP) over our
answer retrieval method, B-QA. Its improvement
over B-Ranker, B-Ranker+CR and B-Ranker+WN
(7.6–10% in P@1 and 5.7–6.6% in MAP) shows
the effectiveness of our proposed feature set over
the features used in previous works. Both B-
Ranker+CR and B-Ranker+WN did not show signif-
icant performance improvement over B-Ranker. At
least in our setting, the causal relation and WordNet
features did not prove effective. The performance
gap between B-Ranker and B-QA (3.4–5.2% in P@1
and 4.9–5.3% in MAP) suggests the effectiveness
of re-ranking. All systems consistently show better
performance in CV(Set1+Set2) than CV(Set1). This
suggests that training data built with why-questions
that does not reflect real-world distribution of why-
questions is useful in training re-rankers.
We investigate the contribution of each type of
features to the performance by removing one fea-
ture set from the all feature sets in training our re-
ranker. In this experiment, we split SA into SA@W
(features expressing words and their polarity) and
SA@P (features expressing phrases and their po-
larity) to investigate their contribution either. The
results are summarized in Table 5.
In Table 5, MSA+SWC+SA represents our pro-
posed method using all feature sets. The perfor-
mance gap between MSA+SWC+SA and the others
confirms that all the features contributed to a higher
376
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
SWC+SA 0.302 0.324 0.314 0.332
MSA+SWC 0.308 0.349 0.318 0.358
MSA+SA 0.300 0.352 0.314 0.364
MSA+SWC+SA@W 0.312 0.358 0.325 0.365
MSA+SWC+SA@P 0.323 0.369 0.358 0.384
MSA+SWC+SA 0.336 0.377 0.374 0.391
UpperBound 0.604 0.604 0.604 0.604
Table 5: Evaluation with different combination of feature
sets used in training our re-ranker
performance. The significant performance improve-
ment by SA (features from sentiment analysis) and
SWC (features from semantic word classes) (The
gap between MSA+SWC+SA and MSA+SWC was
2.8–6% and that between MSA+SWC+SA and
MSA+SA was 3.6%–6% in P@1) supports the hy-
pothesis for sentiment analysis and semantic word
classes in this paper.
Though the performance gap between
MSA+SWC+SA and MSA+SWC+SA@P
(1.3%–1.6% in P@1) shows that SA@W is
useful in training our re-ranker, we found that
MSA+SWC+SA@W made only 0.4–0.7% im-
provement over MSA+SWC. We believe that this
is mainly because SA@W and SWC are based on
semantic and sentiment information at the word
level, and these often capture a similar type of
information. For instance, disease names that are
grouped together into one class in SWC are typi-
cally classified as negative in SA@W. Therefore the
similarity in the information provided by SA@W
and SWC causes a classifier trained with both of
these features to obtain only a minor improvement
over a classifier using only one of the features.
To estimate the ideal-case performance of our
proposed method, we made another experiment by
using Set1 as training data for our re-ranker and
Set2 as test data for evaluating our proposed method.
Here, we assume a perfect answer retrieval module
that adds the source passage that was used for gener-
ating the original why-question in Set2 as a correct
answer to the set of existing answer candidates, giv-
ing 21 answer candidates. The performance of our
method in this setting was 64.8% in P@1 and 66.6%
in MAP. This evaluation result suggests that our re-
ranker can potentially perform with high precision
when at least one correct answer in answer candi-
dates is given by the answer retrieval module.
6 Related Work
In the QA literature, Higashinaka and Isozaki
(2008), Verberne et al. (2010), and Surdeanu et al.
(2011) are closest to our work. The first two deal
with why-questions, the last with how-questions.
Similar to our method, they use machine learn-
ing techniques to re-rank answer candidates to non-
factoid questions based on various combinations of
syntactic, semantic and other statistical features such
as the density and frequency of question terms in the
answer candidates and patterns for causal relations
in the answer candidates. Especially for why-QA,
Higashinaka and Isozaki (2008) used causal relation
features and Verberne et al. (2010) exploited Word-
Net features as a kind of semantic features for train-
ing their re-ranker, where we used these features, re-
spectively, for B-Ranker+CR and B-Ranker+WN in
our experiment.
Our work differs from the above approaches in
that we propose semantic word classes and senti-
ment analysis as a new type of semantic features,
and show their usefulness in why-QA. Sentiment
analysis has been used before on the slightly un-
usual task of opinion question answering, where the
system is asked to answer subjective opinion ques-
tions (Stoyanov et al., 2005; Dang, 2008; Li et al.,
2009). To the best of our knowledge though, no pre-
vious work has systematically explored the use of
sentiment analysis in a general QA setting beyond
opinion questions.
7 Conclusion
In this paper, we have explored the utility of senti-
ment analysis and semantic word classes for ranking
answer candidates to why-questions. We proposed a
set of semantic features that exploit sentiment anal-
ysis and semantic word classes obtained from large-
scale noun clustering, and used them to train an an-
swer candidate re-ranker. Through a series of exper-
iments on 850 why-questions, we showed that the
proposed semantic features were effective in identi-
fying correct answers, and our proposed method ob-
tained more than 15% improvement in precision of
its top answer (P@1) over our baseline, a state-of-
the-art IR based QA system. We plan to use new se-
mantic knowledge such as semantic orientation, ex-
citatory or inhibitory, proposed in Hashimoto et al.
(2012) for improving why-QA.
377
References
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In Pro-
ceedings of the 7th Workshop on Asian Language Re-
sources, pages 1–8.
Hoa Tran Dang. 2008. Overview of the TAC 2008 opin-
ion question answering and summarization tasks. In
Proc. TAC 2008.
Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large scale
relation acquisition using class dependent patterns. In
Proc. of ICDM 2009, pages 764–769.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building Watson: An overview of the DeepQA
project. AI Magazine, 31(3):59–79.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th ques-
tion answering challenge (QAC-4) at NTCIR work-
shop 6. In Proc. of NTCIR-6.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, and Jun’ichi Kazama. 2012. Excita-
tory or inhibitory: A new semantic orientation extracts
contradiction and causality from the web. In Proceed-
ings of EMNLP-CoNLL 2012.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-questions.
In Proc. of IJCNLP, pages 418–425.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ’99, pages 50–57.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL-
08: HLT, pages 407–415.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, pages
737–745.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A system
for answering non-factoid Japanese questions by using
passage retrieval weighted based on type of answer. In
Proc. of NTCIR-6.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786–794, Los An-
geles, California, June. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79–86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41.
Anselmo Peñas, Eduard H. Hovy, Pamela Forner, Álvaro
Rodrigo, Richard F. E. Sutcliffe, Corina Forascu, and
Caroline Sporleder. 2011. Overview of QA4MRE at
CLEF 2011: Question answering for machine reading
evaluation. In CLEF.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, HLT ’05, pages 923–
930.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351–383.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 417–424.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In SIGIR,
pages 735–736.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2010. What is not in the bag of words
for why-QA? Computational Linguistics, 36:229–
245.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
378
