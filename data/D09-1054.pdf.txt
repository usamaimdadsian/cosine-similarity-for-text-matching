Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 514–523,
Singapore, 6-7 August 2009.
c
©2009 ACL and AFNLP
A Structural Support Vector Method for Extracting Contexts and
Answers of Questions from Online Forums
Wen-Yun Yang
†?
Yunbo Cao
†‡
Chin-Yew Lin
‡
†
Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, China
‡
Microsoft Research Asia, Beijing, China
wenyun.yang@gmail.com {yunbo.cao; cyl}@microsoft.com
Abstract
This paper addresses the issue of extract-
ing contexts and answers of questions
from post discussion of online forums.
We propose a novel and unified model by
customizing the structural Support Vector
Machine method. Our customization has
several attractive properties: (1) it gives a
comprehensive graphical representation of
thread discussion. (2) It designs special
inference algorithms instead of general-
purpose ones. (3) It can be readily ex-
tended to different task preferences by
varying loss functions. Experimental re-
sults on a real data set show that our meth-
ods are both promising and flexible.
1 Introduction
Recently, extracting questions, contexts and an-
swers from post discussions of online forums in-
curs increasing academic attention (Cong et al.,
2008; Ding et al., 2008). The extracted knowl-
edge can be used either to enrich the knowledge
base of community question answering (QA) ser-
vices such as Yahoo! Answers or to augment the
knowledge base of chatbot (Huang et al., 2007).
Figure 1 gives an example of a forum thread
with questions, contexts and answers annotated.
This thread contains three posts and ten sentences,
among which three questions are discussed. The
three questions are proposed in three sentences,
S3, S5 and S6. The context sentences S1 and
S2 provide contextual information for question
sentence S3. Similarly, the context sentence S4
provides contextual information for question sen-
tence S5 and S6. There are three question-context-
answer triples in this example, (S3) ? (S1,S2) ?
(S8,S9), (S5)? (S4)? (S10) and (S6)? (S4)?
?
This work was done while the first author visited Mi-
crosoft Research Asia.
Post1: <context id=1> S1: Hi I am looking for
a pet friendly hotel in Hong Kong because all of
my family is going there for vacation. S2: my fam-
ily has 2 sons and a dog. </context> <question
id=1> S3: Is there any recommended hotel near
Sheung Wan or Tsing Sha Tsui? </question>
<context id=2, 3> S4: We also plan to go shopping
in Causeway Bay. </context> <question id=2>
S5: What’s the traffic situation around those com-
mercial areas? </question> <question id=3> S6:
Is it necessary to take a taxi? </question> S7: Any
information would be appreciated.
Post2: <answer id=1> S8: The Comfort Lodge
near Kowloon Park allows pet as I know, and usu-
ally fits well within normal budgets. S9: It is also
conveniently located, nearby the Kowloon railway
station and subway. </answer>
Post3: <answer id=2, 3> S10: It’s very crowd in
those areas, so I recommend MTR in Causeway Bay
because it is cheap to take you around. </answer>
Figure 1: An example thread with three posts and
ten sentences
(S10). As shown in the example, a forum question
usually requires contextual information to com-
plement its expression. For example, the ques-
tion sentence S3 would be of incomplete meaning
without the contexts S1 and S2, since the impor-
tant keyword pet friendly would be lost.
The problem of extracting questions, contexts,
and answers can be solved in two steps: (1) iden-
tify questions and then (2) extract contexts and an-
swers for them. Since identifying questions from
forum discussions is already well solved in (Cong
et al., 2008), in this paper, we are focused on step
(2) while assuming questions already identified.
Previously, Ding et al. (2008) employ general-
purpose graphical models without any customiza-
tions to the specific extraction problem (step 2).
In this paper, we improve the existing models in
514
three aspects: graphical representation, inference
algorithm and loss function.
Graphical representation. We propose a more
comprehensive and unified graphical representa-
tion to model the thread for relational learning.
Our graphical representation has two advantages
over previous work (Ding et al., 2008): unifying
sentence relations and incorporating question in-
teractions.
Three types of relation should be considered for
context and answer extraction: (a) relations be-
tween successive sentences (e.g., context sentence
S2 occurs immediately before question sentence
S3); (b) relations between context sentences and
answer sentences (e.g., context S4 presents the
phrase Causeway Bay linking to answer which is
absent from question S6); and (c) relations be-
tween multiple labels for one sentence (e.g., one
question sentence is unlikely to be the answer to
another question although one sentence can serve
as contexts for more than one questions). Our pro-
posed graphical representation improves the mod-
eling of the three types of sentence relation (Sec-
tion 2.2).
Certain interactions exist among questions. For
example, question sentences S5 and S6 interact by
sharing context sentence S4. Our proposed graphi-
cal representation can naturally model the interac-
tions. Previous work (Ding et al., 2008) performs
the extraction of contexts and answers in multiple
passes of the thread (with each pass corresponding
to one question), which cannot address the interac-
tions well. In comparison, our model performs the
extraction in one pass of the thread.
Inference algorithm. Inference is usually a
time-consuming process for structured prediction.
We design special inference algorithms, instead of
general-purpose inference algorithms used in pre-
vious works (Cong et al., 2008; Ding et al., 2008),
by taking advantage of special properties of our
task. Specifically, we utilize two special properties
of thread structure to reduce the inference (time)
cost. First, context sentences and question sen-
tences usually occur in the same post while answer
sentences can only occur in the following posts.
With this properties, we can greatly reduce context
(or answer) candidate sets of a question, which
results in a significant decrease in inference cost
(Section 3). Second, context candidate set is usu-
ally much smaller than the number of sentences
in a thread. This property enables our proposal to
have an exact and efficient inference (Section 4.1).
Moreover, an approximate inference algorithm is
also given (Section 4.2).
Loss function. In practice, different applica-
tion settings usually imply different requirements
for system performance. For example, we expect
a higher recall for the purpose of archiving ques-
tions but a higher precision for the purpose of re-
trieving questions. A flexible framework should
be able to cope with various requirements. We
employ structural Support Vector Machine (SVM)
model that could naturally incorporate different
loss functions (Section 5).
We use a real data set to evaluate our approach
to extracting contexts and answers of questions.
The experimental results show both the effective-
ness and the flexibility of our approach.
In the next section, we formalize the problem
of context and answer extraction and introduce the
structural model. In Sections 3, 4 and 5 we give
the details of customizing structural model for our
task. In Section 6, we evaluate our methods. In
Section 7, we discuss the related work. Finally,
we conclude this paper in Section 8.
2 Problem Statement
We first introduce our notations in Section 2.1 and
then in Section 2.2 introduce how we model the
problem of extracting contexts and answers for
questions with a novel form of graphical represen-
tation. In Section 2.3 we introduce the structured
model based on the new representation.
2.1 Notations
Assuming that a given thread contains p posts
{p
1
, . . . , p
p
}, which are authored by a set of
users {u
1
, . . . , u
p
}. The p posts can be further
segmented into n sentences x = {x
1
, . . . , x
n
}.
Among the n sentences, m question sentences q =
{x
q
1
, . . . , x
q
m
} have been identified. Our task is
to identify the context sentences and the answer
sentences for those m question sentences. More
formally, we use four types of label {C,A,Q, P}
to stand for context, answer, question and plain la-
bels. Then, our task is to predict an m × n label
matrix y = (y
ij
)
1?i?m,1?j?n
, except m elements
{y
1,q
1
, . . . , y
m,q
m
} which correspond to (known)
question labels. The element y
ij
in label matrix y
represents the role that the jth sentence plays for
the ith question. We denote the ith row and jth
column of the label matrix y by y
i.
and y
.j
.
515
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(a) Skip-chain model
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(b) Complete skip-chain model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(c) 2D model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(d) Label group model
Figure 2: Structured models
2.2 Graphical Representation
Recently, Ding et al. (2008) use skip-chain and
2D Conditional Random Fields (CRFs) (Lafferty
et al., 2001) to perform the relational learning for
context and answer extraction. The skip-chain
CRFs (Sutton and McCallum, 2004; Galley, 2006)
model the long distance dependency between con-
text and answer sentences and the 2D CRFs (Zhu
et al., 2005) model the dependency between con-
tiguous questions. The graphical representation
of those two models are shown in Figures 2(a)
and 2(c), respectively. Those two CRFs are both
extensions of the linear chain CRFs for the sake
of powerful relational learning. However, di-
rectly using the skip-chain and 2D CRFs with-
out any customization has obvious disadvantages:
(a) the skip-chain model does not model the de-
pendency between answer sentence and multiple
context sentences; and (b) the 2D model does not
model the dependency between non-contiguous
questions.
To better model the problem of extracting con-
texts and answers of questions, we propose two
more comprehensive models, complete skip-chain
model and label group model to improve the ca-
pability of the two previous models. These two
models are shown in Figures 2(b) and 2(d).
In Figures 2(a) and 2(b), each label node is an-
notated with its allowed labels and the labels C, A,
Q and P stand for context, answer, question and
plain sentence labels, respectively. Note that the
complete skip-chain model completely links each
two context and answer candidates and the label
group model combines the labels of one sentence
into one label group.
2.3 Structured Model
Following the standard machine learning setup,
we denote the input and output spaces by X and
Y , then formulate our task as learning a hypoth-
esis function h : X ? Y to predict a y when
given x. In this setup, x represents a thread of n
sentences and m identified questions. y represents
the m× n label matrix to be predicted.
Given a set of training examples, S =
{(x
(i)
,y
(i)
) ? X × Y : i = 1, . . . , N}, we
restrict ourselves to the supervised learning sce-
nario. We focus on hypothesis functions that
take the form h(x;w) = argmax
y?Y
F(x,y;w)
with discriminant function F : X × Y ? R
where F(x,y;w) = w
T
?(x,y). As will be
introduced in Section 4, we employ structural
SVMs (Joachims et al., 2009) to find the optimal
parameters w. The structural SVMs have sev-
eral competitive properties as CRFs. First, it fol-
lows from the maximum margin strategy, which
has been shown with competitive or even better
516
performance (Tsochantaridis et al., 2005; Nguyen
and Guo, 2007). Second, it allows flexible choices
of loss functions to users. Moreover, in general,
it has theoretically proved convergence in polyno-
mial time (Joachims et al., 2009).
To use structural SVMs in relational learning,
one needs to customize three steps according to
specific tasks. The three steps are (a) definition of
joint feature mapping for encoding relations, (b)
algorithm of finding the most violated constraint
(inference) for efficient trainings and (c) definition
of loss function for flexible uses.
In the following Sections 3, 4 and 5, we describe
the customizations of the three steps for our con-
text and answer extraction task, respectively.
3 Encoding Relations
We use a joint feature mapping to model the rela-
tions between sentences in a thread. For context
and answer extraction, the joint feature mapping
can be defined as follows,
?(x,y) =
?
?
?
n
(x,y)
?
h
(x,y)
?
v
(x,y)
?
?
,
where the sub-mappings ?
n
(x,y), ?
h
(x,y), and
?
v
(x,y) encode three types of feature mappings,
node features, edge features and label group fea-
tures. The node features provide the basic infor-
mation for the output labels. The edge features
consist of the sequential edge features and skip-
chain edge features for successive label dependen-
cies. The label group features encode the relations
within each label group.
Before giving the detail definitions of the sub-
mappings, we first introduce the context and an-
swer candidate sets, which will be used for the
definitions and inferences. Each row of the label
matrix y corresponds to one question. Assuming
that the ith row y
i.
corresponds to the question
with sentence index q
i
, we thus have two candi-
date sets of contexts and answers for this question
denoted by C and A, respectively. We denote the
post indices and the author indices for the n sen-
tences as p = (p
1
, . . . , p
n
) and u = (u
1
, . . . , u
n
).
Then, we can formally define the two candidate
sets for the y
i.
as
C =
{
c
j
?
?
?
?
?
p
c
j
= p
q
i
? ?? ?
In Question Post
, c
j
6= q
i
? ?? ?
Not Question Sentence
}
,
A =
{
a
j
?
?
?
?
?
p
a
j
> p
q
i
? ?? ?
After Question Post
, u
a
j
6= u
q
i
? ?? ?
Not by the Same User
}
.
In the following, we describe formally about the
definitions of the three feature sub-mappings.
The node feature mapping ?
n
(x,y) encodes
the relations between sentence and label pairs, we
define it as follows,
?
n
(x,y) =
m
?
i=1
n
?
j=1
?
n
(x
j
, y
ij
),
where ?
n
(x
j
, y
ij
) is a feature mapping for a given
sentence and a label. It can be formally defined as
follows,
?
n
(x
j
, y
ij
) = ?(y
ij
)? ?
q
i
(x
j
), (1)
where ? denotes a tensor product, ?
q
i
(x
j
) and
?(y
ij
) denote two vectors. ?
q
i
(x
j
) contains ba-
sic information for output label. ?(y
ij
) is a 0/1
vector defined as
?(y
ij
) = [?
C
(y
ij
), ?
A
(y
ij
), ?
P
(y
ij
)]
T
,
where ?
C
(y
ij
) equal to one if y
ij
= C, otherwise
zero. The ?
A
(y
ij
) and ?
P
(y
ij
) are similarly de-
fined. Thus, for example, writing out ?
n
(x
j
, y
ij
)
for y
ij
= C one gets,
?
n
(x
j
, y
ij
) =
?
?
?
q
i
(x
j
)
0
0
?
?
? context
? answer
? plain
.
Note that the node feature mapping does not in-
corporate the relations between sentences.
The edge feature mapping ?
h
(x,y) is used
to incorporate two types of relation, the relation
between successive sentences and the relation be-
tween context and answer sentences. It can be de-
fined as follows,
?
h
(x,y) =
[
?
hn
(x,y)
?
hc
(x,y)
]
,
where ?
hn
(x,y) and ?
hc
(x,y) denote the two
types of feature mappings corresponding to se-
quential edges and skip-chain edges, respectively.
Their formal definitions are given as follows,
?
hn
(x,y) =
m
?
i=1
n?1
?
j=1
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
517
Descriptions Dimensions
?
q
i
(x
j
) (32 dimensions) in ?
n
(x,y)
The cosine, WordNet and KL-divergence similarities with the question x
q
i
3
The cosine, WordNet and KL-divergence similarities with the questions other than x
q
i
3
The cosine, WordNet and KL-divergence similarities with previous and next sentences 6
Is this sentence x
j
exactly x
q
i
or one of the questions in {x
q
1
, . . . , x
q
m
}? 2
Is this sentence x
j
in the three beginning sentences? 3
The relative position of this sentence x
j
to questions 4
Is this sentence x
j
share the same author with the question sentence x
q
i
? 1
Is this sentence x
j
in the same post with question sentences? 2
Is this sentence x
j
in the same paragraph with question sentences? 2
The presence of greeting (e.g., “hi”) and acknowledgement words in this sentence x
j
2
The length of this sentence x
j
1
The number of nouns, verbs and pronouns in this sentence x
j
, respectively 3
?
h
(x,y) (704 dimensions)
For ?
hn
(x,y), the above 32 dimension features w.r.t. 4× 4 = 16 transition patterns 512
For ?
hc
(x,y), 12 types of pairwise or merged similarities w.r.t. 16 transition patterns 192
?
v
(x,y) (32 dimensions)
The transition patterns for any two non-contiguous labels in a label group 16
The transition patterns for any two contiguous labels in a label group 16
Table 1: Feature descriptions and demisions
?
hc
(x,y) =
m
?
i=1
?
j?C
?
k?A
? ?? ?
Complete Edges
?
hc
(x
j
, x
k
, y
ij
, y
ik
),
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
)
= ?(y
ij
, y
i,j+1
)? ?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
?
hc
(x
j
, x
k
, y
ij
, y
ik
)
= ?(y
ij
, y
ik
)? ?
hc
(x
j
, x
k
, y
ij
, y
ik
)
where ?(y
ij
, y
ik
) is a 16-dimensional vector. It in-
dicates all 4×4 pairwise transition patterns of four
types of labels, the context, answer, question and
plain. Note that apart from previous work (Ding
et al., 2008) we use complete skip-chain (context-
answer) edges in ?
hc
(x,y).
The label group feature mapping ?
v
(x,y) is
defined as follows,
?
v
(x,y) =
n
?
j=1
?
v
(x
j
,y
.j
),
where ?
v
(x
j
,y
.j
) encodes each label group pat-
tern into a vector.
The detail descriptions and vector dimensions
of the used features are listed in Table 1.
4 Structural SVMs and Inference
Given a training set S = {(x
(i)
,y
(i)
) ? X ×
Y : i = 1, . . . , N}, we use the structural
SVMs (Taskar et al., 2003; Tsochantaridis et
al., 2005; Joachims et al., 2009) formulation, as
shown in Optimization Problem 1 (OP1), to learn
a weight vector w.
OP 1 (1-Slack Structural SVM)
min
w,??0
1
2
||w||
2
+
C
N
?
s.t. ?(y¯
(1)
, . . . , y¯
(N)
) ? Y
n
,
1
N
w
T
N
?
i=1
[?(x
(i)
,y
(i)
)??(x
(i)
, y¯
(i)
)]
?
1
N
N
?
i=1
?(y
(i)
, y¯
(i)
)? ?,
where ? is a slack variable, ?(x,y) is the joint
feature mapping and ?(y, y¯) is the loss func-
tion that measures the loss caused by the dif-
ference between y and y¯. Though OP1 is al-
ready a quadratic optimization problem, directly
using off-the-shelf quadratic optimization solver
will fail, due to the large number of constraints.
Instead, a cutting plane algorithm is used to ef-
ficiently solve this problem. For the details of the
518
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
(a) Original graph
{P P P , P P C , P C P , P C C , C P P , C P C , C C P , C C C }
{Q } {P } {A, P } {A, P }
(b) Transformed graph
{P P P }
{Q } {P } {A, P } {A, P }
{C C C }
{Q } {P } {A, P } {A, P }
....
(c) Decomposed graph
Figure 3: The equivalent transform of graphs
Algorithm 1 Exact Inference Algorithm
1: Input: (C
i
,A
i
) for each q
i
, w, x, y
2: for i ? {1, . . . ,m} do
3: for C
s
? C
i
do
4: [R(C
s
), y¯
i.
(C
s
)] ? Viterbi(w,x; C
s
)
5: end for
6: C
?
s
= argmax
C
s
?C
i
R(C
s
)
7: y¯
?
i.
= y¯
i.
(C
?
s
)
8: end for
9: return y¯
?
structural SVMs, please refer to (Tsochantaridis et
al., 2005; Joachims et al., 2009).
The most essential and time-consuming step in
structural SVMs is finding the most violated con-
straint, which is equivalent to solve
argmax
y?Y
w
T
?(x
(i)
,y) + ?(y
(i)
,y). (2)
Without the ability to efficiently find the most vio-
lated constraint, the cutting plane algorithm is not
tractable.
In the next sub-sections, we introduce the al-
gorithms for finding the most violated constraint,
also called loss-augmented inference. The algo-
rithms are essential for the success of customizing
structural SVMs to our problem.
4.1 Exact Inference
The exact inference algorithm is designed for a
simplified model with two sub-mappings ?
n
and
?
h
, except ?
v
.
One naive approach to finding the most violated
constraint for the simplified model is to enumer-
ate all the 2
|C|+|A|
cases for each row of the label
matrix. However, it would be intractable for large
candidate sets.
An important property is that the context can-
didate set is usually much smaller than the whole
number of sentences in a thread. This property en-
ables us to design efficient and exact inference al-
gorithm by transforming from the original graph
representation in Figure 2 to the graphs in Fig-
ure 3. This graph transform merges all the nodes
in the context candidate set C to one node with 2
|C|
possible labels.
We design an exact inference algorithm in Algo-
rithm 1 based on the graph in Figure 3(c). The al-
gorithm can be summarized in three steps: (1) enu-
merate all the 2
|C|
possible labels
1
for the merged
node (line 3). (2) For each given label of the
merged node, perform the Viterbi algorithm (Ra-
biner, 1989) on the decomposed graph (line 4) and
store the Viterbi algorithm outputs in R and yˆ
i.
.
(3) From the 2
|C|
Viterbi algorithm outputs, select
the one with highest score as the output (lines 6
and 7).
The use of the Viterbi algorithm is assured by
the fact that there exists certain equivalence be-
tween the decomposed graph (Figure 3(c)) and a
linear chain. By fixing the the label of the merged
node, we could remove the dashed edges in the
decomposed graph and regard the rest graph as a
linear chain, which results in the Viterbi decoding.
4.2 Approximate Inference
The exact inference cannot handle the complete
model with three sub-mappings, ?
n
, ?
h
, and
?
v
, since the label group defeats the graph trans-
form in Figure 3. Thus, we design two ap-
proximate algorithms by employing undergener-
ating and overgenerating approaches (Finley and
Joachims, 2008).
First, we develop an undergenerating local
greedy search algorithm shown in Algorithm 2. In
the algorithm, there are two loops, inner and outer
loops. The outer loop terminates when no labels
change (steps 3-11). The inner loop enumerates
the whole label matrix and greedily determines
each label (step 7) by maximizing the Equation
(2). Since the whole algorithm terminates only if
1
Since the merged node is from context candidate set C,
enumerating its label is equivalent to enumerating subsets C
s
of the candidate set C
519
Algorithm 2 Greedy Inference Algorithm
1: Input: w, x, y
2: initialize solution: y¯ ? y
0
3: repeat
4: y
?
? y¯
5: for i ? {1, . . . ,m} do
6: for j ? {1, . . . , n} do
7:
y¯
?
ij
? argmax
y¯
ij
w
T
?(x, y¯)
+4(y, y¯)
8: y¯
ij
? y¯
?
ij
9: end for
10: end for
11: until y¯ = y
?
12: y¯
?
? y¯
13: return y¯
?
the label matrix does not change during the last
outer loop. This indicates that at least a local opti-
mal solution is obtained.
Second, an overgenerating method can be
designed by using linear programming relax-
ation (Finley and Joachims, 2008). To save the
space, we skip the details of this algorithm here.
5 Loss Functions
Structural SVMs allow users to customize the loss
function 4 : Y × Y ? R according to different
system requirements. In this section, we introduce
the loss functions used in our work.
Basic loss function. The simplest way to quan-
tify the prediction quality is counting the number
of wrongly predicted labels. Formally,
4
b
(y, y¯) =
m
?
i=1
n
?
j=1
I[y
ij
6= y¯
ij
], (3)
where I[.] is an indicative function that equals to
one if the condition holds and zero otherwise.
Recall-vs-precision loss function. In practice,
we may place different emphasis on recall and pre-
cision according to application settings. We could
include this preference into the model by defining
the following loss function,
4
p
(y,
¯
y) =
m
?
i=1
n
?
j=1
I[y
ij
6= P, y¯
ij
= P ] · c
r
+I[y
ij
= P, y¯
ij
6= P ] · c
p
. (4)
This function penalizes the wrong prediction de-
creasing recall and that decreasing precision with
Items in the data set #items
Thread 515
Post 2, 035
Sentence 8, 500
question annotation 1, 407
context annotation 1, 962
answer annotation 4, 652
plain annotation 18, 198
Table 2: The data statistics
two weights c
r
and c
p
respectively. Specifically,
we denote the loss function with c
p
/c
r
= 2 and
that with c
r
/c
p
= 2 by 4
p
p
and 4
r
p
, respectively.
Various types of loss function can be defined in
a similar fashion. To save the space, we skip the
definitions of other loss functions and only use the
above two types of loss functions to show the flex-
ibility of our approach.
6 Experiments
6.1 Experimental Setup
Corpus. We made use of the same data set as
introduced in (Cong et al., 2008; Ding et al.,
2008). Specifically, the data set includes about
591 threads from the forum TripAdvisor
2
. Each
sentence in the threads is tagged with the labels
‘question’, ‘context’, ‘answer’, or ‘plain’ by two
annotators. We removed 76 threads that have no
question sentences or more than 40 sentences and
6 questions. The remaining 515 forum threads
form our data set.
Table 2 gives the statistics on the data set. On
average, each thread contains 3.95 posts and 2.73
questions, and each question has 1.39 context sen-
tences and 3.31 answer sentences. Note that the
number of annotations is much larger than the
number of sentences because one sentence can be
annotated with multiple labels.
Experimental Details. In all the experiments,
we made use of linear models for the sake of com-
putational efficiency. As a preprocessing step, we
normalized the value of each feature value into
the interval [0, 1] and then followed the heuristic
used in SVM-light (Joachims, 1998) to set C to
1/||x||
2
, where ||x|| is the average length of input
samples (in our case, sentences). The tolerance pa-
rameter ² was set to 0.1 (the value also used in (Cai
2
TripAdvisor (http://www.tripadvisor.com/
ForumHome) is one of the most popular travel forums
520
and Hofmann, 2004)) in all the runs of the experi-
ments.
Evaluation. We calculated the standard preci-
sion (P), recall (R) and F
1
-score (F
1
) for both tasks
(context extraction and answer extraction). All the
experimental results were obtained through 5-fold
cross validation.
6.2 Baseline Methods
We employed binary SVMs (B-SVM), multiclass
SVMs (M-SVM), and C4.5 (Quinlan, 1993) as our
baseline methods:
B-SVM. We trained two binary SVMs for con-
text extraction (context vs. non-context) and an-
swer extraction (answer vs. non-answer), respec-
tively. We used the feature mapping ?
q
i
(x
j
) de-
fined in Equation (1) while training the binary
SVM models.
M-SVM. We extended the binary SVMs by
training multiclass SVMs for three category labels
(context, answer, plain).
C4.5. This decision tree algorithm solved the
same classification problem as binary SVMs and
made use of the same set of features.
6.3 Modeling Sentence Relations and
Question Interactions
We demonstrate in Table 3 that our approach can
make use of the three types of relation among sen-
tences well to boost the performance.
In Table 3, S-SVM represents the structural
SVMs only using the node features ?
n
(x,y). The
suffixes H, C, and V denote the models using
horizontal sequential edges, complete skip-chain
edges and vertical label groups, respectively. The
suffixes C* and V* denote the models using in-
complete skip-chain edges and vertical sequential
edges proposed in (Ding et al., 2008), as shown
in Figures 2(a) and 2(c). All the structural SVMs
were trained using basic loss function ?
b
in Equa-
tion (3). From Table 3, we can observe the follow-
ing advantages of our approaches.
Overall improvement. Our structural approach
steadily improves the extraction as more types of
relation (corresponding to more types of edge) are
included. The best results obtained by using the
three types of relation together improve the base-
line methods binary SVMs by about 6% and 20%
in terms of F
1
values for context extraction and
answer extraction, respectively.
The usefulness of relations. The relations
encoded by horizontal sequential edges and la-
Method 4
b
P (%) R (%) F
1
(%)
Context Extraction
C4.5 ? 74.2 68.7 71.2
B-SVM ? 78.3 72.2 74.9
M-SVM ? 68.0 77.6 72.1
S-SVM 8.86 75.6 71.7 73.4
S-SVM-H 8.60 77.5 75.5 76.3
S-SVM-HC* 8.65 77.9 74.1 75.8
S-SVM-HC 8.62 77.5 75.2 76.2
S-SVM-HCV* 8.08 79.5 79.6 79.5
S-SVM-HCV 7.98 79.7 80.2 79.9
Answer Extraction
C4.5 ? 61.3 45.2 51.8
B-SVM ? 69.7 42.0 51.8
M-SVM ? 63.2 51.5 55.8
S-SVM 8.86 67.0 48.0 55.6
S-SVM-H 8.60 66.9 49.7 56.7
S-SVM-HC* 8.65 66.5 49.4 56.4
S-SVM-HC 8.62 65.7 51.5 57.4
S-SVM-HCV* 8.08 65.5 58.7 61.7
S-SVM-HCV 7.98 65.1 61.2 63.0
Table 3: The effectiveness of our approach
bel groups are useful for both context extraction
and answer extraction. The relation encoded by
complete skip-chain edges is useful for answer
extraction. The complete skip-chain edges not
only avoid preprocessing but also boost the per-
formance when compared with the preprocessed
skip-chain edges. The label groups improve the
vertical sequential edges.
Interactions among questions. The interac-
tions encoded by label groups are especially use-
ful. We conducted significance tests (sign test) on
the experimental results. The test result shows that
S-SVM-HCV outperforms all the other methods
without vertical edges statistically significantly (p-
value < 0.01). Our proposed graphical represen-
tation in Figure 2(d) eases us to model the complex
interactions. In comparison, the 2D model in Fig-
ure 2(c) used in previous work (Ding et al., 2008)
can only model the interaction between adjacent
questions.
6.4 Loss Function Results
We report in Table 4 the comparison between
structural SVMs using different loss functions.
Note that ?
p
p
prefers precision and ?
r
p
prefers re-
call. From Table 4, we can observe that the ex-
perimental results also exhibit this kind of system
521
Method P (%) R (%) F
1
(%)
Context Extraction
S-SVM-HCV-4
b
79.7 80.2 79.9
S-SVM-HCV-4
p
p
82.0 70.3 75.6
S-SVM-HCV-4
r
p
75.7 84.2 79.7
Answer Extraction
S-SVM-HCV-4
b
65.1 61.2 63.0
S-SVM-HCV-4
p
p
71.8 52.2 60.2
S-SVM-HCV-4
r
p
61.8 66.1 63.7
Table 4: The use of different loss functions
preference. Moreover, we further demonstrate the
capability of the loss function ?
p
in Figure 4. The
curves are achieved by varying the ratio between
two parameters c
p
/c
r
in Equation (4). The curves
confirm our intuition: when log(c
p
/c
r
) becomes
larger, the precisions increase but the recalls de-
crease and vice versa.
7 Related work
Previous work on extracting questions, answers
and contexts is most related with our work. Cong
et al. (2008) proposed a supervised approach for
question detection and an unsupervised approach
for answer detection without considering contexts.
Ding et al. (2008) used CRFs to detect contexts
and answers of questions from forum threads.
Some researches on summarizing discussion
threads and emails are related to our work, too.
Zhou and Hovy (2005) segmented internet re-
lay chat, clustered segments into sub-topics, and
identified responding segments of the first seg-
ment in each sub-topic by assuming the first seg-
ment to be focus. In (Nenkova and Bagga, 2003;
Wan and McKeown, 2004; Rambow et al., 2004),
email summaries were organized by extracting
overview sentences as discussion issues. The
work (Shrestha and McKeown, 2004) used RIP-
PER as a classifier to detect interrogative questions
and their answers then used the resulting question
and answer pairs as summaries. We also note the
existing work on extracting knowledge from dis-
cussion threads. Huang et al. (2007) used SVMs
to extract input-reply pairs from forums for chat-
bot knowledge. Feng et al. (2006) implemented
a discussion-bot which used cosine similarity to
match students’ query with reply posts from an an-
notated corpus of archived threaded discussions.
Moreover, extensive researches have been done
within the area of question answering (Burger et
?1.5 ?1 ?0.5 0 0.5 1 1.50.6
0.7
0.8
0.9
1
Log loss ratio
Pre
cisi
on
 
 ContextAnswer
?1.5 ?1 ?0.5 0 0.5 1 1.50.4
0.6
0.8
1
Log loss ratio
Rec
all
 
 ContextAnswer
Figure 4: Balancing between precision and recall
al., 2006; Jeon et al., 2005; Harabagiu and Hickl,
2006; Cui et al., 2005; Dang et al., 2006). They
mainly focused on using sophisticated linguistic
analysis to construct answer from a large docu-
ment collection.
8 Conclusion and Future Work
We have proposed a new form of graphical rep-
resentation for modeling the problem of extract-
ing contexts and answers of questions from online
forums and then customized structural SVM ap-
proach to solve it.
The proposed graphical representation is able
to naturally express three types of relation among
sentences: relation between successive sentences,
relation between context sentences and answer
sentences, and relation between multiple labels for
one sentence. The representation also enables us
to address interactions among questions. We also
developed the inference algorithms for the struc-
tural SVM model by exploiting the special struc-
ture of thread discussions.
Experimental results on a real data set show that
our approach significantly improves the baseline
methods by effectively utilizing various types of
relation among sentences.
Our future work includes: (a) to summa-
rize threads and represent the forum threads in
question-context-answer triple, which will change
the organization of online forums; and (b) to en-
hance QA services (e.g., Yahoo! Answers) by the
contents extracted from online forums.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for their comments to improve this paper.
522
References
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2006. Issues, tasks and program struc-
tures to roadmap research in question and answering
(qna). ARAD: Advanced Research and Development
Activity (US).
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal document categorization with support vector ma-
chines. In Proceedings of CIKM, pages 78–87.
Gao Cong, Long Wang, Chin-Yew Lin, and Young-In
Song. 2008. Finding question-answer pairs from
online forums. In Proceedings of SIGIR, pages 467–
474.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of SIGIR, pages 400–407.
Hoa Dang, Jimmy Lin, and Diane Kelly. 2006.
Overview of the trec 2006 question answering track.
In Proceedings of TREC, pages 99–116.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random field to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL, pages 710–718.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Proceedings of IUI, pages 171–177.
Thomas Finley and Thorsten Joachims. 2008. Training
structural SVMs when exact inference is intractable.
In Proceedings of ICML, pages 304–311.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
364–372.
Sanda M. Harabagiu and Andrew Hickl. 2006. Meth-
ods for using textual entailment in open-domain
question answering. In Proceedings of ACL, pages
905–912.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In Proceedings of IJCAI, pages 423–428.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of CIKM, pages 84–
90.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs.
Machine Learning.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of ECML, pages 137–
142.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282–
289.
Ani Nenkova and Amit Bagga. 2003. Facilitating
email thread access by extractive summary genera-
tion. In Proceedings of RANLP, pages 287–296.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of sequence labeling algorithms and extensions. In
Proceedings of ICML, pages 681–688.
John Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publisher Incorpora-
tion.
Lawrence Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of IEEE, pages 257–286.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email
threads. In Proceedings of HLT-NAACL, pages 105–
108.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of COLING, pages 889–895.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. Technical Report 04-49.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Advances
in Neural Information Processing Systems 16. MIT
Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453–1484.
Stephen Wan and Kathy McKeown. 2004. Generating
overview summaries of ongoing email thread discus-
sions. In Proceedings of COLING, pages 549–555.
Liang Zhou and Eduard Hovy. 2005. Digesting vir-
tual ”geek” culture: The summarization of technical
internet relay chats. In Proceedings of ACL, pages
298–305.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and
Wei-Ying Ma. 2005. 2d conditional random fields
for web information extraction. In Proceedings of
ICML, pages 1044–1051.
523
