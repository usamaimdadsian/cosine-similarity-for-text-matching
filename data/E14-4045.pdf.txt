Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 231–235,
Gothenburg, Sweden, April 26-30 2014.
c
©2014 Association for Computational Linguistics
Coreference Resolution Evaluation for Higher Level Applications
Don Tuggener
Unversity of Zurich
Institute of Computational Linguistics
tuggener@cl.uzh.ch
Abstract
This paper presents an evaluation frame-
work for coreference resolution geared to-
wards interpretability for higher-level ap-
plications. Three application scenarios
for coreference resolution are outlined and
metrics for them are devised. The metrics
provide detailed system analysis and aim
at measuring the potential benefit of using
coreference systems in preprocessing.
1 Introduction
Coreference Resolution is often described as an
important preprocessing step for higher-level ap-
plications. However, the commonly used coref-
erence evaluation metrics (MUC, BCUB, CEAF,
BLANC) treat coreference as a generic clustering
problem and perform cluster similarity measures
to evaluate coreference system outputs. Mentions
are seen as unsorted generic items rather than lin-
early ordered linguistic objects (Chen and Ng,
2013). This makes it arguably hard to interpret
the scores and assess the potential benefit of using
a coreference system as a preprocessing step.
Therefore, this paper proposes an evaluation
framework for coreference systems which aims at
bridging the gap between coreference system de-
velopment, evaluation, and higher level applica-
tions. For this purpose, we outline three types
of application scenarios which coreference resolu-
tion can benefit and devise metrics for them which
are easy to interpret and provide detailed system
output analysis based on any available mention
feature.
2 Basic Concepts
Like other coreference metrics, we adapt the con-
cepts of Recall and Precision from evaluation in
Information Retrieval (IR) to compare mentions
in a system output (the response) to the anno-
tated mentions in a gold standard (the key). To
stay close to the originally clear definitions of Re-
call and Precision in IR, Recall is aimed at iden-
tifying how many of the annotated key mentions
are correctly resolved by a system, and Precision
will measure the correctness of the returned sys-
tem mentions.
However, if we define Recall as
tp
tp+fn
, the de-
nominator will not include key mentions that have
been put in the wrong coreference chain, and will
not denote all mentions in the key. Therefore,
borrowing nomenclature from (Durrett and Klein,
2013), we introduce an additional error class,
wrong linkage (wl), which signifies key mentions
that have been linked to incorrect antecedents. Re-
call can then be defined as
tp
tp+wl+fn
and Precision
as
tp
tp+wl+fp
. Recall then extends over all key men-
tions, and Precision calculation includes all sys-
tem mentions.
Furthermore, including wrong linkage in the
Recall equation prevents it from inflating com-
pared to Precision when a large number of key
mentions are incorrectly resolved. Evaluation
is also sensitive to the anaphoricity detection
problem. For example, an incorrectly resolved
anaphoric “it” pronoun is counted as wrong link-
age and thus also affects Recall, while a resolved
pleonastic “it” pronoun is considered a false posi-
tive which is only penalized by Precision. Beside
the “it” pronoun, this is of particular relevance for
noun markables, as determining their referential
status is a non-trivial subtask in coreference res-
olution.
As we evaluate each mention individually, we
are able to measure performance regarding any
feature type of a mention, e.g. PoS, number, gen-
der, semantic class etc. We will focus on men-
tion types based on PoS tags (i.e. pronouns, nouns
etc.), as they are often the building blocks of coref-
erence systems. Furthermore, mention type based
231
performance analysis is informative for higher-
level applications, as they might be specifically in-
terested in certain mention types.
3 Application Scenarios
Next, we will outline three higher-level applica-
tion types which consume coreference and devise
relevant metrics for them.
3.1 Models of entity distributions
The first application scenario subsumes models
that investigate distributions and patterns of en-
tity occurrences in discourse. For example, Cen-
tering theory (Grosz et al., 1995) and the thereof
derived entity grid model (Barzilay and Lapata,
2008; Elsner and Charniak, 2011) record transi-
tions of grammatical functions that entities occur
with in coherent discourse. These models can
benefit from coreference resolution if entities are
pronominalized or occur as a non-string matching
nominal mentions.
Another application which tracks sequences of
entity occurrences is event sequence modeling.
Such models investigate prototypical sequences of
events to derive event schemes or templates of suc-
cessive events (Lee et al., 2012; Irwin et al., 2011;
Kuo and Chen, 2007). Here, coreference res-
olution can help link pronominalized arguments
of events to their previous mention and, thereby,
maintain the event argument sequence.
The outlined applications in this scenario pri-
marily rely on the identification of correct and
gapless sequences of entity occurrences. We can
approximate this requirement in a metric by re-
quiring the immediate antecedent of a mention in
a response chain to be the immediate antecedent
of that mention in the key chain.
Note that this restriction deems mentions as in-
correct, if they skip an antecedent but are resolved
to another antecedent in the correct chain. For ex-
ample, given a key [A-B-C-D], mention D in a re-
sponse [A-B-D] would not be considered correct,
as the immediate antecedent is not the same as in
the key. The original sequence of the entity’s oc-
currence is broken between mention B and D in
the response, as mention C is missing.
We use the following algorithm (table 1) to cal-
culate Recall and Precision for evaluating imme-
diate antecedents. Let K be the key and S be the
system response. Let e be an entity denoted by m
n
mentions.
01 for e
k
? K:
02 for m
i
? e
k
? i > 0:
03 if ¬?e
s
,m
j
: (e
s
? S ?m
j
? e
s
?m
j
= m
i
?
?predecessor(m
j
, e
s
)) ? fn++
04 elif ?e
s
,m
j
: (e
s
? S ?m
j
? e
s
?m
j
= m
i
?
predecessor(m
i
, e
k
) = predecessor(m
j
, e
s
))
? tp++
05 else wl++
06 for e
s
? S:
07 for m
i
? e
s
? i > 0:
08 if ¬?e
k
,m
j
: (e
k
? K ?m
j
? e
k
?m
j
= m
i
?
?predecessor(m
j
, e
k
)) ? fp++
Table 1: Algorithm for calculating Recall and Pre-
cision.
We traverse the key K and each entity e
k
in
it
1
. We evaluate each mention m
i
in e
k
, except for
the first one (line 2), as we investigate coreference
links. If no response chain exists that contains
m
i
and its predecessor, we count m
i
as a false
negative (line 3). This condition subsumes the
case where m
i
is not in the response, and the case
where m
i
is the first mention of a response chain.
In the latter case, the system has deemed m
i
to be
non-anaphoric (i.e. the starter of a chain), while it
is anaphoric in the key
2
. We check whether the
immediate predecessor of m
i
in the key chain e
k
is also the immediate predecessor of m
j
in the re-
sponse chain e
s
(line 4). If true, we count m
i
as a
true positive, or as wrong linkage otherwise.
We traverse the response chains to detect spu-
rious system mentions, i.e. mentions not in the
key, and count them as false positives, i.e. non-
anaphoric markables that have been resolved by
the system (lines 6-8). Here, we also count men-
tions in the response, which have no predecessor
in a key chain, as false positives. If a mention
in the response chain is the chain starter in a key
chain, it means that the system has falsely deemed
it to be anaphoric and we regard it as a false posi-
tive
3
.
3.2 Inferred local entities
The second application scenario relies on corefer-
ence resolution to infer local nominal antecedents.
For example, in Summarization, a target sentence
may contain a pronoun which should be replaced
by a nominal antecedent to avoid ambiguities and
ensure coherence in the summary. Machine Trans-
1
We disregard singleton entities, as it is not clear what
benefit a higher level application could gain from them.
2
(Durrett and Klein, 2013) call this error false new (FN).
3
This error is called false anaphoric (FA) by (Durrett and
Klein, 2013).
232
lation can benefit from pronoun resolution in lan-
guage pairs where nouns have grammatical gen-
der. In such language pairs, the gender of a pro-
noun antecedent has to be retrieved in the source
language in order to insert the pronoun with the
correct gender in the target language.
In these applications, it is not sufficient to link
pronouns to other pronouns of the same corefer-
ence chain because they do not help infer the un-
derlying entity. Therefore, in our metric, we re-
quire the closest preceding nominal antecedent of
a mention in a response chain to be an antecedent
in the key chain.
The algorithm for calculation of Recall and Pre-
cision is similar to the one in table 1. We modify
lines 3 and 4 to require the closest nominal an-
tecedent of m
i
in the response chain e
s
to be an
antecedent of m
j
in the corresponding key chain
e
k
, where m
j
= m
i
, i.e.:
?m
h
? e
s
: is closest noun(m
h
,m
i
) ?
?e
k
,m
j
,m
l
: (e
k
? K ? m
j
? e
k
? m
j
=
m
i
?m
l
? e
k
? l < j ?m
l
= m
h
) ? tp++
Note that we cannot process chains without a
nominal mention in this scenario
4
. Therefore, we
skip evaluation for such e
k
? K. We still want
to find incorrectly inferred nominal antecedents of
anaphoric mentions, i.e. mentions in e
s
? S that
have been assigned a nominal antecedent in the re-
sponse but have none in the key and count them as
wrong linkage, as they infer an incorrect nominal
antecedent. Therefore, we traverse all e
s
? S and
add to the algorithm:
?m
i
? e
s
: ¬is noun(m
i
) ? ?m
h
? e
s
:
is noun(m
h
) ? ?e
k
,m
j
: (e
k
? K ? m
j
?
e
k
?m
j
= m
i
? ¬?m
l
? e
k
: is noun(m
l
)) ?
wl++
3.3 Finding contexts for a specific entity
The last scenario we consider covers applications
that are primarily query driven. Such applications
search for references to a given entity and analyze
or extract its occurrence contexts. For example,
Sentiment Analysis searches large text collections
for occurrences of a target entity and then derives
polarity information from its contexts. Biomedical
relation mining looks for interaction contexts of
specific genes or proteins etc.
4
We found that 476 of 4532 key chains (10.05%) do not
contain a nominal mention. Furthermore, we do not treat
cataphora (i.e. pronouns at chain start) in this scenario. We
found that 241 (5.31%) of the key chains start with cataphoric
pronouns.
For these applications, references to relevant en-
tities have to be accessible by queries. For ex-
ample, if a sentiment system investigates polarity
contexts of the entity “Barack Obama”, given a
key chain [Obama - the president - he], a response
chain [the president - he] is not sufficient, because
the higher level application is not looking for in-
stances of the generic “president” entity.
Therefore, we determine an anchor mention for
each coreference chain which represents the most
likely unique surface form an entity occurs with.
As a simple approximation, we choose the first
nominal mention of a coreference chain to be the
anchor of the entity, because first mentions of enti-
ties introduce them to discourse and are, therefore,
generally informative, unambiguous, semantically
extensive and are likely to contain surface forms a
higher level application will query.
Entity Detection
01 for e
k
? K:
02 if ?m
n
? e
k
: is noun(m
n
)
? m
anchor
= determine anchor(e
k
)
03 if ?m
anchor
? ?e
s
? S : m
anchor
? e
s
? tp++
04 else ? fn++
05 for e
s
? S:
06 if ?m
n
? e
s
: is noun(m
n
)
? m
anchor
= determine anchor(e
s
)
07 if ¬?e
k
? K : m
anchor
? e
k
? fp++
Entity Mentions
01 for e
k
? K : ?m
anchor
? ?e
s
? S : m
anchor
? e
s
:
02 for m
i
? e
k
:
03 if m
i
? e
s
? tp++
04 else? fn++
05 for m
i
? e
s
:
06 if m
i
¬ ? e
k
? fp++
Table 2: Algorithm for calculating Recall and Pre-
cision using anchor mentions.
To calculate Recall and Precision, we align
coreference chains in the responses to those in the
key via their anchors and then measure how many
(in)correct references to that anchor the corefer-
ence systems find (table 2). We divide evaluation
into entity detection (ED), which measures how
many of the anchor mentions a system identifies.
We then measure the quality of the entity men-
tions (EM) for only those entities which have been
aligned through their anchors.
The quality of the references to the anchor men-
tions are not directly comparable between sys-
tems, as their basis is not the same if the num-
ber of aligned anchors differs. Therefore, we cal-
culate the harmonic mean of entity detection and
entity mentions to enable direct system compari-
233
son. Where applicable, we obtain the named en-
tity class of the entity and measure performance
for each such class.
4 Evaluation
We apply our metrics to three available corefer-
ence systems, namely the Berkley system (Dur-
rett and Klein, 2013), the IMS system (Bj¨orkelund
and Farkas, 2012), and the Stanford system (Lee
et al., 2013) and their responses for the CoNLL
2012 shared task test set for English (Pradhan et
al., 2012). Tables 3 and 4 report the results.
Immediate antecedent Inferred antecedent
R P F R P F
BERK (Durrett and Klein, 2013)
NOUN 45.06 47.06 46.04 55.54 60.37 57.85
PRP 67.66 64.87 66.24 48.92 53.62 51.16
PRP$ 74.49 74.32 74.41 61.95 66.80 64.28
TOTAL 56.60 56.91 56.76 52.94 58.04 55.37
IMS (Bj¨orkelund and Farkas, 2012)
NOUN 38.01 43.09 40.39 46.90 54.96 50.61
PRP 69.06 68.64 68.85 43.04 57.42 49.20
PRP$ 72.57 72.11 72.34 51.51 63.54 56.90
TOTAL 53.55 57.55 55.48 45.27 56.47 50.25
STAN (Lee et al., 2013)
NOUN 38.51 42.92 40.60 50.03 57.62 53.56
PRP 65.55 61.09 63.25 36.67 45.97 40.80
PRP$ 66.12 65.70 65.91 40.64 52.38 45.77
TOTAL 51.70 52.69 52.19 43.01 51.73 46.97
Table 3: Antecedent based evaluation
We note that the system ranking based on the
MELA score
5
is retained by our metrics. MELA
rates the Berkley system best (61.62), followed by
the IMS system (57.42), and then the Stanford sys-
tem (55.69).
Beside detailed analysis based on PoS tags, our
metrics reveal interesting nuances. Somewhat ex-
pectedly, noun resolution is worse when the imme-
diate antecedent is evaluated, than if the next nom-
inal antecedent is analyzed. Symmetrically in-
verse, pronouns achieve higher scores when their
direct antecedent is measured, as compared to
when the next nominal antecedent has to be cor-
rect.
Our evaluation shows that the IMS system
achieves a higher score for pronouns than the
Berkley system when immediate antecedents are
measured and has a higher Precision for pronouns
regarding the inferred antecedents. The Berkley
system performs best mainly due to Recall. For
e.g. personal pronouns (PRP), Berkley has the
5
MUC+BCUB+CEAFE
3
following counts for the inferred antecedents:
tp=2687, wl=1935, fn=871, fp=389, while IMS
shows tp=2243, wl=1376, fn=1592, fp=287. This
indicates that the IMS Recall is lower because of
the high false negative count, rather than being due
to too many wrong linkages.
Finally, table 4 suggests that the IMS systems
performs significantly worse in the PERSON class
than the other systems and is outperformed by the
Stanford system in the ORG class, but performs
best in the GPE class.
R P F F?
PERSON (18.69%)
BERK
ED 64.02 75.88 69.45
67.11
EM 63.60 66.29 64.92
IMS
ED 45.66 51.69 48.48
52.74
EM 47.67 73.45 57.82
STAN
ED 56.33 59.74 57.98
61.61
EM 53.84 84.37 65.73
GPE (13.28%)
BERK
ED 73.21 77.36 75.23
75.71
EM 69.89 83.73 76.19
IMS
ED 73.51 74.17 73.84
76.21
EM 69.94 90.04 78.73
STAN
ED 70.24 76.62 73.29
75.24
EM 68.44 88.81 77.30
ORG (9.63%)
BERK
ED 62.78 67.13 64.88
67.62
EM 66.87 74.78 70.60
IMS
ED 44.98 54.30 49.20
56.85
EM 57.26 81.66 67.32
STAN
ED 49.68 58.56 53.75
59.41
EM 57.25 79.05 66.41
TOTAL (100%)
BERK
ED 58.65 53.19 55.79
63.41
EM 72.65 74.28 73.45
IMS
ED 47.16 42.66 44.80
55.24
EM 65.88 79.40 72.01
STAN
ED 48.62 41.40 44.72
55.27
EM 65.66 80.48 72.32
Table 4: Anchor mention based evaluation
5 Conclusion
We have presented a simple evaluation framework
for coreference evaluation with higher level ap-
plications in mind. The metrics allow specific
performance measurement regarding different an-
tecedent requirements and any mention feature,
such as PoS type, lemma, or named entity class,
which can aid system development and compari-
son. Furthermore, the metrics do not alter system
rankings compared to the commonly used evalua-
tion approach
6
.
6
The scorers are freely available on our website:
http://www.cl.uzh.ch/research/coreferenceresolution.html
234
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
put. Linguist., 34(1):1–34, March.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, CoNLL ’12, pages 49–55,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, pages 1366–1374.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Seattle, Washington, Oc-
tober. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 125–129, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Comput. Linguist.,
21(2):203–225, June.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CONLL Shared Task
’11, pages 86–92, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
June-Jei Kuo and Hsin-Hsi Chen. 2007. Cross-
document event clustering using knowledge mining
from co-reference chains. Inf. Process. Manage.,
43(2):327–343, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 489–500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.
235
