Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Joint Relational Embeddings for Knowledge-based Question Answering
Min-Chul Yang
†
Nan Duan
‡
Ming Zhou
‡
Hae-Chang Rim
†
†
Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
‡
Microsoft Research Asia, Beijing, China
mcyang@nlp.korea.ac.kr
{nanduan, mingzhou}@microsoft.com
rim@nlp.korea.ac.kr
Abstract
Transforming a natural language (NL)
question into a corresponding logical form
(LF) is central to the knowledge-based
question answering (KB-QA) task. Un-
like most previous methods that achieve
this goal based on mappings between lex-
icalized phrases and logical predicates,
this paper goes one step further and pro-
poses a novel embedding-based approach
that maps NL-questions into LFs for KB-
QA by leveraging semantic associations
between lexical representations and KB-
properties in the latent space. Experimen-
tal results demonstrate that our proposed
method outperforms three KB-QA base-
line methods on two publicly released QA
data sets.
1 Introduction
Knowledge-based question answering (KB-QA)
involves answering questions posed in natural
language (NL) using existing knowledge bases
(KBs). As most KBs are structured databases,
how to transform the input question into its corre-
sponding structured query for KB (KB-query) as
a logical form (LF), also known as semantic pars-
ing, is the central task for KB-QA systems. Pre-
vious works (Mooney, 2007; Liang et al., 2011;
Cai and Yates, 2013; Fader et al., 2013; Berant et
al., 2013; Bao et al., 2014) usually leveraged map-
pings between NL phrases and logical predicates
as lexical triggers to perform transformation tasks
in semantic parsing, but they had to deal with two
limitations: (i) as the meaning of a logical pred-
icate often has different natural language expres-
sion (NLE) forms, the lexical triggers extracted for
a predicate may at times are limited in size; (ii)
entities detected by the named entity recognition
(NER) component will be used to compose the
logical forms together with the logical predicates,
so their types should be consistent with the pred-
icates as well. However, most NER components
used in existing KB-QA systems are independent
from the NLE-to-predicate mapping procedure.
We present a novel embedding-based KB-QA
method that takes all the aforementioned lim-
itations into account, and maps NLE-to-entity
and NLE-to-predicate simultaneously using sim-
ple vector operations for structured query con-
struction. First, low-dimensional embeddings of
n-grams, entity types, and predicates are jointly
learned from an existing knowledge base and from
entries <entity
subj
, NL relation phrase, entity
obj
>
that are mined from NL texts labeled as KB-
properties with weak supervision. Each such en-
try corresponds to an NL expression of a triple
<entity
subj
, predicate, entity
obj
> in the KB. These
embeddings are used to measure the semantic as-
sociations between lexical phrases and two prop-
erties of the KB, entity type and logical predicate.
Next, given an NL-question, all possible struc-
tured queries as candidate LFs are generated and
then they are ranked by the similarity between the
embeddings of observed features (n-grams) in the
NL-question and the embeddings of logical fea-
tures in the structured queries. Last, answers are
retrieved from the KB using the selected LFs.
The contributions of this work are two-fold: (1)
as a smoothing technique, the low-dimensional
embeddings can alleviate the coverage issues of
lexical triggers; (2) our joint approach integrates
entity span selection and predicate mapping tasks
for KB-QA. For this we built independent entity
embeddings as the additional component, solving
the entity disambiguation problem.
2 Related Work
Supervised semantic parsers (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Mooney,
2007) heavily rely on the <sentence, semantic an-
645
notation> pairs for lexical trigger extraction and
model training. Due to the data annotation re-
quirement, such methods are usually restricted to
specific domains, and struggle with the coverage
issue caused by the limited size of lexical triggers.
Studies on weakly supervised semantic parsers
have tried to reduce the amount of human supervi-
sion by using question-answer pairs (Liang et al.,
2011) or distant supervision (Krishnamurthy and
Mitchell, 2012) instead of full semantic annota-
tions. Still, for KB-QA, the question of how to
leverage KB-properties and analyze the question
structures remains.
Bordes et al. (2012) and Weston et al. (2013) de-
signed embedding models that connect free texts
with KBs using the relational learning method
(Weston et al., 2010). Their inputs are often
statement sentences which include subject and ob-
ject entities for a given predicate, whereas NL-
questions lack either a subject or object entity that
is the potential answer. Hence, we can only use
the information of a subject or object entity, which
leads to a different training instance generation
procedure and a different training criterion.
Recently, researchers have developed open do-
main systems based on large scale KBs such as
FREEBASE
1
(Cai and Yates, 2013; Fader et al.,
2013; Berant et al., 2013; Kwiatkowski et al.,
2013; Bao et al., 2014; Berant and Liang, 2014;
Yao and Van Durme, 2014). Their semantic
parsers for Open QA are unified formal and scal-
able: they enable the NL-question to be mapped
into the appropriate logical form. Our method ob-
tains similar logical forms, but using only low-
dimensional embeddings of n-grams, entity types,
and predicates learned from texts and KB.
3 Setup
3.1 Relational Components for KB-QA
Our method learns semantic mappings between
NLEs and the KB
2
based on the paired relation-
ships of the following three components: C de-
notes a set of bag-of-words (or n-grams) as context
features (c) for NLEs that are the lexical represen-
tations of a logical predicate (p) in KB; T denotes
a set of entity types (t) in KB and each type can be
used as the abstract expression of a subject entity
1
http://www.freebase.com
2
For this paper, we used a large scale knowledge base that
contains 2.3B entities, 5.5K predicates, and 18B assertions.
A 16-machine cluster was used to host and serve the whole
data.
(s) that occurs in the input question; P denotes a
set of logical predicates (p) in KB, each of which
is the canonical form of different NLEs sharing an
identical meaning (bag-of-words; c).
Based on the components defined above, the
paired relationships are described as follows: T -
P can investigate the relationship between sub-
ject entity and logical predicate, as object entity
is always missing in KB-QA; C-T can scruti-
nize subject entity’s attributes for the entity span
selection such as its positional information and
relevant entity types to the given context, which
may solve the entity disambiguation problem in
KB-QA; C-P can leverage the semantic overlap
between question contexts (n-gram features) and
logical predicates, which is important for mapping
NL-questions to their corresponding predicates.
3.2 NLE-KB Pair Extraction
This section describes how we extract the semantic
associated pairs of NLE-entries and KB-triples to
learn the relational embeddings (Section 4.1).
<Relation Mention, Predicate> Pair (MP)
Each relation mention denotes a lexical phrase
of an existing KB-predicate. Following informa-
tion extraction methods, such as PATTY (Nakas-
hole et al., 2012), we extracted the <relation
mention, logical predicate> pairs from English
WIKIPEDIA
3
, which is closely connected to our
KB, as follows: Given a KB-triple <entity
subj
,
logical predicate, entity
obj
>, we extracted NLE-
entries <entity
subj
, relation mention, entity
obj
>
where relation mention is the shortest path be-
tween entity
subj
and entity
obj
in the dependency
tree of sentences. The assumption is that any re-
lation mention (m) in the NLE-entry containing
such entity pairs that occurred in the KB-triple is
likely to express the predicate (p) of that triple.
With obtaining high-qualityMP pairs, we kept
only relation mentions that were highly associated
with a predicate measured by the scoring function:
S(m, p) = PMI(e
m
; e
p
) + PMI(u
m
;u
p
) (1)
where e
x
is the set of total pairs of both-side
entities of entry x (m or p) and u
x
is the set
of unique (distinct) pairs of both-side entities of
entry x. In this case, the both-side entities in-
dicate entity
subj
and entity
obj
. For a frequency-
based probability, PMI(x; y) = log
P (x,y)
P (x)P (y)
3
http://en.wikipedia.org/
646
(Church and Hanks, 1990) can be re-written as
PMI(x; y) = log
|x
?
y|·C
|x|·|y|
, where C denotes the
total number of items shown in the corpus. The
function is partially derived from the support score
(Gerber and Ngonga Ngomo, 2011), but we fo-
cus on the correlation of shared entity pairs be-
tween relation mentions and predicates using the
PMI computation.
<Question Pattern, Predicate> Pair (QP)
Since WIKIPEDIA articles have no information to
leverage interrogative features which highly de-
pend on the object entity (answer), it is difficult to
distinguish some questions that are composed of
only different 5W1H words, e.g., {When|Where}
was Barack Obama born? Hence, we used the
method of collecting question patterns with human
labeled predicates that are restricted by the set of
predicates used inMP (Bao et al., 2014).
4 Embedding-based KB-QA
Our task is as follows. First, our model learns the
semantic associations of C-T , C-P , and T -P (Sec-
tion 3.1) based on NLE-KB pairs (Section 3.2),
and then predicts the semantic-related KB-query
which can directly find the answer to a given NL-
question.
For our feature space, given an NLE-KB pair,
the NLE (relation mention in MP or question
pattern in QP) is decomposed into n-gram fea-
tures: C = {c | c is a segment of NLE}, and
the KB-properties are represented by entity type
t of entity
subj
and predicate p. Then we can ob-
tain a training triplet w = [C, t, p]. Each feature
(c ? C, t ? T , p ? P) is encoded in the distributed
representation which is n-dimensional embedding
vectors (E
n
): ?x, x
encode
? E(x) ? E
n
.
All n-gram features (C) for an NLE are merged
into one embedding vector to help speed up the
learning process: E(C) =
?
c?C
E(c)/|C|. This
feature representation is inspired by previous work
in embedding-based relation extraction (Weston et
al., 2013), but differs in the following ways: (1)
entity information is represented on a separate em-
bedding, but its positional information remains as
symbol ?entity?; (2) when the vectors are com-
bined, we use the average of each index to normal-
ize features.
For our joint relational approach, we focus on
the set of paired relationships R = {C-t, C-p, t-
p} that can be semantically leveraged. Formally,
these features are embedded into the same latent
space (E
n
) and their semantic similarities can be
computed by a dot product operation:
Sim(a, b) = Sim(r
ab
) = E(a)
?
E(b) (2)
where r
ab
denotes a paired relationship a-b (or (a,
b)) in the above set R. We believe that our joint re-
lational learning can smooth the surface (lexical)
features for semantic parsing using the aligned en-
tity and predicate.
4.1 Joint Relational Embedding Learning
Our ranking-based relational learning is based on
a ranking loss (Weston et al., 2010) that supports
the idea that the similarity scores of observed pairs
in the training set (positive instances) should be
larger than those of any other pairs (negative in-
stances):
?i, ?y
?
6= y
i
, Sim(x
i
, y
i
) > 1+Sim(x
i
, y
?
) (3)
More precisely, for each triplet w
i
= [C
i
, t
i
, p
i
]
obtained from an NLE-KB pair, the relationships
R
i
= {C
i
-t
i
, C
i
-p
i
, t
i
-p
i
} are trained under the
soft ranking criterion, which conducts Stochastic
Gradient Descent (SGD). We thus aim to minimize
the following:
?i,?y
?
6= y
i
,max(0, 1?Sim(x
i
, y
i
)+Sim(x
i
, y
?
))
(4)
Our learning strategy is as follows. First, we ini-
tialize embedding space E
n
by randomly giving
mean 0 and standard deviation 1/n to each vec-
tor. Then for each training triplet w
i
, we select the
negative pairs against positive pairs (C
i
-t
i
, C
i
-p
i
,
and t
i
-p
i
) in the triplet. Last, we make a stochastic
gradient step to minimize Equation 4 and update
E
n
at each step.
4.2 KB-QA using Embedding Models
Our goal for KB-QA is to translate a given NL-
question to a KB-query with the form <subject
entity, predicate, ?>, where ? denotes the an-
swer entity we are looking for. The decoding pro-
cess consists of two stages. The first stage in-
volves generating all possible KB-queries (K
q
) for
an NL-question q. We first extract n-gram fea-
tures (C
q
) from the NL-question q. Then for a
KB-query k
q
, we find all available entity types
(t
q
) of the identified subject entities (s
q
) using
the dictionary-based entity detection on the NL-
question q (all of spans can be candidate entities),
and assign all items of predicate set (P) as the can-
didate predicates (p
q
). Like the training triplets,
647
q where is the city of david?
ˆ
k(q) [The City of David, contained by, ?]
C
q
n-grams of “where is ?entity? ?”
t
q
location
p
q
contained by
Table 1: The corresponding KB-query
ˆ
k(q) for a
NL-question q and its decoding triplet w
q
.
we also represent the above features as the triplet
form w
q
i
= [C
q
i
, t
q
i
, p
q
i
] which is directly linked to
a KB-query k
q
i
= [s
q
i
, p
q
i
, ?]. The second stage
involves ranking candidate KB-queries based on
the similarity scores between the following paired
relationships from the triplet w
q
i
: R
q
i
= {C
q
i
-t
q
i
,
C
q
i
-p
q
i
, t
q
i
-p
q
i
}. Unlike in the training step, the sim-
ilarities of C
q
i
-t
q
i
and C
q
i
-p
q
i
are computed by sum-
mation of all pairwise elements (each context em-
bedding E(c), not E(C), with each paired E(t) or
E(p)) for a more precise measurement. Since sim-
ilarites of R
q
are calculated on different scales, we
normalize each value using Z-score (Z(x) =
x?µ
?
)
(Kreyszig, 1979). The final score is measured by:
Sim
q2k
(q, k
q
) =
?
r?R
q
Z(Sim(r)) (5)
Then, given any NL-question q, we can predict the
corresponding KB-query
ˆ
k(q):
ˆ
k(q) = argmax
k?K
q
Sim
q2k
(q, k) (6)
Last, we can retrieve an answer from the KB using
a structured query
ˆ
k(q). Table 1 shows an example
of our decoding process.
Multi-related Question Some questions in-
clude two-subject entities, both of which are cru-
cial to understanding the question. For the ques-
tion who plays gandalf in the lord of the rings?
Gandalf (character) and The Lord Of The
Rings (film) are explicit entities that should be
joined to a pair of the two entities (implicit entity).
More precisely, the two entities can be combined
into one concatenated entity (character-in-film)
using our manual rule, which compares the possi-
ble pairs of entity types in the question with the
list of pre-defined entity type pairs that can be
merged into a concatenated entity. Our solution
enables a multi-related question to be transformed
to a single-related question which can be directly
translated to a KB-query. Then, the two entity
# Entries Accuracy
MP pairs 291,585 89%
QP pairs 4,764 98%
Table 2: Statistics of NLE-KB pairs
mentions are replaced with the symbol ?entity?
(who play ?entity? in ?entity? ?). We re-
gard the result of this transformation as one of the
candidate KB-queries in the decoding step.
5 Experiments
Experimental Setting We first performed pre-
processing, including lowercase transformation,
lemmatization and tokenization, on NLE-KB pairs
and evaluation data. We used 71,310 n-grams
(uni-, bi-, tri-), 990 entity types, and 660 predi-
cates as relational components shown in Section
3.1. The sum of these three numbers (72,960)
equals the size of the embeddings we are going
to learn. In Table 2, we evaluated the quality of
NLE-KB pairs (MP and QP) described in Sec-
tion 3.2. We can see that the quality ofQP pairs is
good, mainly due to human efforts. Also, we ob-
tained MP pairs that have an acceptable quality
using threshold 3.0 for Equation 1, which lever-
ages the redundancy information in the large-scale
data (WIKIPEDIA). For our embedding learning,
we set the embedding dimension n to 100, the
learning rate (?) for SGD to 0.0001, and the it-
eration number to 30. To make the decoding
procedure computable, we kept only the popular
KB-entity in the dictionary to map different entity
mentions into a KB-entity.
We used two publicly released data sets for QA
evaluations: Free917 (Cai and Yates, 2013) in-
cludes the annotated lambda calculus forms for
each question, and covers 81 domains and 635
Freebase relations; WebQ. (Berant et al., 2013)
provides 5,810 question-answer pairs that are built
by collecting common questions from Web-query
logs and by manually labeling answers. We used
the previous three approaches (Cai and Yates,
2013; Berant et al., 2013; Bao et al., 2014) as our
baselines.
Experimental Results Table 3 reports the over-
all performances of our proposed KB-QA method
on the two evaluation data sets and compares them
with those of the three baselines. Note that we
did not re-implement the baseline systems, but just
borrowed the evaluation results reported in their
648
Methods Free917 WebQ.
Cai and Yates (2013) 59.00% N/A
Berant et al. (2013) 62.00% 31.40%
Bao et al. (2014) N/A 37.50%
Our method 71.38% 41.34%
Table 3: Accuracy on the evaluation data
Methods Free917 WebQ.
Our method 71.38% 41.34%
w/o T -P 70.65% 40.55%
w/o C-T 67.03% 38.44%
w/o C-P 31.16% 19.24%
Table 4: Ablation of the relationship types
papers. Although the KB used by our system is
much larger than FREEBASE, we still think that
the experimental results are directly comparable
because we disallow all the entities that are not in-
cluded in FREEBASE.
Table 3 shows that our method outperforms the
baselines on both Free917 and WebQ. data sets.
We think that using the low-dimensional embed-
dings of n-grams rather than the lexical triggers
greatly improves the coverage issue. Unlike the
previous methods which perform entity disam-
biguation and predicate prediction separately, our
method jointly performs these two tasks. More
precisely, we consider the relationships C-T and
C-P simultaneously to rank candidate KB-queries.
In Table 1, the most independent NER in KB-QA
systems may detect David as the subject entity,
but our joint approach can predict the appropriate
subject entity The City of David by leveraging
not only the relationships with other components
but also other relationships at once. The syntax-
based (grammar formalism) approaches such as
Combinatory Categorial Grammar (CCG) may ex-
perience errors if a question has grammatical er-
rors. However, our bag-of-words model-based ap-
proach can handle any question as long as the
question contains keywords that can help in un-
derstanding it.
Table 4 shows the contributions of the relation-
ships (R) between relational components C, T ,
and P . For each row, we remove the similarity
from each of the relationship types described in
Section 3.1. We can see that the C-P relationship
plays a crucial role in translating NL-questions to
KB-queries, while the other two relationships are
slightly helpful.
Result Analysis Since the majority of questions
in WebQ. tend to be more natural and diverse, our
method cannot find the correct answers to many
questions. The errors can be caused by any of
the following reasons. First, some NLEs cannot
be easily linked to existing KB-predicates, mak-
ing it difficult to find the answer entity. Second,
some entities can be mentioned in several different
ways, e.g., nickname (shaq?Shaquille O’neal)
and family name (hitler?Adolf Hitler). Third, in
terms of KB coverage issues, we cannot detect the
entities that are unpopular. Last, feature represen-
tation for a question can fail when the question
consists of rare n-grams.
The two training sets shown in Section 3.2 are
complementary: QP pairs provide more oppor-
tunities for us to learn the semantic associations
between interrogative words and predicates. Such
resources are especially important for understand-
ing NL-questions, as most of them start with such
5W1H words; on the other hand, MP pairs en-
rich the semantic associations between context in-
formation (n-gram features) and predicates.
6 Conclusion
In this paper, we propose a novel method that
transforms NL-questions into their corresponding
logical forms using joint relational embeddings.
We also built a simple and robust KB-QA system
based on only the learned embeddings. Such em-
beddings learn the semantic associations between
natural language statements and KB-properties
from NLE-KB pairs that are automatically ex-
tracted from English WIKIPEDIA using KB-triples
with weak supervision. Then, we generate all pos-
sible structured queries derived from latent logical
features of the given NL-question, and rank them
based on the similarity scores between those re-
lational attributes. The experimental results show
that our method outperforms the latest three KB-
QA baseline systems. For our future work, we will
build concept-level context embeddings by lever-
aging latent meanings of NLEs rather than their
surface n-grams with the aligned logical features
on KB.
Acknowledgement This research was sup-
ported by the Next-Generation Information
Computing Development Program through the
National Research Foundation of Korea (NRF)
funded by the Ministry of Science, ICT & Future
Planning (NRF-2012M3C4A7033344).
649
References
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 967–976. Association for Computa-
tional Linguistics.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, pages 1415–1425. Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In In Proceedings of 15th International
Conference on Artificial Intelligence and Statistics.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Association for Computational Lin-
guistics (ACL), pages 423–433. The Association for
Computer Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22–29, March.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Association for Computational Lin-
guistics (ACL), pages 1608–1618. The Association
for Computer Linguistics.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In 1st Workshop
on Web Scale Knowledge Extraction @ ISWC 2011.
E. Kreyszig. 1979. Advanced Engineering Mathemat-
ics. Wiley.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 754–765, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545–1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 590–599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
RaymondJ. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311–324. Springer Berlin Heidelberg.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1135–1145, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine
Learning, 81(1):21–35, October.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366–1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 956–966, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI’96, pages 1050–1055. AAAI Press.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658–666. AUAI Press.
650
