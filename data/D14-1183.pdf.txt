Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758–1763,
October 25-29, 2014, Doha, Qatar.
c©2014 Association for Computational Linguistics
Large-scale Reordering Model for Statistical Machine Translation
using Dual Multinomial Logistic Regression
Abdullah Alrajeh
ab
and Mahesan Niranjan
b
a
Computer Research Institute, King Abdulaziz City for Science and Technology (KACST)
Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
b
School of Electronics and Computer Science, University of Southampton
Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk
Abstract
Phrase reordering is a challenge for statis-
tical machine translation systems. Posing
phrase movements as a prediction prob-
lem using contextual features modeled by
maximum entropy-based classifier is su-
perior to the commonly used lexicalized
reordering model. However, Training
this discriminative model using large-scale
parallel corpus might be computationally
expensive. In this paper, we explore recent
advancements in solving large-scale clas-
sification problems. Using the dual prob-
lem to multinomial logistic regression, we
managed to shrink the training data while
iterating and produce significant saving in
computation and memory while preserv-
ing the accuracy.
1 Introduction
Phrase reordering is a common problem when
translating between two grammatically different
languages. Analogous to speech recognition sys-
tems, statistical machine translation (SMT) sys-
tems relied on language models to produce more
fluent output. While early work penalized phrase
movements without considering reorderings aris-
ing from vastly differing grammatical structures
across language pairs like Arabic-English (Koehn,
2004a), many researchers considered lexicalized
reordering models that attempted to learn orienta-
tion based on the training corpus (Tillmann, 2004;
Kumar and Byrne, 2005; Koehn et al., 2005).
Building on this, some researchers have bor-
rowed powerful ideas from the machine learning
literature, to pose the phrase movement problem
as a prediction problem using contextual input fea-
tures whose importance is modeled as weights of
a linear classifier trained by entropic criteria. The
approach (so called maximum entropy classifier
or simply MaxEnt) is a popular choice (Zens and
Ney, 2006; Xiong et al., 2006; Nguyen et al.,
2009; Xiang et al., 2011). Max-margin structure
classifiers were also proposed (Ni et al., 2011).
Alternatively, Cherry (2013) proposed recently us-
ing sparse features optimize the translation quality
with the decoder instead of training a classifier in-
dependently.
While large-scale parallel corpus is advanta-
geous for improving such reordering model, this
improvement comes at a price of computational
complexity. This issue is particularly pronounced
when discriminative models are considered such
as maximum entropy-based model due to the re-
quired iterative learning.
Advancements in solving large-scale classifica-
tion problems have been shown to be effective
such as dual coordinate descent method for linear
support vector machines (Hsieh et al., 2008). Sim-
ilarly, Yu et al. (2011) proposed a two-level dual
coordinate descent method for maximum entropy
classifier.
In this work we explore the dual problem to
multinomial logistic regression for building large-
scale reordering model (section 3). One of the
main advantages of solving the dual problem is
providing a mechanism to shrink the training data
which is a serious issue in building such large-
scale system. We present empirical results com-
paring between the primal and the dual problems
(section 4). Our approach is shown to be fast and
memory-efficient.
2 Baseline System
In statistical machine translation, the most likely
translation e
best
of an input sentence f can be
found by maximizing the probability p(e|f), as
follows:
e
best
= arg max
e
p(e|f). (1)
1758
A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(e|f) (Papineni et al., 1998; Och
and Ney, 2002):
e
best
= arg max
e
n
?
i=1
?
i
h
i
(f , e) (2)
where the feature h
i
(f , e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features.
The language model, which ensures fluent
translation, plays an important role in reordering;
however, it has a bias towards short translations
(Koehn, 2010). Therefore, a need for developing a
specific model for the reordering problem.
2.1 Lexicalized Reordering Model
Adding a lexicalized reordering model consis-
tently improved the translation quality for sev-
eral language pairs (Koehn et al., 2005). Re-
ordering modeling involves formulating phrase
movements as a classification problem where each
phrase position considered as a class (Tillmann,
2004). Some researchers classified phrase move-
ments into three categories (monotone, swap, and
discontinuous) but the classes can be extended to
any arbitrary number (Koehn and Monz, 2005). In
general, the distribution of phrase orientation is:
p(o
k
|
¯
f
i
, e¯
i
) =
1
Z
h(
¯
f
i
, e¯
i
, o
k
) . (3)
This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(
¯
f
i
, e¯
i
) with such an orientation (o
k
) is counted
and then normalized to yield the probability as fol-
lows:
p(o
k
|
¯
f
i
, e¯
i
) =
count(
¯
f
i
, e¯
i
, o
k
)
?
o
count(
¯
f
i
, e¯
i
, o)
. (4)
The orientation of a current phrase pair is de-
fined with respect to the previous target phrase.
Galley and Manning (2008) extended the model to
tackle long-distance reorderings. Their hierarchi-
cal model enables phrase movements that are more
complex than swaps between adjacent phrases.
3 Multinomial Logistic Regression
Multinomial logistic regression (MLR), also
known as maximum entropy classifier (Zens and
Ney, 2006), is a probabilistic model for the multi-
class problem. The class probability is given by:
p(o
k
|
¯
f
i
, e¯
i
) =
exp(w
>
k
?(
¯
f
i
, e¯
i
))
?
k
?
exp(w
>
k
?
?(
¯
f
i
, e¯
i
))
, (5)
where ?(
¯
f
i
, e¯
i
) is the feature vector of the i-th
phrase pair. An equivalent notation to w
>
k
?(
¯
f
i
, e¯
i
)
is w
>
f(?(
¯
f
i
, e¯
i
), o
k
) where w is a long vector
composed of all classes parameters (i.e. w
>
=
[w
>
1
. . .w
>
K
] ) and f(., .) is a joint feature vec-
tor decomposed via the orthogonal feature rep-
resentation (Rousu et al., 2006). This repre-
sentation simply means there is no crosstalk be-
tween two different feature vectors. For example,
f(?(
¯
f
i
, e¯
i
), o
1
)
>
= [?(
¯
f
i
, e¯
i
)
>
0 . . . 0].
The model’s parameters can be estimated by
minimizing the following regularized negative
log-likelihood P(w) as follows (Bishop, 2006):
min
w
1
2?
2
K
?
k=1
?w
k
?
2
?
N
?
i=1
K
?
k=1
p˜
ik
log p(o
k
|
¯
f
i
, e¯
i
)
(6)
Here ? is a penalty parameter and p˜ is the em-
pirical distribution where p˜
ik
equals zero for all
o
k
6= o
i
.
Solving the primal optimization problem (6) us-
ing the gradient:
?P(w)
?w
k
=
w
k
?
2
?
N
?
i=1
(
p˜
ik
? p(o
k
|
¯
f
i
, e¯
i
)
)
?(
¯
f
i
, e¯
i
),
(7)
do not constitute a closed-form solution. In our
experiments, we used stochastic gradient decent
method (i.e. online learning) to estimate w which
is shown to be fast and effictive for large-scale
problems (Bottou, 2010). The method approxi-
mates (7) by a gradient at a single randomly picked
phrase pair. The update rule is:
w
?
k
= w
k
? ?
i
?
k
P
i
(w), (8)
where ?
i
is a positive learning rate.
1759
3.1 The Dual Problem
Lebanon and Lafferty (2002) derived an equiva-
lent dual problem to (6). Introducing Lagrange
multipliers ?, the dual becomes
min
w
1
2?
2
K
?
k=1
?w
k
(?)?
2
+
N
?
i=1
K
?
k=1
?
ik
log?
ik
,
s.t.
K
?
k=1
?
ik
= 1 and ?
ik
? 0 ,?i, k, (9)
where
w
k
(?) = ?
2
N
?
i=1
(p˜
ik
? ?
ik
)?(
¯
f
i
, e¯
i
) (10)
As mentioned in the introduction, Yu et al.
(2011) proposed a two-level dual coordinate de-
scent method to minimize D(?) in (9) but it has
some numerical difficulties. Collins et al. (2008)
proposed simple exponentiated gradient (EG) al-
gorithm for Conditional Random Feild (CRF). The
algorithm is applicable to our problem, a special
case of CRF. The rule update is:
?
?
ik
=
?
ik
exp(??
i
?
ik
D(?))
?
k
?
?
ik
?
exp(??
i
?
ik
?
D(?))
(11)
where
?
ik
D(?) ?
?D(?)
??
ik
= 1 + log?
ik
+
(
w
y
(?)
>
?(
¯
f
i
, e¯
i
)?w
k
(?)
>
?(
¯
f
i
, e¯
i
)
)
.
(12)
Here y represents the true class (i.e. o
y
= o
i
).
To improve the convergence, ?
i
is adaptively ad-
justed for each example. If the objective function
(9) did not decrease, ?
i
is halved for number of tri-
als (Collins et al., 2008). Calculating the function
difference below is the main cost in EG algorithm,
D(?
?
)?D(?) =
K
?
k=1
(
?
?
ik
log?
?
ik
? ?
ik
log?
ik
)
?
K
?
k=1
(?
?
ik
? ?
ik
)w
k
(?)
>
?(
¯
f
i
, e¯
i
)
+
?
2
2
??(
¯
f
i
, e¯
i
)?
2
K
?
k=1
(?
?
ik
? ?
ik
)
2
. (13)
Clearly, the cost is affordable because w
k
(?) is
maintained throughout the algorithm as follows:
w
k
(?
?
) = w
k
(?)??
2
(?
?
ik
??
ik
)?(
¯
f
i
, e¯
i
) (14)
Following Yu et al. (2011), we initialize ?
ik
as
follows:
?
ik
=
{
(1? ) if o
k
= o
i
;

K?1
else.
(15)
where  is a small positive value. This is because
the objective function (9) is not well defined at
?
ik
= 0 due to the logarithm appearance.
Finally, the optimal dual variables are achieved
when the following condition is satisfied for all ex-
amples (Yu et al., 2011):
max
k
?
ik
D(?) = min
k
?
ik
D(?) (16)
This condition is the key to accelerate EG al-
gorithm. Unlike the primal problem (6), the dual
variables ?
ik
are associated with each example
(i.e. phrase pair) therefore a training example can
be disregarded once its optimal dual variables ob-
tained. More data shrinking can be achieved by
tolerating a small difference between the two val-
ues in (16). Algorithm 1 presents the overall pro-
cedure (shrinking step is from line 6 to 9).
Algorithm 1 Shrinking stochastic exponentiated
gradient method for training the dual problem
Require: training set S = {?(
¯
f
i
, e¯
i
), o
i
}
N
i=1
1: Given ? and the corresponding w(?)
2: repeat
3: Randomly pick i from S
4: Claculate?
ik
D(?) ?k by (12)
5: v
i
= max
k
?
ik
D(?)?min
k
?
ik
D(?)
6: if v
i
?  then
7: Remove i from S
8: Continue from line 3
9: end if
10: ? = 0.5
11: for t = 1 to maxTrial do
12: Calculate ?
?
ik
?k by (11)
13: if D(?
?
)?D(?) ? 0 then
14: Update ? and w(?) by (14)
15: Break
16: end if
17: ? = 0.5 ?
18: end for
19: until v
i
?  ?i
1760
4 Experiments
We used MultiUN which is a large-scale parallel
corpus extracted from the United Nations website
(Eisele and Chen, 2010). We have used Arabic
and English portion of MultiUN where the English
side is about 300 million words.
We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.
As shown in Table 1, each extracted phrase pair
is represented by linguistic features as follows:
• Aligned source and target words in a phrase
pair. Each word alignment is a feature.
• Words within a window around the source
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.
The extracted phrase pairs after filtering are
47,227,789. The features that occur more than 10
times are 670,154.
Sentence pair:
f : f
1
f
2
1
f
3
f
4
f
5
2
f
6
3
.
e : e
1
1
e
2
e
3
3
e
4
e
5
2
.
Extracted phrase pairs (
¯
f , e¯) :
¯
f
i
||| e¯
i
||| o
i
||| alignment ||| context
f
1
f
2
||| e
1
||| mono ||| 0-0 1-0 ||| f
3
f
3
f
4
f
5
||| e
4
e
5
||| swap ||| 0-1 2-0 ||| f
2
f
6
f
6
||| e
2
e
3
||| other ||| 0-0 0-1 ||| f
5
All linguistic features:
1. f
1
&e
1
2. f
2
&e
1
3. f
3
4. f
3
&e
5
5. f
5
&e
4
6. f
2
7. f
6
8. f
6
&e
2
9. f
6
&e
3
10. f
5
Bag-of-words representation:
a phrase pair is represented as a vector where each feature
is a discrete number (0=not exist).
?(
¯
f
i
, e¯
i
) 1 2 3 4 5 6 7 8 9 10
?(
¯
f
1
, e¯
1
) = 1 1 1 0 0 0 0 0 0 0
?(
¯
f
2
, e¯
2
) = 0 0 0 1 1 1 1 0 0 0
?(
¯
f
3
, e¯
3
) = 0 0 0 0 0 0 1 1 1 1
Table 1: A generic example of the process of
phrase pair extraction and representation.
4.1 Classification
We trained our reordering models by both primal
and dual classifiers for 100 iterations. For the dual
MLR, different shrinking levels have been tried by
varying the parameter () in Algorithm 1. Table 2
reports the training time and classification error
rate of these models.
Training the dual MLR with moderate shrinking
level (i.e.  = 0.1) is almost four times faster than
training the primal one. Choosing larger value for
() leads to faster training but might harm the per-
formance as shown below.
Classifier Training Time Error Rate
Primal MLR 1 hour 9 mins 17.81%
Dual MLR :0.1 18 minutes 17.95%
Dual MLR :1.0 13 minutes 21.13%
Dual MLR :0.01 22 minutes 17.89%
Table 2: Performance of the primal and dual MLR
based on held-out data.
Figure 1 shows the percentage of active set dur-
ing training dual MLR with various shrinking lev-
els. Interestingly, the dual MLR could disregard
more than 99% of the data after a couple of iter-
ations. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial.
2 4 6 8 10 12 14 16 18 200
10
20
30
40
50
60
70
80
90
100
Training iteration
Per
cen
tag
e o
f ac
tive
 ph
ras
e p
airs
 
 
? = 0.1
? = 1.0
? = 0.01
Figure 1: Percentage of active set in dual MLR.
As the data size decreases, each iteration takes far
less computation time (see Table 2 for total time).
1761
4.2 Translation
We used the Moses toolkit (Koehn et al., 2007)
with its default settings to build three phrase-based
translation systems. They differ in how their re-
ordering models were estimated. The language
model is a 5-gram with interpolation and Kneser-
Ney smoothing (Kneser and Ney, 1995). We tuned
the system by using MERT technique (Och, 2003).
As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and MT08 where the En-
glish sides are 35,481 words (1056 sentences) and
116,840 words (3252 sentences), respectively. Ta-
ble 3 shows the BLEU scores for the translation
systems. We also computed statistical significance
for the models using the paired bootstrap resam-
pling method (Koehn, 2004b).
Translation System MT06 MT08
Baseline + Lexical. model 30.86 34.22
Baseline + Primal MLR 31.37* 34.85*
Baseline + Dual MLR :0.1 31.36* 34.87*
Table 3: BLEU scores for Arabic-English transla-
tion systems with different reordering models (*:
better than the lexicalized model with at least 95%
statistical significance).
5 Conclusion
In training such system with large data sizes and
big dimensionality, computational complexity be-
come a serious issue. In SMT, maximum entropy-
based reordering model is often introduced as a
better alternative to the commonly used lexical-
ized one. However, training this discriminative
model using large-scale corpus might be compu-
tationally expensive due to the iterative learning.
In this paper, we propose training the model
using the dual MLR with shrinking method. It
is almost four times faster than the primal MLR
(also know as MaxEnt) and much more memory-
efficient. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial. The
proposed method is also useful for many classi-
fication problems in natural language processing
that require large-scale data.
References
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secau-
cus, NJ, USA.
L´eon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Yves Lecheval-
lier and Gilbert Saporta, editors, Proceedings of
the 19th International Conference on Computa-
tional Statistics (COMPSTAT’2010), pages 177–
187, Paris, France, August. Springer.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22–
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. Journal
of Machine Learning Research, 9:1775–1822, June.
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868–2872. Euro-
pean Language Resources Association (ELRA), 5.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848–856, Hawaii, October. Association
for Computational Linguistics.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear svm. In Proceedings of the 25th International
Conference on Machine Learning, ICML ’08, pages
408–415.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 181–184.
Philipp Koehn and Christof Monz. 2005. Shared
task: Statistical machine translation between euro-
pean languages. In Proceedings of ACL Workshop
on Building and Using Parallel Texts, pages 119–
124. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
1762
for the 2005 IWSLT speech translation evaluation.
In Proceedings of International Workshop on Spo-
ken Language Translation, Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ond?rej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 177–180.
Philipp Koehn. 2004a. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA), pages 115—124, Washington DC.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
161–168, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Guy Lebanon and John D. Lafferty. 2002. Boosting
and maximum likelihood for exponential models. In
T.G. Dietterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Processing
Systems 14, pages 447–454. MIT Press.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lex-
icalized hierarchical reordering model using maxi-
mum entropy. In Proceedings of the Twelfth Ma-
chine Translation Summit (MT Summit XII). Inter-
national Association for Machine Translation.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2011. Exploitation of machine
learning techniques in modelling phrase movements
for machine translation. Journal of Machine Learn-
ing Research, 12:1–30, February.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proceedings
of ICASSP, pages 189–192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning of
hierarchical multilabel classification models. Jour-
nal of Machine Learning Research, pages 1601–
1626.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL: Short Papers, pages 101–
104.
Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011.
Improving reordering for statistical machine transla-
tion with smoothed priors and syntactic features. In
Proceedings of SSST-5, Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
pages 61–69, Portland, Oregon, USA. Association
for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL,
pages 521–528, Sydney, July. Association for Com-
putational Linguistics.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41–75, October.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 55–63, New York
City, June. Association for Computational Linguis-
tics.
1763
