Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Fast Joint Compression and Summarization via Graph Cuts
Xian Qian and Yang Liu
The University of Texas at Dallas
800 W. Campbell Rd., Richardson, TX, USA
{qx,yangl}@hlt.utdallas.edu
Abstract
Extractive summarization typically uses sen-
tences as summarization units. In contrast,
joint compression and summarization can use
smaller units such as words and phrases, re-
sulting in summaries containing more infor-
mation. The goal of compressive summariza-
tion is to find a subset of words that max-
imize the total score of concepts and cut-
ting dependency arcs under the grammar con-
straints and summary length constraint. We
propose an efficient decoding algorithm for
fast compressive summarization using graph
cuts. Our approach first relaxes the length con-
straint using Lagrangian relaxation. Then we
propose to bound the relaxed objective func-
tion by the supermodular binary quadratic pro-
gramming problem, which can be solved ef-
ficiently using graph max-flow/min-cut. S-
ince finding the tightest lower bound suffers
from local optimality, we use convex relax-
ation for initialization. Experimental results
on TAC2008 dataset demonstrate our method
achieves competitive ROUGE score and has
good readability, while is much faster than the
integer linear programming (ILP) method.
1 Introduction
Automatic multi-document summarization helps
readers get the most important information from
large amounts of texts. Summarization techniques
can be roughly divided into two categories: extrac-
tive and abstractive. Extractive summarization casts
the summarization task as a sentence selection prob-
lem: identifying important summary sentences from
one or multiple documents. Many methods have
been developed in the past decades, including super-
vised approaches that use classifiers to predict sum-
mary sentences, graph based approaches to rank the
sentences, and recent global optimization methods
such as integer linear programming (Gillick et al.,
2008) (ILP) and submodular maximization methods
(Lin and Bilmes, 2011). Though extractive summa-
rization is popular because of its simplicity and high
readability, it has limitations in that it selects each
sentence as a whole, and thus may miss informative
partial sentences.
To improve the informativeness, joint com-
pression and summarization was proposed (Berg-
Kirkpatrick et al., 2011), which uses words as sum-
marization units, unlike extractive summarization
where each sentence is a basic undecomposable u-
nit. To achieve better readability, manually defined
grammar constraints or automatically learned mod-
els based on syntax trees are added during the sum-
marization process. Up to now, the state of the art
compressive systems are based on integer linear pro-
gramming (ILP). Because ILP suffers from expo-
nential complexity, word-based compression sum-
marization is an order of magnitude slower than
sentence-based extraction.
One common way to solve an ILP problem is to
use its LP relaxation and round the results. How-
ever Berg-Kirkpatrick et al. (2011) found that LP
relaxation gave poor results, finding unacceptably
suboptimal solutions. For speedup, they proposed a
two stage method where they performed some sen-
tence selection in the first step to reduce the number
of candidates. Despite their empirical success, such
1492
a pruning approach has its inherent problem in that
it may eliminate correct sentences in the first step.
Recently, Almeida and Martins (2013) proposed a
fast joint decoding algorithm based on dual decom-
position. For fast convergence, they added quadratic
penalty terms to alleviate the learning rate problem.
In this paper, we propose an efficient decoding al-
gorithm for fast ILP based compressive summariza-
tion using graph cuts. Our assumption is that all con-
cepts are word n-grams and non-negatively scored.
The rationale for the non-negativity assumption is s-
traightforward: the score of a concept reflects its in-
formativeness, hence should be non-negative. Given
a set of documents, each word is associated with a
binary variable, indicating whether the word is se-
lected in the summary. Our idea is to approximate
the ILP as a binary quadratic programming problem
where coefficients of all quadratic terms are non-
negative. It is well known that such binary quadrat-
ic function is supermodular, and its maximum can
be solved efficiently using graph max-flow/min-cut.
Hence the key is to find the coefficients of the super-
modular binary quadratic function (SBQF) so that
its maximum is close to the optimal ILP objective
function. Our solution consists of 3 steps. First,
we show that the subtree deletion model and gram-
mar constraints can be eliminated by adding SBQF-
s to the objective function. Second, we relax the
summary length constraint using Lagrangian relax-
ation. Third, we propose a family of SBQFs that
are lower bounds of the ILP objective function. S-
ince finding the tightest lower bound suffers from
local optimality, we choose to use convex relaxation
for initialization. To demonstrate our technique, we
conduct experiments on Text Analysis Conference
(TAC) datasets using the same train/test splits as pre-
vious work (Berg-Kirkpatrick et al., 2011). We com-
pare our approach with the state-of-the-art ILP based
approach in terms of summary quality (ROUGE s-
cores and sentence quality) and speed. Experimen-
tal results show that our proposed method achieves
competitive performance with ILP, while about 100
times faster.
2 Compressive Summarization
2.1 Extractive Summarization
As our method is an approximation of ILP based
method, we first briefly review the ILP based extrac-
tive summarization and compressive summarization.
Gillick and Favre (2009) introduced the concept-
based ILP for summarization. A concept is a basic
semantic unit. They used word bigrams as such lan-
guage concepts. Their system achieved the highest
ROUGE score on the TAC 2009 evaluation. This
approach selects sentences so that the total score
of language concepts appearing in the summary is
maximized. The association between the language
concepts and sentences serves as the constraints, in
addition to the summary length constraint.
Formally, given a set of sentences S = {sn}Nn=1,
extractive summarization can be represented by a bi-
nary vector y, where yn indicates whether sentence
sn is selected. Let C = {c1, . . . cJ} denote the set
of concepts in S, e.g., word bigrams (Gillick and
Favre, 2009). Each concept cj is associated with a
given score wj and a binary variable vj indicating
if cj is selected in the summary. Let njk denote the
index of the sentence containing the kth occurrence
of concept cj , and ln denote the length of sentence
sn. The ILP based extractive summarization system
can be formulated as below:
max
y,v
J
?
j=1
wjvj
s.t. vj =
?
k
ynjk 1 ? j ? J (1)
N
?
i=1
ynln ? L
v,y are binary
The first constraint is imposed by the relation be-
tween concept selection and sentence selection: s-
electing a sentence leads to the selection of all the
concepts it contains, and selecting a concept only
happens when it is present in at least one of the se-
lected sentences. The second constraint is the sum-
mary length constraint.
As solving an ILP problem is generally NP-hard,
pre-pruning of candidate concepts and sentences is
necessary for efficient summarization. For exam-
1493
ple, the ICSI system (Gillick et al., 2008) removed
the sentences that are too short or have non-overlap
with the queries, and concepts with document fre-
quency less than 3, resulting in 95.8 sentences and
about 80 concepts per topic on the TAC2009 dataset.
Therefore the actual scale of ILP is rather small after
pruning (e.g., 176 variables and 372 constraints per
topic). Empirical studies showed that such small s-
cale ILP can be solved within a few seconds (Gillick
and Favre, 2009).
2.2 Compressive Summarization
The quality of sentence-based extractive summariza-
tion is limited by the informativeness of the orig-
inal sentences and the summary length constraint.
To remove the unimportant part from a long sen-
tence, sentence compression is proposed to generate
more informative summaries (Liu and Liu, 2009; Li
et al., 2013a). Recent studies show that joint sen-
tence compression and extraction, namely compres-
sive summarization, outperforms pipeline systems
that run extractive summarization on the compressed
sentences or compress selected summary sentences
(Martins and Smith, 2009; Berg-Kirkpatrick et al.,
2011; Chali and Hasan, 2012). In Berg-Kirkpatrick
et al. (2011), compressive summarization inte-
grates the concept model for extractive summariza-
tion (Gillick and Favre, 2009) and subtree deletion
model for sentence compression. The score of a
compressive summary consists of two parts, scores
of selected concepts, and scores of the broken arcs
in the dependency parse trees. The selected word-
s must satisfy the length constraint and grammar
constraints that include subtree constraint and some
manually defined hard constraints.
Formally, let x = x1 . . . xI denote the word se-
quence of documents, where s1 = x1, . . . xl1 corre-
sponds to the first sentence, s2 = xl1+1, . . . , xl1+l2
corresponds to the second sentence, and so on. A
compressive summary can be represented by a bi-
nary vector z, where zi indicates whether word xi
is selected in the summary. Let ahm denote the arc
xh ? xm in the dependency parse tree of the cor-
responding sentence containing words xh and xm,
and A = {ahm} denote the set of dependency arcs.
The subtree constraint ensures that word xm is se-
lected only if its head xh is selected. In order to
guarantee the readability, grammar constraints are
added to prohibit the breaks of some specific arc-
s. For example, Clarke and Lapata (2008) nev-
er deleted an arc whose dependency label is SUB,
OBJ, PMOD, SBAR or VC. In this paper, we use
B ? A to denote the set of these arcs that must
not be broken in summarization. We use ojk to de-
note the indices of words corresponding to the kth
occurrence of cj . For example, suppose the jth
concept European Union appears twice in the doc-
ument: x22x23 = x50x51 =European Union, then
oj1 = {22, 23}, oj2 = {50, 51}.
The compressive summarization model can be
formulated as an integer programming problem
max
z,v
J
?
j=1
wj · vj +
?
ahm?A
wahmzh(1? zm)
s.t. vj =
?
k
?
i?ojk
zi ?j
?
i
zi ? L
zh ? zm ?ahm ? A (2)
zh = zm ?ahm ? B
z,v are binary
According to the subtree deletion model, the score
of arc ahm is included if zh = 1 and zm = 0, which
can be formulated as wahm · zh(1 ? zm). The first
constraint is similar to that in extractive summariza-
tion, that is, a concept is selected if and only if any
of its occurrence is selected. The third and fourth
constraints are the subtree constraints and manual-
ly defined grammar constraints respectively. In the
rest of the paper, without loss of generality, we re-
move the fourth constraint by directly substituting
one variable for the other.
Finding the optimal summary is generally NP-
hard. Unlike extractive summarization where the s-
cale of the problem (the number of sentences and
concepts) is small, the number of variables in com-
pressive summarization is linear in the number of
words, which is usually thousands on the TAC
datasets. Hence solving such a problem using ILP
based decoding algorithms is not efficient especially
when the document set is large.
1494
3 Fast Decoding via Graph Cuts
In this section, we introduce our fast decoding al-
gorithm. We assume that all the concepts are word
n-grams, and their scores are non-negative. The non-
negativity assumption can reduce the computational
complexity, but is also reasonable: the score of a
concept denotes its informativeness, hence should
be non-negative. For example, Li et al. (2013b)
proposed to use the estimated normalized frequen-
cies of concepts as scores, which are essentially non-
negative. The basic idea of our method is to approx-
imate the above optimization problem (2) by the su-
permodular binary quadratic programming (SBQP)
problem:
max
z
?
i
?izi +
?
ij
?ijzizj
s.t. z is binary (3)
where ?ij ? 0. It is known that such a binary
quadratic function is supermodular, and its maxi-
mum can be solved efficiently using graph max-
flow/min-cut (Billionnet and Minoux, 1985; Kol-
mogorov and Zabih, 2004). Now the problem is to
find the optimal ?, ? for a good approximation.
3.1 Formulate Grammar Constraints and
Subtree Deletion Model by SBQF
We show that the subtree deletion model can be for-
mulated equivalently using SBQF. First, we can e-
liminate the constraint zh ? zm by adding a penalty
term to the objective function. That is,
max f(z)
s.t. zh ? zm
z is binary
is equivalent to
max f(z)??(1? zh)zm
s.t. z is binary
We can see that the penalty term??(1?zh)zm ex-
cludes zh = 0, zm = 1 from the feasible set, and
for zh ? zm, both problems have the same objective
function value. Hence the two problems are equiva-
lent. Notice that the coefficient of quadratic term in
??(1 ? zh)zm is positive, hence the penalty term
is supermodular.
Now we eliminate the third constraint in problem
(2) using the penalized objective function described
above. Note that the fourth constraint has been elim-
inated by variable substitution, we have
max
z,v
J
?
j=1
wj · vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
s.t. vj =
?
k
?
i?ojk
zi ?j (4)
?
i
zi ? L
z,v are binary
We can see that for each arc ahm, there must be a
positive quadratic term +?zhzm in the objective
function, which guarantees the supermodularity of
the objective function, no matter what wahm is.
3.2 Eliminate Length Constraint Using
Lagrangian Relaxation
Problem (4) is NP-hard, because for any feasible v,
it is a SBQP with a length constraint. Since size con-
strained minimum cut problem is generally NP-hard
(Nagano et al., 2011), Problem (4) can not be cast
as a SBQP as long as P ?= NP. One popular way to
deal with the size constrained optimization problem
is Lagrangian relaxation. We introduce Lagrangian
multiplier ? to the length constraint in Problem (4),
and get
min
?
max
z,v
J
?
j=1
wj · vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. vj =
?
k
?
i?ojk
zi ?j (5)
? ? 0
z,v are binary
We solve the relaxed problem iteratively. In each
iteration, we fix ? and solve the inner maximiza-
tion problem (details described below). The score of
1495
each word is penalized by ? — with larger ?, few-
er words are selected. Hence the summary length
can be adjusted by ?. The optimal ? can be found
using binary search. We maintain an upper bound
?max , and a lower bound ?min, which is initially 0.
In each iteration, we choose ? = 12(?max + ?min)
and search the optimal z. If the duality gap vanish-
es, i.e., ?(L ?
?
i zi) = 0 and
?
i zi ? L, then we
get the global solution of Problem (4). Otherwise,
if
?
i zi > L, then the current ? is too small, so we
set ?min = ?; otherwise, ? > 0 and
?
i zi < L,
we set ?max = ?. The search process terminates if
?max ? ?min is less than a predefined threshold.
3.3 Eliminate v Using Supermodular
Relaxation
Now we consider the inner maximization Problem
(5). It is still not a SBQP, since the objective func-
tion is not a linear function of zizj . We propose to
approximate the objective function using SBQP. Our
solution consists of two steps. First we relax the first
constraint of Problem (5) by bounding the objec-
tive function with a family of supermodular pseudo
boolean functions (Boros and Hammer, 2002). Sec-
ond we reformulate these pseudo boolean functions
equivalently as quadratic functions.
Similar to the bounding strategy in (Qian and Liu,
2013), we relax the logical disjunction by lineariza-
tion. Using the fact that for any binary vector a, we
have
?
ai = max
p??
?
i
piai
where ? denotes the probability simplex
? = {p|
?
k
pk = 1, pk ? 0}
We have
vj =
?
k
?
i?ojk
zi
= max
pj??
?
k
pjk
?
i?ojk
zi
Plug the equation above into the objective function
of Problem (5), we get the following optimization
problem
max
z,p
J
?
j=1
?
?
?
k
pjkwj
?
i?ojk
zi
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z is binary (6)
pj ? ? ?j
LetQ(p, z) denote the objective function of Prob-
lem (6). Given p, we can see that Q is a supermod-
ular pseudo boolean function because coefficients
of all non-linear terms are non-negative. Using the
fact that for any binary vector a = [a1, . . . ar]T ,
ai ? {0, 1}, 1 ? i ? r,
r
?
i=1
ai = max
b?{0,1}
( r
?
i=1
ai ? r + 1
)
b
(Freedman and Drineas, 2005), we get the following
equivalent optimization problem of Problem (6)
max
z,p,q
J
?
j=1
?
k
pjkwjqjk
?
?
?
i?ojk
zi ? |ojk|+ 1
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z,q are binary (7)
pj ? ? ?j
where |ojk| is the size of ojk.
Let R(z,p,q) denote the objective function of
Problem (7), to search the optimal point, we al-
ternatively update p and z,q. First we initialize
p = p(0). In each iteration, we first fix p. It is ob-
vious that Problem (7) is a SBQP, hence the optimal
z,q can be solved efficiently using max-flow/min-
cut. Then we fix z,q, and update p using projected
1496
subgradient. That is
pnewj = P?
(
pj +
?R
?pj
?
)
(8)
where ? > 0 is the step size in line search, and func-
tion P?(q) denotes the projection of q onto the fea-
sible set ?
P?(q) = min
p??
?p? q?2
which can be solved efficiently by sorting (Duchi et
al., 2008).
3.4 Initialize p Using Convex Relaxation
Since R is non-concave, searching its maximum us-
ing subgradient method suffers from local optimali-
ty. Though one can use techniques such as branch-
and-bound for exact inference (Qian and Liu, 2013;
Gormley and Eisner, 2013), here for fast decoding,
we use convex relaxation to choose a good seed
p(0). Recall that pjk denotes the percentage of the
kth occurrence contributing to cj . The larger pjk is,
the more likely the kth occurrence is selected. To
estimate such likelihood, we replace the binary con-
straint in extractive summarization (Problem (1)) by
0 ? y,v ? 1, since solving a relaxed LP is much
faster than ILP. Suppose y? is the optimal solution
for such a relaxed LP problem, we initialize p by
pjk =
y?njk
?
k y?njk
(9)
If for all k, y?njk = 0, then we initialize pjk using
uniform distribution
pjk =
1
|oj |
where |oj | is the frequency of cj .
3.5 Summary
For clarity, we summarize our decoding algorithm in
Algorithm 1. Initial ?max can be arbitrarily large. In
our experiments, we set ?max =
?
j wj , which em-
pirically guarantees the summary length
?
i zi ? L
when ? = ?max. The choice of the step size for
updating p is similar to the projected subgradien-
t method in dual decomposition (Koo et al., 2010).
Algorithm 1 Compressive Summarization via
Graph Cuts
Require: Scores of concepts {wj} and arcs {wahm},
max summary length L.
Ensure: Compressive summarization z?, where zi indi-
cates whether the ith word is selected.
Solve the relaxed LP of Problem (1) (replace the binary
constraint by 0 ? y,v ? 1) to get y.
Initialize p(0) using Eq (9).
Initialize sufficient large ?max, and ?min = 0
while ?max ? ?min > ? do
Set ? = 12 (?min + ?max)
Set p = p(0).
repeat
Fix p, solve Problem (7) to get z using max-
flow/min-cut.
Update p using Eq (8).
until convergence
if
?
i zi > L then
?min = ?
else if
?
i zi < L then
?max = ?
else
break
end if
end while
4 Features and Hard Constraints
We choose discriminative models to learn the scores
of concepts and arcs. For concept cj , its score is
wj = ?Tconceptfconcept(cj)
where fconcept(cj) is the feature vector of cj , and
?concept is the corresponding weight vector of fea-
ture fconcept(cj). Similarly, score wahm is defined
as
wahm = ?
T
arcfarc(ahm)
Though our algorithm can handle general word n-
gram concepts, we restrict the concepts to word bi-
grams, which have been widely used recently in the
sentence-based ILP extractive summarization sys-
tems. For a concept cj , we define the following
features, some of which have been used in previous
work (Brandow et al., 1995; Aker and Gaizauskas,
2009; Edmundson, 1969; Radev, 2001; Li et al.,
2013b). All of these features are non-negative.
• Term frequency: the frequency of cj in the giv-
en topic.
1497
• Stop word ratio: ratio of stop words in cj . The
value can be {0, 0.5, 1}.
• Similarity with topic title: the number of com-
mon words in these two strings, divided by the
length of the longer string.
• Document ratio: percentage of documents con-
taining cj .
• Sentence ratio: percentage of sentences con-
taining cj .
• Sentence-title similarity: word uni-
gram/bigrams cosine similarity between
the sentence containing cj and the topic title.
For concepts appearing in multiple sentences,
we choose the maximal similarity.
• Sentence-query similarity: word uni-
gram/bigram cosine similarity between
the sentence containing cj and the topic query
(concatenation of topic title and description).
For concepts appearing in multiple sentences,
we choose the maximal similarity.
• Sentence position: position of the sentence
containing cj in the document. For concepts
appearing in multiple sentences, we choose the
minimum.
• Sentence length: length of the sentence con-
taining cj . For concepts appearing in multiple
sentences, we choose the maximum.
• Paragraph starter: binary feature indicating
whether cj appears in the first sentence of a
paragraph.
For subtree deletion model, we define the follow-
ing features for arc ahm.
• POS tags of head word xh and child word xm
and their concatenations.
• Dependency label of arc ahm and its parent arc.
• Word xm if xm is a conjunction word or prepo-
sition word. Word xh if xm is a conjunction
word or preposition word.
• Binary feature indicating whether the modifier
xm is a temporal word such as Friday.
We also define some hard constraints for subtree
deletion to improve the readability of the generated
compressed sentences.
• C0 Arc ahm can be cut only if one of the two
conditions holds: (1) there is a comma, colon,
or semicolon between the head and the modifi-
er; (2) the modifier word is a preposition (POS
tag is IN) or a wh-word, such as what, who,
whose (corresponds to POS tag IN, WDT, WP,
WP$, WRB).
• C1 Arcs with dependency labels SUB, OBJ,
PRD, SBAR or VC can not be cut.
• C2 Arcs in set phrases like so far, more than,
according to can not be cut.
• C3 All arcs in coordinate structures can not be
cut, such as cats and dogs.
Note that compared with previous work, our com-
pression is more conservative. Constraint C0 al-
lows only a small portion of arcs to be cut. This
is based on our observation of the sentence com-
pression corpus: removing preposition phrases (PP)
or sub-clauses can greatly reduce the length of sen-
tence, while hurting the readability little. Cutting
other arcs like NMOD usually removes only one or
two words, and possibly affects the sentence’s read-
ability.
5 Experimental Results
5.1 Experimental Setup
Due to the lack of training data for compressive
summarization, we learn the subtree deletion mod-
el and the concept model separately. Specifically,
the sentence compression dataset (Clarke and La-
pata, 2008) (referred as CL08) is used for subtree
deletion model training (?arc). A sentence pair in
the corpus is kept for training the subtree deletion
model if the compressed sentence can be derived by
deleting subtrees from the parse tree of the origi-
nal sentence. There are 3, 178 out of 5, 739 such
pairs. The concept model (?concept) is learned from
the TAC2009 dataset. We create the oracle extrac-
tive summaries with the maximal bigram recall as
the reference summary. TAC2010 data is used as
1498
Corpus Sent. Words Topics
Train TAC2009 4, 216 117, 304 44
CL08 3, 178 52, 624 N/A
Develop TAC2010 2, 688 72, 609 46
Test TAC2008 4, 518 123, 946 48
Table 1: Corpus statistics. Training data consist of
two parts, TAC2009 for learning the concept mod-
el, CL08 (Clarke and Lapata, 2008) for learning the
subtree deletion model.
development set for various parameter tuning. Table
1 has the descriptions of all the data used.
We choose averaged perceptron for fast training.
The number of iterations is tuned on the develop-
ment data. Remind that our algorithm is based on the
assumption that scores of concepts are non-negative,
?j, wj ? 0. We assume that feature vector fconcept
is non-negative (e.g., term frequency, n-gram fea-
tures), then ?concept ? 0 is required to guarantee the
non-negativity of wj . Therefore, we project ?concept
onto the non-negative space after each iteration. S-
ince training is offline, we use ILP based exact in-
ference for accurate learning. 1
To control the contributions of the concept mod-
el and the subtree deletion model, we introduce a
parameter µ, and modify the original maximization
problem (Problem 2) to:
max
z,v
J
?
j=1
wj · vj + µ×
?
ahm?A
wahmzh(1? zm)
We tune µ on TAC2010 dataset. For max-flow/min-
cut, in our experiments, we implemented the im-
proved shortest augmented path (SAP) method (Ed-
monds and Karp, 1972).
For performance measure of the summaries, we
use both ROUGE and linguistic quality. ROUGE
has been widely used for summarization perfor-
mance and can measure the informativeness of the
summaries (content match between system and ref-
erence summaries). Since joint compression and
summarization tends to pick isolated words to max-
imize the information coverage in the system gener-
ated summaries, it may have poor readability. There-
fore we conduct human evaluation for the linguis-
1we choose the GLPK as our ILP solver, which is used in
(Berg-Kirkpatrick et al., 2011)
tic quality for various systems. The linguistic qual-
ity consists of two parts. One evaluates the gram-
mar quality within a sentence. Annotators marked
if a compressed sentence is grammatically correc-
t. Typical grammar errors include lack of verb or
subordinate clause. The other evaluates the coher-
ence between sentences, including the order of sen-
tences and irrelevant sentences. For compressive
summaries, we removed the sentences with gram-
mar errors when evaluating coherence. The overall
linguistic quality score is the combined score of the
percentage of grammatically correct sentences and
the correct ordering of the summary sentences. The
score is scaled and ranges from 1 (bad) to 10 (good).
5.2 Results on the Development Set
We conducted a series of experiments on the de-
velopment dataset to investigate the effect of the
non-negative score assumption, SBQP approxima-
tion, and initialization. First, we build a stan-
dard ILP based compressive summarizer without the
non-negative score assumption. We varied µ over
{2?4, 2?3, . . . 24} and selected the optimal µ = 2?2
according to both ROUGE-2 score and linguistic
quality. This interpolation weight is used in all the
other experiments.
To study the impact of the non-negative score as-
sumption, we build another summarizer by replac-
ing the concept model with the one trained under the
non-negative constraint. We also compared three d-
ifferent initialization strategies for p. The first one
is uniform initialization, i.e., pjk = 1|oj | . The second
one is a greedy approach, where extractive summa-
rization is obtained by greedy search (i.e., add the
top ranked sentence iteratively), then we use the cor-
responding y and Eq (9) to initialize p. The last one
is our convex relaxation method described in Sec-
tion 3.4.
Table 2 shows the comparison results. For com-
parison, we also include the sentence-based ILP ex-
tractive summarization results. We can see that the
impact of initial p is substantial. Using convex re-
laxation helps our method to survive from local opti-
mality. The non-negativity assumption has very lit-
tle effect on the standard compressive summariza-
tion (comparing the first two rows). This empir-
ical result demonstrates the appropriateness of the
assumption we use in our proposed method.
1499
System R-2 LQ
ILP (µ = 2?2) 11.22 6.3
ILP (Non Neg.) 11.18 6.4
Graph Cut (uniform) 9.54 5.9
Graph Cut (greedy) 10.13 6.2
Graph Cut (LP) 11.06 6.1
Sent Extractive 10.11 7.3
Table 2: Experimental results on developmen-
t dataset. R-2 and LQ are short for ROUGE-2 score
multiplied by 100, and linguistic quality respective-
ly.
5.3 Results on Test Dataset
Table 3 shows the summarization results for various
systems on the TAC2008 data set. We show both the
summarization performance and the speed2 of the
system. The presented systems include our graph-
cut based method, the ILP based compression and
summarization, and the sentence-based extractive
summarization. ILP 2-step refers to the 2-step fast
decoding strategy proposed by (Berg-Kirkpatrick et
al., 2011).
We also list the performance of some state-of-the-
art systems, including the two ICSI systems (Gillick
et al., 2008), the compressive summarization sys-
tem of Berg-Kirkpatrick et al. (2011) (GBK’11),
the multi-aspect ILP system of Woodsend and Lapa-
ta (2012)(WL’12) and the dual decomposition based
system (Almeida andMartins, 2013) (AM’13). Note
that for these referred systems since the linguistic
quality results are not comparable due to different
judgment methods. For our graph-cut based method,
to study the tradeoff between the readability of the
summary and the ROUGE scores, we present two
versions for this method: one uses all the constraints
(C0-C3), the other does not use C0.
We can see that our proposed method balanced
speed and quality. Compared with ILP, we achieved
competitive ROUGE scores, but with about 100x
speedup. Our method is also faster than the 2-step
ILP system. We also tried another state-of-the-art
LP solver, Gurobi version 5.53, it achieves 0.17 sec-
onds per topic, much faster than GLPK, but stil-
2For fair comparison, we only recode the running time for
decoding. Other time costs like feature extraction, IO opera-
tions are excluded.
3www.gurobi.com
System R-2 R-SU4 LQ sec.
Graph Cut 11.74 14.54 6.5 0.056
Graph Cut w/o C0 12.05 14.71 5.4 0.053
ILP 11.86 14.62 6.6 5.5
ILP (Non Neg.) 11.82 14.60 6.6 5.2
ILP (2-step) 11.72 14.49 6.5 1.1
Sent Extractive 11.06 13.93 7.1 0.13
ICSI-1 11.0 13.4 - 0.38†
ICSI-2 11.1 14.3 - -
BGK’11 11.70 14.38 6.5† -
WL’12 11.37 14.47 - -
AM’13 12.30+ 15.18+ 4.2† 0.41†
Table 3: Experimental results on TAC2008 dataset.
Columns 2-5 are scores of ROUGE-2, ROUGE-
SU4, linguistic quality, and speed (seconds per top-
ic). ROUGE-2 and ROUGE-SU4 scores are multi-
plied by 100. All the experiments are conducted on
the platform Intel Core i5-2500 CPU 3.30GHz. †
numbers are not directly comparable due to differ-
ent annotations or platforms. + extra resources are
used.
l slower than ours. Regarding the grammar con-
straints used in our system, from the two result-
s for our graph-cut based method, we can see that
adding constraint C0 significantly decreases the R-2
score but improves the language quality. This shows
that word-based joint compression and summariza-
tion can improve ROUGE score; however, we need
to keep in mind about linguistic quality and find a
tradeoff between the ROUGE score and the linguis-
tic quality. Almeida and Martins (2013) trained their
model on extra corpora using multi-task learning,
and achieved better results than ours. The results
of our system and theirs showed that Lagrangian re-
laxation based method combined with combinatorial
optimization algorithms such as dynamic program-
ming or minimum cut can exploit the inner structure
of problems and achieve significant speedup over
ILP.
Four example summaries produced by our system
are shown below. Words in gray are not selected in
the summary.
1500
India’s space agency is ready to send a man to space within sev-
en years if the government gives the nod, while preparations have
lready begun for the launch of an unmanned lunar mission, a top
official said. India will launch more missions to the moon if its
maiden unmanned spacecraft Chandrayaan-1, slated to be launched
by 2008, is successful a top space fficial said Tuesday. The Unit-
ed States, the European Space Agency, China, Japan and India are
all planning lunar missions during the ext decade.India is “a step
ahead” of China in satellite technology and can surpass Beijing
in space research by tapping the talent of its huge pool of young
scientists, India’s space research chief said Monday. The space
agencies of India and France signed an agreement on Friday to co-
operate in launching a satellite in four years that will help make
climate predictions more accurate. The Indian Space Research Or-
ganization (ISRO) has short-listed experiments from five nation-
s including the United States, Britain and Germany, for a slot on
India’s unmanned moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) reported Monday. A
three-member Afghan delegation is in Bangalore seeking help to
set up a high-tech telemedicine facility in 10 Afghan cities linked
via Indian satellites, Indo-Asian News Service reported Saturday.
A woman was killed in Mississippi when a tree crashed on her car,
becoming the 11th fatality blamed on the powerful Hurricane Kat-
rina that slammed the US Gulf coast after pounding Florida, local
TV reportedMonday. The bill for the Hurricane Katrina disaster ef-
fort has so far reached 2.87 billion dollars, federal officials said on
Tuesday. The official death toll from Hurricane Katrina has risen
to 118 people in and around the swamped city of New Orleans,
officials said Thursday. The Foreign Ministry on Friday reported
the first confirmed death of a Guatemalan due to Hurricane Kat-
rina in the United States. The Ugandan government has pledged
200,000 US dollars toward relief and rebuilding efforts in the after-
math of Hurricane Katrina, local press reported on Friday. Swiss
Reinsurance Co., the world’s second largest reinsurance company
on Monday doubled to 40 billion US dollars its initial estimate of
the global insured losses caused by Hurricane Katrina in the United
States.
The A380 ’superjumbo’, which will be presented to the world in
a lavish ceremony in southern France on Tuesday, will be prof-
itable from 2008, its maker Airbus told the French financial news-
paper La Tribune. The A380 will take over from the Boeing 747
as the biggest jet in the skies. An association of residents living n-
ear Paris’s Charles-de-Gaulle airport on Wednesday denounced the
noise pollution generated by the giant Airbus A380, after the new
airliner’s maiden flight. One problem that Airbus is encountering
with its new A380 is that the craft pushes the envelope on the max-
imum size of a commercial airplane. With a whisper more than a
roar, the largest passenger airliner ever built, the Airbus 380, took
off on its maiden flight Wednesday.
“When she came in, she was in good spirits,” a prison staffer told
the New York Daily News. Martha Stewart, the American celebrity
homemaker who had her own cooking and home improvement TV
show, reported to a federal prison in Alderson, West Virginia, on
Friday to serve a five-month sentence for lying about a stock sale.
Home fashion guru Martha Stewart said on Friday that she has ad-
justed to prison life and is keeping busy behind bars since reporting
a week ago to a federal penal camp in West Virginia, where she
is serving a five-month sentence for lying about a stock sale. The
lawyer said he did not know what she is writing, but Stewart has
suggested since her conviction that she might write a book about
her recent experience with the legal system. Walter Dellinger, the
lawyer leading the appeal, said on NBC’s “Today” that Stewart is
exploring “innovative ways to do microwave cooking” The lawyer
said he did not know with her fellow inmates. As Martha Stewart
arrives at the red-brick federal prison in Alderson, W. Va., on Fri-
day to begin a five-month sentence, the company she founded is
focused both on life without her and on life once she returns.
In most cases, the removed phrases do not hurt the
readability of the summaries. The errors are mainly
caused by the break of sub-clauses or main claus-
es that are separated by commas, for example, the
fourth sentence in the last summary, The lawyer said
he did not know what she is writing. The compressed
sentence is grammatically correct, but semantically
incomplete. Other errors are due to the lack of verb,
subject, or object, or incorrect removal of PP, such
as the last sentence of the last summary.
6 Conclusion
In this paper, we propose a fast decoding algorith-
m for compressive summarization using graph cuts.
Our idea is to approximate the original ILP prob-
lem using supermodular binary quadratic program-
ming (SBQP) problem. Under the assumption that
scores of concepts are non-negative, we eliminate
subtree constraints and grammar constraints, and
relax the length constraint and non-supermodular
part of the problem step by step. Our experimen-
tal results showed that the graph cut based method
achieved competitive performance compared to ILP,
while about 100 times faster.
There are several possibilities for further research
involving our graph cut algorithms. One idea is to
apply it to the language model based compression
method (Clarke and Lapata, 2008). The other is
to adapt it to social media text summarization task,
where text is much more noisy. As graph cut is a
general method, applying it to other binary struc-
tured learning tasks is also an interesting direction.
Acknowledgments
We’d like to thank three anonymous reviewers for
their valuable comments. This work is partly sup-
ported by NSF award IIS-0845484 and DARPA un-
der Contract No. FA8750-13-2-0041. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
A. Aker and R. Gaizauskas. 2009. Summary generation
for toponym-referenced images using object type lan-
guage models. In Proceedings of RANLP.
1501
Miguel Almeida and Andre Martins. 2013. Fast and ro-
bust compressive summarization with dual decompo-
sition and multi-task learning. In Proceedings of ACL,
pages 196–206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481–490, June.
A. Billionnet and M. Minoux. 1985. Maximizing a su-
permodular pseudoboolean function: A polynomial al-
gorithm for supermodular cubic functions. Discrete
Applied Mathematics, 12(1):1 – 11.
Endre Boros and Peter L. Hammer. 2002. Pseudo-
boolean optimization. Discrete Applied Mathematics,
123(1C3):155 – 225.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing & Man-
agement, 31(5):675 – 685.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In COLING,
pages 457–474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399–429.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
L1-ball for learning in high dimensions. In Proceed-
ings of ICML, pages 272–279.
Jack Edmonds and Richard M. Karp. 1972. Theoret-
ical improvements in algorithmic efficiency for net-
work flow problems. J. ACM, 19(2):248–264, April.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264–285, April.
Daniel Freedman and Petros Drineas. 2005. Energy min-
imization via graph cuts: Settling what is possible. In
CVPR (2), pages 939–946.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18, June.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008.
The ICSI summarization system at tac 2008. In Pro-
ceedings of the Text Understanding Conference.
Matthew R. Gormley and Jason Eisner. 2013. Noncon-
vex global optimization for latent-variable models. In
Proceedings of ACL, pages 444–454, August.
Vladimir Kolmogorov and Ramin Zabih. 2004. What en-
ergy functions can be minimized via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 26:65–81.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP 2010, pages 1288–1298, Oc-
tober.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP (to appear), Oc-
tober.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using super-
vised bigram-based ILP for extractive summarization.
In Proceedings of ACL, pages 1004–1013, August.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL, pages 510–520, June.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP 2009,
pages 261–264, August.
Andre´ F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ’09, pages 1–9.
Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuk-
i Aihara. 2011. Size-constrained submodular min-
imization through minimum norm base. In ICML,
pages 977–984.
Xian Qian and Yang Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, 1:105–151.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In First
Document Understanding Conference.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL, pages
233–243, July.
1502
