Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 278–288,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics
Exploiting Zero Pronouns to Improve Chinese Coreference Resolution
Fang Kong
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
dcskf@nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
Coreference resolution plays a critical role
in discourse analysis. This paper focuses
on exploiting zero pronouns to improve Chi-
nese coreference resolution. In particular, a
simplified semantic role labeling framework
is proposed to identify clauses and to detect
zero pronouns effectively, and two effective
methods (refining syntactic parser and refining
learning example generation) are employed to
exploit zero pronouns for Chinese coreference
resolution. Evaluation on the CoNLL-2012
shared task data set shows that zero pronouns
can significantly improve Chinese coreference
resolution.
1 Introduction
As one of the most important tasks in discourse
analysis, coreference resolution aims to link a given
mention (i.e., entity or event) to its co-referring ex-
pression in a text and has been a focus of research in
natural language processing (NLP) for decades.
Over the last decade, various machine learning
techniques have been applied to coreference reso-
lution and have performed reasonably well (Soon
et al., 2001; Ng and Cardie, 2002; Fernandes et al.,
2012). Current techniques rely primarily on surface
level features such as string match, syntactic features
such as apposition, and shallow semantic features
such as number, gender, semantic class, etc.
Despite similarities between Chinese and English,
there are differences that have a significant impact
on coreference resolution. In this paper, we focus
on exploiting one of the key characteristics of Chi-
nese text, zero pronouns (ZPs), to improve Chinese
coreference resolution. In particular, a simplified se-
mantic role labeling (SRL) framework is proposed
to identify Chinese clauses and to detect zero pro-
nouns effectively, and two effective methods are em-
ployed to exploit zero pronouns for Chinese corefer-
ence resolution. Experimental results show the ef-
fectiveness of our approach in improving the perfor-
mance of Chinese coreference resolution. Our work
is novel in that it is the first work that incorporates
the use of zero pronouns to significantly improve
Chinese coreference resolution
The rest of this paper is organized as follows.
Section 2 describes our baseline Chinese corefer-
ence resolution system. Section 3 motivates how
the detection of zero pronouns can improve Chinese
coreference resolution, using an illustrating exam-
ple. Section 4 presents our approach to detect zero
pronouns. Section 5 proposes two methods to ex-
ploit zero pronouns to improve Chinese coreference
resolution, based on a corpus study and preliminary
experiments. Section 6 briefly outlines the related
work. Finally, we conclude our work in Section 7.
2 Chinese Coreference Resolution
According to Webber (1978), coreference resolu-
tion can be decomposed into two complementary
subtasks: (1) anaphoricity determination: decid-
ing whether a given noun phrase (NP) is anaphoric
or not; and (2) anaphora resolution: linking to-
gether multiple mentions of a given entity in the
world. Our Chinese coreference resolution system
also contains these two components. Using the train-
ing data set of CoNLL-2012 shared task, we first
train an anaphoricity classifier to determine whether
278
a mention is anaphoric or not, and then employ
an independently-trained coreference resolution sys-
tem to resolve those mentions which are classi-
fied as anaphoric. The lack of gender and number
makes both anaphoricity determination and corefer-
ence resolution in Chinese more difficult.
2.1 Anaphoricity Determination
Since only the mentions that take part in coreference
chains are annotated in the CoNLL-2012 shared
task data set, we first generate a high-recall, low-
precision mention extraction module to extract as
many mentions as possible. The mention extrac-
tion module relies mainly on syntactic parse trees.
We extract all NP nodes, QP (quantifier phrase, i.e.,
complex amount/measure phrase) nodes, and all ter-
minals with part-of-speech tags PN (pronoun) and
NR (proper noun) in parse trees to form a mention
candidate set. Then, we employ some rules to re-
move unlikely mentions, e.g., those which contain
(1) measure words such as ‘??/one year’ and
‘??/one time’; (2) named entities whose cat-
egories are PERCENT, MONEY, QUANTITY, and
CARDINAL; (3) interrogative pronouns such as ‘
??/what’ and‘??/where’.
After pruning, we employ a learning-based
method to train an independent classifier to deter-
mine whether the remaining mentions are anaphoric.
Table 1 lists all the features employed in our
anaphoricity determination system.
2.2 Coreference Resolution
Our Chinese coreference resolution system adopts
the same learning-based model and the same set of
12 features as Soon et al. (2001). Considering the
special characteristics of conversation and web texts
(i.e., a large proportion of personal pronouns and the
organization of a text into several parts1) and prepar-
ing for dealing with zero pronouns, we add some
features shown in Table 2.2
1A text in the CoNLL-2012 data set is broken down into
different “parts”.
2AN denotes anaphor, CA denotes antecedent candidate, IP
denotes a simple clause, and CP denotes a clause headed by a
complementizer. For the feature ANPronounRanking, the rel-
ative ranking of a given pronoun is based on its semantic role
and surface position, and we assign the highest rank to zero pro-
nouns, similar to Kong et al. (2009).
R P F
GS 76.32 87.14 81.37
Auto 64.87 78.42 71.00
Table 3: Performance of anaphoricity determination on
the CoNLL-2012 test set
R P F
AM
Mention Detection 65.26 67.20 66.22
MUC 51.64 61.82 56.27
BCUBED 73.40 80.38 76.73
CEAF 53.16 45.66 49.13
Average 60.71
GMB
Mention Detection 82.01 69.58 75.29
MUC 76.21 66.18 70.84
BCUBED 76.15 86.59 81.04
CEAF 59.75 50.52 54.75
Average 68.88
GM
Mention Detection 79.80 100.00 88.77
MUC 80.86 85.48 83.11
BCUBED 73.66 91.94 81.79
CEAF 67.54 64.87 66.18
Average 77.02
Table 4: Performance of our Chinese coreference resolu-
tion system on the CoNLL-2012 test set
2.3 Results and Analysis
All experiments in this section are conducted on the
CoNLL-2012 shared task data set. The SVM-light
toolkit (Joachims, 1999) with radial basis kernel
and default learning parameters is employed in both
anaphoricity determination and coreference resolu-
tion.
Table 3 reports the performance of anaphoricity
determination on the CoNLL-2012 test set using
gold-standard parse trees (GS) and automatic parse
trees (Auto). All performance figures in this paper
are given in percentages. The results show that using
both gold parse trees and automatic parse trees, our
anaphoricity determination system achieves higher
precision than recall. In comparison with using gold
parse trees, precision decreases by about 9% and re-
call 11% on automatic parse trees.
Table 4 reports the performance of our Chinese
coreference resolution system on the CoNLL-2012
test set under three different experimental settings:
with automatic mentions (AM), with gold mention
279
Feature Description
NPType Type of the current mention (pronoun, demonstrative, proper NP).
NPNumber Number of the current mention (singular, plural).
NPGender Gender of the current mention (male, female).
IsHeadWord Whether the current mention is the same as its headword.
StrMatch Whether there is a string match between the current mention and another phrase in the
previous context.
AliasMatch Whether the current mention is a name alias or abbreviation of another phrase in the
previous context.
Appositive Whether the current mention and another phrase in the previous context are in an
appositive relation.
NestIn Whether another NP is nested in the current mention.
NestOut Whether the current mention is nested in another NP.
FirstNP Whether the current mention is the first NP of the sentence.
FrontDistance The number of words between the current mention and the nearest previous clause.
BackDistance The number of words between the current mention and the nearest following clause.
WordSense Whether the current mention and another phrase in the previous context have the same
word sense. Word sense annotation is provided in the CoNLL-2012 data set, based on
the IMS software (Zhong and Ng, 2010).
Table 1: Features employed in our anaphoricity determination system
Feature Description
AN/CAPronounType Whether the anaphor or the antecedent candidate is a zero pronoun, first per-
son, second person, third person, neutral pronoun, or others. In our corefer-
ence resolution system, a zero pronoun is viewed as a kind of special pro-
noun.
AN/CAGrammaticalRole Whether the anaphor or the antecedent candidate is a subject, object, or oth-
ers.
AN/CAOwnerClauseType Whether the anaphor or the antecedent candidate is in a matrix clause, an
independent clause, a subordinate clause, or none of the above.
AN/CARootPath Whether the path of nodes from the anaphor (or the antecedent candidate) to
the root of the parse tree contains NP, IP, CP, or VP.
ANPronounRanking Whether the anaphor is a pronoun and is ranked highest among the pronouns
(including zero pronouns) of the sentence.
AN/CAClosestNP Whether the antecedent candidate is the closest preceding NP of the anaphor.
AN/CAPartDistance This feature captures the distance (in parts) between the antecedent candidate
and the anaphor. If they are in the same part, the value is 0; if they are one
part apart, the value is 1; and so on.
AN/CASameSpeaker Whether the antecedent candidate and the anaphor appear in sentences spo-
ken by the same person.
Table 2: Additional features employed in our Chinese coreference resolution system
280
boundaries (GMB), and with gold mentions (GM).
From the results, we find that:
• Using automatic mentions, our system achieves
56.27, 76.73, and 49.13 in F-measure on MUC,
BCUBED, and CEAF evaluation metrics, re-
spectively.
• Using gold mention boundaries improves the
performance of our system by 14.57, 4.31, and
5.62 in F-measure, due to large gains in both
recall and precision. We also find that using
gold mention boundaries can boost the recall
of mention detection. As described above, our
anaphoricity determination model relies mainly
on the parser. Using gold mention boundaries
can improve the parser performance. Thus
our coreference resolution system can benefit
much from using gold mention boundaries (es-
pecially the recall).
• Employing gold mentions further boosts our
system significantly. In comparison with using
gold mention boundaries, the performance im-
provement is attributed more to an increase in
precision.
In comparison with the three best systems of
CoNLL-2012 in the Chinese closed track (shown in
Table 5), considering average F-measure, we find
that using automatic mentions, our system is only
inferior to that of Chen and Ng (2012); using gold
mention boundaries, our system achieves the best
performance; and using gold mentions, our system is
only a little worse than that of Chen and Ng (2012).
3 Motivation
In order to analyze the impact of zero pronouns on
Chinese coreference resolution, we first use the re-
leased OntoNotes v5.0 data (i.e., the training and de-
velopment portions of the CoNLL-2012 shared task)
in a corpus study.
Statistics show that anaphoric zero pronouns ac-
count for 10.7% of the mentions in coreference
chains in the training data, while in the develop-
ment data, the proportion is 11.3%. The experi-
mental results of our Chinese coreference resolution
system (i.e., the baseline) show that using both gold
mention boundaries and gold mentions significantly
improves system performance, especially for recall,
largely due to improved parser performance. We
then analyze the impact of zero pronouns on Chi-
nese syntactic parsing. As a preliminary exploration,
we integrate Chinese zero pronouns into the Berke-
ley parser (Petrov et al., 2006), experimenting with
gold-standard or automatically determined zero pro-
nouns kept or stripped off (using gold-standard word
segmentation provided in the CoNLL-2012 data).
The results indicate that given gold-standard zero
pronouns, parsing performance improves by 1.8%
in F-measure. Using automatically determined zero
pronouns by our zero pronoun detector to be intro-
duced in Section 4, parsing performance also im-
proves by 1.4% in F-measure.
In order to illustrate the impact of zero pronouns
on parsing performance, consider the following ex-
ample:3
Example (1):
????????????
#?????????#??????
???
...
????????????????
????????????#????
????
(In future, we have a reconstruction
plan.
Divide the park into seven regions, and
bring some more attractions.
. . .
Now we wait for approval of the gov-
ernment before implementing this plan
again. It is expected that work can start
next year.)
Without considering zero pronouns, the parse tree
of the second sentence output by the Berkeley parser
is shown in Figure 1.
Prior to parsing, using our zero pronoun detector
to be introduced in Section 4, the presence of zero
pronouns (denoted by #) can be detected. Figure 2
3In this paper, zero pronouns are denoted by “#” and men-
tions in the same coreference chain are shown in bold for all
examples.
281
MD MUC BCUBED CEAF Avg
AM
(Chen and Ng, 2012) 71.64 62.21 73.55 50.97 62.24
(Yuan et al., 2012) 68.15 60.33 72.90 48.83 60.69
(Bjo¨rkelund and Farkas, 2012) 66.37 58.61 73.10 48.19 59.97
Our baseline system (without ZPs) 66.22 56.27 76.73 49.13 60.71
Our refined system (with auto ZPs) 70.33 59.58 78.15 51.47 63.07
GMB
(Chen and Ng, 2012) 80.45 71.43 77.04 57.17 68.55
(Yuan et al., 2012) 74.02 66.44 75.02 51.81 64.42
(Bjo¨rkelund and Farkas, 2012) 71.02 63.56 74.52 50.20 62.76
Our baseline system (without ZPs) 75.29 70.84 81.04 54.75 68.88
Our refined system (with auto ZPs) 75.77 72.62 81.45 58.04 70.70
GM
(Chen and Ng, 2012) 91.73 83.77 81.15 68.38 77.77
(Yuan et al., 2012) 89.95 82.79 79.79 65.58 76.05
(Bjo¨rkelund and Farkas, 2012) 83.47 76.85 76.30 56.61 69.92
Our baseline system (without ZPs) 88.77 83.11 81.79 66.18 77.02
Our refined system (with auto ZPs) 91.49 83.46 82.43 65.88 77.26
Table 5: Performance (F-measure) of the three best Chinese coreference resolution systems on the CoNLL-2012 test
set
shows the new parse tree, which includes the de-
tected zero pronouns, output by the Berkeley parser
on the same sentence. Comparing these two parse
trees, we can see that the detected zero pronouns
contribute to better division of clauses and improved
parsing performance, which in turn leads to im-
proved Chinese coreference resolution.
Detecting the presence of zero pronouns also
helps to improve local salience modeling, leading to
improved Chinese coreference resolution. Long sen-
tences containing multiple clauses occur more fre-
quently in Chinese compared to English. Further-
more, a coreference chain can span many sentences.
Zero pronouns can occur not only within one sen-
tence (e.g., the first and second zero pronouns of Ex-
ample (1)), but can also be scattered across multiple
sentences (e.g., the first and third zero pronouns of
Example (1)). The subjects in the second sentence
of Example (1) are omitted.4 Detection of zero pro-
nouns improves local salience modeling, and leads
to the correct identification of all the noun phrases
of the coreference chain in Example (1).
4 Zero Pronoun Detection
Empty elements are those nodes in a parse tree that
do not have corresponding surface words or phrases.
Although empty elements exist in many languages
4In Chinese, pro-dropped subjects account for more than
36% of subjects in sentences (Kim, 2000).
and serve different purposes, they are particularly
important for some languages, such as Chinese,
where subjects and objects are frequently dropped to
keep a discourse concise. Among empty elements,
type *pro*, namely zero pronoun, is either used for
dropped subjects or objects, which can be recovered
from the context (anaphoric), or it is of little interest
for the reader or listener to know (non-anaphoric). In
the Chinese Treebank, type *pro* constitutes about
20% (Yang and Xue, 2010), and more than 85% of
them are anaphoric (Kong and Zhou, 2010). Thus,
zero pronouns are very important in bridging the in-
formation gap in a Chinese text. In this section, we
will introduce our zero pronoun detector.
In Chinese, a zero pronoun always occurs just be-
fore a predicate phrase node (e.g., VP). In particular,
if the predicate phrase node occurs in a coordinate
structure or is modified by an adverbial node, we
only need to consider its parent. A simplified seman-
tic role labeling (SRL) framework (only including
predicate recognition, argument pruning, and argu-
ment identification) is adopted to identify the pred-
icate phrase subtree (Xue, 2008), i.e., the minimal
subtree governed by a predicate and all its argu-
ments.
We carry out zero pronoun detection for every
predicate phrase subtree in an iterative manner from
a parse tree, i.e., determining whether there is a
zero pronoun before the given predicate phrase sub-
282
IP






HH
H
HH
H
HH
H
VP






H
HH
H
HH
H
H
VP


HH
H
VP
H
VV
?
NP
NN
??
VP
 HH
VV
?
NP
 HH
QP
H
CD
?
CLP
M
?
NP
NN
??
PU
?
VP



HH
H
HH
VV
??
NP


HH
H
DNP


HH
H
QP
 HH
ADVP
AD
?
QP
H
CD
?
CLP
M
?
DEG
?
NP
NN
??
PU
?
Figure 1: The parse tree without considering zero pronouns
tree. Viewing the position before the given predi-
cate phrase subtree as a zero pronoun candidate, we
can perform zero pronoun detection using a machine
learning approach.
During training, if a zero pronoun candidate has
a counterpart in the same position in the annotated
training corpus (either anaphoric or non-anaphoric),
a positive example is generated. Otherwise, a nega-
tive example is generated. During testing, each zero
pronoun candidate is presented to the zero pronoun
detector to determine whether it is a zero pronoun.
The features that are employed to detect zero pro-
nouns mainly model the context of the clause itself,
the left and right siblings, and the path of the clause
to the root node. Table 6 lists the features in detail.
4.1 Results and Analysis
We evaluate our zero pronoun detector using gold
parse trees and automatic parse trees produced by
the Berkeley parser. The SVM-light toolkit with ra-
dial basis kernel and default learning parameters is
employed as our learning algorithm.
Table 7 lists the results. From the results, we
R P F
GS 89.32 87.29 88.29
Auto 74.19 77.79 75.95
Table 7: Performance of zero pronoun detection on the
test set using gold and automatic parse trees
find that the performance of our zero pronoun detec-
tor drops about 12% in F-measure when using au-
tomatic parse trees, compared to using gold parse
trees. That is, the performance of zero pronoun de-
tection also depends on the performance of the syn-
tactic parser.
5 Exploiting Zero Pronouns to Improve
Chinese Coreference Resolution
In this section, we will propose two methods, refin-
ing the syntactic parser and refining learning exam-
ple generation, to exploit zero pronouns to improve
Chinese coreference resolution.
283
IP









 
 
 
 
 
 
@
@
@
@
@
@
PP
PP
PP
PP
PP
PP
PP
PP
PP
IP


HH
H
NP
EE
#
VP


HH
H
VP
H
VV
?
NP
NN
??
VP


H
H
VV
?
NP
 HH
QP
H
CD
?
CLP
M
?
NP
NN
??
PU
?
IP


H
HH
NP
EE
#
VP



HH
H
HH
VV
??
NP


HH
H
DNP


HH
H
QP
 HH
ADVP
AD
?
QP
H
CD
?
CLP
M
?
DEG
?
NP
NN
??
PU
?
Figure 2: The parse tree with the detected zero pronouns
5.1 Refining the Syntactic Parser
Similar to our preliminary experiments, we retrain
the Berkeley parser with explicit, automatically de-
tected zero pronouns in the training set and parse
the test set with explicit, automatically detected
zero pronouns using the retrained model. In both
anaphoricity determination and coreference resolu-
tion, the output results of the retrained parser are
employed to generate all features.
5.2 Refining Learning Example Generation
In order to model the salience of all entities, we re-
gard all zero pronouns as a special kind of NPs when
generating the learning examples. Considering the
modest performance of our anaphoricity determina-
tion module, we do not determine the anaphoricity
of zero pronouns. Instead, in the coreference res-
olution stage, all zero pronouns will be considered
during learning example generation (including both
training and test example generation).
For example, consider a coreference chain A1-
A2-Z0-A3-A4 containing one zero pronoun found
in an annotated training document. A1, A2, A3,
and A4 are traditional entity mentions, and Z0 is a
zero pronoun. During training, pairs of mentions in
the chain that are immediately adjacent (i.e., A1-A2,
A2-Z0, Z0-A3, and A3-A4) are used to generate the
positive training examples. Among them, two ex-
amples (i.e., A2-Z0 and Z0-A3) are associated with
a zero pronoun, which can act as both an anaphor
and an antecedent. For each positive pair, e.g., Z0-
A3, we find any noun phrase and zero pronoun oc-
curring between the anaphor A3 and the antecedent
Z0, and pair each of them with A3 to form a nega-
tive example. Similarly, test examples can be gen-
erated except that only the preceding mentions and
zero pronouns in the current and previous two sen-
tences will be paired with an anaphor.
Incorporating zero pronouns models salience of
all entities more accurately. The ratio of positive to
negative examples is also less skewed as a result of
considering zero pronouns – the ratio changes from
1:7.9 to 1:6.8 after considering zero pronouns.
5.3 Reprocessing
Although in the OntoNotes corpus, dropped subjects
and objects (i.e., zero pronouns) are considered dur-
ing coreference resolution for Chinese, they are not
284
Feature Description
ClauseClass Whether the given clause is a terminal clause or non-terminal clause.
LeftSibling Whether the given clause has a sibling immediately to its left.
LeftSiblingNP Whether the left siblings of the given clause contain an NP.
RightSibling Whether the given clause has a sibling immediately to its right.
RightSiblingVP Whether the right siblings of the given clause contain a VP.
ParentIP/VP Whether the syntactic category of the immediate parent of the given clause is an
IP or VP.
RootPath Whether the path from the given clause to the root of the parse tree contains
an NP or VP or CP. This feature models how the given clause is syntactically
connected to the sentence as a whole, reflecting its function within the sentence.
ClauseType The given clause is an independent clause, a subordinate clause, or others.
Has-Arg0/Arg1 Whether the given clause has an agent or patient argument.
Table 6: Features employed to detect zero pronouns
used in the CoNLL-2012 shared task (i.e., in the
gold evaluation keys, all the links formed by zero
pronouns are removed).
As described in Subsection 5.2, during training
and testing, all links associated with zero pronouns
will be considered in our coreference resolution sys-
tem. That is, we do not distinguish zero pronoun res-
olution from traditional coreference resolution, and
only view zero pronouns as special pronouns. After
generating all the links, zero pronouns are included
in coreference chains. For every coreference chain,
all zero pronouns will be removed before evaluation.
5.4 Experimental Results and Analysis
For fair comparison, all our experiments in this sub-
section have been conducted using the same experi-
mental settings as our baseline system. When com-
pared to our baseline system, all improvements are
statistically significant (p < 0.005).
Table 8 lists the coreference resolution perfor-
mance incorporating automatically detected zero
pronouns. The results show that:
• Using automatically detected zero pronouns
achieves better performance under all experi-
mental settings. In particular, using automatic
mentions, performance improves by 3.31%,
1.42%, and 2.34% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively. Using gold mention boundaries, au-
tomatic zero pronouns contribute 1.82% in av-
erage F-measure. Using gold mentions, the
R P F
AM
Mention Detection 71.09 69.58 70.33
MUC 55.06 64.91 59.58
BCUBED 76.04 80.38 78.15
CEAF 53.98 49.19 51.47
Average 63.07
GMB
Mention Detection 82.44 70.10 75.77
MUC 75.58 69.89 72.62
BCUBED 76.35 87.27 81.45
CEAF 65.17 52.31 58.04
Average 70.70
GM
Mention Detection 84.31 100.00 91.49
MUC 80.83 86.27 83.46
BCUBED 74.18 92.74 82.43
CEAF 69.91 62.29 65.88
Average 77.26
Table 8: Performance of our Chinese coreference resolu-
tion system incorporating zero pronouns
285
contribution of zero pronouns is only 0.24% in
average F-measure. This is because employing
either gold mention boundaries or gold men-
tions improves parsing performance.
• Our system incorporating zero pronouns out-
performs the three best systems in the CoNLL-
2012 shared task when using automatic men-
tions or gold mention boundaries. Using gold
mentions, our average F-measure is slightly
lower than that of Chen and Ng (2012).5
Table 9 presents the contribution of our two meth-
ods of exploiting zero pronouns and the impact of
gold-standard zero pronouns. We conclude that:
• Both the refined parser and refined example
generation improve performance. While the
refined parser improves the recall of mention
detection and coreference resolution, refined
example generation contributes more to preci-
sion. Combining these two methods further im-
proves coreference resolution.
• There is a performance gap of 6.01%, 4.08%,
and 3.19% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively, between the coreference resolution
system with gold-standard zero pronouns and
without zero pronouns. This suggests the use-
fulness of zero pronoun detection in Chinese
coreference resolution.
• Our proposed methods incorporating automatic
zero pronouns reduce the performance gap by
about half. This shows the effectiveness of our
proposed methods.
5.5 Discussion
Although the evaluation of the CoNLL-2012 shared
task does not consider zero pronouns, we also eval-
uate the performance of zero pronoun resolution on
the development data set (i.e., extracting all the re-
solved coreference links containing zero pronouns,
acting as anaphor or antecedent, to conduct the eval-
uation independently). The results show that, for
the correct anaphoric zero pronouns, the precision
5Statistical significance testing cannot be conducted since
their output files are not released.
of our system is 94.76%. So viewing zero pronouns
as a special kind of NP, zero pronouns can bridge
salience and contribute to coreference resolution. In
Example (1), the zero pronouns occurring in the sec-
ond sentence help to bridge the coreferential relation
between the mention “????/this plan” in the
last sentence and the mention “??????/a re-
construction plan” in the first sentence.
6 Related Work
In the last decade, both manual rule-based ap-
proaches (Lee et al., 2011) and statistical ap-
proaches (Soon et al., 2001; Ng and Cardie, 2002;
Fernandes et al., 2012) have been proposed for
coreference resolution. Besides frequently used syn-
tactic and semantic features, more linguistic features
are exploited in recent work (Ponzetto and Strube,
2006; Ng, 2007; Versley, 2007). There is less re-
search on Chinese coreference resolution compared
to English.
Although zero pronouns are prevalent in Chinese,
there is relatively little work on this topic. For Chi-
nese zero pronoun resolution, representative work
includes Converse (2006), Zhao and Ng (2007), and
Kong and Zhou (2010).
For the use of zero pronouns, Chung and Gildea
(2010) applied some extracted patterns to recover
two types of empty elements (*PRO* and *pro*).
Although the performance is still not satisfactory
(e.g., 63.0 and 44.0 in F-measure for *PRO* and
*pro* respectively), it nevertheless improves ma-
chine translation performance by 0.96 in BLEU
score.
7 Conclusion
In this paper, we focus on exploiting one of the key
characteristics of Chinese text, zero pronouns, to im-
prove Chinese coreference resolution. In particu-
lar, a simplified semantic role labeling framework
is proposed to detect zero pronouns effectively, and
two effective methods are employed to incorporate
zero pronouns into Chinese coreference resolution.
Experiments on the CoNLL-2012 shared task show
the effectiveness of our proposed approach. To the
best of our knowledge, this is the first attempt at in-
corporating zero pronouns into Chinese coreference
resolution.
286
MD MUC BCUBED CEAF Avg
R P F R P F R P F R P F
Baseline 65.26 67.20 66.22 51.64 61.82 56.27 73.40 80.38 76.73 53.16 45.66 49.13 60.71
+RP 72.01 66.24 69.00 55.02 61.47 58.07 77.83 78.97 78.40 50.40 49.81 50.10 62.19
+REG 65.92 70.02 67.91 49.98 66.27 56.98 73.64 83.45 78.24 51.12 47.44 49.21 61.48
+AZPs 71.09 69.58 70.33 55.06 64.91 59.58 76.04 80.38 78.15 53.98 49.19 51.47 63.07
+GZPs 72.18 70.59 71.38 58.61 66.45 62.28 78.79 82.94 80.81 54.12 50.63 52.32 65.14
Table 9: Contributions of the two methods of incorporating zero pronouns and the impact of gold zero pronouns
(RP: refining parser using auto zero pronouns, REG: refining example generation using auto zero pronouns, AZPs:
combining both RP and REG using auto zero pronouns, and GZPs: combining both RP and REG using gold zero
pronouns)
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Anders Bjo¨rkelund and Richa´rd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Joint Confer-
ence on EMNLP and CoNLL – Shared Task, pages 49–
55.
Chen Chen and Vincent Ng. 2012. Combining the best of
two worlds: A hybrid approach to multilingual coref-
erence resolution. In Proceedings of the Joint Con-
ference on EMNLP and CoNLL – Shared Task, pages
56–63.
Tagyoung Chung and Daniel Gildea. 2010. Effects of
empty categories on machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 636–645.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Eraldo Rezende Fernandes, C?´cero Nogueira dos Santos,
and Ruy Luiz Milidiu´. 2012. Latent structure percep-
tron with feature induction for unrestricted coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL – Shared Task, pages 41–48.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho¨lkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing. MIT-Press.
Young-Joo Kim. 2000. Subject/object drop in the acqui-
sition of Korean: A cross-linguistic comparison. Jour-
nal of East Asian Linguistics, 9:325–351.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882–891.
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009.
Employing the centering theory in pronoun resolution
from the semantic perspective. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 987–996.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28–34.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104–111.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 536–543.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433–440.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference of the
NAACL, pages 192–199.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
287
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 496–505.
Bonnie Lynn Webber. 1978. A Formal Approach to Dis-
course Anaphora. Garland Press.
Nianwen Xue. 2008. Labeling Chinese predicates
with semantic roles. Computational Linguistics,
34(2):225–255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
Recovering empty categories in the Chinese Treebank.
In Coling 2010: Posters, pages 1382–1390.
Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,
Liping Ge, Zengjian Liu, Meng Liao, and Xianbo Si.
2012. A mixed deterministic model for coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL – Shared Task, pages 76–82.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 541–550.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78–83.
288
